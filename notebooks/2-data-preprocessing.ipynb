{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, preprocess, and save train and test data\n",
    "\n",
    "This notebook preprocesses and collates the training and testing data for model creation.\n",
    "\n",
    "# John Brandt\n",
    "# July 11, 2021\n",
    "\n",
    "- Fuse Sentinel 1/2 data\n",
    "- Reconstruct 2D-array from CEO output CSV by plot\n",
    "- Match sentinel data to CEO labels\n",
    "- Stack data_x, data_y, length\n",
    "- Save arrays for data_x, data_y, length\n",
    "\n",
    "\n",
    "# Package imports and source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from scipy.ndimage import median_filter\n",
    "import hickle as hkl\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "%run ../src/preprocessing/slope.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_images(plot_id):\n",
    "    '''Takes a plot ID and subsets the input pd.DataFrame to that plot ID\n",
    "       returns a (14, 14) array-like list with binary labels\n",
    "       \n",
    "        Parameters:\n",
    "          batch_ids (list):\n",
    "          batch_size (int):\n",
    "          \n",
    "         Returns:\n",
    "          x_batch (arr):\n",
    "          y_batch (arr):\n",
    "    '''\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    rows = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        rows.append(list(subs_lat['TREE']))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'train'\n",
    "sentinel_1 = True\n",
    "s2_path = \"../data/{}-s2/\".format(source)\n",
    "s1_path = \"../data/{}-s1/\".format(source)\n",
    "csv_path = \"../data/{}-csv/\".format(source)\n",
    "output_path = \"../data/{}-processed/\".format(source)\n",
    "dem_path = \"../data/{}-dem/\".format(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2328\n"
     ]
    }
   ],
   "source": [
    "# Load and edit bad plot ids if needed\n",
    "verified_lu_change = np.load(\"bad_plot_ids.npy\")\n",
    "len(verified_lu_change)\n",
    "\n",
    "to_add = [141238348]\n",
    "to_add = [x for x in to_add if x not in verified_lu_change]\n",
    "verified_lu_change = np.concatenate([verified_lu_change, \n",
    "                     np.array(to_add).flatten()])\n",
    "\n",
    "to_remove = []\n",
    "\n",
    "verified_lu_change = [x for x in verified_lu_change if x not in to_remove]\n",
    "np.save(\"bad_plot_ids.npy\", np.array(verified_lu_change))\n",
    "print(len(verified_lu_change))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ceo-2022chaco_rubber-sample-29.csv', 'ceo-chaco-eucalyptus-30.csv', 'ceo-chaco-global-uuid-11.csv']\n",
      "Index(['PLOT_ID', 'SAMPLE_ID', 'LON', 'LAT', 'FLAGGED', 'COLLECTION_TIME',\n",
      "       'ANALYSIS_DURATION', 'TREE', 'country'],\n",
      "      dtype='object')\n",
      "Index(['PLOT_ID', 'SAMPLE_ID', 'LON', 'LAT', 'FLAGGED', 'COLLECTION_TIME',\n",
      "       'ANALYSIS_DURATION', 'TREE', 'COUNTRY'],\n",
      "      dtype='object')\n",
      "29001\n",
      "[ 29001  29002  29003  29004  29005  29006  29007  29008  29009 290010\n",
      " 290011 290012 290013 290014 290015 290016 290017 290018 290019 290020\n",
      " 290021 290022 290023 290024 290025 290026 290027 290028 290029 290030\n",
      " 290031 290032 290033 290034 290035 290036 290037 290038 290039 290040\n",
      " 290041 290042 290043 290044 290045 290046 290047 290048 290049 290050\n",
      " 290051 290052 290053 290054 290055 290056 290057 290058 290059 290060\n",
      " 290061 290062 290063 290064 290065 290066 290067 290068 290069 290070\n",
      " 290071 290072 290073 290074 290075 290076 290077 290078 290079]\n",
      "9\n",
      "Index(['plotid', 'sampleid', 'lon', 'lat', 'email', 'flagged',\n",
      "       'collection_time', 'analysis_duration', 'imagery_title',\n",
      "       'imagery_attributions', 'sample_geom', 'Tree'],\n",
      "      dtype='object')\n",
      "Index(['PLOT_ID', 'SAMPLE_ID', 'LON', 'LAT', 'EMAIL', 'FLAGGED',\n",
      "       'COLLECTION_TIME', 'ANALYSIS_DURATION', 'IMAGERY_TITLE',\n",
      "       'IMAGERY_ATTRIBUTIONS', 'SAMPLE_GEOM', 'TREE'],\n",
      "      dtype='object')\n",
      "1\n",
      "1\n",
      "No unique ID for ceo-chaco-eucalyptus-30.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30001  30002  30003  30004  30005  30006  30007  30008  30009 300010\n",
      " 300011 300012 300013 300014 300015 300016 300017 300018 300019 300020\n",
      " 300021 300022 300023 300024 300025 300026 300027 300028 300029 300030\n",
      " 300031 300032 300033 300034 300035 300036 300037 300038 300039 300040\n",
      " 300041 300042 300043 300044 300045 300046 300047 300048 300049 300050\n",
      " 300051 300052 300053 300054 300055 300056 300057 300058 300059 300060\n",
      " 300061 300062 300063 300064]\n",
      "9\n",
      "Index(['PLOT_ID', 'SAMPLE_ID', 'LON', 'LAT', 'FLAGGED', 'COLLECTION_TIME',\n",
      "       'ANALYSIS_DURATION', 'TREE', 'country'],\n",
      "      dtype='object')\n",
      "Index(['PLOT_ID', 'SAMPLE_ID', 'LON', 'LAT', 'FLAGGED', 'COLLECTION_TIME',\n",
      "       'ANALYSIS_DURATION', 'TREE', 'COUNTRY'],\n",
      "      dtype='object')\n",
      "11001\n",
      "[ 11001  11002  11003  11004  11005  11006  11007  11008  11009 110010\n",
      " 110011 110012 110013 110014 110015 110016 110017 110018 110019 110020\n",
      " 110021 110022 110023 110024 110025 110026 110027 110028 110029 110030\n",
      " 110031 110032 110033 110034 110035 110036 110037 110038 110039 110040\n",
      " 110041 110042 110043 110044 110045 110046 110047 110048 110049 110050\n",
      " 110051 110052 110053 110054 110055 110056 110057 110058 110059 110060\n",
      " 110061 110062]\n",
      "9\n",
      "205\n",
      "200\n",
      "There are 200 plots\n"
     ]
    }
   ],
   "source": [
    "# For either train or test data, loop through each plot and determine whether there is\n",
    "# labelled Y data for it -- returning one dataframe for the entire data set\n",
    "import re \n",
    "\n",
    "cols_to_keep = ['PLOT_ID', 'SAMPLE_ID', 'LON', 'LAT', 'FLAGGED', 'ANALYSES', 'USER_ID',\n",
    "       'COLLECTION_TIME', 'ANALYSIS_DURATION', 'TREE', 'plotid', 'sampleid']\n",
    "csvs = [x for x in sorted(os.listdir(csv_path)) if \".csv\" in x]\n",
    "#csvs = [x for x in csvs if 'uuid'in x]\n",
    "csvs = [x for x in csvs if \".csv\" in x]\n",
    "csvs = [x for x in csvs if \"chaco\" in x]\n",
    "#csvs = [x for x in csvs if \"senegal\" not in x]\n",
    "\n",
    "print(csvs)\n",
    "\n",
    "dfs = []\n",
    "for i in csvs:\n",
    "    df = pd.read_csv(csv_path + i, encoding = \"ISO-8859-1\")\n",
    "    df.columns = [re.sub(r'\\W+', '', x) for x in df.columns]\n",
    "    df.rename(columns={'Ã¯plotid':'plotid'}, inplace=True)\n",
    "    print(df.columns)\n",
    "    df.columns = [x.upper() for x in df.columns]\n",
    "    df.columns = ['PLOT_ID' if x == 'PLOTID' else x for x in df.columns]\n",
    "    df.columns = ['SAMPLE_ID' if x == 'SAMPLEID' else x for x in df.columns]\n",
    "    print(df.columns)\n",
    "    print(df['PLOT_ID'][0])\n",
    "    \n",
    "    # If there are no unique IDs already, go ahead and assign them\n",
    "\n",
    "    if abs(df['PLOT_ID'][0]) == 1:\n",
    "        print(df['PLOT_ID'][0])\n",
    "        print(f\"No unique ID for {i}\")\n",
    "        for index, row in df.iterrows():\n",
    "            row['PLOT_ID'] = abs(row['PLOT_ID'])\n",
    "            df['PLOT_ID'][index] = int(str(i[-6:-4]) + '00' + str(row['PLOT_ID']))\n",
    "    \n",
    "    print(df['PLOT_ID'].unique())\n",
    "    \n",
    "    #print(df.columns)\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column not in cols_to_keep:\n",
    "            df = df.drop(column, axis = 1)\n",
    "            \n",
    "    df['country'] = i.split(\".\")[0]\n",
    "    #print(( len(df) - np.sum(pd.isna(df['TREE']))) / 196 )\n",
    "    print(len(df.columns))\n",
    "    df.to_csv(csv_path + i, index = False)\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index = True, sort = True)\n",
    "print(len(df) // 196)\n",
    "df = df[~pd.isna(df['TREE'])]\n",
    "print(len(df) // 196)\n",
    "\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "plot_ids_loaded = plot_ids\n",
    "\n",
    "print(f\"There are {len(plot_ids)} plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int16(array: np.array) -> np.array:\n",
    "    '''Converts a float32 array to int16, reducing storage costs by three-fold'''\n",
    "    assert np.min(array) >= 0, np.min(array)\n",
    "    assert np.max(array) <= 1, np.max(array)\n",
    "    \n",
    "    array = np.clip(array, 0, 1)\n",
    "    array = np.trunc(array * 65535)\n",
    "    assert np.min(array >= 0)\n",
    "    assert np.max(array <= 65535)\n",
    "    \n",
    "    return array.astype(np.uint16)\n",
    "\n",
    "def process_dem(dem):\n",
    "    dem =  median_filter(dem, size = 5)\n",
    "    dem = calcSlope(dem.reshape((1, 32+2, 32+2)),\n",
    "                      np.full((32+2, 32+2), 10),\n",
    "                      np.full((32+2, 32+2), 10), \n",
    "                      zScale = 1, minSlope = 0.02)\n",
    "    dem = dem / 90\n",
    "    dem = dem.reshape((32+2, 32+2, 1))\n",
    "    dem = dem[1:-1, 1:-1]\n",
    "    dem = median_filter(dem, 5)[2:-2, 2:-2]\n",
    "    return dem\n",
    "\n",
    "def grndvi(array):\n",
    "    nir = np.clip(array[..., 3], 0, 1)\n",
    "    green = np.clip(array[..., 1], 0, 1)\n",
    "    red = np.clip(array[..., 2], 0, 1)\n",
    "    denominator = (nir+(green+red)) + 1e-5\n",
    "    return (nir-(green+red)) / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1521 plots\n",
      "Finished loading: (1521, 12, 28, 28, 14) of uint16 type\n"
     ]
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "%run ../src/preprocessing/indices.py\n",
    "\n",
    "def to_float32(array: np.array) -> np.array:\n",
    "    \"\"\"Converts an int_x array to float32\"\"\"\n",
    "    if not isinstance(array.flat[0], np.floating):\n",
    "        assert np.max(array) > 1\n",
    "        array = np.float32(array) / 65535.\n",
    "    assert np.max(array) <= 1\n",
    "    assert array.dtype == np.float32\n",
    "    return array\n",
    "\n",
    "count = 0\n",
    "dataframe = pd.DataFrame({'plot_id': [''], 'lat': [0.325], 'long': [0.325],\n",
    "                          'y': [0]})\n",
    "\n",
    "# Identify shape of data to load\n",
    "plot_ids_to_load = []\n",
    "for i in range(len(plot_ids)):\n",
    "    s1_i = f'{s1_path}{str(plot_ids[i])}.hkl'\n",
    "    s2_i = f'{s2_path}{str(plot_ids[i])}.hkl'\n",
    "    dem_i = f'{dem_path}{str(plot_ids[i])}.npy'\n",
    "    s1_new_i = f'../data/{source}-s1/{str(plot_ids[i])}.npy'\n",
    "    s1_exists = (os.path.exists(s1_i))\n",
    "    \n",
    "    if os.path.isfile(s2_i) and s1_exists:\n",
    "        if plot_ids[i] not in verified_lu_change:\n",
    "            plot_ids_to_load.append(plot_ids[i])\n",
    "\n",
    "print(f\"There are {len(plot_ids_to_load)} plots\")\n",
    "plot_ids_to_load = [x for x in plot_ids_to_load if x not in  [139077414,\n",
    "                                                              139187051,\n",
    "                                                              139187043,\n",
    "                                                             139187133, 139187134]]\n",
    "data_x = np.zeros((len(plot_ids_to_load), 12, 28, 28, 14)).astype(np.uint16)\n",
    "data_y = np.zeros((len(plot_ids_to_load), 14, 14))\n",
    "            \n",
    "# Iterate over each plot\n",
    "to_remove = []\n",
    "\n",
    "for i in range(len(plot_ids_to_load)):\n",
    "    s1_i = f'{s1_path}{str(plot_ids_to_load[i])}.hkl'\n",
    "    s2_i = f'{s2_path}{str(plot_ids_to_load[i])}.hkl'\n",
    "    dem_i = f'{dem_path}{str(plot_ids_to_load[i])}.npy'\n",
    "\n",
    "    x = to_float32(hkl.load(s2_i))\n",
    "    s1 = hkl.load(s1_i)\n",
    "    s1 = np.reshape(s1, (12, 16, 2, 16, 2, 2))\n",
    "    s1 = np.mean(s1, axis = (2, 4))\n",
    "    s1 = resize(s1, (12, 32, 32, 2), order = 1)\n",
    "    s1 = s1[:, 2:-2, 2:-2, :]\n",
    "    \n",
    "    dem = np.load(dem_i)\n",
    "    dem = process_dem(dem)\n",
    "    dem = np.tile(dem.reshape((1, 28, 28)), (x.shape[0], 1, 1))\n",
    "    x[..., 10] = dem\n",
    "    x = np.concatenate([x, s1], axis = -1)\n",
    "\n",
    "    count += 1\n",
    "    y = reconstruct_images(plot_ids_to_load[i])\n",
    "    long = np.mean(df[df['PLOT_ID'] == plot_ids_to_load[i]]['LON'])\n",
    "    lat = np.mean(df[df['PLOT_ID'] == plot_ids_to_load[i]]['LAT'])\n",
    "    dataframe = dataframe.append({'plot_id': str(plot_ids_to_load[i]),\n",
    "                                  'lat': lat, 'long': long,\n",
    "                                 'y': np.sum(np.array(y))}, \n",
    "                                 ignore_index = True)\n",
    "    dataframe.append([plot_ids_to_load[i], lat, long])\n",
    "\n",
    "    if np.sum(np.isnan(x)) > 0:\n",
    "        to_remove.append(i)\n",
    "    else:\n",
    "        x = np.clip(x, 0, 1)\n",
    "        x = to_int16(x)\n",
    "        data_x[i] = x\n",
    "        try:\n",
    "            data_y[i] = np.array(y)\n",
    "        except:\n",
    "            to_remove.append(i)\n",
    "            \n",
    "# Remove any data samples that had missing values\n",
    "if len(to_remove) > 0:\n",
    "    print(f\"Removing {to_remove}\")\n",
    "    data_x = np.delete(data_x, to_remove, 0)\n",
    "    data_y = np.delete(data_y, to_remove, 0)\n",
    "            \n",
    "print(f\"Finished loading: {data_x.shape} of {data_x.dtype} type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "import hickle as hkl\n",
    "dataframe = dataframe.drop(0, 0)\n",
    "dataframe.reset_index(inplace = True, drop = True)\n",
    "if len(to_remove) > 0:\n",
    "    dataframe = dataframe.drop(to_remove, 0)\n",
    "    dataframe.reset_index(inplace = True, drop = True)\n",
    "\n",
    "print(f\"Writing {source} data\")\n",
    "hkl.dump(data_x, f\"../data/{source}/{source}_x.hkl\", mode='w', compression='gzip')\n",
    "hkl.dump(data_y, f\"../data/{source}/{source}_y.hkl\", mode='w', compression='gzip')\n",
    "dataframe.to_csv(f\"../data/{source}/{source}_plot_ids.csv\", index = False)\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
