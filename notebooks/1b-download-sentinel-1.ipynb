{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process Sentinel 1 data\n",
    "\n",
    "This notebook downloads and processes one year of Sentinel 1 data for training and testing plots labelled in Collect Earth Online.\n",
    "\n",
    "## John Brandt\n",
    "## July 12, 2021\n",
    "\n",
    "## Package imports, API import, source scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import scipy.sparse as sparse\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "from scipy.sparse.linalg import splu\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType\n",
    "from sentinelhub import CRS, BBox, constants, DataSource, CustomUrlParam\n",
    "from skimage.transform import resize\n",
    "from sentinelhub.config import SHConfig\n",
    "\n",
    "import reverse_geocoder as rg\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "import hickle as hkl\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "    key = (yaml.safe_load(stream))\n",
    "    API_KEY = key['key']\n",
    "    SHUB_SECRET = key['shub_secret']\n",
    "    SHUB_KEY = key['shub_id']\n",
    "    AWSKEY = key['awskey']\n",
    "    AWSSECRET = key['awssecret']\n",
    "            \n",
    "shconfig = SHConfig()\n",
    "shconfig.instance_id = API_KEY\n",
    "shconfig.sh_client_id = SHUB_KEY\n",
    "shconfig.sh_client_secret = SHUB_SECRET\n",
    "        \n",
    "%matplotlib inline\n",
    "%run ../src/downloading/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ('2019-12-15', '2021-01-15')\n",
    "YEAR = 2020\n",
    "IMSIZE = 32\n",
    "\n",
    "starting_days = np.cumsum([0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_s1_layer(coords: list) -> str:\n",
    "    coords = (coords[1], coords[0])\n",
    "    results = rg.search(coords)\n",
    "    admin1 = (results[-1]['admin1'])\n",
    "    admin2 = results[-1]['admin2']\n",
    "    country = results[-1]['cc']\n",
    "    continent_name = pc.country_alpha2_to_continent_code(country)\n",
    "    print(admin1, admin2, country, continent_name)\n",
    "    if continent_name in ['AF', 'OC', 'EU']:\n",
    "        layer = \"SENT\"\n",
    "    if continent_name in ['SA']:\n",
    "        if coords[0] > -7.11:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['AS']:\n",
    "        if coords[0] > 23.3:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['NA']:\n",
    "        layer = \"SENT_DESC\"\n",
    "    return layer\n",
    "\n",
    "\n",
    "def calc_bbox(plot_id: int, df: pd.DataFrame) -> list:\n",
    "    \"\"\" Calculates the corners of a bounding box from an input\n",
    "        pandas dataframe as output by Collect Earth Online\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (int): plot_id of associated plot\n",
    "         df (pandas.DataFrame): dataframe of associated CEO survey\n",
    "    \n",
    "        Returns:\n",
    "         bounding_box (list): [(min(x), min(y)),\n",
    "                              (max(x), max_y))]\n",
    "    \"\"\"\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    return [(min(subs['LON']), min(subs['LAT'])),\n",
    "            (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "\n",
    "def bounding_box(points: list, expansion: int = 160) -> (tuple, 'CRS'):\n",
    "    \"\"\" Calculates the corners of a bounding box with an\n",
    "        input expansion in meters from a given bounding_box\n",
    "        \n",
    "        Subcalls:\n",
    "         calculate_epsg, convertCoords\n",
    "\n",
    "        Parameters:\n",
    "         points (list): output of calc_bbox\n",
    "         expansion (float): number of meters to expand or shrink the\n",
    "                            points edges to be\n",
    "    \n",
    "        Returns:\n",
    "         bl (tuple): x, y of bottom left corner with edges of expansion meters\n",
    "         tr (tuple): x, y of top right corner with edges of expansion meters\n",
    "    \"\"\"\n",
    "    bl = list(points[0])\n",
    "    tr = list(points[1])\n",
    "    inproj = Proj('epsg:4326')\n",
    "    outproj_code = calculate_epsg(bl)\n",
    "    outproj = Proj('epsg:' + str(outproj_code))\n",
    "    \n",
    "    bl_utm =  transform(inproj, outproj, bl[1], bl[0])\n",
    "    tr_utm =  transform(inproj, outproj, tr[1], tr[0])\n",
    "\n",
    "    distance1 = tr_utm[0] - bl_utm[0]\n",
    "    distance2 = tr_utm[1] - bl_utm[1]\n",
    "    expansion1 = (expansion - distance1)/2\n",
    "    expansion2 = (expansion - distance2)/2\n",
    "    \n",
    "    bl_utm = [bl_utm[0] - expansion1, bl_utm[1] - expansion2]\n",
    "    tr_utm = [tr_utm[0] + expansion1, tr_utm[1] + expansion2]\n",
    "    \n",
    "    zone = str(outproj_code)[3:]\n",
    "    zone = zone[1:] if zone[0] == \"0\" else zone\n",
    "    direction = 'N' if tr[1] >= 0 else 'S'\n",
    "    utm_epsg = \"UTM_\" + zone + direction\n",
    "    return (bl_utm, tr_utm), CRS[utm_epsg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(date_dict: dict, year: int) -> List:\n",
    "    \"\"\" Transforms a SentinelHub date dictionary to a\n",
    "         list of integer calendar dates\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "    starting_days = np.cumsum(days_per_month)\n",
    "    for date in date_dict:\n",
    "        if date.year == year - 1:\n",
    "            dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    return dates\n",
    "\n",
    "\n",
    "def identify_dates_to_download(dates: list) -> list:\n",
    "    \"\"\" Identify the S1 dates to download\"\"\"\n",
    "    days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "    days_per_month = np.array(days_per_month)\n",
    "    #days_per_month = np.reshape(days_per_month, (4, 3))\n",
    "    #days_per_month = np.sum(days_per_month, axis = 1)\n",
    "\n",
    "    starting_days = np.cumsum(days_per_month)\n",
    "\n",
    "    dates = np.array(dates)\n",
    "    dates_to_download = []\n",
    "    for i in starting_days:\n",
    "        s1_month = dates[dates > i]\n",
    "        s1_month = s1_month[s1_month < (i + 30)]\n",
    "        if len(s1_month) > 0:\n",
    "            dates_to_download.append(s1_month[0])\n",
    "    return dates_to_download\n",
    "\n",
    "\n",
    "def download_sentinel_1(bbox, epsg, time = time, \n",
    "                        layer = \"SENT\", year = 2020, \n",
    "                        image_format = MimeType.TIFF, \n",
    "                        data = DataSource.SENTINEL1_IW_ASC):\n",
    "    \"\"\" Downloads all 10 and 20 meter L2A bands from sentinel-hub\n",
    "        for input bbox and epsg, within time range\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): UTM EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         s1 (arr): (Time, X, Y, 2) array of sentinel 1 data\n",
    "         image_dates (list): number of days since time[0] for each\n",
    "                              image in s1.shape[0]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"The data is {data}\")\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WcsRequest(\n",
    "                layer=layer,\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                image_format = image_format,\n",
    "                data_source= data,\n",
    "                maxcc=1.0,\n",
    "                resx='20m', resy='20m',\n",
    "                config=shconfig,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=72),\n",
    "            )\n",
    "        \n",
    "        \n",
    "        s1_dates_dict = [x for x in image_request.get_dates()]\n",
    "        s1_dates = extract_dates(s1_dates_dict, year)\n",
    "        dates_to_download = identify_dates_to_download(s1_dates)\n",
    "        \n",
    "        steps_to_download = [i for i, val in enumerate(s1_dates) if val in dates_to_download]\n",
    "        print(f\"The following steps will be downloaded: {steps_to_download}, for {dates_to_download}\")\n",
    "        \n",
    "        \n",
    "        data_filter = steps_to_download\n",
    "        if len(image_request.download_list) <= 3 or len(steps_to_download) <= 3:\n",
    "            return np.empty((0,)), np.empty((0,))\n",
    "        s1 = image_request.get_data(data_filter = data_filter)\n",
    "        s1 = np.stack(s1)\n",
    "        s1 = to_float32(s1)\n",
    "        \n",
    "        assert np.max(s1) <= 1.\n",
    "        assert s1.shape[1] == 16.\n",
    "        assert s1.shape[2] == 16.\n",
    "        \n",
    "        print(f\"Sentinel 1 used {(2/3)*s1.shape[0] * (s1.shape[1]*s1.shape[2])/(512*512)} PU for\"\n",
    "              f\" {s1.shape[0]} out of {len(image_request.download_list)} images\")\n",
    "        \n",
    "        original = s1.shape\n",
    "        #s1 = s1.repeat(3, axis = 0)\n",
    "        # Store it with nearest upsample, but this will be converted to bilinear at train time\n",
    "        s1 = resize(s1, (s1.shape[0], 32, 32, 2), 0)\n",
    "        new = s1.shape\n",
    "        print(f\"{original} -> {new}\")\n",
    "        \n",
    "        image_dates = []\n",
    "        for date in image_request.get_dates():\n",
    "            if date.year == year - 1:\n",
    "                image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "            if date.year == year:\n",
    "                image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "            if date.year == year + 1:\n",
    "                image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "        image_dates = [val for idx, val in enumerate(image_dates) if idx in data_filter]\n",
    "        image_dates = np.array(image_dates)\n",
    "        \n",
    "        s1c = np.copy(s1)\n",
    "        s1c[np.where(s1c < 1.)] = 0\n",
    "        n_pix_oob = np.sum(s1c, axis = (1, 2, 3))\n",
    "        to_remove = np.argwhere(n_pix_oob > (32*32*2)/10)\n",
    "        if len(to_remove) > 0:\n",
    "            print(f'A total of {len(to_remove)} steps of {s1.shape[0]} were removed.')\n",
    "            s1 = np.delete(s1, to_remove, 0)\n",
    "            image_dates = np.delete(image_dates, to_remove)\n",
    "\n",
    "        return s1, image_dates\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_plots(data_location: str, output_folder: str, image_format = MimeType.TIFF) -> None:\n",
    "    \"\"\" Downloads sentinel-1 data for the plot IDs associated\n",
    "        with an input CSV from a collect earth online survey\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path)\n",
    "         output_folder (os.path)\n",
    "        \n",
    "        Subcalls:\n",
    "         calc_bbox, bounding_box\n",
    "         download_sentinel_1,\n",
    "         calculate_and_save_best_images\n",
    "         \n",
    "        Creates:\n",
    "         output_folder/{plot_id}.npy\n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_location, encoding = \"ISO-8859-1\")\n",
    "    df.columns = [x.upper() for x in df.columns]\n",
    "    for column in ['IMAGERY_TITLE', 'STACKINGPROFILEDG', 'PL_PLOTID', 'IMAGERYYEARDG',\n",
    "                  'IMAGERYMONTHPLANET', 'IMAGERYYEARPLANET', 'IMAGERYDATESECUREWATCH',\n",
    "                  'IMAGERYENDDATESECUREWATCH', 'IMAGERYFEATUREPROFILESECUREWATCH',\n",
    "                  'IMAGERYSTARTDATESECUREWATCH',\n",
    "                  'IMAGERY_ATTRIBUTIONS',\n",
    "                  'SAMPLE_GEOM']:\n",
    "        if column in df.columns:\n",
    "            df = df.drop(column, axis = 1)\n",
    "    df = df.rename(columns={df.columns[0]: 'PLOT_ID'})\n",
    "    print(df.columns)\n",
    "    df = df.dropna(axis = 0)\n",
    "    df = df[df['LAT'] > -24]\n",
    "    df = df[df['LAT'] < 24]\n",
    "    plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "    existing = [int(x[:-4]) for x in os.listdir(output_folder) if \".DS\" not in x]\n",
    "\n",
    "    to_download = [x for x in plot_ids if x not in existing]\n",
    "    print(f\"Starting download of {len(to_download)}\"\n",
    "          f\" plots from {data_location} to {output_folder}\")\n",
    "    errors = []\n",
    "    for i, val in enumerate(to_download):\n",
    "        print(f\"Downloading {i+1}/{len(to_download)}, {val}\")\n",
    "        location_wgs = calc_bbox(val, df = df)\n",
    "        print(location_wgs)\n",
    "        location, epsg = bounding_box(location_wgs, expansion = IMSIZE*10)\n",
    "        try:\n",
    "            # Identify cloud steps, download DEM, and download L2A series\n",
    "            s1_layer = identify_s1_layer(location_wgs[0])\n",
    "            data_source = DataSource.SENTINEL1_IW_DES if s1_layer == \"SENT_DESC\" else DataSource.SENTINEL1_IW_ASC\n",
    "            s1, s1_dates = download_sentinel_1(location, \n",
    "                                               layer = s1_layer, \n",
    "                                               epsg = epsg,\n",
    "                                               data = data_source)\n",
    "            if s1.shape[0] < 2:\n",
    "                s1_layer = \"SENT_DESC\" if s1_layer == \"SENT\" else \"SENT\"\n",
    "                data_source = DataSource.SENTINEL1_IW_DES if s1_layer == \"SENT_DESC\" else DataSource.SENTINEL1_IW_ASC\n",
    "                print(f'Switching to {s1_layer}')\n",
    "                s1, s1_dates = download_sentinel_1(location, \n",
    "                                                   layer = s1_layer,\n",
    "                                                   epsg = epsg,\n",
    "                                                   data = data_source)\n",
    "            \n",
    "            s1_a = np.copy(s1)\n",
    "            print(s1.shape, len(s1_dates))\n",
    "            s1, max_distance = calculate_and_save_best_images(s1, s1_dates)\n",
    "            print(s1.shape)\n",
    "\n",
    "            s1_b = np.copy(s1)\n",
    "            # Retain only iamgery every month\n",
    "            monthly = np.empty((12, IMSIZE, IMSIZE, 2))\n",
    "            index = 0\n",
    "            for start, end in zip(range(0, 24 + 2, 24 // 12), #0, 72, 6\n",
    "                                  range(24 // 12, 24 + 2, 24 // 12)): # 6, 72, 6\n",
    "                monthly[index] = np.median(s1[start:end], axis = 0)\n",
    "                index += 1\n",
    "\n",
    "            s1 = monthly\n",
    "            s1_c = np.copy(s1)\n",
    "            print(s1.shape)\n",
    "            \n",
    "            assert s1.shape[1] == IMSIZE\n",
    "            assert s1.shape[2] == IMSIZE\n",
    "            if max_distance < 200:\n",
    "                hkl.dump(s1, output_folder + str(val) + \".hkl\", mode='w', compression='gzip')\n",
    "                print('\\n')\n",
    "            else:\n",
    "                print(f\"Skipping {val} because max distance is {max_distance}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.fatal(e, exc_info=True)\n",
    "            errors.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "def to_float32(array: np.array) -> np.array:\n",
    "    \"\"\"Converts an int_x array to float32\"\"\"\n",
    "    print(f'The original max value is {np.max(array)}')\n",
    "    if not isinstance(array.flat[0], np.floating):\n",
    "        assert np.max(array) > 1\n",
    "        array = np.float32(array) / 65535.\n",
    "    assert np.max(array) <= 1\n",
    "    return array\n",
    "\n",
    "for i in (os.listdir(\"../data/test-csv/\")):\n",
    "    if \".csv\" in i:\n",
    "        print(i)\n",
    "        download_plots(\"../data/test-csv/\" + i, \n",
    "                       \"../data/test-s1/\",\n",
    "                       image_format = MimeType.TIFF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
