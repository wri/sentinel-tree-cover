{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process sentinel 2 data\n",
    "\n",
    "This notebook downloads and processes one year of the training and validation plots as labelled on Collect Earth Online. \n",
    "\n",
    "## John Brandt\n",
    "## Last edit: Sept 20, 2021\n",
    "\n",
    "## Package imports, API import, source scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import hickle as hkl\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "from scipy.sparse.linalg import splu\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType\n",
    "from sentinelhub import CRS, BBox, constants, DataSource, CustomUrlParam\n",
    "from skimage.transform import resize\n",
    "from typing import Tuple, List\n",
    "from scipy.ndimage import median_filter\n",
    "from sentinelhub.config import SHConfig\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key'] \n",
    "        \n",
    "%matplotlib inline\n",
    "%run ../src/preprocessing/slope.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/downloading/utils.py\n",
    "%run ../src/preprocessing/cloud_removal.py\n",
    "%run ../src/preprocessing/whittaker_smoother.py\n",
    "%run ../src/downloading/io.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "    key = (yaml.safe_load(stream))\n",
    "    API_KEY = key['key']\n",
    "    SHUB_SECRET = key['shub_secret']\n",
    "    SHUB_KEY = key['shub_id']\n",
    "    AWSKEY = key['awskey']\n",
    "    AWSSECRET = key['awssecret']\n",
    "            \n",
    "shconfig = SHConfig()\n",
    "shconfig.instance_id = API_KEY\n",
    "shconfig.sh_client_id = SHUB_KEY\n",
    "shconfig.sh_client_secret = SHUB_SECRET\n",
    "    \n",
    "\n",
    "uploader = FileUploader(awskey = AWSKEY, awssecret = AWSSECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "YEAR = 2020\n",
    "TIME = ('{}-11-15'.format(str(YEAR - 1)), '{}-02-15'.format(str(YEAR + 1)))\n",
    "EPSG = CRS.WGS84\n",
    "IMSIZE = 32\n",
    "\n",
    "# Constants\n",
    "starting_days = np.cumsum([0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geographic utility functions\n",
    "\n",
    "These cell blocks calculate the min_x, min_y, max_x, max_y of the area of interest (AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bbox(plot_id: int, df: pd.DataFrame) -> List:\n",
    "    \"\"\" Calculates the corners of a bounding box from an input\n",
    "        pandas dataframe as output by Collect Earth Online\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (int): plot_id of associated plot\n",
    "         df (pandas.DataFrame): dataframe of associated CEO survey\n",
    "    \n",
    "        Returns:\n",
    "         bounding_box (list): [(min(x), min(y)),\n",
    "                              (max(x), max_y))]\n",
    "    \"\"\"\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])),\n",
    "            (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "\n",
    "def bounding_box(points: List[Tuple[float, float]], \n",
    "                 expansion: int = 160) -> ((Tuple, Tuple), str):\n",
    "    \"\"\" Calculates the corners of a bounding box with an\n",
    "        input expansion in meters from a given bounding_box\n",
    "        \n",
    "        Subcalls:\n",
    "         calculate_epsg, convertCoords\n",
    "\n",
    "        Parameters:\n",
    "         points (list): output of calc_bbox\n",
    "         expansion (float): number of meters to expand or shrink the\n",
    "                            points edges to be\n",
    "    \n",
    "        Returns:\n",
    "         bl (tuple): x, y of bottom left corner with edges of expansion meters\n",
    "         tr (tuple): x, y of top right corner with edges of expansion meters\n",
    "    \"\"\"\n",
    "    bl = list(points[0])\n",
    "    tr = list(points[1])\n",
    "    inproj = Proj('epsg:4326')\n",
    "    outproj_code = calculate_epsg(bl)\n",
    "    outproj = Proj('epsg:' + str(outproj_code))\n",
    "    bl_utm =  transform(inproj, outproj, bl[1], bl[0])\n",
    "    tr_utm =  transform(inproj, outproj, tr[1], tr[0])\n",
    "\n",
    "    distance1 = tr_utm[0] - bl_utm[0]\n",
    "    distance2 = tr_utm[1] - bl_utm[1]\n",
    "    expansion1 = (expansion - distance1)/2\n",
    "    expansion2 = (expansion - distance2)/2\n",
    "        \n",
    "    bl_utm = [bl_utm[0] - expansion1, bl_utm[1] - expansion2]\n",
    "    tr_utm = [tr_utm[0] + expansion1, tr_utm[1] + expansion2]\n",
    "\n",
    "    zone = str(outproj_code)[3:]\n",
    "    zone = zone[1:] if zone[0] == \"0\" else zone\n",
    "    direction = 'N' if tr[1] >= 0 else 'S'\n",
    "    utm_epsg = \"UTM_\" + zone + direction\n",
    "    return (bl_utm, tr_utm), CRS[utm_epsg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(date_dict: dict, year: int) -> List:\n",
    "    \"\"\" Transforms a SentinelHub date dictionary to a\n",
    "         list of integer calendar dates\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "    starting_days = np.cumsum(days_per_month)\n",
    "    for date in date_dict:\n",
    "        if date.year == year - 1:\n",
    "            dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    return dates\n",
    "\n",
    "def to_float32(array: np.array) -> np.array:\n",
    "    \"\"\"Converts an int_x array to float32\"\"\"\n",
    "    print(f'The original max value is {np.max(array)}')\n",
    "    if not isinstance(array.flat[0], np.floating):\n",
    "        assert np.max(array) > 1\n",
    "        array = np.float32(array) / 65535.\n",
    "    assert np.max(array) <= 1\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud and cloud shadow calculation functions\n",
    "\n",
    "This cell block identifies clouds using s2Cloudless, and identifies clouds (and shadows) using Candra et al. 2019.\n",
    "\n",
    "The output is per-px cloud/shadow masks, and a list of sentinel 2 dates to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_clouds(bbox: List[Tuple[float, float]], epsg: 'CRS', time: dict = TIME):\n",
    "    \"\"\" Downloads and calculates cloud cover and shadow\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         cloud_img (np.array): (X, 96, 96) array of cloud probs\n",
    "         shadows (np.array):  (X, 96, 96) array of shadow binary\n",
    "         clean_steps (np.array): (N,) array of clean idx\n",
    "         cloud_dates (np.array): (N,) array of clean cloud datets\n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    cloud_request = WcsRequest(\n",
    "        layer='CLOUD_NEW',\n",
    "        bbox=box, time=time,\n",
    "        resx='160m', resy='160m',\n",
    "        image_format = MimeType.TIFF,\n",
    "        maxcc=0.75, config=shconfig,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=96))\n",
    "\n",
    "    shadow_request = WcsRequest(\n",
    "        layer='SHADOW',\n",
    "        bbox=box, time=time,\n",
    "        resx='60m', resy='60m',\n",
    "        image_format =  MimeType.TIFF,\n",
    "        maxcc=0.75, config=shconfig,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=96))\n",
    "\n",
    "    cloud_img = np.array(cloud_request.get_data())\n",
    "    if not isinstance(cloud_img.flat[0], np.floating):\n",
    "        assert np.max(cloud_img) > 1\n",
    "        cloud_img = cloud_img / 255.\n",
    "    assert np.max(cloud_img) <= 1\n",
    "\n",
    "    cloud_img = resize(cloud_img, (cloud_img.shape[0], 96, 96), order = 0)\n",
    "    n_cloud_px = np.sum(cloud_img > 0.33, axis = (1, 2))\n",
    "    cloud_steps = np.argwhere(n_cloud_px > (96**2 * 0.15))\n",
    "    clean_steps = [x for x in range(cloud_img.shape[0]) if x not in cloud_steps]\n",
    "    \n",
    "    \n",
    "    cloud_dates_dict = [x for x in cloud_request.get_dates()]\n",
    "    cloud_dates = extract_dates(cloud_dates_dict, YEAR)\n",
    "    cloud_dates = [val for idx, val in enumerate(cloud_dates) if idx in clean_steps]\n",
    "    \n",
    "    shadow_dates_dict = [x for x in shadow_request.get_dates()]\n",
    "    shadow_dates = extract_dates(shadow_dates_dict, YEAR)\n",
    "    shadow_steps = [idx for idx, val in enumerate(shadow_dates) if val in cloud_dates]    \n",
    "    \n",
    "    shadow_img = np.array(shadow_request.get_data(data_filter = shadow_steps))\n",
    "    shadow_pus = (shadow_img.shape[1]*shadow_img.shape[2])/(512*512) * shadow_img.shape[0]\n",
    "    shadow_img = resize(shadow_img, (shadow_img.shape[0], 96, 96, shadow_img.shape[-1]), order = 0,\n",
    "                        anti_aliasing = False, preserve_range = True).astype(np.uint16)\n",
    "\n",
    "    cloud_img = np.delete(cloud_img, cloud_steps, 0)\n",
    "    assert shadow_img.shape[0] == cloud_img.shape[0], (shadow_img.shape, cloud_img.shape)\n",
    "    shadows = mcm_shadow_mask(shadow_img, cloud_img) # Make usre this makes sense??\n",
    "    print(f\"Shadows ({shadows.shape}) used {round(shadow_pus, 1)} processing units\")\n",
    "    return cloud_img, shadows, clean_steps, np.array(cloud_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEM and slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dem(plot_id: int, df: 'DataFrame', epsg: 'CRS') -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads MapZen digital elevation model and return slope\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (tuple): plot id from collect earth online (CEO)\n",
    "         df (pandas.DataFrame): data associated with plot_id from CEO\n",
    "         epsg (int): UTM EPSG associated with plot_id\n",
    "    \n",
    "        Returns:\n",
    "         slope (arr): (X, Y, 1) array of per-pixel slope from [0, 1]\n",
    "    \"\"\"\n",
    "    location = calc_bbox(plot_id, df = df)\n",
    "    bbox, epsg = bounding_box(location, expansion = (32+2)*10)\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_request = WcsRequest(\n",
    "                         layer='DEM', bbox=box,\n",
    "                         resx = \"10m\", resy = \"10m\",\n",
    "                         config=shconfig,\n",
    "                         image_format= MimeType.TIFF,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image_init = dem_request.get_data()[0]\n",
    "    dem_image_init = dem_image_init - 12000\n",
    "    dem_image_init = dem_image_init.astype(np.float32)\n",
    "    dem_image = np.copy(dem_image_init)\n",
    "    dem_image = median_filter(dem_image_init, size = 5)\n",
    "    slope = calcSlope(dem_image.reshape((1, 32+2, 32+2)),\n",
    "                      np.full((32+2, 32+2), 10),\n",
    "                      np.full((32+2, 32+2), 10), \n",
    "                      zScale = 1, minSlope = 0.02)\n",
    "    slope = slope / 90\n",
    "    slope = slope.reshape((32+2, 32+2, 1))\n",
    "    slope = slope[1:32+1, 1:32+1, :]\n",
    "    return slope, dem_image_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 and 20 meter L2A bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_layer(bbox: List[Tuple[float, float]],\n",
    "                   clean_steps: np.ndarray, epsg: 'CRS',\n",
    "                   dates: dict = TIME, year: int = YEAR) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads the L2A sentinel layer with 10 and 20 meter bands\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         img (arr):\n",
    "         img_request (obj): \n",
    "    \"\"\"\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A20',\n",
    "                bbox=box, time=dates,\n",
    "                image_format = MimeType.TIFF,\n",
    "                data_source = DataSource.SENTINEL2_L2A,\n",
    "                maxcc=0.75,\n",
    "                resx='20m', resy='20m',\n",
    "                config=shconfig,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=96),\n",
    "            )\n",
    "        \n",
    "        image_dates = []\n",
    "        for date in image_request.get_dates():\n",
    "            if date.year == YEAR - 1:\n",
    "                image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "            if date.year == YEAR:\n",
    "                image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "            if date.year == YEAR + 1:\n",
    "                image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "        \n",
    "        steps_to_download = [i for i, val in enumerate(image_dates) if val in clean_steps]\n",
    "        dates_to_download = [val for i, val in enumerate(image_dates) if val in clean_steps]\n",
    "              \n",
    "        img_bands = image_request.get_data(data_filter = steps_to_download)\n",
    "        img_20 = np.stack(img_bands)\n",
    "        img_20 = to_float32(img_20)\n",
    "\n",
    "        s2_20_usage = (img_20.shape[1]*img_20.shape[2])/(512*512) * (6/3) * img_20.shape[0]\n",
    "        if (img_20.shape[1] * img_20.shape[2]) != 14*14:\n",
    "            print(f\"Original 20 meter bands size: {img_20.shape}, using {s2_20_usage} PU\")\n",
    "        img_20 = resize(img_20, (img_20.shape[0], IMSIZE, IMSIZE, img_20.shape[-1]), order = 0)\n",
    "        \n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A10',\n",
    "                bbox=box, time=dates,\n",
    "                image_format = MimeType.TIFF,\n",
    "                data_source = DataSource.SENTINEL2_L2A,\n",
    "                maxcc=0.75,\n",
    "                resx='10m', resy='10m',\n",
    "                config=shconfig,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=96),\n",
    "        )\n",
    "        \n",
    "        img_bands = image_request.get_data(data_filter = steps_to_download)\n",
    "        img_10 = np.stack(img_bands)\n",
    "        if (img_10.shape[1] * img_10.shape[2]) != 28*28:\n",
    "            print(f\"The original L2A image size is: {img_10.shape}\")\n",
    "        img_10 = to_float32(img_10)\n",
    "            \n",
    "        img_10 = resize(img_10, (img_10.shape[0], IMSIZE, IMSIZE, img_10.shape[-1]), order = 0)\n",
    "        img = np.concatenate([img_10, img_20], axis = -1)\n",
    "\n",
    "        \n",
    "        return img, np.array(dates_to_download)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super resolution\n",
    "\n",
    "Super-resolve the 20 meter bands to 10 meters using DSen2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/supres/model\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "MDL_PATH = \"../models/supres/\"\n",
    "\n",
    "model = tf.train.import_meta_graph(MDL_PATH + 'model.meta')\n",
    "model.restore(sess, tf.train.latest_checkpoint(MDL_PATH))\n",
    "\n",
    "logits = tf.get_default_graph().get_tensor_by_name(\"Add_6:0\")\n",
    "inp = tf.get_default_graph().get_tensor_by_name(\"Placeholder:0\")\n",
    "inp_bilinear = tf.get_default_graph().get_tensor_by_name(\"Placeholder_1:0\")\n",
    "\n",
    "def superresolve(input_data):\n",
    "    bilinear_upsample = input_data[..., 4:]\n",
    "    x = sess.run([logits], \n",
    "                 feed_dict={inp: input_data,\n",
    "                            inp_bilinear: bilinear_upsample})\n",
    "    return x[0]\n",
    "\n",
    "def superresolve_tile(x):\n",
    "    twentym = x[..., 4:]\n",
    "    imsize = x.shape[1]\n",
    "    twentym = np.reshape(twentym, (x.shape[0], imsize // 2, 2, imsize // 2, 2, 6))\n",
    "    twentym = np.mean(twentym, (2, 4))\n",
    "    twentym = resize(twentym, (x.shape[0], imsize, imsize, 6), 1)\n",
    "    x[..., 4:] = twentym\n",
    "    x[..., 4:] = superresolve(x)\n",
    "    if imsize > 28:\n",
    "        crop_amt = (imsize - 28) // 2\n",
    "        x = x[:, crop_amt:-crop_amt, crop_amt:-crop_amt, :]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_new_dem(data_location: 'os.Path',\n",
    "                     output_folder: 'os.Path',\n",
    "                     image_format: 'MimeType' = MimeType.TIFF):\n",
    "    \"\"\" Downloads and saves DEM and slope files\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path): \n",
    "         output_folder (os.path): \n",
    "         image_format (MimeType): \n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(data_location)\n",
    "    df.columns = [x.upper() for x in df.columns]\n",
    "    for column in ['IMAGERY_TITLE', 'STACKINGPROFILEDG', 'PL_PLOTID', 'IMAGERYYEARDG',\n",
    "                  'IMAGERYMONTHPLANET', 'IMAGERYYEARPLANET', 'IMAGERYDATESECUREWATCH',\n",
    "                  'IMAGERYENDDATESECUREWATCH', 'IMAGERYFEATUREPROFILESECUREWATCH',\n",
    "                  'IMAGERYSTARTDATESECUREWATCH','IMAGERY_ATTRIBUTIONS',\n",
    "                  'SAMPLE_GEOM']:\n",
    "        if column in df.columns:\n",
    "            df = df.drop(column, axis = 1)\n",
    "            \n",
    "    df = df.dropna(axis = 0)\n",
    "    plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "    existing = [int(x[:-4]) for x in os.listdir(output_folder) if \".DS\" not in x]\n",
    "    existing = existing + [139089844]\n",
    "    to_download = [x for x in plot_ids if x not in existing]\n",
    "    print(f\"Starting download of {len(to_download)}\"\n",
    "          f\" plots from {data_location} to {output_folder}\")\n",
    "    errors = []\n",
    "    for i, val in enumerate(to_download):\n",
    "        print(f\"Downloading {i + 1}/{len(to_download)}, {val}\")\n",
    "        initial_bbx = calc_bbox(val, df = df)\n",
    "        dem_bbx, epsg = bounding_box(initial_bbx, expansion = 32*10)\n",
    "        slope, dem = download_dem(val, epsg = epsg, df = df)\n",
    "        np.save(output_folder + str(val), dem)\n",
    "        np.save(\"../data/train-slope/\" + str(val), slope)\n",
    "        \n",
    "\n",
    "\n",
    "def concatenate_dem(x, dem):\n",
    "    dem = np.tile(dem.reshape((1, 32, 32, 1)), (x.shape[0], 1, 1, 1))\n",
    "    dem = dem[:, 2:-2, 2:-2, :]\n",
    "    x = np.concatenate([x, dem], axis = -1)\n",
    "    assert x.shape[1] == x.shape[2] == 28\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
=======
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
>>>>>>> 00e127d (adds download_job_fast, and cirrus cloud removal)
   "outputs": [],
   "source": [
    "def id_missing_px(sentinel2: np.ndarray, thresh: int = 100) -> np.ndarray:\n",
    "    missing_images_0 = np.sum(sentinel2[..., :10] == 0.0, axis = (1, 2, 3))\n",
    "    missing_images_p = np.sum(sentinel2[..., :10] >= 1., axis = (1, 2, 3))\n",
    "    missing_images = missing_images_0 + missing_images_p\n",
    "    \n",
    "    missing_images = np.argwhere(missing_images >= (sentinel2.shape[1]**2) / thresh).flatten()\n",
    "    return missing_images\n",
    "\n",
    "\n",
    "def download_raw_data(data_location, output_folder, fmt = \"train\", image_format = MimeType.TIFF):\n",
    "    \"\"\" Downloads slope and sentinel-2 data for all plots associated\n",
    "        with an input CSV from a collect earth online survey\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path)\n",
    "         output_folder (os.path)\n",
    "        \n",
    "        Creates:\n",
    "         output_folder/{plot_id}.npy\n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_location)\n",
    "    df.columns = [x.upper() for x in df.columns]\n",
    "    for column in ['IMAGERY_TITLE', 'STACKINGPROFILEDG', 'PL_PLOTID', 'IMAGERYYEARDG',\n",
    "                  'IMAGERYMONTHPLANET', 'IMAGERYYEARPLANET', 'IMAGERYDATESECUREWATCH',\n",
    "                  'IMAGERYENDDATESECUREWATCH', 'IMAGERYFEATUREPROFILESECUREWATCH',\n",
    "                  'IMAGERYSTARTDATESECUREWATCH','IMAGERY_ATTRIBUTIONS',\n",
    "                  'SAMPLE_GEOM']:\n",
    "        if column in df.columns:\n",
    "            df = df.drop(column, axis = 1)\n",
    "\n",
    "    df = df.dropna(axis = 0)\n",
    "    plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "    existing = [int(x[:-4]) for x in os.listdir(f\"../data/{fmt}-dates/\") if \".DS\" not in x]\n",
    "    existing = existing + [139190271, 139187199, 139319876, 139319877]\n",
    "    to_download = [x for x in plot_ids if x not in existing]\n",
    "    print(f\"Starting download of {len(to_download)}\"\n",
    "          f\" plots from {data_location} to {output_folder}\")\n",
    "    for i, val in enumerate(reversed(to_download)):\n",
    "        print(f\"Downloading {i + 1}/{len(to_download)}, {val}\")\n",
    "        initial_bbx = calc_bbox(val, df = df)\n",
    "        sentinel2_bbx, epsg = bounding_box(initial_bbx, expansion = IMSIZE*10)\n",
    "        cloud_bbx, _ = bounding_box(initial_bbx, expansion = 96*10)\n",
    "        try:\n",
    "            # Identify cloud steps, download DEM, and download L2A series\n",
    "            cloud_probs, shadows, _, clean_dates = identify_clouds(cloud_bbx, epsg = epsg)\n",
    "            dem, _ = download_dem(val, epsg = epsg, df = df)\n",
    "            #to_remove, _ = calculate_cloud_steps(cloud_probs, clean_dates)\n",
    "            \n",
    "            #if len(to_remove) > 0:\n",
    "            #    cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "            #    clean_dates = np.delete(clean_dates, to_remove)\n",
    "            #    shadows = np.delete(shadows, to_remove, 0)\n",
    "                \n",
    "            to_remove = subset_contiguous_sunny_dates(clean_dates, cloud_probs)\n",
    "            if len(to_remove) > 0:\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "                clean_dates = np.delete(clean_dates, to_remove)\n",
    "                shadows = np.delete(shadows, to_remove, 0)\n",
    "                \n",
    "            _ = print_dates(clean_dates, np.mean(cloud_probs, axis = (1, 2)))\n",
    "        \n",
    "            s2, s2_dates = download_layer(sentinel2_bbx, clean_steps = clean_dates, epsg = epsg)    \n",
    "            \n",
    "            # Step to ensure that shadows, clouds, sentinel l2a have aligned dates\n",
    "            to_remove_clouds = [i for i, val in enumerate(clean_dates) if val not in s2_dates]\n",
    "            to_remove_dates = [val for i, val in enumerate(clean_dates) if val not in s2_dates]\n",
    "            if len(to_remove_clouds) > 0:\n",
    "                print(f\"Removing {to_remove_dates} from clouds because not in S2\")\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove_clouds, 0)\n",
    "                shadows = np.delete(shadows, to_remove_clouds, 0)\n",
    "            print(f\"Shadows {shadows.shape}, clouds {cloud_probs.shape},\"\n",
    "                  f\" S2, {s2.shape}, S2d, {s2_dates.shape}\")\n",
    "            \n",
    "            print(s2.shape)\n",
    "            \n",
    "            cloud_probs = cloud_probs[:, 24:-24, 24:-24]\n",
    "            shadows = shadows[:, 24:-24, 24:-24]\n",
    "            x, interp = remove_cloud_and_shadows(s2, cloud_probs, shadows, s2_dates)\n",
    "            to_remove = np.argwhere(np.mean(interp, axis = (1, 2)) > 0.5)\n",
    "            if len(to_remove) > 0:\n",
    "                print(f\"Removing {len(to_remove)} steps with >50% interpolation: {to_remove}\")\n",
    "                x = np.delete(x, to_remove, 0)\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "                s2_dates = np.delete(s2_dates, to_remove)\n",
    "                shadows = np.delete(shadows, to_remove, 0)\n",
    "                print(np.sum(shadows, axis = (1, 2)))\n",
    "            \n",
    "            x_to_save = np.copy(x)\n",
    "            x_to_save = np.clip(x_to_save, 0, 1)\n",
    "            x_to_save = np.trunc(x_to_save * 65535).astype(np.uint16)\n",
    "            np.save(f\"../data/{fmt}-dates/{str(val)}\", s2_dates)\n",
    "            np.save(f\"../data/{fmt}-raw/{str(val)}\", x_to_save)\n",
    "            file = f\"../data/{fmt}-raw/{str(val)}.npy\"\n",
    "            key = f'restoration-mapper/model-data/{fmt}/raw/{str(val)}.npy'\n",
    "            uploader.upload(bucket = 'restoration-monitoring', key = key, file = file)\n",
    "            print(\"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dates(dates):\n",
    "    \"\"\"For imagery that was downloaded prior to capping the number \n",
    "       of monthly images to be 3, it is necessary to enforce that cap\n",
    "       on the training / testing data.\n",
    "       \n",
    "       This function identifies the indices of the imagery to deletet\n",
    "       such that there is a maximum of three images per month.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    before = len(dates)\n",
    "    selected_indices = np.arange(len(dates))\n",
    "    begin = [-60, 0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n",
    "    end = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334, 390]\n",
    "    indices_to_remove = []\n",
    "    for x, y in zip(begin, end):\n",
    "        indices_month = np.argwhere(np.logical_and(dates >= x, dates < y)).flatten()\n",
    "        if len(indices_month) > 3:\n",
    "            to_delete = np.empty((0,))\n",
    "            if begin == -60:\n",
    "                to_delete = indices_month[:-3]\n",
    "            elif begin == 334:\n",
    "                to_delete = indices_month[3:]\n",
    "            elif len(indices_month) == 4:\n",
    "                to_delete = indices_month[1]\n",
    "            elif len(indices_month) == 5:\n",
    "                to_delete = np.array([indices_month[1],\n",
    "                                      indices_month[3]])\n",
    "            elif len(indices_month) == 6:\n",
    "                to_delete = np.array([indices_month[1],\n",
    "                                      indices_month[3],\n",
    "                                      indices_month[4]])\n",
    "                \n",
    "            to_delete = np.array(to_delete)\n",
    "            if to_delete.size > 0:\n",
    "                indices_to_remove.append(to_delete.flatten())\n",
    "                \n",
    "    if len(indices_to_remove) > 0:\n",
    "        indices_to_remove = np.concatenate(indices_to_remove)\n",
    "        after = before - len(indices_to_remove)\n",
    "        print(f\"Keeping {after}/{before}\")\n",
    "        return indices_to_remove\n",
    "    \n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "\n",
    "def subset_contiguous_sunny_dates(dates, probs):\n",
    "    \"\"\"\n",
    "    The general imagery subsetting strategy is as below:\n",
    "        - Select all images with < 30% cloud cover\n",
    "        - For each month, select up to 2 images that are <30% CC and are the closest to\n",
    "          the beginning and the midde of the month\n",
    "        - Select only one image per month for each month if the following criteria are met\n",
    "              - Within Q1 and Q4, apply if at least 3 images in quarter\n",
    "              - Otherwise, apply if at least 8 total images for year\n",
    "              - Select the second image if max CC < 15%, otherwise select least-cloudy image\n",
    "        - If more than 10 images remain, remove any images for April and September\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    probs = np.mean(probs, axis = (1, 2))\n",
    "    begin = [-60, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n",
    "    end = [31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334, 410]\n",
    "    n_per_month = []\n",
    "    months_to_adjust = []\n",
    "    months_to_adjust_again = []\n",
    "    indices_to_rm = []\n",
    "    indices = [x for x in range(len(dates))]\n",
    "    \n",
    "    def _indices_month(dates, x, y):\n",
    "        indices_month = np.argwhere(np.logical_and(\n",
    "                    dates >= x, dates < y)).flatten()\n",
    "        return indices_month\n",
    "\n",
    "    \n",
    "    _ = print_dates(dates, probs)\n",
    "    # Select the best 2 images per month to start with\n",
    "    best_two_per_month = []\n",
    "    for x, y in zip(begin, end):\n",
    "        indices_month = np.argwhere(np.logical_and(\n",
    "            dates >= x, dates < y)).flatten()\n",
    "\n",
    "        month_dates = dates[indices_month]\n",
    "        month_clouds = probs[indices_month]\n",
    "        month_good_dates = month_dates[month_clouds < 0.20]\n",
    "        indices_month = indices_month[month_clouds < 0.20]\n",
    "\n",
    "        if len(month_good_dates) >= 2:\n",
    "            if x > 0:\n",
    "                ideal_dates = [x, x + 15]\n",
    "            else:\n",
    "                ideal_dates = [0, 15]\n",
    "\n",
    "            # We first pick the 2 images with <30% cloud cover that are the closest\n",
    "            # to the 1st and 15th of the month\n",
    "            # todo: if both these images are above 15%, and one below 15% is available, include it\n",
    "            closest_to_first_img = np.argmin(abs(month_good_dates - ideal_dates[0]))\n",
    "            closest_to_second_img = np.argmin(abs(month_good_dates - ideal_dates[1]))\n",
    "            if closest_to_second_img == closest_to_first_img:\n",
    "                distances = abs(month_good_dates - ideal_dates[1])\n",
    "                closest_to_second_img = np.argsort(distances)[1]\n",
    "\n",
    "            first_image = indices_month[closest_to_first_img]\n",
    "            second_image = indices_month[closest_to_second_img]\n",
    "            best_two_per_month.append(first_image)\n",
    "            best_two_per_month.append(second_image)\n",
    "                    \n",
    "        elif len(month_good_dates) >= 1:\n",
    "            if x > 0:\n",
    "                ideal_dates = [x, x + 15]\n",
    "            else:\n",
    "                ideal_dates = [0, 15]\n",
    "\n",
    "            closest_to_second_img = np.argmin(abs(month_good_dates - ideal_dates[1]))\n",
    "            second_image = indices_month[closest_to_second_img]\n",
    "            best_two_per_month.append(second_image)\n",
    "                \n",
    "    dates_round_2 = dates[best_two_per_month]\n",
    "    probs_round_2 = probs[best_two_per_month]\n",
    "    \n",
    "    # We then select between those two images to keep a max of one per month\n",
    "    # We select the least cloudy image if the most cloudy has >15% cloud cover\n",
    "    # Otherwise we select the second image\n",
    "\n",
    "    # If there are more than 8 images, subset so only 1 image per month,\n",
    "    # To bring down to a min of 8 images\n",
    "    if len(dates_round_2) >= 8:\n",
    "        n_to_rm = len(dates_round_2) - 8\n",
    "        monthly_dates = []\n",
    "        monthly_probs = []\n",
    "        monthly_dates_date = []\n",
    "        removed = 0\n",
    "        for x, y in zip(begin, end):\n",
    "            indices_month = np.argwhere(np.logical_and(\n",
    "                dates >= x, dates < y)).flatten()\n",
    "            dates_month = dates[indices_month]\n",
    "            indices_month = [val for i, val in enumerate(indices_month) if dates_month[i] in dates_round_2]\n",
    "            if len(indices_month) > 1:\n",
    "                month_dates = dates[indices_month]\n",
    "                month_clouds = probs[indices_month]\n",
    "\n",
    "                subset_month = True\n",
    "                if x == -60:\n",
    "                    feb_mar = np.argwhere(np.logical_and(\n",
    "                        dates >= 31, dates < 90)).flatten()\n",
    "                    subset_month = False if len(feb_mar) < 2 else True\n",
    "                if x == 334:\n",
    "                    oct_nov = np.argwhere(np.logical_and(\n",
    "                        dates >= 273, dates < 334)).flatten()\n",
    "                    subset_month = False if len(oct_nov) < 2 else True\n",
    "\n",
    "                if subset_month:\n",
    "                    subset_month = True if removed <= n_to_rm else False\n",
    "                if subset_month:\n",
    "                    if np.max(month_clouds) >= 0.10:\n",
    "                        month_best_date = [indices_month[np.argmin(month_clouds)]]\n",
    "                    else:\n",
    "                        month_best_date = [indices_month[1]]\n",
    "                else:\n",
    "                    month_best_date = indices_month\n",
    "                monthly_dates.extend(month_best_date)\n",
    "                monthly_probs.extend(probs[month_best_date])\n",
    "                monthly_dates_date.extend(dates[month_best_date])\n",
    "                removed += 1\n",
    "            elif len(indices_month) == 1:\n",
    "                monthly_dates.append(indices_month[0])\n",
    "                monthly_probs.append(probs[indices_month[0]])\n",
    "                monthly_dates_date.append(dates[indices_month[0]])\n",
    "    else:\n",
    "        monthly_dates = best_two_per_month\n",
    "        \n",
    "    indices_to_rm = [x for x in indices if x not in monthly_dates]\n",
    "\n",
    "\n",
    "    dates_round_3 = dates[monthly_dates]\n",
    "    probs_round_3 = probs[monthly_dates]\n",
    "\n",
    "    if len(dates_round_3) >= 10:\n",
    "        delete_max = False\n",
    "        if np.max(probs_round_3) >= 0.15:\n",
    "            delete_max = True\n",
    "            indices_to_rm.append(monthly_dates[np.argmax(probs_round_3)])\n",
    "        for x, y in zip(begin, end):\n",
    "            indices_month = np.argwhere(np.logical_and(\n",
    "                dates >= x, dates < y)).flatten()\n",
    "            dates_month = dates[indices_month]\n",
    "            indices_month = [x for x in indices_month if x in monthly_dates]\n",
    "\n",
    "            n_removed = 0\n",
    "            if len(indices_month) >= 1:\n",
    "                if len(monthly_dates) == 11 and delete_max:\n",
    "                    continue\n",
    "                elif len(monthly_dates) >= 11:\n",
    "                    if x in [90, 243]:\n",
    "                        indices_to_rm.append(indices_month[0])\n",
    "\n",
    "    return indices_to_rm\n",
    "\n",
    "\n",
    "def to_int16(array: np.array) -> np.array:\n",
    "    '''Converts a float32 array to uint16, reducing storage costs by three-fold'''\n",
    "    array = np.clip(array, 0, 1)\n",
    "    array = np.trunc(array * 65535)\n",
    "    assert np.min(array >= 0)\n",
    "    assert np.max(array <= 65535)\n",
    "    \n",
    "    return array.astype(np.uint16)\n",
    "\n",
    "\n",
    "def process_raw(plot_id, path = 'train'):\n",
    "    \"\"\" Downloads slope and sentinel-2 data for all plots associated\n",
    "        with an input CSV from a collect earth online survey\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path)\n",
    "         output_folder (os.path)\n",
    "        \n",
    "        Creates:\n",
    "         output_folder/{plot_id}.npy\n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"         \n",
    "\n",
    "    x = np.load(f\"../data/{path}-raw/{plot_id}.npy\")\n",
    "    x = np.float32(x) / 65535\n",
    "    if x.shape[-1] == 10:\n",
    "        s2_dates = np.load(f\"../data/{path}-dates/{plot_id}.npy\")\n",
    "        dem = np.load(f\"../data/{path}-slope/{plot_id}.npy\")\n",
    "\n",
    "        assert x.shape[0] == s2_dates.shape[0]\n",
    "\n",
    "        missing_px = id_missing_px(x)\n",
    "        if len(missing_px) > 0:\n",
    "            print(f\"Deleting {missing_px} because of missing data\")\n",
    "            x = np.delete(x, missing_px, 0)\n",
    "            s2_dates = np.delete(s2_dates, missing_px)\n",
    "\n",
    "\n",
    "        n_images = x.shape[0]\n",
    "\n",
    "        to_remove = select_dates(s2_dates)\n",
    "        if len(to_remove) > 0:\n",
    "            x = np.delete(x, to_remove, 0)\n",
    "            s2_dates = np.delete(s2_dates, to_remove)\n",
    "\n",
    "        print(x.shape)\n",
    "        #to_remove = subset_contiguous_sunny_dates(s2_dates)\n",
    "        #if len(to_remove) > 0:\n",
    "        #    x = np.delete(x, to_remove, 0)\n",
    "        #    s2_dates = np.delete(s2_dates, to_remove)\n",
    "\n",
    "        for band in range(0, 10):\n",
    "            for time in range(0, x.shape[0]):\n",
    "                x_i = x[time, :, :, band]\n",
    "                x_i[np.argwhere(np.isnan(x_i))] = np.mean(x_i)\n",
    "                x[time, :, :, band] = x_i\n",
    "\n",
    "        # Interpolate linearly to 5 day frequency\n",
    "        tiles, max_distance = calculate_and_save_best_images(x, s2_dates)\n",
    "        sm = Smoother(lmbd = 150, size = tiles.shape[0],\n",
    "                      nbands = 10, dimx = tiles.shape[1], dimy = tiles.shape[2])\n",
    "        x = sm.interpolate_array(tiles)\n",
    "        x = superresolve_tile(x)\n",
    "        tiles = concatenate_dem(x, dem)\n",
    "        dates = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n",
    "        dates = np.array(dates) + 15\n",
    "        closest_date = []\n",
    "        for date in dates:\n",
    "            date_diff = s2_dates[np.argmin(abs(s2_dates - date))]\n",
    "            closest_date.append(date_diff)\n",
    "\n",
    "        closest_date = np.array(closest_date)\n",
    "        closest_date = closest_date[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "        closest_date = np.broadcast_to(closest_date, (12, 28, 28, 1))\n",
    "        closest_date = (closest_date + 45)  / 411\n",
    "\n",
    "        tiles = np.concatenate([tiles, closest_date], axis = -1)\n",
    "\n",
    "        if np.sum(np.isnan(tiles)) == 0:\n",
    "            print(f\"There are {np.sum(np.isnan(tiles))} NA values\")\n",
    "            if max_distance <= 300 and n_images >= 5:\n",
    "                tiles = to_int16(tiles)\n",
    "                #np.save(f\"../data/{path}-s2-new/{plot_id}\", tiles)\n",
    "                tile_path = f\"../data/{path}-s2/{plot_id}\"\n",
    "                tile_path = tile_path + \".hkl\"\n",
    "                hkl.dump(tiles, tile_path, mode='w', compression='gzip')\n",
    "                print(f\"Saved {tiles.shape} shape, {n_images} img,\"\n",
    "                      f\" to {tile_path} \\n\")\n",
    "            else:\n",
    "                print(f\"Skipping {plot_id} because {max_distance} distance, and {n_images} img \\n\")\n",
    "\n",
    "        return tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function execution\n",
    "## 1. Download DEM and Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download of 60 plots from ../data/train-csv/ceo-chaco-eucalyptus-30.csv to ../data/train-dem/\n",
      "Downloading 1/60, 30001\n",
      "Downloading 2/60, 30002\n",
      "Downloading 3/60, 30003\n",
      "Downloading 4/60, 30004\n",
      "Downloading 5/60, 30005\n",
      "Downloading 6/60, 30006\n",
      "Downloading 7/60, 30007\n",
      "Downloading 8/60, 30008\n",
      "Downloading 9/60, 30009\n",
      "Downloading 10/60, 300010\n",
      "Downloading 11/60, 300011\n",
      "Downloading 12/60, 300012\n",
      "Downloading 13/60, 300013\n",
      "Downloading 14/60, 300014\n",
      "Downloading 15/60, 300015\n",
      "Downloading 16/60, 300016\n",
      "Downloading 17/60, 300017\n",
      "Downloading 18/60, 300018\n",
      "Downloading 19/60, 300019\n",
      "Downloading 20/60, 300020\n",
      "Downloading 21/60, 300021\n",
      "Downloading 22/60, 300022\n",
      "Downloading 23/60, 300023\n",
      "Downloading 24/60, 300024\n",
      "Downloading 25/60, 300025\n",
      "Downloading 26/60, 300026\n",
      "Downloading 27/60, 300027\n",
      "Downloading 28/60, 300028\n",
      "Downloading 29/60, 300029\n",
      "Downloading 30/60, 300030\n",
      "Downloading 31/60, 300031\n",
      "Downloading 32/60, 300032\n",
      "Downloading 33/60, 300033\n",
      "Downloading 34/60, 300034\n",
      "Downloading 35/60, 300035\n",
      "Downloading 36/60, 300036\n",
      "Downloading 37/60, 300037\n",
      "Downloading 38/60, 300038\n",
      "Downloading 39/60, 300039\n",
      "Downloading 40/60, 300040\n",
      "Downloading 41/60, 300041\n",
      "Downloading 42/60, 300042\n",
      "Downloading 43/60, 300043\n",
      "Downloading 44/60, 300044\n",
      "Downloading 45/60, 300045\n",
      "Downloading 46/60, 300046\n",
      "Downloading 47/60, 300047\n",
      "Downloading 48/60, 300048\n",
      "Downloading 49/60, 300049\n",
      "Downloading 50/60, 300050\n",
      "Downloading 51/60, 300051\n",
      "Downloading 52/60, 300052\n",
      "Downloading 53/60, 300053\n",
      "Downloading 54/60, 300054\n",
      "Downloading 55/60, 300055\n",
      "Downloading 56/60, 300056\n",
      "Downloading 57/60, 300057\n",
      "Downloading 58/60, 300058\n",
      "Downloading 59/60, 300059\n",
      "Downloading 60/60, 300060\n",
      "Starting download of 0 plots from ../data/train-csv/ceo-chaco-global-uuid-11.csv to ../data/train-dem/\n",
      "Starting download of 0 plots from ../data/train-csv/ceo-2022chaco_rubber-sample-29.csv to ../data/train-dem/\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "for i in (os.listdir(\"../data/train-csv/\")):\n",
    "    if \"chaco\" in i:\n",
    "        tile = download_new_dem(\"../data/train-csv/\" + i,\n",
    "                                \"../data/train-dem/\",\n",
    "                                image_format = MimeType.TIFF)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Raw data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download of 1 plots from ../data/train-csv/ceo-chaco-eucalyptus-30.csv to ../data/train-raw/\n",
      "Downloading 1/1, 300040\n",
      "Shadows ((20, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [11, 385], Probs: [0.03, 0.03]\n",
      "2, Dates: [400], Probs: [0.05]\n",
      "3, Dates: [], Probs: []\n",
      "4, Dates: [], Probs: []\n",
      "5, Dates: [125, 130, 150], Probs: [0.09, 0.01, 0.04]\n",
      "6, Dates: [160, 165], Probs: [0.02, 0.19]\n",
      "7, Dates: [190, 195, 210], Probs: [0.02, 0.01, 0.1]\n",
      "8, Dates: [215, 225], Probs: [0.16, 0.12]\n",
      "9, Dates: [250, 270], Probs: [0.17, 0.03]\n",
      "10, Dates: [280], Probs: [0.05]\n",
      "11, Dates: [315], Probs: [0.09]\n",
      "12, Dates: [-14, -4, 335], Probs: [0.15, 0.1, 0.13]\n",
      "1, Dates: [11, 385], Probs: [0.03, 0.03]\n",
      "2, Dates: [], Probs: []\n",
      "3, Dates: [], Probs: []\n",
      "4, Dates: [], Probs: []\n",
      "5, Dates: [130], Probs: [0.01]\n",
      "6, Dates: [160], Probs: [0.02]\n",
      "7, Dates: [195], Probs: [0.01]\n",
      "8, Dates: [225], Probs: [0.12]\n",
      "9, Dates: [270], Probs: [0.03]\n",
      "10, Dates: [280], Probs: [0.05]\n",
      "11, Dates: [315], Probs: [0.09]\n",
      "12, Dates: [-4], Probs: [0.1]\n",
      "The original max value is 32374\n",
      "Original 20 meter bands size: (10, 16, 16, 6), using 0.01953125 PU\n",
      "The original L2A image size is: (10, 32, 32, 4)\n",
      "The original max value is 31168\n",
      "Shadows (10, 96, 96), clouds (10, 96, 96), S2, (10, 32, 32, 10), S2d, (10,)\n",
      "(10, 32, 32, 10)\n",
      "\n",
      "\n",
      "Starting download of 0 plots from ../data/train-csv/ceo-chaco-global-uuid-11.csv to ../data/train-raw/\n",
      "Starting download of 0 plots from ../data/train-csv/ceo-2022chaco_rubber-sample-29.csv to ../data/train-raw/\n"
     ]
    }
   ],
   "source": [
    "for i in (os.listdir(\"../data/train-csv/\")):\n",
    "    if \"chaco\" in i:\n",
    "        download_raw_data(\"../data/train-csv/\" + i,\n",
    "                          \"../data/train-raw/\", \n",
    "                          fmt = 'train',\n",
    "                          image_format = MimeType.TIFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process train / test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300022\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300022.hkl \n",
      "\n",
      "31 300022\n",
      "300036\n",
      "(9, 32, 32, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbrandt.terminal/Documents/GitHub/sentinel-tree-cover/src/downloading/utils.py:236: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  selected_images[i] = {'image_date': np.array([prior_dates, after_dates]).flatten(),\n",
      "/Users/jbrandt.terminal/Documents/GitHub/sentinel-tree-cover/src/downloading/utils.py:253: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  step = np.median(img_bands[info['image_idx']], axis = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300036.hkl \n",
      "\n",
      "33 300036\n",
      "30065\n",
      "(2, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 30065 because 215 distance, and 2 img \n",
      "\n",
      "56 30065\n",
      "20061\n",
      "Deleting [4 5 6] because of missing data\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 20061 because 0 distance, and 4 img \n",
      "\n",
      "116 20061\n",
      "50021\n",
      "Deleting [1] because of missing data\n",
      "(3, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 50021 because 104 distance, and 3 img \n",
      "\n",
      "130 50021\n",
      "5003\n",
      "(2, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 5003 because 324 distance, and 2 img \n",
      "\n",
      "157 5003\n",
      "300037\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300037.hkl \n",
      "\n",
      "159 300037\n",
      "300023\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300023.hkl \n",
      "\n",
      "164 300023\n",
      "200223\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 200223 because 0 distance, and 4 img \n",
      "\n",
      "197 200223\n",
      "300035\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300035.hkl \n",
      "\n",
      "212 300035\n",
      "300021\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300021.hkl \n",
      "\n",
      "213 300021\n",
      "300020\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300020.hkl \n",
      "\n",
      "353 300020\n",
      "300034\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300034.hkl \n",
      "\n",
      "358 300034\n",
      "300030\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300030.hkl \n",
      "\n",
      "393 300030\n",
      "300024\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300024.hkl \n",
      "\n",
      "399 300024\n",
      "300018\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300018.hkl \n",
      "\n",
      "403 300018\n",
      "250015\n",
      "Deleting [0 3 4 5 8 9] because of missing data\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 250015 because 0 distance, and 4 img \n",
      "\n",
      "423 250015\n",
      "240010\n",
      "Deleting [1 2 3 4 5 6 9] because of missing data\n",
      "(3, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 240010 because 274 distance, and 3 img \n",
      "\n",
      "456 240010\n",
      "300019\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300019.hkl \n",
      "\n",
      "538 300019\n",
      "300025\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300025.hkl \n",
      "\n",
      "545 300025\n",
      "300031\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300031.hkl \n",
      "\n",
      "546 300031\n",
      "300027\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300027.hkl \n",
      "\n",
      "590 300027\n",
      "300033\n",
      "Deleting [3] because of missing data\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300033.hkl \n",
      "\n",
      "596 300033\n",
      "5007\n",
      "(3, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 5007 because 170 distance, and 3 img \n",
      "\n",
      "603 5007\n",
      "250016\n",
      "Deleting [0 2 3 4 5 8] because of missing data\n",
      "(3, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 250016 because 279 distance, and 3 img \n",
      "\n",
      "610 250016\n",
      "800226\n",
      "Deleting [0 2 3 7 8] because of missing data\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 800226 because 89 distance, and 4 img \n",
      "\n",
      "655 800226\n",
      "50018\n",
      "(3, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 50018 because 140 distance, and 3 img \n",
      "\n",
      "694 50018\n",
      "500241\n",
      "Deleting [4] because of missing data\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 500241 because 199 distance, and 4 img \n",
      "\n",
      "729 500241\n",
      "300032\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300032.hkl \n",
      "\n",
      "731 300032\n",
      "300026\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300026.hkl \n",
      "\n",
      "732 300026\n",
      "22006\n",
      "(2, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 22006 because 300 distance, and 2 img \n",
      "\n",
      "761 22006\n",
      "300041\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300041.hkl \n",
      "\n",
      "782 300041\n",
      "300055\n",
      "(8, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 8 img, to ../data/train-s2/300055.hkl \n",
      "\n",
      "785 300055\n",
      "30006\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/30006.hkl \n",
      "\n",
      "816 30006\n",
      "30007\n",
      "Deleting [0] because of missing data\n",
      "(7, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 7 img, to ../data/train-s2/30007.hkl \n",
      "\n",
      "889 30007\n",
      "141018069\n",
      "(2, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 141018069 because 115 distance, and 2 img \n",
      "\n",
      "913 141018069\n",
      "300054\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300054.hkl \n",
      "\n",
      "916 300054\n",
      "300040\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300040.hkl \n",
      "\n",
      "917 300040\n",
      "2400241\n",
      "Deleting [1 3 4 5 6 7] because of missing data\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 2400241 because 0 distance, and 4 img \n",
      "\n",
      "947 2400241\n",
      "100207\n",
      "Deleting [0 1 2 7 8] because of missing data\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 100207 because 0 distance, and 4 img \n",
      "\n",
      "951 100207\n",
      "300056\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300056.hkl \n",
      "\n",
      "966 300056\n",
      "300042\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300042.hkl \n",
      "\n",
      "969 300042\n",
      "30005\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/30005.hkl \n",
      "\n",
      "984 30005\n",
      "80077\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 80077 because 130 distance, and 4 img \n",
      "\n",
      "1012 80077\n",
      "240062\n",
      "Deleting [1 2 3 4 5 6] because of missing data\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 240062 because 274 distance, and 4 img \n",
      "\n",
      "1024 240062\n",
      "3002\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 3002 because 0 distance, and 4 img \n",
      "\n",
      "1045 3002\n",
      "50055\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 50055 because 0 distance, and 4 img \n",
      "\n",
      "1076 50055\n",
      "100170\n",
      "Deleting [0 1 4 5] because of missing data\n",
      "(2, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 100170 because 0 distance, and 2 img \n",
      "\n",
      "1082 100170\n",
      "30004\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/30004.hkl \n",
      "\n",
      "1090 30004\n",
      "141018056\n",
      "300043\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300043.hkl \n",
      "\n",
      "1108 300043\n",
      "141018081\n",
      "(2, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 141018081 because 274 distance, and 2 img \n",
      "\n",
      "1110 141018081\n",
      "300057\n",
      "(7, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 7 img, to ../data/train-s2/300057.hkl \n",
      "\n",
      "1111 300057\n",
      "25009\n",
      "Deleting [2 3 4 5 7 8] because of missing data\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 25009 because 169 distance, and 4 img \n",
      "\n",
      "1138 25009\n",
      "300053\n",
      "(8, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 8 img, to ../data/train-s2/300053.hkl \n",
      "\n",
      "1154 300053\n",
      "300047\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300047.hkl \n",
      "\n",
      "1156 300047\n",
      "30014\n",
      "(1, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 30014 because 0 distance, and 1 img \n",
      "\n",
      "1170 30014\n",
      "2500132\n",
      "Deleting [0 1 2 3 4 5 6] because of missing data\n",
      "(0, 32, 32, 10)\n",
      "141018132\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 141018132 because 0 distance, and 4 img \n",
      "\n",
      "1187 141018132\n",
      "3006\n",
      "(1, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 3006 because 0 distance, and 1 img \n",
      "\n",
      "1222 3006\n",
      "30001\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/30001.hkl \n",
      "\n",
      "1283 30001\n",
      "300046\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300046.hkl \n",
      "\n",
      "1291 300046\n",
      "300052\n",
      "(6, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 6 img, to ../data/train-s2/300052.hkl \n",
      "\n",
      "1297 300052\n",
      "22001\n",
      "(1, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 22001 because 0 distance, and 1 img \n",
      "\n",
      "1311 22001\n",
      "22003\n",
      "(3, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 22003 because 0 distance, and 3 img \n",
      "\n",
      "1318 22003\n",
      "300044\n",
      "Deleting [4] because of missing data\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300044.hkl \n",
      "\n",
      "1336 300044\n",
      "300050\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300050.hkl \n",
      "\n",
      "1337 300050\n",
      "141018079\n",
      "(4, 32, 32, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 NA values\n",
      "Skipping 141018079 because 95 distance, and 4 img \n",
      "\n",
      "1348 141018079\n",
      "300130\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 300130 because 100 distance, and 4 img \n",
      "\n",
      "1355 300130\n",
      "30003\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/30003.hkl \n",
      "\n",
      "1359 30003\n",
      "2500131\n",
      "Deleting [0 1 2 3 4 5 6] because of missing data\n",
      "(0, 32, 32, 10)\n",
      "240059\n",
      "Deleting [1 2 3 4 5 6] because of missing data\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 240059 because 274 distance, and 4 img \n",
      "\n",
      "1421 240059\n",
      "2500130\n",
      "Deleting [0 2 3 4 5 6 7 9] because of missing data\n",
      "(2, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 2500130 because 334 distance, and 2 img \n",
      "\n",
      "1451 2500130\n",
      "30002\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/30002.hkl \n",
      "\n",
      "1458 30002\n",
      "300131\n",
      "(2, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 300131 because 0 distance, and 2 img \n",
      "\n",
      "1468 300131\n",
      "141018078\n",
      "(2, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 141018078 because 74 distance, and 2 img \n",
      "\n",
      "1472 141018078\n",
      "300051\n",
      "(7, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 7 img, to ../data/train-s2/300051.hkl \n",
      "\n",
      "1482 300051\n",
      "300045\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300045.hkl \n",
      "\n",
      "1485 300045\n",
      "300048\n",
      "(7, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 7 img, to ../data/train-s2/300048.hkl \n",
      "\n",
      "1521 300048\n",
      "300060\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300060.hkl \n",
      "\n",
      "1523 300060\n",
      "300128\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 300128 because 210 distance, and 4 img \n",
      "\n",
      "1535 300128\n",
      "280066\n",
      "Deleting [0] because of missing data\n",
      "(2, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 280066 because 85 distance, and 2 img \n",
      "\n",
      "1549 280066\n",
      "220010\n",
      "(1, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 220010 because 0 distance, and 1 img \n",
      "\n",
      "1601 220010\n",
      "200115\n",
      "Deleting [0 1 2 4 5] because of missing data\n",
      "(2, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 200115 because 250 distance, and 2 img \n",
      "\n",
      "1639 200115\n",
      "300049\n",
      "(7, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 7 img, to ../data/train-s2/300049.hkl \n",
      "\n",
      "1671 300049\n",
      "8002\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 8002 because 114 distance, and 4 img \n",
      "\n",
      "1868 8002\n",
      "30009\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/30009.hkl \n",
      "\n",
      "1922 30009\n",
      "100169\n",
      "Deleting [0 1 3 6 7 8] because of missing data\n",
      "(3, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 100169 because 0 distance, and 3 img \n",
      "\n",
      "1930 100169\n",
      "30008\n",
      "(7, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 7 img, to ../data/train-s2/30008.hkl \n",
      "\n",
      "2014 30008\n",
      "250056\n",
      "Deleting [1 2 3 4 8] because of missing data\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 250056 because 249 distance, and 4 img \n",
      "\n",
      "2019 250056\n",
      "141018072\n",
      "(2, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 141018072 because 114 distance, and 2 img \n",
      "\n",
      "2034 141018072\n",
      "300059\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300059.hkl \n",
      "\n",
      "2084 300059\n",
      "141018070\n",
      "(2, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 141018070 because 260 distance, and 2 img \n",
      "\n",
      "2089 141018070\n",
      "250055\n",
      "Deleting [1 2 3 4 8] because of missing data\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 250055 because 249 distance, and 4 img \n",
      "\n",
      "2210 250055\n",
      "300058\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300058.hkl \n",
      "\n",
      "2225 300058\n",
      "300017\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300017.hkl \n",
      "\n",
      "2276 300017\n",
      "20068\n",
      "Deleting [0 2 4] because of missing data\n",
      "(3, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 20068 because 0 distance, and 3 img \n",
      "\n",
      "2355 20068\n",
      "300016\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300016.hkl \n",
      "\n",
      "2402 300016\n",
      "200214\n",
      "(2, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 200214 because 0 distance, and 2 img \n",
      "\n",
      "2423 200214\n",
      "300028\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300028.hkl \n",
      "\n",
      "2452 300028\n",
      "300014\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300014.hkl \n",
      "\n",
      "2455 300014\n",
      "5008\n",
      "(1, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 5008 because 0 distance, and 1 img \n",
      "\n",
      "2465 5008\n",
      "20056\n",
      "Deleting [0 1 2 4] because of missing data\n",
      "(3, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 20056 because 215 distance, and 3 img \n",
      "\n",
      "2503 20056\n",
      "300015\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300015.hkl \n",
      "\n",
      "2576 300015\n",
      "300029\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300029.hkl \n",
      "\n",
      "2579 300029\n",
      "2500216\n",
      "Deleting [0 1 2 3 4 5 6 7] because of missing data\n",
      "(2, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 2500216 because 0 distance, and 2 img \n",
      "\n",
      "2595 2500216\n",
      "141018212\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 141018212 because 254 distance, and 4 img \n",
      "\n",
      "2614 141018212\n",
      "300011\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300011.hkl \n",
      "\n",
      "2620 300011\n",
      "300039\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300039.hkl \n",
      "\n",
      "2627 300039\n",
      "30057\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 30057 because 80 distance, and 4 img \n",
      "\n",
      "2725 30057\n",
      "300038\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300038.hkl \n",
      "\n",
      "2738 300038\n",
      "300010\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300010.hkl \n",
      "\n",
      "2745 300010\n",
      "300012\n",
      "(10, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 10 img, to ../data/train-s2/300012.hkl \n",
      "\n",
      "2794 300012\n",
      "50011\n",
      "(4, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Skipping 50011 because 190 distance, and 4 img \n",
      "\n",
      "2898 50011\n",
      "300013\n",
      "(9, 32, 32, 10)\n",
      "There are 0 NA values\n",
      "Saved (12, 28, 28, 12) shape, 9 img, to ../data/train-s2/300013.hkl \n",
      "\n",
      "2934 300013\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "plots = [str(x[:-4]) for x in os.listdir(\"../data/train-raw/\") if \".npy\" in x]\n",
    "for plot in plots:\n",
    "    i += 1\n",
    "    if not os.path.exists(\"../data/train-s2/\" + plot + \".hkl\"):\n",
    "        print(plot)\n",
    "        try:\n",
    "            tiles = process_raw(plot, path = 'train')\n",
    "            print(i, plot)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
