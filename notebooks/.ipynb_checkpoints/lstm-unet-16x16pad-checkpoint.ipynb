{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master model development\n",
    "\n",
    "## John Brandt\n",
    "\n",
    "### Last updated: November 1 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Package loading\n",
    "*  Hyperparameter definitions\n",
    "*  Additional layer definitions\n",
    "*  Model definition\n",
    "*  Data loading\n",
    "*  Data preprocessing\n",
    "*  K means clustering\n",
    "*  Augment training data\n",
    "*  Loss definition\n",
    "*  Equibatch creation\n",
    "*  Model training\n",
    "*  Model validation and sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Remove imports that aren't needed to save RAM\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras.layers import Conv2D, Lambda, Dense, Multiply, Add\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim import conv2d\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import itertools\n",
    "from tflearn.layers.conv import global_avg_pool\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.layers import batch_normalization\n",
    "from tensorflow.python.util import deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/zoneout.py\n",
    "%run ../src/convgru.py\n",
    "%run ../src/lovasz.py\n",
    "%run ../src/utils.py\n",
    "%run ../src/adabound.py\n",
    "%run ../src/slope.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONE_OUT_PROB = 0.10 #(0.05, 0.20, 0.05) --> 4\n",
    "L2_REG = 0.0005 #(1-e6, 1-e1, x10) --> 5\n",
    "INITIAL_LR = 2e-6 #(1e-6, 1e-3, x5) --> 10\n",
    "FINAL_LR = 2e-4 # (1e - 5, 1e-2, x5) --> 10\n",
    "BN_MOMENTUM = 0.9 # --> 3\n",
    "BATCH_SIZE = 4 # -->4\n",
    "TRAIN_RATIO = 0.8\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "\n",
    "AUGMENTATION_RATIO = 4\n",
    "IMAGE_SIZE = 16\n",
    "existing = [int(x[:-4]) for x in os.listdir('../data/final/') if \".DS\" not in x]\n",
    "N_SAMPLES = len(existing)\n",
    "\n",
    "LABEL_SIZE = 14\n",
    "\n",
    "    \n",
    "TRAIN_SAMPLES = int((N_SAMPLES * AUGMENTATION_RATIO) * TRAIN_RATIO)\n",
    "TEST_SAMPLES = int((N_SAMPLES * AUGMENTATION_RATIO) - TRAIN_SAMPLES)\n",
    "print(TRAIN_SAMPLES // AUGMENTATION_RATIO, N_SAMPLES - (TRAIN_SAMPLES // AUGMENTATION_RATIO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional layer definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_elu(inp, is_training, kernel_size, scope, filter_count = 16):\n",
    "    if kernel_size == 3:\n",
    "        padded = ReflectionPadding2D((1, 1,))(inp)\n",
    "        padding = 'valid'\n",
    "    else:\n",
    "        padded = inp\n",
    "        padding = 'same'\n",
    "    conv = Conv2D(filters = filter_count, kernel_size = (kernel_size, kernel_size),\n",
    "                      padding = padding, kernel_regularizer=reg)(padded)\n",
    "    elu = ELU()(conv)\n",
    "    bn = Batch_Normalization(elu, training=is_training, scope = scope + \"bn\")\n",
    "    return bn\n",
    "    \n",
    "    \n",
    "def fpa(inp, filter_count):\n",
    "    one = conv_bn_elu(inp, is_training, 1, 'forward1', filter_count)\n",
    "    three = conv_bn_elu(inp, is_training, 3, 'down1', filter_count)\n",
    "    three_f = conv_bn_elu(three, is_training, 3, 'down1_f', filter_count)\n",
    "    two = conv_bn_elu(three, is_training, 2, 'down2', filter_count)\n",
    "    two_f = conv_bn_elu(two, is_training, 2, 'down2_f', filter_count)\n",
    "    \n",
    "    # top block\n",
    "    pooled = tf.keras.layers.GlobalAveragePooling2D()(inp)\n",
    "    one_top = conv_bn_elu(tf.reshape(pooled, (-1, 1, 1, pooled.shape[-1])),\n",
    "                          is_training, 1, 'top1', filter_count)\n",
    "    four_top = tf.keras.layers.UpSampling2D((4, 4))(one_top)\n",
    "    \n",
    "    \n",
    "    concat_1 = tf.multiply(one, tf.add(three_f, two_f))\n",
    "    concat_2 = tf.add(concat_1, four_top)\n",
    "    print(\"Feature pyramid attention shape {}\".format(concat_2.shape))\n",
    "    return concat_2\n",
    "\n",
    "\n",
    "def upconv2d(X, filters, is_training, scope):\n",
    "    X = tf.image.resize_images(X, [X.shape[1]*2, X.shape[2]*2],\n",
    "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    X = conv_bn_elu(X, is_training, 3, scope, filter_count = filters)\n",
    "    return X\n",
    "\n",
    "    \n",
    "def create_deconv_init(filter_size, num_channels):\n",
    "    bilinear_kernel = np.zeros([filter_size, filter_size], dtype=np.float32)\n",
    "    scale_factor = (filter_size + 1) // 2\n",
    "    if filter_size % 2 == 1:\n",
    "        center = scale_factor - 1\n",
    "    else:\n",
    "        center = scale_factor - 0.5\n",
    "    for x in range(filter_size):\n",
    "        for y in range(filter_size):\n",
    "            bilinear_kernel[x,y] = (1 - abs(x - center) / scale_factor) * \\\n",
    "                                   (1 - abs(y - center) / scale_factor)\n",
    "    weights = np.zeros((filter_size, filter_size, num_channels, num_channels))\n",
    "    for i in range(num_channels):\n",
    "        weights[:, :, i, i] = bilinear_kernel\n",
    "\n",
    "    #assign numpy array to constant_initalizer and pass to get_variable\n",
    "    bilinear_init = tf.constant_initializer(value=weights, dtype=tf.float32)\n",
    "    return bilinear_init\n",
    "\n",
    "\n",
    "def get_deconv2d(inp, filter_count, num_channels, scope, is_training):\n",
    "    bilinear_init = create_deconv_init(4, filter_count)\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = filter_count, kernel_size = (4, 4),\n",
    "                                        strides=(2, 2), padding='same', \n",
    "                                        kernel_initializer = bilinear_init)(inp)\n",
    "    x = ELU()(x)\n",
    "    x = Batch_Normalization(x, training=is_training, scope = scope + \"bn\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    return batch_normalization(inputs=x, \n",
    "                               momentum = BN_MOMENTUM, \n",
    "                               training=training,\n",
    "                               renorm = True,\n",
    "                               reuse=None,\n",
    "                               name = scope)\n",
    "\n",
    "    \n",
    "\n",
    "def gau(x_low_level, x_high_level, scope, is_training, filter_count, uptype, size = 4):\n",
    "    \"\"\"\n",
    "    The global attention upsample to replace the up_cat_conv element\n",
    "    \"\"\"\n",
    "    print(x_low_level.shape)\n",
    "    print(x_high_level.shape)\n",
    "    low_feat = conv_bn_elu(x_low_level, is_training, 3, 'gauforward' + scope, filter_count)\n",
    "    high_gap = tf.keras.layers.GlobalAveragePooling2D()(x_high_level)\n",
    "    high_feat = tf.keras.layers.Dense(filter_count, activation='linear', use_bias=False)(high_gap)\n",
    "    high_feat = ELU()(high_feat)\n",
    "    high_feat = Batch_Normalization(high_feat, training=is_training, scope = scope + \"bn_highfeat\")\n",
    "    high_feat = tf.keras.layers.Reshape((1, 1, -1))(high_feat)\n",
    "    high_feat_gate = tf.keras.layers.UpSampling2D((size, size))(high_feat)\n",
    "    gated_low = tf.keras.layers.multiply([low_feat, high_feat_gate])\n",
    "    gated_low = conv_bn_elu(gated_low, is_training, 3, 'gauforward5' + scope, filter_count)\n",
    "    if uptype == \"upconv\":\n",
    "        gated_high = upconv2d(gated_low, filter_count, is_training, scope + \"conv\")\n",
    "    elif uptype == \"transpose\":\n",
    "        bilinear_init = create_deconv_init(4, filter_count)\n",
    "        gated_high = tf.keras.layers.Conv2DTranspose(filters = filter_count, kernel_size = (4, 4),\n",
    "                                             strides=(2, 2), padding='same', \n",
    "                                                     kernel_initializer = bilinear_init)(gated_low)\n",
    "        gated_high = ELU()(gated_high)\n",
    "        gated_high = Batch_Normalization(gated_high, training=is_training, scope = scope + \"bn_gatedhigh\")\n",
    "    high_clamped = conv_bn_elu(x_high_level, is_training, 3, 'gauforward1' + scope, filter_count)\n",
    "    return tf.keras.layers.add([gated_high, high_clamped])\n",
    "\n",
    "\n",
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cse_block(prevlayer, prefix):\n",
    "    mean = Lambda(lambda xin: K.mean(xin, axis=[1, 2]))(prevlayer)\n",
    "    lin1 = Dense(K.int_shape(prevlayer)[3] // 2, name=prefix + 'cse_lin1', activation='relu')(mean)\n",
    "    lin2 = Dense(K.int_shape(prevlayer)[3], name=prefix + 'cse_lin2', activation='sigmoid')(lin1)\n",
    "    x = Multiply()([prevlayer, lin2])\n",
    "    return x\n",
    "\n",
    "\n",
    "def sse_block(prevlayer, prefix):\n",
    "    # Bug? Should be 1 here?\n",
    "    conv = Conv2D(K.int_shape(prevlayer)[3], (1, 1), padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                  activation='sigmoid', strides=(1, 1),\n",
    "                  name=prefix + \"_conv\")(prevlayer)\n",
    "    conv = Multiply(name=prefix + \"_mul\")([prevlayer, conv])\n",
    "    return conv\n",
    "\n",
    "\n",
    "def csse_block(x, prefix):\n",
    "    '''\n",
    "    Implementation of Concurrent Spatial and Channel ‘Squeeze & Excitation’ in Fully Convolutional Networks\n",
    "    https://arxiv.org/abs/1803.02579\n",
    "    '''\n",
    "    cse = cse_block(x, prefix)\n",
    "    sse = sse_block(x, prefix)\n",
    "    x = Add(name=prefix + \"_csse_mul\")([cse, sse])\n",
    "\n",
    "    return x\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = keras.regularizers.l2(L2_REG)\n",
    "inp = tf.placeholder(tf.float32, shape=(None, 24, IMAGE_SIZE, IMAGE_SIZE, 14))\n",
    "length = tf.placeholder(tf.int32, shape = (None, 1))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 14, 14))#, 1))\n",
    "alpha = tf.placeholder(tf.float32, shape = (None))\n",
    "\n",
    "length2 = tf.reshape(length, (-1,))\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training')\n",
    "    \n",
    "FILTER_SIZE = IMAGE_SIZE\n",
    "\n",
    "down_16 = 18\n",
    "down_8 = 72\n",
    "\n",
    "up_8 = 36\n",
    "up_16 = 18\n",
    "\n",
    "def down_block(inp, length, size, flt, scope, train):\n",
    "    with tf.variable_scope(scope):\n",
    "        cell_fw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID')\n",
    "        cell_bw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID')\n",
    "        cell_fw = ZoneoutWrapper(\n",
    "            cell_fw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        cell_bw = ZoneoutWrapper(\n",
    "            cell_bw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        gru = convGRU(inp, cell_fw, cell_bw, length)\n",
    "        print(gru[0].shape)\n",
    "        flattened = tf.reshape(gru[0], (-1, 24, 16*16*flt*2))\n",
    "        attended = attention(flattened, flt*2, time_major=False, return_alphas=False)\n",
    "        flattened = tf.reshape(attended, (-1, 16, 16, flt*2))\n",
    "        flattened = conv_bn_elu(flattened, train, 3, scope, filter_count = flt*2)\n",
    "        down = MaxPool2D(pool_size = (2, 2))(flattened)\n",
    "        print(\"Down block shape: {}\".format(down.shape))\n",
    "    return down, flattened\n",
    "\n",
    "\n",
    "def down_block_no_gru(inp, flt, scope, train):\n",
    "    with tf.variable_scope(scope):\n",
    "        #padded = ReflectionPadding2D((1, 1))(inp)\n",
    "        \n",
    "        # Conv block 1\n",
    "        x = conv_bn_elu(inp, is_training, 3, scope + \"_1\", filter_count = flt)\n",
    "        x = conv_bn_elu(x, is_training, 3, scope + \"_2\", filter_count = flt)\n",
    "        x = csse_block(x, prefix='csse_block_{}'.format(scope))\n",
    "        down = MaxPool2D(pool_size = (2, 2))(x)\n",
    "        print(\"Down block shape: {}\".format(down.shape))\n",
    "    return down\n",
    "\n",
    "\n",
    "def up_block(inp, concat_inp, flt, sq, scope, concat, is_training, uptype, padding = True):\n",
    "    with tf.variable_scope(scope):\n",
    "        x = gau(inp, concat_inp, scope, is_training, flt, uptype, inp.shape[-2])\n",
    "        x = csse_block(x, prefix='csse_block_{}'.format(scope))\n",
    "        print(\"Up block conv 1 shape: {}\".format(x.shape))\n",
    "        return x\n",
    "        \n",
    "        \n",
    "# Down block - 16 - 8\n",
    "down_1, copy_1 = down_block(inp = inp, \n",
    "                            length = length2, \n",
    "                            size = [FILTER_SIZE, FILTER_SIZE], \n",
    "                            flt = down_16, \n",
    "                            scope = 'down_16', \n",
    "                            train = is_training)\n",
    "\n",
    "# Down block - 8 - 4\n",
    "down_2 = down_block_no_gru(down_1, down_8, 'down_8', is_training)\n",
    "\n",
    "# Feature pyramid attention block - 4 - 4\n",
    "down_fpa = fpa(down_2, down_8)\n",
    "\n",
    "# Up block 4 - 8\n",
    "up_3 = up_block(inp = down_fpa,\n",
    "                concat_inp = down_1, \n",
    "                flt = up_8, \n",
    "                sq = up_8,\n",
    "                scope = 'up_8', \n",
    "                concat = True, \n",
    "                is_training = is_training, \n",
    "                uptype = \"transpose\", \n",
    "                padding =  True) # 4 - 8\n",
    "\n",
    "# Up block 8 - 16\n",
    "up_2 = up_block(inp = up_3,\n",
    "                concat_inp = copy_1,\n",
    "                flt = up_16, \n",
    "                sq = up_16, \n",
    "                scope = 'up_16',\n",
    "                concat = True, \n",
    "                is_training = is_training,\n",
    "                uptype = \"transpose\",\n",
    "                padding = True) # 8 - 16\n",
    "\n",
    "# Hypercolumns\n",
    "#up_4_8 = get_deconv2d(down_2, down_8, up_8, \"upfinal1\", is_training)\n",
    "#up_4_8_16 = getdeconv2d(up_4_8, up_8, up_8, 'upfinal1_1', is_training)\n",
    "#print(\"Hypercolumn 1 {}\".format(up_4_16.shape))\n",
    "up_8_16 = get_deconv2d(up_3, up_8, up_16, 'upfinal2', is_training)   \n",
    "print(\"Hypercolumn 2 {}\".format(up_8_16.shape))\n",
    "concat_final = tf.concat([up_2, up_8_16], axis = -1)\n",
    "\n",
    "\n",
    "# Down block 16 - 14\n",
    "up_2 = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'valid')(concat_final)\n",
    "elu = ELU()(up_2)\n",
    "bn = Batch_Normalization(elu, training=is_training, scope =  \"conv32_bn\")\n",
    "\n",
    "\n",
    "# Final conv block, with concatenation of DEM Slope\n",
    "#slope = tf.reshape(inp[:, 0, 1:15, 1:15, -1], (-1, 14, 14, 1))\n",
    "#up_2 = tf.concat([bn, slope], axis = -1)\n",
    "up_2 = conv_bn_elu(bn, is_training, 3, \"final_out\", filter_count = 48)\n",
    "\n",
    "#B = tf.Variable([-np.log(0.99/0.01)]) \n",
    "init = tf.constant_initializer([-np.log(0.98/0.02)]) # For focal loss\n",
    "fm = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1), \n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid',\n",
    "            #bias_initializer = init, # For focal loss\n",
    "            )(up_2)\n",
    "print(fm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(\"This model has {} parameters\".format(total_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/subplot.csv\")\n",
    "df1 = pd.read_csv(\"../data/subplot2.csv\")\n",
    "df2 = pd.read_csv(\"../data/subplot3.csv\")\n",
    "df3 = pd.read_csv(\"../data/subplot4.csv\")\n",
    "\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df1 = df1.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df2 = df2.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df3 = df3.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "\n",
    "lens = [len(x) for x in [df, df1, df2, df3]]\n",
    "\n",
    "df = pd.concat([df, df1, df2, df3], ignore_index = True)\n",
    "df = df.dropna(axis = 0)\n",
    "\n",
    "existing = [int(x[:-4]) for x in os.listdir('../data/correct_dem/') if \".DS\" not in x]\n",
    "N_SAMPLES = len(existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['PLOT_ID'].isin(existing)]\n",
    "N_SAMPLES = int(df.shape[0]/196)\n",
    "N_YEARS = 1\n",
    "\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "weight = np.load(\"weights.npy\")\n",
    "weight = dict(zip(plot_ids, weight))\n",
    "\n",
    "def reconstruct_images(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    rows = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        rows.append(list(subs_lat['TREE']))\n",
    "    return rows\n",
    "\n",
    "data = [reconstruct_images(x) for x in plot_ids]\n",
    "\n",
    "# Initiate empty lists to store the X and Y data in\n",
    "data_x, data_y, lengths = [], [], []\n",
    "\n",
    "# Iterate over each plot\n",
    "for i in tnrange(len(plot_ids)):\n",
    "    # Load the sentinel imagery\n",
    "    for year in [\"correct_dem\"]:  \n",
    "        x = np.load(\"../data/\" + year + \"/\" + str(plot_ids[i]) + \".npy\")\n",
    "        x = ndvi(x, image_size = 16)\n",
    "        x = evi(x, image_size = 16)\n",
    "        x = savi(x, image_size = 16)\n",
    "        x = remove_blank_steps(x)\n",
    "        y = reconstruct_images(plot_ids[i])\n",
    "        x[:, :, :, 10] /= 90\n",
    "        lengths.append(x.shape[0])\n",
    "        if x.shape[0] < 24:\n",
    "            padding = np.zeros((24 - x.shape[0], IMAGE_SIZE, IMAGE_SIZE, 14))\n",
    "            x = np.concatenate((x, padding), axis = 0)\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "print(\"Finished data loading\")\n",
    "\n",
    "data_x = np.stack(data_x)\n",
    "data_y = np.stack(data_y)\n",
    "lengths = np.stack(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(data_x[:, :, :, :, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "below_1 = [i for i, val in enumerate(data_x) if np.min(val) < -1.05]\n",
    "above_1 = [i for i, val in enumerate(data_x) if np.max(val) > 1.05]\n",
    "min_vals = [np.min(val) for i, val in enumerate(data_x) if np.min(val) < -1.05]\n",
    "max_vals = [np.max(val) for i, val in enumerate(data_x) if np.max(val) > 1.05]\n",
    "outliers = below_1 + above_1\n",
    "print(\"The outliers are: {}, totalling {}\".format(outliers, len(outliers)))\n",
    "print(\"\\n\")\n",
    "print(min_vals, max_vals)\n",
    "data_x = data_x[[x for x in range(0, len(data_x)) if x not in outliers]]\n",
    "data_y = data_y[[x for x in range(0, len(data_y)) if x not in outliers]]\n",
    "lengths = lengths[[x for x in range(0, len(lengths)) if x not in outliers]]\n",
    "\n",
    "min_all = []\n",
    "max_all = []\n",
    "for x in range(0, data_x.shape[-1]):\n",
    "    mins, maxs = (np.min(data_x[:, :, :, :, x]), np.max(data_x[:, :, :, :, x]))\n",
    "    min_all.append(mins)\n",
    "    max_all.append(maxs)\n",
    "    \n",
    "    data_x[:, :, :, :, x] = (data_x[:, :, :, :, x] - mins) / (maxs - mins)\n",
    "    \n",
    "print(\"The data has been scaled to [{}, {}]\".format(np.min(data_x), np.max(data_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ids2 = [val for x, val in enumerate(plot_ids) if x not in list(set([x for x in outliers]))]\n",
    "#plot_ids2[ordering[460]//4] \n",
    "N_SAMPLES = len(data_x)\n",
    "\n",
    "region_lengths = []\n",
    "for x in [df1, df2, df3]:\n",
    "    subs = [i for i in set(x['PLOT_ID']) if i in plot_ids2]\n",
    "    region_lengths.append(len(subs))\n",
    "    \n",
    "region_lengths = [N_SAMPLES - sum(region_lengths)] + region_lengths\n",
    "    #print(len(x[x['PLOT_ID'] in plot_ids2]))\n",
    "\n",
    "print(\"The region sample distribution is {}\".format(region_lengths))\n",
    "print(sum(region_lengths))\n",
    "train_ordering = []\n",
    "test_ordering = []\n",
    "ordering = []\n",
    "total_samples = 0\n",
    "for r in TRAIN_RATIO, TEST_RATIO:\n",
    "    for i, val in enumerate(region_lengths):\n",
    "        start = int(np.sum(region_lengths[:i]))\n",
    "        end = start + val\n",
    "        if r == 0.8:\n",
    "            start = start\n",
    "            end = end-((end-start)*(1-r))\n",
    "            start = int(start)\n",
    "            end = int(end)\n",
    "            total_samples += (end - start)\n",
    "            train_ordering += [x for x in range(start, end)]\n",
    "        if r == 0.2:\n",
    "            start = start + ((end-start)*(1-r))\n",
    "            end = end\n",
    "            start = int(start)\n",
    "            end = int(end)\n",
    "            total_samples += (end-start)\n",
    "            test_ordering += [x for x in range(start, end)]\n",
    "\n",
    "ordering = train_ordering + test_ordering\n",
    "\n",
    "data_x = data_x[ordering]\n",
    "data_y = data_y[ordering]\n",
    "lengths = lengths[ordering]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "NONZERO_CLUSTERS = 10\n",
    "ZERO_CLUSTERS = 6\n",
    "\n",
    "kmeans = KMeans(n_clusters=NONZERO_CLUSTERS, random_state = 50)\n",
    "kmeans_zero = KMeans(n_clusters = ZERO_CLUSTERS, random_state = 50)\n",
    "unaugmented = [x for x in range(0, len(data_y))]\n",
    "zeros = [x for x in unaugmented if np.sum(data_y[x]) == 0]\n",
    "nonzero = [x for x in unaugmented if x not in zeros]\n",
    "kmeans.fit(data_y[nonzero, :, :].reshape((len(nonzero), 14*14)))\n",
    "kmeans_zero.fit(np.mean(data_x[zeros, :, :], axis = 1).reshape((len(zeros), 16*16*14)))             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplot(matrices):\n",
    "    '''Plot multiple heatmaps with subplots'''\n",
    "    fig, axs = plt.subplots(ncols=4)\n",
    "    fig.set_size_inches(20, 4)\n",
    "    for i, matrix in enumerate(matrices):\n",
    "        sns.heatmap(data = matrix, ax = axs[i], vmin = 0, vmax = 0.9)\n",
    "        axs[i].set_xlabel(\"\")\n",
    "        axs[i].set_ylabel(\"\")\n",
    "        axs[i].set_yticks([])\n",
    "        axs[i].set_xticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "samples_x = [val for x, val in enumerate(nonzero) if kmeans.labels_[x] == 1]\n",
    "print(samples_x)\n",
    "randoms = random.sample(samples_x, 4)\n",
    "randoms = [data_y[x] for x in randoms]\n",
    "randoms = [x.reshape((14, 14)) for x in randoms]\n",
    "multiplot(randoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(data_y, labels, labels2, unaugmented = unaugmented):\n",
    "    ids = {}\n",
    "    # Loop over the nonzero clusters\n",
    "    for i in range(0, NONZERO_CLUSTERS):\n",
    "        tmp = [val for x, val in enumerate(nonzero) if labels[x] == i]\n",
    "        ids[i] = tmp\n",
    "    # Loop over the zero clusters\n",
    "    for i in range(0, ZERO_CLUSTERS):\n",
    "        tmp = [val for x, val in enumerate(zeros) if labels2[x] == i]\n",
    "        ids[i + 10] = tmp\n",
    "    #ids[10] = zeros\n",
    "    return ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = balance_data(data_y, kmeans.labels_, kmeans_zero.labels_ )\n",
    "items = [v for k, v in ids.items()]\n",
    "items = [item for sublist in items for item in sublist]\n",
    "print(\"The {} samples have been balanced between the sampling sites\".format(len(items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = []\n",
    "test_ids = []\n",
    "for i in ids:\n",
    "    ln = len(ids[i])\n",
    "    train_len = int(np.floor([ln * TRAIN_RATIO]))\n",
    "    test_len = ln - train_len\n",
    "    print(train_len, test_len, ln)\n",
    "    trains = ids[i][:train_len]\n",
    "    tests = ids[i][train_len:]\n",
    "    train_ids += trains\n",
    "    test_ids += tests\n",
    "    \n",
    "train_labels = []\n",
    "for i in train_ids:\n",
    "    train_labels.append([k for k, v in ids.items() if i in v][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train and test characteristics:\")\n",
    "print(\"Train mean Y {}\".format(np.mean([np.sum(x) for x in data_y[train_ids]])))\n",
    "print(\"Test mean Y {}\".format(np.mean([np.sum(x) for x in data_y[test_ids]])))\n",
    "print(\"Train number with zero trees {}\".format(0.2*len([x for x in data_y[train_ids] if np.sum(x) == 0])))\n",
    "print(\"Test number with zero trees {}\".format(0.8*len([x for x in data_y[test_ids] if np.sum(x) == 0])))\n",
    "print(\"Train mean NDVI\")\n",
    "print(\"Test mean NDVI\")\n",
    "print(\"There are {} train and {} test samples\".format(len(train_ids), len(test_ids)))\n",
    "print(\"There is {} overlap between train and test\".format(len([x for x in train_ids if x in test_ids])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_augmented = []\n",
    "data_y_augmented = []\n",
    "lengths_augmented = []\n",
    "labels_augmented = []\n",
    "for i, val in enumerate(train_ids):\n",
    "    data_x_augmented.append(data_x[val])\n",
    "    data_y_augmented.append(data_y[val])\n",
    "    lengths_augmented.append(data_x[val].shape[0])\n",
    "    labels_augmented.append(train_labels[i])\n",
    "    \n",
    "    x1 = np.flip(data_x[val], 1)\n",
    "    y1 = np.flip(data_y[val], 0)\n",
    "    lengths_augmented.append(x1.shape[0])\n",
    "    labels_augmented.append(train_labels[i])\n",
    "    data_x_augmented.append(x1)\n",
    "    data_y_augmented.append(y1)\n",
    "    \n",
    "    x1 = np.flip(data_x[val], [2, 1])\n",
    "    y1 = np.flip(data_y[val], [1, 0])\n",
    "    lengths_augmented.append(x1.shape[0])\n",
    "    labels_augmented.append(train_labels[i])\n",
    "    data_x_augmented.append(x1)\n",
    "    data_y_augmented.append(y1)\n",
    "    \n",
    "    x1 = np.flip(data_x[val], 2)\n",
    "    y1 = np.flip(data_y[val], 1)\n",
    "    lengths_augmented.append(x1.shape[0])\n",
    "    labels_augmented.append(train_labels[i])\n",
    "    data_x_augmented.append(x1)\n",
    "    data_y_augmented.append(y1)\n",
    "\n",
    "train_x = np.stack(data_x_augmented)\n",
    "train_y = np.stack(data_y_augmented)\n",
    "train_y = np.reshape(train_y, (train_y.shape[0], 14, 14, 1))\n",
    "train_l = np.stack(lengths_augmented)\n",
    "train_l = np.reshape(train_l, (train_y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = data_x[test_ids]\n",
    "test_y = data_y[test_ids]\n",
    "test_lengths = lengths[test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RANDOM TRAIN SAMPLES - SHOULD BE AUGMENTED\")\n",
    "multiplot([x.reshape(14, 14) for x in train_y[:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RANDOM TEST SAMPLES - SHOULD BE NOT AUGMENTED\")\n",
    "multiplot([x.reshape(14, 14) for x in test_y[:4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def smooth_jaccard(y_true, y_pred, smooth=1):\n",
    "    y_true = tf.reshape(y_true, (-1, 14*14))\n",
    "    y_pred = tf.reshape(y_pred, (-1, 14*14))\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def focal_loss_fixed(y_true, y_pred, gamma = 2., alpha = 0.25):\n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    y_pred = K.clip(y_pred, 1e-8, 1-1e-8)\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "    epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "    pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "    pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "    loss = -(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1)) - ((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "def bce_lovasz(y_true, y_pred):\n",
    "    foc_losses = []\n",
    "    y_true_r = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    '''\n",
    "    for i in range(BATCH_SIZE):\n",
    "        y_true_i = tf.reshape(y_true_r[i, :, :, :], (1, 14, 14, 1))\n",
    "        y_pred_i = tf.reshape(y_pred[i, :, :, :], (1, 14, 14, 1))\n",
    "        alpha = (1 - K.clip((tf.reduce_sum(y_true_i)/196), 0.33, 0.67))\n",
    "        gamma = (-tf.math.log(alpha))/2\n",
    "        focal_loss = focal_loss_fixed(y_true_i, y_pred_i, gamma = gamma, alpha = alpha)\n",
    "        foc_losses.append(focal_loss)\n",
    "    foc_losses = tf.concat(foc_losses, axis = 0)'''\n",
    "    loss1 = binary_crossentropy(y_true, y_pred)\n",
    "    lv = lovasz_softmax(y_pred, tf.reshape(y_true, (-1, 14, 14)), classes=[1], per_image=True)\n",
    "    loss = loss1 + 0.25*lv\n",
    "    #lovasz =  0.5*lovasz_softmax(tf.reshape(y_pred, (-1, 14, 14)), y_true, classes=[1], per_image=True) +\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equibatch creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 100, 8):\n",
    "    print(np.percentile([np.sum(x) for x in train_y], i))\n",
    "    \n",
    "train_ids = [x for x in range(0, len(train_y))]\n",
    "\n",
    "def equibatch(train_ids, batch_size):\n",
    "    np.random.shuffle(train_ids)\n",
    "    randomize = train_ids\n",
    "    percs = np.sum(train_y[randomize, :, :, :].reshape(len(train_ids), 14*14), axis = 1)\n",
    "    idx = randomize\n",
    "    zero_ids = [x for x, z in zip(idx, percs) if z == 0]\n",
    "    one_ids = [x for x, z in zip(idx, percs) if 0 < z <= 6]\n",
    "    two_ids = [x for x, z in zip(idx, percs) if 6 < z <= 11]\n",
    "    three_ids = [x for x, z in zip(idx, percs) if 11 < z <= 16]\n",
    "    four_ids = [x for x, z in zip(idx, percs) if 16 < z <= 23]\n",
    "    five_ids = [x for x, z in zip(idx, percs) if 23 < z < 41]\n",
    "    six_ids = [x for x, z in zip(idx, percs) if 41 < z <= 63]\n",
    "    seven_ids = [x for x, z in zip(idx, percs) if 63 < z <= 100]\n",
    "    eight_ids = [x for x, z in zip(idx, percs) if 100 < z]\n",
    "    \n",
    "    new_batches = []\n",
    "    maxes = [len(zero_ids), len(one_ids), len(two_ids), len(three_ids), len(four_ids),\n",
    "             len(five_ids), len(six_ids), len(seven_ids), len(eight_ids)]\n",
    "    cur_ids = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    for i in range(0, len(train_ids)//len(maxes)):\n",
    "        to_append = [ zero_ids[cur_ids[0]], zero_ids[cur_ids[0] + 1], one_ids[cur_ids[1]], two_ids[cur_ids[2]],\n",
    "                           three_ids[cur_ids[3]], four_ids[cur_ids[4]], five_ids[cur_ids[5]],\n",
    "                           six_ids[cur_ids[6]], seven_ids[cur_ids[7]], eight_ids[cur_ids[8]]]\n",
    "        np.random.shuffle(to_append)\n",
    "        new_batches.append(to_append)\n",
    "        cur_ids = [x + 1 for x in cur_ids]\n",
    "        for i, val in enumerate(cur_ids):\n",
    "            if val > maxes[i] - 1:\n",
    "                cur_ids[i] = 0\n",
    "        cur_ids[0] += 1\n",
    "        \n",
    "    new_batches = [item for sublist in new_batches for item in sublist]\n",
    "    #overlap = [x for x in new_batches if x in test_ids]\n",
    "    #print(\"There is {} overlap. Error if > 0\".format(len(overlap)))\n",
    "    return new_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = equibatch(train_ids, 32)\n",
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[:4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[4:8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[8:12]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRESH_START = True\n",
    "FINE_TUNE = False\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "print(\"Starting model with: \\n {} zone out \\n {} l2 \\n {} initial LR \\n {} final LR \\n {} parameters\"\n",
    "     .format(ZONE_OUT_PROB, L2_REG, INITIAL_LR, FINAL_LR, total_parameters))\n",
    "best_val = 0.270\n",
    "if not FRESH_START:\n",
    "    print(\"Resuming training with a best validation score of {}\".format(best_val))\n",
    "if FRESH_START:\n",
    "    print(\"Restarting training from scratch on {} train and {} test samples, total {}\".format(len(train_ids), len(test_ids), N_SAMPLES))\n",
    "    #optimizer = AdaBoundOptimizer(learning_rate=1e-6,\n",
    "    #                              final_lr=1e-4,\n",
    "    #                              beta1=0.9, beta2=0.999, \n",
    "    #                              amsbound=True)\n",
    "    learning_rate = 1e-6\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    \n",
    "    train_loss = bce_lovasz(tf.reshape(labels, (-1, 14, 14, 1)), fm)\n",
    "    test_loss = binary_crossentropy(tf.reshape(labels, (-1, 14, 14, 1)), fm)\n",
    "    l2_loss = tf.losses.get_regularization_loss()\n",
    "    train_loss += l2_loss\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(train_loss)   \n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver(max_to_keep = 2)\n",
    "\n",
    "# Run training loop\n",
    "for i in range(0, 200):\n",
    "    if i % 25 == 0:\n",
    "        learning_rate /= 2\n",
    "        print(\"Decaying learning rate to {}\".format(learning_rate))\n",
    "    print(\"Lovasz loss\")\n",
    "    op = train_op\n",
    "    loss = train_loss\n",
    "    randomize = equibatch(train_ids, 32)\n",
    "    test_ids = [x for x in range(0, len(test_x))]\n",
    "\n",
    "    losses = []\n",
    "    val_loss = []\n",
    "    \n",
    "    for k in tnrange(int(len(train_ids) // BATCH_SIZE)):\n",
    "        batch_ids = randomize[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "        batch_y = train_y[batch_ids, :, :].reshape(len(batch_ids), 14, 14)\n",
    "        if sum(sum(sum(batch_y))) > 0:\n",
    "            opt, tr = sess.run([op, loss],\n",
    "                                  feed_dict={inp: train_x[batch_ids, :, :, :],\n",
    "                                             length: train_l[batch_ids].reshape((-1, 1)),\n",
    "                                             labels: batch_y,\n",
    "                                             is_training: True,\n",
    "                                             learning_rate = learning_rate\n",
    "                                             })\n",
    " \n",
    "        else:\n",
    "            print(\"Skipping minibatch for equibatch reasons\")\n",
    "        losses.append(tr)\n",
    "    for j in range(len(test_ids) // 8):\n",
    "        batch_ids = test_ids[j*8:(j+1)*8]\n",
    "        vl, y = sess.run([test_loss, fm], \n",
    "                         feed_dict={inp: test_x[batch_ids, :, :, :],\n",
    "                                    length: test_lengths[batch_ids].reshape((-1, 1)),\n",
    "                                    labels: test_y[batch_ids, :, :].reshape(8, 14, 14),\n",
    "                                    is_training: False,\n",
    "                                    })\n",
    "        val_loss.append(np.mean(vl))\n",
    "        \n",
    "    tps, fps, fns = [], [], []\n",
    "    for m in test_ids:\n",
    "        y = sess.run([fm], feed_dict={inp: test_x[m, :, :, :].reshape(1, 24, 16, 16, 14),\n",
    "                                  length: test_lengths[m].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })[0]\n",
    "        true = test_y[m].reshape((LABEL_SIZE, LABEL_SIZE))\n",
    "        pred = y.reshape((14, 14))\n",
    "        pred[np.where(pred > 0.4)] = 1\n",
    "        pred[np.where(pred < 0.4)] = 0\n",
    "        tp, fp, fn = thirty_meter(true, pred)\n",
    "        tps.append(tp)\n",
    "        fps.append(fp)\n",
    "        fns.append(fn)\n",
    "        \n",
    "    precision = np.sum(tps) / (np.sum(tps) + np.sum(fps))\n",
    "    recall = np.sum(tps) / (np.sum(tps) + np.sum(fns))\n",
    "    if np.mean(val_loss) < best_val:\n",
    "        best_val = np.mean(val_loss)\n",
    "        print(\"Saving model with {}\".format(best_val))\n",
    "        save_path = saver.save(sess, \"../models/equibatch26/model\")\n",
    "    print(\"Epoch {}: Loss {} Val: {} P {} R {} F1 {} iou {}\".format(i + 1,\n",
    "                                                             np.mean(losses), np.mean(val_loss),\n",
    "                                                             precision, recall, 'hey', \"hey\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation and sanity checks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "test_ids = [x for x in range(0, len(test_x))]\n",
    "\n",
    "def multiplot(matrices, nrows = 2, ncols = 4):\n",
    "    '''Plot multiple heatmaps with subplots'''\n",
    "    fig, axs = plt.subplots(ncols=4, nrows = nrows)\n",
    "    fig.set_size_inches(20, 4*nrows)\n",
    "    to_iter = [[x for x in range(i, i + ncols + 1)] for i in range(0, nrows*ncols, ncols)]\n",
    "    for r in range(1, nrows + 1):\n",
    "        min_i = min(to_iter[r-1])\n",
    "        max_i = max(to_iter[r-1])\n",
    "        for i, matrix in enumerate(matrices[min_i:max_i]):\n",
    "            sns.heatmap(data = matrix, ax = axs[r - 1, i], vmin = 0, vmax = 0.9)\n",
    "            axs[r - 1, i].set_xlabel(\"\")\n",
    "            axs[r - 1, i].set_ylabel(\"\")\n",
    "            axs[r - 1, i].set_yticks([])\n",
    "            axs[r - 1, i].set_xticks([])\n",
    "    plt.show()\n",
    "start = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "    \n",
    "test_losses = []\n",
    "print(start/len(test_ids))\n",
    "test_ids = sorted(test_ids)\n",
    "matrix_ids = [test_ids[start], test_ids[start + 1], test_ids[start + 2], test_ids[start + 3],\n",
    "              test_ids[start + 4], test_ids[start + 5], test_ids[start + 6], test_ids[start + 7]]\n",
    "#matrix_ids = random.sample(test_ids, 4)\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    print(i)\n",
    "    y = sess.run([fm], feed_dict={inp: test_x[idx].reshape(1, 24, IMAGE_SIZE, IMAGE_SIZE, 14),\n",
    "                                  length: test_lengths[idx].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })\n",
    "    y = np.array(y).reshape(14, 14)\n",
    "    preds.append(y)\n",
    "    true = test_y[idx].reshape(LABEL_SIZE, LABEL_SIZE)\n",
    "    trues.append(true)\n",
    "    \n",
    "\n",
    "to_plot = trues[0:4] + preds[0:4]# + trues[5:] + preds[5:]\n",
    "multiplot(to_plot, nrows = 2, ncols = 4)\n",
    "#plot_ids[ordering[976]//4] \n",
    "start = start + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ids2 = [val for x, val in enumerate(plot_ids) if x not in list(set([x // 4 for x in outliers]))]\n",
    "plot_ids2[ordering[460]//4] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate ROC for best threshold selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for j in range(4, 18):\n",
    "    tps = []\n",
    "    fps = []\n",
    "    fns = []\n",
    "    for i in test_ids:\n",
    "        y = sess.run([fm], feed_dict={inp: data_x[i].reshape(1, 24, 16, 16, 14),\n",
    "                                  length: lengths[i].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })[0]\n",
    "        true = data_y[i].reshape((14, 14))\n",
    "        pred = y.reshape((14, 14))\n",
    "        pred[np.where(pred > j*0.05)] = 1\n",
    "        pred[np.where(pred < j*0.05)] = 0\n",
    "        tp, fp, fn = thirty_meter(true, pred)\n",
    "        tps.append(tp)\n",
    "        fps.append(fp)\n",
    "        fns.append(fn)\n",
    "        \n",
    "    precision = np.sum(tps) / (np.sum(tps) + np.sum(fps))\n",
    "    recall = np.sum(tps) / (np.sum(tps) + np.sum(fns))\n",
    "    print(j*0.05, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO @jombrandt top 10 worst training, test samples by IOU \n",
    "\n",
    "These should be written to a tmp/ .txt file and indexed by validate-data.ipynb to ensure that original classifications were correct, and to identify regions that need more training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
