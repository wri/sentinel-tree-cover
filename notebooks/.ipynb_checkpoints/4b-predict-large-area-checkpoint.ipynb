{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This Jupyter notebook predicts large-area tiles downloaded in `4a-download-large-area` with a trained model from `3-model-master`. The notebook is broken down into the following sections:\n",
    "\n",
    "   * **Model loading**:\n",
    "   * **Coordinate identification**\n",
    "   * **Tiling**\n",
    "   * **Loading and predicting**\n",
    "   * **Mosaicing**\n",
    "   * **Writing TIF**\n",
    "   * **Writing COG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "from osgeo import ogr, osr\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize\n",
    "import hickle as hkl\n",
    "from time import sleep\n",
    "\n",
    "%run ../src/downloading/utils.py\n",
    "%run ../src/models/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Parameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANDSCAPE = 'cameroon-mogazang'\n",
    "YEAR = 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_csv(\"../project-monitoring/database.csv\")\n",
    "coords = database[database['landscape'] == LANDSCAPE]\n",
    "path = coords['path'].tolist()[0]\n",
    "coords = (float(coords['longitude']), float(coords['latitude']))\n",
    "\n",
    "IO_PARAMS = {'prefix': '../',\n",
    "             'bucket': 'restoration-monitoring',\n",
    "             'coords': coords,\n",
    "             'bucket-prefix': '',\n",
    "             'path': path}\n",
    "\n",
    "OUTPUT = IO_PARAMS['prefix'] + IO_PARAMS['path'] + str(YEAR) + '/output/'\n",
    "TIF_OUTPUT = IO_PARAMS['prefix'] + IO_PARAMS['path'] + str(YEAR) + \".tif\"\n",
    "INPUT = IO_PARAMS['prefix'] + IO_PARAMS['path'] + str(YEAR) + '/processed/'\n",
    "\n",
    "if not os.path.exists(OUTPUT):\n",
    "    os.makedirs(OUTPUT)\n",
    "    \n",
    "print(coords, OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../models/master-2021-11750/'\n",
    "new_saver = tf.train.import_meta_graph(path + 'model.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    try:\n",
    "        logits = tf.get_default_graph().get_tensor_by_name(\"conv2d_{}/Sigmoid:0\".format(i))\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "inp = tf.get_default_graph().get_tensor_by_name(\"Placeholder:0\")\n",
    "length = tf.get_default_graph().get_tensor_by_name(\"Placeholder_1:0\")\n",
    "\n",
    "\n",
    "#inp_median = tf.get_default_graph().get_tensor_by_name(\"Placeholder_4:0\")\n",
    "rmax = tf.get_default_graph().get_tensor_by_name(\"Placeholder_4:0\")\n",
    "rmin = tf.get_default_graph().get_tensor_by_name(\"Placeholder_5:0\")\n",
    "dmax = tf.get_default_graph().get_tensor_by_name(\"Placeholder_6:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tiling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fspecial_gauss(size, sigma):\n",
    "\n",
    "    \"\"\"Function to mimic the 'fspecial' gaussian MATLAB function\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
    "    g = np.exp(-((x**2 + y**2)/(2.0*sigma**2)))\n",
    "    return g\n",
    "\n",
    "arr = fspecial_gauss(14, 2)\n",
    "arr = arr[:7, :7]\n",
    "\n",
    "SIZE = 10\n",
    "SIZE_N = SIZE*SIZE\n",
    "SIZE_UR = (SIZE - 1) * (SIZE - 1)\n",
    "SIZE_R = (SIZE - 1) * SIZE\n",
    "SIZE_U = SIZE_R\n",
    "TOTAL = SIZE_N + SIZE_UR + SIZE_R + SIZE_U\n",
    "print(SIZE_N, SIZE_UR, SIZE_R, SIZE_U, TOTAL)\n",
    "\n",
    "arr = np.concatenate([arr, np.flip(arr, 0)], 0)\n",
    "base_filter = np.concatenate([arr, np.flip(arr, 1)], 1)\n",
    "normal = np.tile(base_filter, (SIZE, SIZE))\n",
    "normal[:, 0:7] = 1.\n",
    "normal[:, -7:] = 1.\n",
    "normal[0:7, :] = 1.\n",
    "normal[-7:, :] = 1.\n",
    "upright = np.tile(base_filter, (SIZE - 1, SIZE - 1))\n",
    "upright = np.pad(upright, (7, 7), 'constant', constant_values = 0)\n",
    "right_filter = np.tile(base_filter, (SIZE, SIZE - 1))\n",
    "right_filter = np.pad(right_filter, ((0, 0), (7, 7)), 'constant', constant_values = 0)\n",
    "up_filter = np.tile(base_filter, (SIZE - 1, SIZE))\n",
    "up_filter = np.pad(up_filter, ((7, 7), (0, 0)), 'constant', constant_values = 0)\n",
    "\n",
    "sums = (up_filter + right_filter + upright + normal)\n",
    "up_filter /= sums\n",
    "right_filter /= sums\n",
    "upright /= sums\n",
    "normal /= sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7300 samples\n",
    "min_all = [0.013473716334782941, 0.026749065384908823, 0.015594720378423743, 0.04151979858091097, 0.041794460975051496, 0.04419012741283284, 0.046387426565957124, 0.03997863736934462, 0.029922941939421684, 0.022613870450904097, 0.0, -0.03150988021667811, -0.4017853055619135, -0.020706492713817082, -0.17936980239566647, 0.49647595452889137, 0.036705339246950075]\n",
    "max_all = [0.2135957885099565, 0.31784542610818645, 0.4665445944914931, 0.5549248493171588, 0.5132982375829709, 0.5258716716258488, 0.5463492790112153, 0.557778286411841, 0.7029221026932173, 0.6533150225070573, 0.420202944991226, 0.7505455100328069, 0.3344625009536888, 0.6910658426794842, 0.8989242389562828, 0.9640911367847687, 0.8307561131659074]\n",
    "\n",
    "# 10250 samples\n",
    "min_all = [0.012680247196154727, 0.025696192874036773, 0.015335317006179903, 0.045014114595254444, 0.040924696726939803, 0.047470817120622566, 0.049927519645990695, 0.04351873044937819, 0.027344167238879988, 0.019943541618982222, 0.0, -0.03862058442053862, -0.4059052414740215, -0.02613870450904099, -0.18538185702296484, 0.4994787833207808, 0.036705339246950075]\n",
    "max_all = [0.2144045166704814, 0.3134355687800412, 0.4580300602731365, 0.5453721675441149, 0.504097047379263, 0.5171740291447319, 0.5381551842526894, 0.5497062638284886, 0.6925154497596704, 0.6404211490043488, 0.42020294499122607, 0.7460593575951782, 0.3305867093919279, 0.6874341954680705, 0.898161287861448, 0.9669493083796132, 0.8337636829655277]\n",
    "\n",
    "# GRNDVI\n",
    "\n",
    "min_all = [0.012726024261844816, 0.025741969939726862, 0.015365835049973294, 0.04444953078507668, 0.04090943770504311, 0.04702830548561837, 0.04936293583581292, 0.042969405661097124, 0.02673380636301213, 0.019562066071564813, 0.0, -0.039597161821927185, -0.4055085069047074, -0.02751201647974366, -0.5080387335466494, 0.4994787833207808, 0.036705339246950075] \n",
    "max_all = [0.21391622796978713, 0.3129167620355535, 0.4574807354848554, 0.5446860456244754, 0.5035172045471885, 0.5166094453345541, 0.537621118486305, 0.5491264209964142, 0.6919050888838025, 0.6396734569314106, 0.42020294499122607, 0.7455100328068971, 0.3302204928664072, 0.6869459067673762, 0.7157240565415754, 0.9673748605671116, 0.8341312927789295]\n",
    "\n",
    "# 11750\n",
    "min_all = [0.012558175020981156, 0.025696192874036773, 0.01539635309376669, 0.04478522926680399, 0.04068055237659266, 0.04710460059510185, 0.049881742580300606, 0.04361028458075837, 0.02818341344319829, 0.02026398107881285, 0.0, 0.5419731947249189, 0.2969113383797463, -0.03367114973145012, -0.40432315843766126, -0.02335820986187474, -0.4988996431782181] \n",
    "max_all =  [0.2126802471961547, 0.3093308918898299, 0.45065995269703213, 0.5368581673914702, 0.4968947890440223, 0.5098954757000076, 0.5315938048371099, 0.5438315403982604, 0.6847180895704585, 0.6326390478370336, 0.41567101548790725, 0.948365434839523, 0.8137210160767949, 0.7460347679428321, 0.3274571766911071, 0.6886312522154149, 0.7143557051870734]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_csv(\"qaqcecoregion.csv\")\n",
    "database.head(5)\n",
    "landscape = 134\n",
    "\n",
    "coords = database[database['id'] == landscape]\n",
    "coords = (float(coords['X']), float(coords['Y']))\n",
    "\n",
    "OUTPUT = '../project-monitoring/qa-qc-ecoregion/'+ str(landscape) + '/2020/output/'\n",
    "INPUT = '../project-monitoring/qa-qc-ecoregion/'+ str(landscape) + '/2020/processed/'\n",
    "TIF_OUTPUT = '../project-monitoring/qa-qc-ecoregion/'+ str(landscape) + f'/2020/out_{str(landscape)}.tif'\n",
    "\n",
    "if not os.path.exists(OUTPUT):\n",
    "    os.makedirs(OUTPUT)\n",
    "    \n",
    "print(coords, OUTPUT)\n",
    "database.to_csv(\"qaqc.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_db(x, min_db):\n",
    "    x = 10 * np.log10(x + 1/65535)\n",
    "    x[x < -min_db] = -min_db\n",
    "    x = x + min_db\n",
    "    x = x / min_db\n",
    "    x = np.clip(x, 0, 1)\n",
    "    return x\n",
    "\n",
    "def grndvi(array):\n",
    "    nir = np.clip(array[..., 3], 0, 1)\n",
    "    green = np.clip(array[..., 1], 0, 1)\n",
    "    red = np.clip(array[..., 2], 0, 1)\n",
    "    denominator = (nir+(green+red)) + 1e-5\n",
    "    return (nir-(green+red)) / denominator\n",
    "\n",
    "def evi(x: np.ndarray, verbose: bool = False) -> np.ndarray:\n",
    "    '''\n",
    "    Calculates the enhanced vegetation index\n",
    "    2.5 x (08 - 04) / (08 + 6 * 04 - 7.5 * 02 + 1)\n",
    "    '''\n",
    "\n",
    "    BLUE = x[..., 0]\n",
    "    GREEN = x[..., 1]\n",
    "    RED = x[..., 2]\n",
    "    NIR = x[..., 3]\n",
    "    evis = 2.5 * ( (NIR-RED) / (NIR + (6*RED) - (7.5*BLUE) + 1))\n",
    "    evis = np.clip(evis, -1.5, 1.5)\n",
    "    return evis\n",
    "\n",
    "def msavi2(x: np.ndarray, verbose: bool = False) -> np.ndarray:\n",
    "    '''\n",
    "    Calculates the modified soil-adjusted vegetation index 2\n",
    "    (2 * NIR + 1 - sqrt((2*NIR + 1)^2 - 8*(NIR-RED)) / 2\n",
    "    '''\n",
    "    BLUE = x[..., 0]\n",
    "    GREEN = x[..., 1]\n",
    "    RED = np.clip(x[..., 2], 0, 1)\n",
    "    NIR = np.clip(x[..., 3], 0, 1)\n",
    "\n",
    "    msavis = (2 * NIR + 1 - np.sqrt( (2*NIR+1)**2 - 8*(NIR-RED) )) / 2\n",
    "    return msavis\n",
    "\n",
    "def bi(x: np.ndarray, verbose: bool = False) -> np.ndarray:\n",
    "    B11 = np.clip(x[..., 8], 0, 1)\n",
    "    B4 = np.clip(x[..., 2], 0, 1)\n",
    "    B8 = np.clip(x[..., 3], 0, 1)\n",
    "    B2 = np.clip(x[..., 0], 0, 1)\n",
    "    bis = ((B11 + B4) - (B8 + B2)) / ((B11 + B4) + (B8 + B2))\n",
    "    return bis\n",
    "\n",
    "\n",
    "def load_and_predict_folder(y_col, folder, overlap_filter = upright,\n",
    "                            normal_filter = normal, histogram_match = False):\n",
    "    \"\"\"Insert documentation here\n",
    "    \"\"\"\n",
    "    pred_files = INPUT + str(y_col) + \"/\" + str(folder) + \".hkl\"    \n",
    "    clipping_params = {\n",
    "        'rmax': rmax,\n",
    "        'rmin': rmin,\n",
    "        'dmax': dmax\n",
    "    }\n",
    "    \n",
    "    pred_x = []\n",
    "    x = hkl.load(pred_files)\n",
    "    if not isinstance(x.flat[0], np.floating):\n",
    "        assert np.max(x) > 1\n",
    "        x = x / 65535.\n",
    "        \n",
    "    x[..., -1] = convert_to_db(x[..., -1], 50)\n",
    "    x[..., -2] = convert_to_db(x[..., -2], 50)\n",
    "\n",
    "    indices = np.empty((12, 142, 142, 4))\n",
    "    indices[..., 0] = evi(x)\n",
    "    indices[..., 1] = bi(x)\n",
    "    indices[..., 2] = msavi2(x)\n",
    "    indices[..., 3] = grndvi(x)\n",
    "    \n",
    "    #s1 = x[..., -2:]\n",
    "    #x = x[..., :-2]\n",
    "    \n",
    "    print(x.shape)\n",
    "    x = np.concatenate([x, indices], axis = -1)\n",
    "    #x = np.concatenate([x, s1], axis = -1)\n",
    "    \n",
    "    med = np.median(x, axis = 0)\n",
    "    med = med[np.newaxis, :, :, :]\n",
    "    print(med.shape)\n",
    "    print(x.shape)\n",
    "    x = np.concatenate([x, med], axis = 0)\n",
    "    print(x.shape)\n",
    "\n",
    "    filtered = median_filter(x[0, :, :, 10], size = 5)\n",
    "    x[:, :, :, 10] = np.stack([filtered] * x.shape[0])\n",
    "    x = tile_images(x)\n",
    "    pred_x = np.stack(x)   \n",
    "    for band in range(0, pred_x.shape[-1]):\n",
    "        mins = min_all[band]\n",
    "        maxs = max_all[band]\n",
    "        pred_x[..., band] = np.clip(pred_x[..., band], mins, maxs)\n",
    "        midrange = (maxs + mins) / 2\n",
    "        rng = maxs - mins\n",
    "        standardized = (pred_x[..., band] - midrange) / (rng / 2)\n",
    "        pred_x[..., band] = standardized\n",
    "        \n",
    "    #pred_x = pred_x[..., :-2]\n",
    "    print(pred_x.shape)\n",
    "    print(np.max(pred_x[..., -1]))\n",
    "\n",
    "    preds = []\n",
    "    batches = [x for x in range(0, 341, 20)] + [361]\n",
    "    for i in range(len(batches)-1):\n",
    "        batch_x = pred_x[batches[i]:batches[i+1]]\n",
    "        lengths = np.full((batch_x.shape[0], 1), 12)\n",
    "        batch_pred = sess.run(logits,\n",
    "                              feed_dict={inp:batch_x, \n",
    "                                         clipping_params['rmax']: 5,\n",
    "                                         clipping_params['rmin']: 0,\n",
    "                                         clipping_params['dmax']: 3,\n",
    "                                         length:lengths}).reshape(batch_x.shape[0], 14, 14)\n",
    "        for sample in range(batch_pred.shape[0]):\n",
    "            preds.append(batch_pred[sample, :, :])\n",
    "            \n",
    "        \n",
    "    preds_stacked = []\n",
    "    for i in range(0, SIZE_N, SIZE):\n",
    "        preds_stacked.append(np.concatenate(preds[i:i + SIZE], axis = 1))\n",
    "    stacked = np.concatenate(preds_stacked, axis = 0) * normal\n",
    "\n",
    "    preds_overlap = []\n",
    "    for scene in range(SIZE_N, SIZE_N+SIZE_UR, SIZE - 1):\n",
    "        to_concat = np.concatenate(preds[scene:scene+ (SIZE - 1)], axis = 1)\n",
    "        preds_overlap.append(to_concat)    \n",
    "    overlapped = np.concatenate(preds_overlap, axis = 0)\n",
    "    overlapped = np.pad(overlapped, (7, 7), 'constant', constant_values = 0)\n",
    "    overlapped = overlapped * upright\n",
    "\n",
    "    preds_up = []\n",
    "    for scene in range(SIZE_N+SIZE_UR, SIZE_N+SIZE_UR+SIZE_R, SIZE):\n",
    "        to_concat = np.concatenate(preds[scene:scene+SIZE], axis = 1)\n",
    "        preds_up.append(to_concat)   \n",
    "    up = np.concatenate(preds_up, axis = 0)\n",
    "    up = np.pad(up, ((7,7), (0,0)), 'constant', constant_values = 0)\n",
    "    up = up * up_filter\n",
    "        \n",
    "    preds_right = []\n",
    "    for scene in range(SIZE_N+SIZE_UR+SIZE_R, TOTAL, SIZE - 1):\n",
    "        to_concat = np.concatenate(preds[scene:scene+SIZE-1], axis = 1)\n",
    "        preds_right.append(to_concat)   \n",
    "    right = np.concatenate(preds_right, axis = 0)\n",
    "    right = np.pad(right, ((0, 0), (7, 7)), 'constant', constant_values = 0)\n",
    "    right = right * right_filter\n",
    "    \n",
    "    stacked = stacked + overlapped + right + up\n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "for row in tnrange(0, (1*5)): \n",
    "    for column in range(0*5, (1*5)):\n",
    "        output_file = f\"{OUTPUT}{str(row)}/{str(column)}.npy\"\n",
    "        input_file = f\"{INPUT}{str(row)}/{str(column)}.hkl\"\n",
    "        \n",
    "        if os.path.exists(input_file) and not os.path.exists(output_file):\n",
    "            print(output_file)\n",
    "            prediction = load_and_predict_folder(row, column, histogram_match = False)\n",
    "            if not os.path.exists(OUTPUT + str(row) + \"/\"):\n",
    "                os.makedirs(OUTPUT + str(row) + \"/\")\n",
    "            prediction = prediction[7:-7, 7:-7]\n",
    "            np.save(output_file, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2.5 Mosaic predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_x = 1*5#41*5\n",
    "max_y = 1*5#24*5\n",
    "\n",
    "start_x = 0*5\n",
    "start_y = 0*5\n",
    "\n",
    "predictions = np.full(\n",
    "    ((max_y-start_y)*126,\n",
    "     (max_x-start_x)*126), 0, dtype = np.uint8 )\n",
    "\n",
    "max_y_out = predictions.shape[0]\n",
    "max_x_out = predictions.shape[1]\n",
    "\n",
    "numb = 0\n",
    "for row in tnrange(start_y, max_y):\n",
    "    for column in range(start_x, max_x):\n",
    "        input_file = f\"{OUTPUT}{str(row)}/{str(column)}.npy\"\n",
    "        if os.path.exists(input_file):\n",
    "            prediction = np.load(input_file)\n",
    "            x_value = (column-start_x) *126\n",
    "            y_value = (max_y_out - ((row - start_y + 1) *126))\n",
    "            if (row % 5 == 0) and (column % 5 == 0):\n",
    "                numb += 1\n",
    "            prediction = np.around(prediction * 10, 0) * 10\n",
    "            \n",
    "            predictions[y_value:y_value+126, \n",
    "                        x_value:x_value+126,\n",
    "                        ] = (prediction).astype(np.uint8)\n",
    "            \n",
    "predictions[predictions < 25] = 0.\n",
    "print(f\"There are {numb*4000} hectares processed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Sharpen predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_new(arr, thresh):\n",
    "    \"\"\"Not currently used. Identifies small trees that may be below the\n",
    "       threshold for binary map creation.\n",
    "    \"\"\"\n",
    "    adding = 0\n",
    "    stacked = np.copy(arr)\n",
    "    for window_x in tnrange(2, stacked.shape[0]-2, 1):\n",
    "        for window_y in range(2, stacked.shape[1]-2, 1):\n",
    "            #\n",
    "            five_w = stacked[window_x-2:window_x+3, window_y-2:window_y+3]\n",
    "            three_w = stacked[window_x-1:window_x+2, window_y-1:window_y+2]\n",
    "            \n",
    "            n_five_above = len(five_w[np.argwhere(five_w > 0.15)])\n",
    "            n_three_above = len(three_w[np.argwhere(three_w > 0.15)])\n",
    "            \n",
    "            n_five_below = len(five_w[np.argwhere(five_w < thresh)])\n",
    "            \n",
    "            \n",
    "            if n_five_below >= 24:                 \n",
    "                # if less than 2 of the 5x5 are positive\n",
    "                if n_three_above >= 2 and n_three_above < 6:            \n",
    "                    # and at least 2 of the 3x3 are above 0.1\n",
    "                    if n_three_above <= (n_five_above + 4):  \n",
    "                        # and less than 1/4 of the outer border is above 0.1\n",
    "                        if np.argmax(three_w) == 4:          \n",
    "                            # and the center of the 3 x 3 is the largest\n",
    "                            stacked[window_x, window_y] = -1.\n",
    "                            adding += 1\n",
    "    stacked[np.where(stacked == -1)] = 1.\n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = predictions\n",
    "\n",
    "threshold = False\n",
    "if threshold:\n",
    "    stacked = recover_new(predictions, 0.3)\n",
    "    stacked[np.where(stacked > thresh_p)] = 0.71\n",
    "    stacked[np.where(stacked < thresh_p)] = -1\n",
    "    stacked[np.where(stacked == 1.0)] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = True\n",
    "if plot:\n",
    "    plt.figure(figsize=(20,17))\n",
    "    plt.imshow(stacked, cmap='Greens', vmin=0, vmax=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Write GeoTiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = bounding_box(coords, (max_x*1260)-0, ((max_y)*1260)-0, expansion = 0)\n",
    "west = point[1][0]\n",
    "east = point[0][0]\n",
    "north = point[0][1]\n",
    "south = point[1][1]\n",
    "\n",
    "stacked[np.where(stacked < 0)] = 0.\n",
    "stacked = stacked.astype(np.uint8)\n",
    "transform = rasterio.transform.from_bounds(west = west, south = south,\n",
    "                                           east = east, north = north,\n",
    "                                           width = stacked.shape[1], \n",
    "                                           height = stacked.shape[0])\n",
    "\n",
    "print(\"Writing\", TIF_OUTPUT)\n",
    "new_dataset = rasterio.open(TIF_OUTPUT, 'w', driver = 'GTiff',\n",
    "                           height = stacked.shape[0], width = stacked.shape[1], count = 1,\n",
    "                           dtype = 'uint8',#str(stacked.dtype),\n",
    "                           crs = '+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs',\n",
    "                           transform=transform)\n",
    "new_dataset.write(stacked, 1)\n",
    "new_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdal_translate ../../ce-hosting/includes/niger-koure.tif ../tile_data/cog/niger-koure.tif \\\n",
    "               -co TILED=YES -co COMPRESS=LZW\n",
    "!gdaladdo -r average -ro ../tile_data/cog/niger-koure.tif 2 4 8 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 One-hectare tree cover Geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed = np.reshape(stacked, (stacked.shape[0] // 10, 10, stacked.shape[1] // 10, 10))\n",
    "summed = np.mean(summed, (1, 3))\n",
    "\n",
    "summed = summed.astype(np.float32)\n",
    "transform = rasterio.transform.from_bounds(west = west, south = south,\n",
    "                                           east = east, north = north,\n",
    "                                           width = summed.shape[1], height = summed.shape[1])\n",
    "\n",
    "new_dataset = rasterio.open('../../ce-hosting/includes/bonanza1.tif', 'w', driver = 'GTiff',\n",
    "                           height = summed.shape[1], width = summed.shape[1], count = 1,\n",
    "                           dtype = 'float32',#str(stacked.dtype),\n",
    "                           crs = '+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs',\n",
    "                           transform=transform)\n",
    "new_dataset.write(summed, 1)\n",
    "new_dataset.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
