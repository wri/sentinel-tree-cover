{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process sentinel 2 data\n",
    "\n",
    "This notebook downloads and processes one year of the training and validation plots as labelled on Collect Earth Online. \n",
    "\n",
    "## John Brandt\n",
    "## Last edit: Sept 20, 2021\n",
    "\n",
    "## Package imports, API import, source scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import hickle as hkl\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "from scipy.sparse.linalg import splu\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType\n",
    "from sentinelhub import CRS, BBox, constants, DataSource, CustomUrlParam\n",
    "from skimage.transform import resize\n",
    "from typing import Tuple, List\n",
    "from scipy.ndimage import median_filter\n",
    "from sentinelhub.config import SHConfig\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key'] \n",
    "        \n",
    "%matplotlib inline\n",
    "%run ../src/preprocessing/slope.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/downloading/utils.py\n",
    "%run ../src/preprocessing/cloud_removal.py\n",
    "%run ../src/preprocessing/whittaker_smoother.py\n",
    "%run ../src/downloading/io.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "    key = (yaml.safe_load(stream))\n",
    "    API_KEY = key['key']\n",
    "    SHUB_SECRET = key['shub_secret']\n",
    "    SHUB_KEY = key['shub_id']\n",
    "    AWSKEY = key['awskey']\n",
    "    AWSSECRET = key['awssecret']\n",
    "            \n",
    "shconfig = SHConfig()\n",
    "shconfig.instance_id = API_KEY\n",
    "shconfig.sh_client_id = SHUB_KEY\n",
    "shconfig.sh_client_secret = SHUB_SECRET\n",
    "    \n",
    "\n",
    "uploader = FileUploader(awskey = AWSKEY, awssecret = AWSSECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "YEAR = 2020\n",
    "TIME = ('{}-11-15'.format(str(YEAR - 1)), '{}-02-15'.format(str(YEAR + 1)))\n",
    "EPSG = CRS.WGS84\n",
    "IMSIZE = 32\n",
    "\n",
    "# Constants\n",
    "starting_days = np.cumsum([0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geographic utility functions\n",
    "\n",
    "These cell blocks calculate the min_x, min_y, max_x, max_y of the area of interest (AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bbox(plot_id: int, df: pd.DataFrame) -> List:\n",
    "    \"\"\" Calculates the corners of a bounding box from an input\n",
    "        pandas dataframe as output by Collect Earth Online\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (int): plot_id of associated plot\n",
    "         df (pandas.DataFrame): dataframe of associated CEO survey\n",
    "    \n",
    "        Returns:\n",
    "         bounding_box (list): [(min(x), min(y)),\n",
    "                              (max(x), max_y))]\n",
    "    \"\"\"\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])),\n",
    "            (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "\n",
    "def bounding_box(points: List[Tuple[float, float]], \n",
    "                 expansion: int = 160) -> ((Tuple, Tuple), str):\n",
    "    \"\"\" Calculates the corners of a bounding box with an\n",
    "        input expansion in meters from a given bounding_box\n",
    "        \n",
    "        Subcalls:\n",
    "         calculate_epsg, convertCoords\n",
    "\n",
    "        Parameters:\n",
    "         points (list): output of calc_bbox\n",
    "         expansion (float): number of meters to expand or shrink the\n",
    "                            points edges to be\n",
    "    \n",
    "        Returns:\n",
    "         bl (tuple): x, y of bottom left corner with edges of expansion meters\n",
    "         tr (tuple): x, y of top right corner with edges of expansion meters\n",
    "    \"\"\"\n",
    "    bl = list(points[0])\n",
    "    tr = list(points[1])\n",
    "    inproj = Proj('epsg:4326')\n",
    "    outproj_code = calculate_epsg(bl)\n",
    "    outproj = Proj('epsg:' + str(outproj_code))\n",
    "    bl_utm =  transform(inproj, outproj, bl[1], bl[0])\n",
    "    tr_utm =  transform(inproj, outproj, tr[1], tr[0])\n",
    "\n",
    "    distance1 = tr_utm[0] - bl_utm[0]\n",
    "    distance2 = tr_utm[1] - bl_utm[1]\n",
    "    expansion1 = (expansion - distance1)/2\n",
    "    expansion2 = (expansion - distance2)/2\n",
    "        \n",
    "    bl_utm = [bl_utm[0] - expansion1, bl_utm[1] - expansion2]\n",
    "    tr_utm = [tr_utm[0] + expansion1, tr_utm[1] + expansion2]\n",
    "\n",
    "    zone = str(outproj_code)[3:]\n",
    "    zone = zone[1:] if zone[0] == \"0\" else zone\n",
    "    direction = 'N' if tr[1] >= 0 else 'S'\n",
    "    utm_epsg = \"UTM_\" + zone + direction\n",
    "    return (bl_utm, tr_utm), CRS[utm_epsg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(date_dict: dict, year: int) -> List:\n",
    "    \"\"\" Transforms a SentinelHub date dictionary to a\n",
    "         list of integer calendar dates\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "    starting_days = np.cumsum(days_per_month)\n",
    "    for date in date_dict:\n",
    "        if date.year == year - 1:\n",
    "            dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    return dates\n",
    "\n",
    "def to_float32(array: np.array) -> np.array:\n",
    "    \"\"\"Converts an int_x array to float32\"\"\"\n",
    "    print(f'The original max value is {np.max(array)}')\n",
    "    if not isinstance(array.flat[0], np.floating):\n",
    "        assert np.max(array) > 1\n",
    "        array = np.float32(array) / 65535.\n",
    "    assert np.max(array) <= 1\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud and cloud shadow calculation functions\n",
    "\n",
    "This cell block identifies clouds using s2Cloudless, and identifies clouds (and shadows) using Candra et al. 2019.\n",
    "\n",
    "The output is per-px cloud/shadow masks, and a list of sentinel 2 dates to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_clouds(bbox: List[Tuple[float, float]], epsg: 'CRS', time: dict = TIME):\n",
    "    \"\"\" Downloads and calculates cloud cover and shadow\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         cloud_img (np.array): (X, 96, 96) array of cloud probs\n",
    "         shadows (np.array):  (X, 96, 96) array of shadow binary\n",
    "         clean_steps (np.array): (N,) array of clean idx\n",
    "         cloud_dates (np.array): (N,) array of clean cloud datets\n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    cloud_request = WcsRequest(\n",
    "        layer='CLOUD_NEW',\n",
    "        bbox=box, time=time,\n",
    "        resx='160m', resy='160m',\n",
    "        image_format = MimeType.TIFF,\n",
    "        maxcc=0.75, config=shconfig,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=96))\n",
    "\n",
    "    shadow_request = WcsRequest(\n",
    "        layer='SHADOW',\n",
    "        bbox=box, time=time,\n",
    "        resx='60m', resy='60m',\n",
    "        image_format =  MimeType.TIFF,\n",
    "        maxcc=0.75, config=shconfig,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=96))\n",
    "\n",
    "    cloud_img = np.array(cloud_request.get_data())\n",
    "    if not isinstance(cloud_img.flat[0], np.floating):\n",
    "        assert np.max(cloud_img) > 1\n",
    "        cloud_img = cloud_img / 255.\n",
    "    assert np.max(cloud_img) <= 1\n",
    "\n",
    "    cloud_img = resize(cloud_img, (cloud_img.shape[0], 96, 96), order = 0)\n",
    "    n_cloud_px = np.sum(cloud_img > 0.33, axis = (1, 2))\n",
    "    cloud_steps = np.argwhere(n_cloud_px > (96**2 * 0.15))\n",
    "    clean_steps = [x for x in range(cloud_img.shape[0]) if x not in cloud_steps]\n",
    "    \n",
    "    \n",
    "    cloud_dates_dict = [x for x in cloud_request.get_dates()]\n",
    "    cloud_dates = extract_dates(cloud_dates_dict, YEAR)\n",
    "    cloud_dates = [val for idx, val in enumerate(cloud_dates) if idx in clean_steps]\n",
    "    \n",
    "    shadow_dates_dict = [x for x in shadow_request.get_dates()]\n",
    "    shadow_dates = extract_dates(shadow_dates_dict, YEAR)\n",
    "    shadow_steps = [idx for idx, val in enumerate(shadow_dates) if val in cloud_dates]    \n",
    "    \n",
    "    shadow_img = np.array(shadow_request.get_data(data_filter = shadow_steps))\n",
    "    shadow_pus = (shadow_img.shape[1]*shadow_img.shape[2])/(512*512) * shadow_img.shape[0]\n",
    "    shadow_img = resize(shadow_img, (shadow_img.shape[0], 96, 96, shadow_img.shape[-1]), order = 0,\n",
    "                        anti_aliasing = False, preserve_range = True).astype(np.uint16)\n",
    "\n",
    "    cloud_img = np.delete(cloud_img, cloud_steps, 0)\n",
    "    assert shadow_img.shape[0] == cloud_img.shape[0], (shadow_img.shape, cloud_img.shape)\n",
    "    shadows = mcm_shadow_mask(shadow_img, cloud_img) # Make usre this makes sense??\n",
    "    print(f\"Shadows ({shadows.shape}) used {round(shadow_pus, 1)} processing units\")\n",
    "    return cloud_img, shadows, clean_steps, np.array(cloud_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEM and slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dem(plot_id: int, df: 'DataFrame', epsg: 'CRS') -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads MapZen digital elevation model and return slope\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (tuple): plot id from collect earth online (CEO)\n",
    "         df (pandas.DataFrame): data associated with plot_id from CEO\n",
    "         epsg (int): UTM EPSG associated with plot_id\n",
    "    \n",
    "        Returns:\n",
    "         slope (arr): (X, Y, 1) array of per-pixel slope from [0, 1]\n",
    "    \"\"\"\n",
    "    location = calc_bbox(plot_id, df = df)\n",
    "    bbox, epsg = bounding_box(location, expansion = (32+2)*10)\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_request = WcsRequest(\n",
    "                         layer='DEM', bbox=box,\n",
    "                         resx = \"10m\", resy = \"10m\",\n",
    "                         config=shconfig,\n",
    "                         image_format= MimeType.TIFF,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image_init = dem_request.get_data()[0]\n",
    "    dem_image_init = dem_image_init - 12000\n",
    "    dem_image_init = dem_image_init.astype(np.float32)\n",
    "    dem_image = np.copy(dem_image_init)\n",
    "    dem_image = median_filter(dem_image_init, size = 5)\n",
    "    slope = calcSlope(dem_image.reshape((1, 32+2, 32+2)),\n",
    "                      np.full((32+2, 32+2), 10),\n",
    "                      np.full((32+2, 32+2), 10), \n",
    "                      zScale = 1, minSlope = 0.02)\n",
    "    slope = slope / 90\n",
    "    slope = slope.reshape((32+2, 32+2, 1))\n",
    "    slope = slope[1:32+1, 1:32+1, :]\n",
    "    return slope, dem_image_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 and 20 meter L2A bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_layer(bbox: List[Tuple[float, float]],\n",
    "                   clean_steps: np.ndarray, epsg: 'CRS',\n",
    "                   dates: dict = TIME, year: int = YEAR) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads the L2A sentinel layer with 10 and 20 meter bands\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         img (arr):\n",
    "         img_request (obj): \n",
    "    \"\"\"\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A20',\n",
    "                bbox=box, time=dates,\n",
    "                image_format = MimeType.TIFF,\n",
    "                data_source = DataSource.SENTINEL2_L2A,\n",
    "                maxcc=0.75,\n",
    "                resx='20m', resy='20m',\n",
    "                config=shconfig,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=96),\n",
    "            )\n",
    "        \n",
    "        image_dates = []\n",
    "        for date in image_request.get_dates():\n",
    "            if date.year == YEAR - 1:\n",
    "                image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "            if date.year == YEAR:\n",
    "                image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "            if date.year == YEAR + 1:\n",
    "                image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "        \n",
    "        steps_to_download = [i for i, val in enumerate(image_dates) if val in clean_steps]\n",
    "        dates_to_download = [val for i, val in enumerate(image_dates) if val in clean_steps]\n",
    "              \n",
    "        img_bands = image_request.get_data(data_filter = steps_to_download)\n",
    "        img_20 = np.stack(img_bands)\n",
    "        img_20 = to_float32(img_20)\n",
    "\n",
    "        s2_20_usage = (img_20.shape[1]*img_20.shape[2])/(512*512) * (6/3) * img_20.shape[0]\n",
    "        if (img_20.shape[1] * img_20.shape[2]) != 14*14:\n",
    "            print(f\"Original 20 meter bands size: {img_20.shape}, using {s2_20_usage} PU\")\n",
    "        img_20 = resize(img_20, (img_20.shape[0], IMSIZE, IMSIZE, img_20.shape[-1]), order = 0)\n",
    "        \n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A10',\n",
    "                bbox=box, time=dates,\n",
    "                image_format = MimeType.TIFF,\n",
    "                data_source = DataSource.SENTINEL2_L2A,\n",
    "                maxcc=0.75,\n",
    "                resx='10m', resy='10m',\n",
    "                config=shconfig,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=96),\n",
    "        )\n",
    "        \n",
    "        img_bands = image_request.get_data(data_filter = steps_to_download)\n",
    "        img_10 = np.stack(img_bands)\n",
    "        if (img_10.shape[1] * img_10.shape[2]) != 28*28:\n",
    "            print(f\"The original L2A image size is: {img_10.shape}\")\n",
    "        img_10 = to_float32(img_10)\n",
    "            \n",
    "        img_10 = resize(img_10, (img_10.shape[0], IMSIZE, IMSIZE, img_10.shape[-1]), order = 0)\n",
    "        img = np.concatenate([img_10, img_20], axis = -1)\n",
    "\n",
    "        \n",
    "        return img, np.array(dates_to_download)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super resolution\n",
    "\n",
    "Super-resolve the 20 meter bands to 10 meters using DSen2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/supres/model\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "MDL_PATH = \"../models/supres/\"\n",
    "\n",
    "model = tf.train.import_meta_graph(MDL_PATH + 'model.meta')\n",
    "model.restore(sess, tf.train.latest_checkpoint(MDL_PATH))\n",
    "\n",
    "logits = tf.get_default_graph().get_tensor_by_name(\"Add_6:0\")\n",
    "inp = tf.get_default_graph().get_tensor_by_name(\"Placeholder:0\")\n",
    "inp_bilinear = tf.get_default_graph().get_tensor_by_name(\"Placeholder_1:0\")\n",
    "\n",
    "def superresolve(input_data):\n",
    "    bilinear_upsample = input_data[..., 4:]\n",
    "    x = sess.run([logits], \n",
    "                 feed_dict={inp: input_data,\n",
    "                            inp_bilinear: bilinear_upsample})\n",
    "    return x[0]\n",
    "\n",
    "def superresolve_tile(x):\n",
    "    twentym = x[..., 4:]\n",
    "    imsize = x.shape[1]\n",
    "    twentym = np.reshape(twentym, (x.shape[0], imsize // 2, 2, imsize // 2, 2, 6))\n",
    "    twentym = np.mean(twentym, (2, 4))\n",
    "    twentym = resize(twentym, (x.shape[0], imsize, imsize, 6), 1)\n",
    "    x[..., 4:] = twentym\n",
    "    x[..., 4:] = superresolve(x)\n",
    "    if imsize > 28:\n",
    "        crop_amt = (imsize - 28) // 2\n",
    "        x = x[:, crop_amt:-crop_amt, crop_amt:-crop_amt, :]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_new_dem(data_location: 'os.Path',\n",
    "                     output_folder: 'os.Path',\n",
    "                     image_format: 'MimeType' = MimeType.TIFF):\n",
    "    \"\"\" Downloads and saves DEM and slope files\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path): \n",
    "         output_folder (os.path): \n",
    "         image_format (MimeType): \n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(data_location)\n",
    "    df.columns = [x.upper() for x in df.columns]\n",
    "    for column in ['IMAGERY_TITLE', 'STACKINGPROFILEDG', 'PL_PLOTID', 'IMAGERYYEARDG',\n",
    "                  'IMAGERYMONTHPLANET', 'IMAGERYYEARPLANET', 'IMAGERYDATESECUREWATCH',\n",
    "                  'IMAGERYENDDATESECUREWATCH', 'IMAGERYFEATUREPROFILESECUREWATCH',\n",
    "                  'IMAGERYSTARTDATESECUREWATCH','IMAGERY_ATTRIBUTIONS',\n",
    "                  'SAMPLE_GEOM']:\n",
    "        if column in df.columns:\n",
    "            df = df.drop(column, axis = 1)\n",
    "            \n",
    "    df = df.dropna(axis = 0)\n",
    "    plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "    existing = [int(x[:-4]) for x in os.listdir(output_folder) if \".DS\" not in x]\n",
    "    existing = existing + [139089844]\n",
    "    to_download = [x for x in plot_ids if x not in existing]\n",
    "    print(f\"Starting download of {len(to_download)}\"\n",
    "          f\" plots from {data_location} to {output_folder}\")\n",
    "    errors = []\n",
    "    for i, val in enumerate(to_download):\n",
    "        print(f\"Downloading {i + 1}/{len(to_download)}, {val}\")\n",
    "        initial_bbx = calc_bbox(val, df = df)\n",
    "        dem_bbx, epsg = bounding_box(initial_bbx, expansion = 32*10)\n",
    "        slope, dem = download_dem(val, epsg = epsg, df = df)\n",
    "        np.save(output_folder + str(val), dem)\n",
    "        np.save(\"../data/train-slope/\" + str(val), slope)\n",
    "        \n",
    "\n",
    "\n",
    "def concatenate_dem(x, dem):\n",
    "    dem = np.tile(dem.reshape((1, 32, 32, 1)), (x.shape[0], 1, 1, 1))\n",
    "    dem = dem[:, 2:-2, 2:-2, :]\n",
    "    x = np.concatenate([x, dem], axis = -1)\n",
    "    assert x.shape[1] == x.shape[2] == 28\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def id_missing_px(sentinel2: np.ndarray, thresh: int = 100) -> np.ndarray:\n",
    "    missing_images_0 = np.sum(sentinel2[..., :10] == 0.0, axis = (1, 2, 3))\n",
    "    missing_images_p = np.sum(sentinel2[..., :10] >= 1., axis = (1, 2, 3))\n",
    "    missing_images = missing_images_0 + missing_images_p\n",
    "    \n",
    "    missing_images = np.argwhere(missing_images >= (sentinel2.shape[1]**2) / thresh).flatten()\n",
    "    return missing_images\n",
    "\n",
    "\n",
    "def download_raw_data(data_location, output_folder, fmt = \"train\", image_format = MimeType.TIFF):\n",
    "    \"\"\" Downloads slope and sentinel-2 data for all plots associated\n",
    "        with an input CSV from a collect earth online survey\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path)\n",
    "         output_folder (os.path)\n",
    "        \n",
    "        Creates:\n",
    "         output_folder/{plot_id}.npy\n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_location)\n",
    "    df.columns = [x.upper() for x in df.columns]\n",
    "    for column in ['IMAGERY_TITLE', 'STACKINGPROFILEDG', 'PL_PLOTID', 'IMAGERYYEARDG',\n",
    "                  'IMAGERYMONTHPLANET', 'IMAGERYYEARPLANET', 'IMAGERYDATESECUREWATCH',\n",
    "                  'IMAGERYENDDATESECUREWATCH', 'IMAGERYFEATUREPROFILESECUREWATCH',\n",
    "                  'IMAGERYSTARTDATESECUREWATCH','IMAGERY_ATTRIBUTIONS',\n",
    "                  'SAMPLE_GEOM']:\n",
    "        if column in df.columns:\n",
    "            df = df.drop(column, axis = 1)\n",
    "\n",
    "    df = df.dropna(axis = 0)\n",
    "    plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "    existing = [int(x[:-4]) for x in os.listdir(f\"../data/{fmt}-dates/\") if \".DS\" not in x]\n",
    "    existing = existing + [139190271, 139187199, 139319876, 139319877]\n",
    "    to_download = [x for x in plot_ids if x not in existing]\n",
    "    print(f\"Starting download of {len(to_download)}\"\n",
    "          f\" plots from {data_location} to {output_folder}\")\n",
    "    for i, val in enumerate(reversed(to_download)):\n",
    "        print(f\"Downloading {i + 1}/{len(to_download)}, {val}\")\n",
    "        initial_bbx = calc_bbox(val, df = df)\n",
    "        sentinel2_bbx, epsg = bounding_box(initial_bbx, expansion = IMSIZE*10)\n",
    "        cloud_bbx, _ = bounding_box(initial_bbx, expansion = 96*10)\n",
    "        try:\n",
    "            # Identify cloud steps, download DEM, and download L2A series\n",
    "            cloud_probs, shadows, _, clean_dates = identify_clouds(cloud_bbx, epsg = epsg)\n",
    "            dem, _ = download_dem(val, epsg = epsg, df = df)\n",
    "            #to_remove, _ = calculate_cloud_steps(cloud_probs, clean_dates)\n",
    "            \n",
    "            #if len(to_remove) > 0:\n",
    "            #    cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "            #    clean_dates = np.delete(clean_dates, to_remove)\n",
    "            #    shadows = np.delete(shadows, to_remove, 0)\n",
    "                \n",
    "            to_remove = subset_contiguous_sunny_dates(clean_dates, cloud_probs)\n",
    "            if len(to_remove) > 0:\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "                clean_dates = np.delete(clean_dates, to_remove)\n",
    "                shadows = np.delete(shadows, to_remove, 0)\n",
    "                \n",
    "            _ = print_dates(clean_dates, np.mean(cloud_probs, axis = (1, 2)))\n",
    "        \n",
    "            s2, s2_dates = download_layer(sentinel2_bbx, clean_steps = clean_dates, epsg = epsg)    \n",
    "            \n",
    "            # Step to ensure that shadows, clouds, sentinel l2a have aligned dates\n",
    "            to_remove_clouds = [i for i, val in enumerate(clean_dates) if val not in s2_dates]\n",
    "            to_remove_dates = [val for i, val in enumerate(clean_dates) if val not in s2_dates]\n",
    "            if len(to_remove_clouds) > 0:\n",
    "                print(f\"Removing {to_remove_dates} from clouds because not in S2\")\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove_clouds, 0)\n",
    "                shadows = np.delete(shadows, to_remove_clouds, 0)\n",
    "            print(f\"Shadows {shadows.shape}, clouds {cloud_probs.shape},\"\n",
    "                  f\" S2, {s2.shape}, S2d, {s2_dates.shape}\")\n",
    "            \n",
    "            print(s2.shape)\n",
    "            \n",
    "            cloud_probs = cloud_probs[:, 24:-24, 24:-24]\n",
    "            shadows = shadows[:, 24:-24, 24:-24]\n",
    "            x, interp = remove_cloud_and_shadows(s2, cloud_probs, shadows, s2_dates)\n",
    "            to_remove = np.argwhere(np.mean(interp, axis = (1, 2)) > 0.5)\n",
    "            if len(to_remove) > 0:\n",
    "                print(f\"Removing {len(to_remove)} steps with >50% interpolation: {to_remove}\")\n",
    "                x = np.delete(x, to_remove, 0)\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "                s2_dates = np.delete(s2_dates, to_remove)\n",
    "                shadows = np.delete(shadows, to_remove, 0)\n",
    "                print(np.sum(shadows, axis = (1, 2)))\n",
    "            \n",
    "            x_to_save = np.copy(x)\n",
    "            x_to_save = np.clip(x_to_save, 0, 1)\n",
    "            x_to_save = np.trunc(x_to_save * 65535).astype(np.uint16)\n",
    "            np.save(f\"../data/{fmt}-dates/{str(val)}\", s2_dates)\n",
    "            np.save(f\"../data/{fmt}-raw/{str(val)}\", x_to_save)\n",
    "            file = f\"../data/{fmt}-raw/{str(val)}.npy\"\n",
    "            key = f'restoration-mapper/model-data/{fmt}/raw/{str(val)}.npy'\n",
    "            uploader.upload(bucket = 'restoration-monitoring', key = key, file = file)\n",
    "            print(\"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dates(dates):\n",
    "    \"\"\"For imagery that was downloaded prior to capping the number \n",
    "       of monthly images to be 3, it is necessary to enforce that cap\n",
    "       on the training / testing data.\n",
    "       \n",
    "       This function identifies the indices of the imagery to deletet\n",
    "       such that there is a maximum of three images per month.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    before = len(dates)\n",
    "    selected_indices = np.arange(len(dates))\n",
    "    begin = [-60, 0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n",
    "    end = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334, 390]\n",
    "    indices_to_remove = []\n",
    "    for x, y in zip(begin, end):\n",
    "        indices_month = np.argwhere(np.logical_and(dates >= x, dates < y)).flatten()\n",
    "        if len(indices_month) > 3:\n",
    "            to_delete = np.empty((0,))\n",
    "            if begin == -60:\n",
    "                to_delete = indices_month[:-3]\n",
    "            elif begin == 334:\n",
    "                to_delete = indices_month[3:]\n",
    "            elif len(indices_month) == 4:\n",
    "                to_delete = indices_month[1]\n",
    "            elif len(indices_month) == 5:\n",
    "                to_delete = np.array([indices_month[1],\n",
    "                                      indices_month[3]])\n",
    "            elif len(indices_month) == 6:\n",
    "                to_delete = np.array([indices_month[1],\n",
    "                                      indices_month[3],\n",
    "                                      indices_month[4]])\n",
    "                \n",
    "            to_delete = np.array(to_delete)\n",
    "            if to_delete.size > 0:\n",
    "                indices_to_remove.append(to_delete.flatten())\n",
    "                \n",
    "    if len(indices_to_remove) > 0:\n",
    "        indices_to_remove = np.concatenate(indices_to_remove)\n",
    "        after = before - len(indices_to_remove)\n",
    "        print(f\"Keeping {after}/{before}\")\n",
    "        return indices_to_remove\n",
    "    \n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "\n",
    "def subset_contiguous_sunny_dates(dates, probs):\n",
    "    \"\"\"\n",
    "    The general imagery subsetting strategy is as below:\n",
    "        - Select all images with < 30% cloud cover\n",
    "        - For each month, select up to 2 images that are <30% CC and are the closest to\n",
    "          the beginning and the midde of the month\n",
    "        - Select only one image per month for each month if the following criteria are met\n",
    "              - Within Q1 and Q4, apply if at least 3 images in quarter\n",
    "              - Otherwise, apply if at least 8 total images for year\n",
    "              - Select the second image if max CC < 15%, otherwise select least-cloudy image\n",
    "        - If more than 10 images remain, remove any images for April and September\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    probs = np.mean(probs, axis = (1, 2))\n",
    "    begin = [-60, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n",
    "    end = [31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334, 410]\n",
    "    n_per_month = []\n",
    "    months_to_adjust = []\n",
    "    months_to_adjust_again = []\n",
    "    indices_to_rm = []\n",
    "    indices = [x for x in range(len(dates))]\n",
    "    \n",
    "    def _indices_month(dates, x, y):\n",
    "        indices_month = np.argwhere(np.logical_and(\n",
    "                    dates >= x, dates < y)).flatten()\n",
    "        return indices_month\n",
    "\n",
    "    \n",
    "    _ = print_dates(dates, probs)\n",
    "    # Select the best 2 images per month to start with\n",
    "    best_two_per_month = []\n",
    "    for x, y in zip(begin, end):\n",
    "        indices_month = np.argwhere(np.logical_and(\n",
    "            dates >= x, dates < y)).flatten()\n",
    "\n",
    "        month_dates = dates[indices_month]\n",
    "        month_clouds = probs[indices_month]\n",
    "        month_good_dates = month_dates[month_clouds < 0.20]\n",
    "        indices_month = indices_month[month_clouds < 0.20]\n",
    "\n",
    "        if len(month_good_dates) >= 2:\n",
    "            if x > 0:\n",
    "                ideal_dates = [x, x + 15]\n",
    "            else:\n",
    "                ideal_dates = [0, 15]\n",
    "\n",
    "            # We first pick the 2 images with <30% cloud cover that are the closest\n",
    "            # to the 1st and 15th of the month\n",
    "            # todo: if both these images are above 15%, and one below 15% is available, include it\n",
    "            closest_to_first_img = np.argmin(abs(month_good_dates - ideal_dates[0]))\n",
    "            closest_to_second_img = np.argmin(abs(month_good_dates - ideal_dates[1]))\n",
    "            if closest_to_second_img == closest_to_first_img:\n",
    "                distances = abs(month_good_dates - ideal_dates[1])\n",
    "                closest_to_second_img = np.argsort(distances)[1]\n",
    "\n",
    "            first_image = indices_month[closest_to_first_img]\n",
    "            second_image = indices_month[closest_to_second_img]\n",
    "            best_two_per_month.append(first_image)\n",
    "            best_two_per_month.append(second_image)\n",
    "                    \n",
    "        elif len(month_good_dates) >= 1:\n",
    "            if x > 0:\n",
    "                ideal_dates = [x, x + 15]\n",
    "            else:\n",
    "                ideal_dates = [0, 15]\n",
    "\n",
    "            closest_to_second_img = np.argmin(abs(month_good_dates - ideal_dates[1]))\n",
    "            second_image = indices_month[closest_to_second_img]\n",
    "            best_two_per_month.append(second_image)\n",
    "                \n",
    "    dates_round_2 = dates[best_two_per_month]\n",
    "    probs_round_2 = probs[best_two_per_month]\n",
    "    \n",
    "    # We then select between those two images to keep a max of one per month\n",
    "    # We select the least cloudy image if the most cloudy has >15% cloud cover\n",
    "    # Otherwise we select the second image\n",
    "\n",
    "    # If there are more than 8 images, subset so only 1 image per month,\n",
    "    # To bring down to a min of 8 images\n",
    "    if len(dates_round_2) >= 8:\n",
    "        n_to_rm = len(dates_round_2) - 8\n",
    "        monthly_dates = []\n",
    "        monthly_probs = []\n",
    "        monthly_dates_date = []\n",
    "        removed = 0\n",
    "        for x, y in zip(begin, end):\n",
    "            indices_month = np.argwhere(np.logical_and(\n",
    "                dates >= x, dates < y)).flatten()\n",
    "            dates_month = dates[indices_month]\n",
    "            indices_month = [val for i, val in enumerate(indices_month) if dates_month[i] in dates_round_2]\n",
    "            if len(indices_month) > 1:\n",
    "                month_dates = dates[indices_month]\n",
    "                month_clouds = probs[indices_month]\n",
    "\n",
    "                subset_month = True\n",
    "                if x == -60:\n",
    "                    feb_mar = np.argwhere(np.logical_and(\n",
    "                        dates >= 31, dates < 90)).flatten()\n",
    "                    subset_month = False if len(feb_mar) < 2 else True\n",
    "                if x == 334:\n",
    "                    oct_nov = np.argwhere(np.logical_and(\n",
    "                        dates >= 273, dates < 334)).flatten()\n",
    "                    subset_month = False if len(oct_nov) < 2 else True\n",
    "\n",
    "                if subset_month:\n",
    "                    subset_month = True if removed <= n_to_rm else False\n",
    "                if subset_month:\n",
    "                    if np.max(month_clouds) >= 0.10:\n",
    "                        month_best_date = [indices_month[np.argmin(month_clouds)]]\n",
    "                    else:\n",
    "                        month_best_date = [indices_month[1]]\n",
    "                else:\n",
    "                    month_best_date = indices_month\n",
    "                monthly_dates.extend(month_best_date)\n",
    "                monthly_probs.extend(probs[month_best_date])\n",
    "                monthly_dates_date.extend(dates[month_best_date])\n",
    "                removed += 1\n",
    "            elif len(indices_month) == 1:\n",
    "                monthly_dates.append(indices_month[0])\n",
    "                monthly_probs.append(probs[indices_month[0]])\n",
    "                monthly_dates_date.append(dates[indices_month[0]])\n",
    "    else:\n",
    "        monthly_dates = best_two_per_month\n",
    "        \n",
    "    indices_to_rm = [x for x in indices if x not in monthly_dates]\n",
    "\n",
    "\n",
    "    dates_round_3 = dates[monthly_dates]\n",
    "    probs_round_3 = probs[monthly_dates]\n",
    "\n",
    "    if len(dates_round_3) >= 10:\n",
    "        delete_max = False\n",
    "        if np.max(probs_round_3) >= 0.15:\n",
    "            delete_max = True\n",
    "            indices_to_rm.append(monthly_dates[np.argmax(probs_round_3)])\n",
    "        for x, y in zip(begin, end):\n",
    "            indices_month = np.argwhere(np.logical_and(\n",
    "                dates >= x, dates < y)).flatten()\n",
    "            dates_month = dates[indices_month]\n",
    "            indices_month = [x for x in indices_month if x in monthly_dates]\n",
    "\n",
    "            n_removed = 0\n",
    "            if len(indices_month) >= 1:\n",
    "                if len(monthly_dates) == 11 and delete_max:\n",
    "                    continue\n",
    "                elif len(monthly_dates) >= 11:\n",
    "                    if x in [90, 243]:\n",
    "                        indices_to_rm.append(indices_month[0])\n",
    "\n",
    "    return indices_to_rm\n",
    "\n",
    "\n",
    "def to_int16(array: np.array) -> np.array:\n",
    "    '''Converts a float32 array to uint16, reducing storage costs by three-fold'''\n",
    "    array = np.clip(array, 0, 1)\n",
    "    array = np.trunc(array * 65535)\n",
    "    assert np.min(array >= 0)\n",
    "    assert np.max(array <= 65535)\n",
    "    \n",
    "    return array.astype(np.uint16)\n",
    "\n",
    "\n",
    "def process_raw(plot_id, path = 'train'):\n",
    "    \"\"\" Downloads slope and sentinel-2 data for all plots associated\n",
    "        with an input CSV from a collect earth online survey\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path)\n",
    "         output_folder (os.path)\n",
    "        \n",
    "        Creates:\n",
    "         output_folder/{plot_id}.npy\n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"         \n",
    "\n",
    "    x = np.load(f\"../data/{path}-raw/{plot_id}.npy\")\n",
    "    x = np.float32(x) / 65535\n",
    "    if x.shape[-1] == 10:\n",
    "        s2_dates = np.load(f\"../data/{path}-dates/{plot_id}.npy\")\n",
    "        dem = np.load(f\"../data/{path}-slope/{plot_id}.npy\")\n",
    "\n",
    "        assert x.shape[0] == s2_dates.shape[0]\n",
    "\n",
    "        missing_px = id_missing_px(x)\n",
    "        if len(missing_px) > 0:\n",
    "            print(f\"Deleting {missing_px} because of missing data\")\n",
    "            x = np.delete(x, missing_px, 0)\n",
    "            s2_dates = np.delete(s2_dates, missing_px)\n",
    "\n",
    "\n",
    "        n_images = x.shape[0]\n",
    "\n",
    "        to_remove = select_dates(s2_dates)\n",
    "        if len(to_remove) > 0:\n",
    "            x = np.delete(x, to_remove, 0)\n",
    "            s2_dates = np.delete(s2_dates, to_remove)\n",
    "\n",
    "        print(x.shape)\n",
    "        #to_remove = subset_contiguous_sunny_dates(s2_dates)\n",
    "        #if len(to_remove) > 0:\n",
    "        #    x = np.delete(x, to_remove, 0)\n",
    "        #    s2_dates = np.delete(s2_dates, to_remove)\n",
    "\n",
    "        for band in range(0, 10):\n",
    "            for time in range(0, x.shape[0]):\n",
    "                x_i = x[time, :, :, band]\n",
    "                x_i[np.argwhere(np.isnan(x_i))] = np.mean(x_i)\n",
    "                x[time, :, :, band] = x_i\n",
    "\n",
    "        # Interpolate linearly to 5 day frequency\n",
    "        tiles, max_distance = calculate_and_save_best_images(x, s2_dates)\n",
    "        sm = Smoother(lmbd = 150, size = tiles.shape[0],\n",
    "                      nbands = 10, dimx = tiles.shape[1], dimy = tiles.shape[2])\n",
    "        x = sm.interpolate_array(tiles)\n",
    "        x = superresolve_tile(x)\n",
    "        tiles = concatenate_dem(x, dem)\n",
    "        dates = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n",
    "        dates = np.array(dates) + 15\n",
    "        closest_date = []\n",
    "        for date in dates:\n",
    "            date_diff = s2_dates[np.argmin(abs(s2_dates - date))]\n",
    "            closest_date.append(date_diff)\n",
    "\n",
    "        closest_date = np.array(closest_date)\n",
    "        closest_date = closest_date[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "        closest_date = np.broadcast_to(closest_date, (12, 28, 28, 1))\n",
    "        closest_date = (closest_date + 45)  / 411\n",
    "\n",
    "        tiles = np.concatenate([tiles, closest_date], axis = -1)\n",
    "\n",
    "        if np.sum(np.isnan(tiles)) == 0:\n",
    "            print(f\"There are {np.sum(np.isnan(tiles))} NA values\")\n",
    "            if max_distance <= 300 and n_images >= 5:\n",
    "                tiles = to_int16(tiles)\n",
    "                #np.save(f\"../data/{path}-s2-new/{plot_id}\", tiles)\n",
    "                tile_path = f\"../data/{path}-s2/{plot_id}\"\n",
    "                tile_path = tile_path + \".hkl\"\n",
    "                hkl.dump(tiles, tile_path, mode='w', compression='gzip')\n",
    "                print(f\"Saved {tiles.shape} shape, {n_images} img,\"\n",
    "                      f\" to {tile_path} \\n\")\n",
    "            else:\n",
    "                print(f\"Skipping {plot_id} because {max_distance} distance, and {n_images} img \\n\")\n",
    "\n",
    "        return tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function execution\n",
    "## 1. Download DEM and Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download of 0 plots from ../data/train-csv/ceo-asia-otherland-sample-data-20.csv to ../data/train-dem/\n",
      "Starting download of 0 plots from ../data/train-csv/ceo-asia-tml-finetune-sample-data-22.csv to ../data/train-dem/\n",
      "Starting download of 0 plots from ../data/train-csv/ceo-asia-rice-sample-data-21.csv to ../data/train-dem/\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "for i in (os.listdir(\"../data/train-csv/\")):\n",
    "    if \"ceo-asia\" in i:\n",
    "        tile = download_new_dem(\"../data/train-csv/\" + i,\n",
    "                                \"../data/train-dem/\",\n",
    "                                image_format = MimeType.TIFF)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Raw data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download of 0 plots from ../data/train-csv/ceo-asia-otherland-sample-data-20.csv to ../data/train-raw/\n",
      "Starting download of 24 plots from ../data/train-csv/ceo-asia-tml-finetune-sample-data-22.csv to ../data/train-raw/\n",
      "Downloading 1/24, 220039\n",
      "Shadows ((28, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [17, 22, 366, 371, 386], Probs: [0.09, 0.05, 0.07, 0.06, 0.18]\n",
      "2, Dates: [32, 37, 42, 52, 396, 401, 406], Probs: [0.11, 0.18, 0.12, 0.1, 0.08, 0.04, 0.05]\n",
      "3, Dates: [86], Probs: [0.04]\n",
      "4, Dates: [91, 96, 101, 106], Probs: [0.07, 0.14, 0.12, 0.17]\n",
      "5, Dates: [], Probs: []\n",
      "6, Dates: [156], Probs: [0.16]\n",
      "7, Dates: [206], Probs: [0.09]\n",
      "8, Dates: [236], Probs: [0.04]\n",
      "9, Dates: [261, 271], Probs: [0.14, 0.1]\n",
      "10, Dates: [281], Probs: [0.12]\n",
      "11, Dates: [-33, 311, 316], Probs: [0.07, 0.07, 0.05]\n",
      "12, Dates: [336, 356], Probs: [0.12, 0.0]\n",
      "1, Dates: [22], Probs: [0.05]\n",
      "2, Dates: [32], Probs: [0.11]\n",
      "3, Dates: [86], Probs: [0.04]\n",
      "4, Dates: [91], Probs: [0.07]\n",
      "5, Dates: [], Probs: []\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [206], Probs: [0.09]\n",
      "8, Dates: [236], Probs: [0.04]\n",
      "9, Dates: [271], Probs: [0.1]\n",
      "10, Dates: [281], Probs: [0.12]\n",
      "11, Dates: [316], Probs: [0.05]\n",
      "12, Dates: [356], Probs: [0.0]\n",
      "The original max value is 20499\n",
      "Original 20 meter bands size: (10, 16, 16, 6), using 0.01953125 PU\n",
      "The original L2A image size is: (10, 32, 32, 4)\n",
      "The original max value is 20460\n",
      "Shadows (10, 96, 96), clouds (10, 96, 96), S2, (10, 32, 32, 10), S2d, (10,)\n",
      "(10, 32, 32, 10)\n",
      "uploading ../data/train-raw/220039.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/220039.npy\n",
      "100 %\n",
      "\n",
      "\n",
      "Downloading 2/24, 220038\n",
      "Shadows ((31, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [17, 22, 366, 371, 386], Probs: [0.06, 0.03, 0.05, 0.03, 0.13]\n",
      "2, Dates: [32, 37, 42, 52, 396, 401, 406], Probs: [0.08, 0.14, 0.1, 0.09, 0.06, 0.02, 0.04]\n",
      "3, Dates: [71, 86], Probs: [0.14, 0.03]\n",
      "4, Dates: [91, 96, 101, 106], Probs: [0.05, 0.11, 0.1, 0.14]\n",
      "5, Dates: [], Probs: []\n",
      "6, Dates: [156], Probs: [0.11]\n",
      "7, Dates: [206], Probs: [0.06]\n",
      "8, Dates: [236], Probs: [0.03]\n",
      "9, Dates: [261, 271], Probs: [0.03, 0.06]\n",
      "10, Dates: [281], Probs: [0.06]\n",
      "11, Dates: [-43, -33, 311, 316], Probs: [0.11, 0.04, 0.04, 0.02]\n",
      "12, Dates: [-28, 336, 356], Probs: [0.14, 0.08, 0.0]\n",
      "1, Dates: [22], Probs: [0.03]\n",
      "2, Dates: [42], Probs: [0.1]\n",
      "3, Dates: [86], Probs: [0.03]\n",
      "4, Dates: [], Probs: []\n",
      "5, Dates: [], Probs: []\n",
      "6, Dates: [156], Probs: [0.11]\n",
      "7, Dates: [206], Probs: [0.06]\n",
      "8, Dates: [236], Probs: [0.03]\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [281], Probs: [0.06]\n",
      "11, Dates: [316], Probs: [0.02]\n",
      "12, Dates: [356], Probs: [0.0]\n",
      "The original max value is 20604\n",
      "Original 20 meter bands size: (9, 16, 16, 6), using 0.017578125 PU\n",
      "The original L2A image size is: (9, 32, 32, 4)\n",
      "The original max value is 19582\n",
      "Shadows (9, 96, 96), clouds (9, 96, 96), S2, (9, 32, 32, 10), S2d, (9,)\n",
      "(9, 32, 32, 10)\n",
      "uploading ../data/train-raw/220038.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/220038.npy\n",
      "\n",
      "\n",
      "Downloading 3/24, 220037\n",
      "Shadows ((28, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [15, 366, 371, 394], Probs: [0.16, 0.04, 0.13, 0.18]\n",
      "2, Dates: [32, 37, 57, 399], Probs: [0.09, 0.11, 0.06, 0.1]\n",
      "3, Dates: [84, 89], Probs: [0.08, 0.16]\n",
      "4, Dates: [94, 99, 109], Probs: [0.23, 0.22, 0.13]\n",
      "5, Dates: [129, 134], Probs: [0.14, 0.18]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [274, 281, 286], Probs: [0.19, 0.09, 0.06]\n",
      "11, Dates: [-33, 306, 311, 316], Probs: [0.14, 0.09, 0.03, 0.02]\n",
      "12, Dates: [-28, -3, 334, 351, 356, 361], Probs: [0.2, 0.11, 0.13, 0.12, 0.18, 0.11]\n",
      "1, Dates: [], Probs: []\n",
      "2, Dates: [32], Probs: [0.09]\n",
      "3, Dates: [84], Probs: [0.08]\n",
      "4, Dates: [109], Probs: [0.13]\n",
      "5, Dates: [129], Probs: [0.14]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [286], Probs: [0.06]\n",
      "11, Dates: [316], Probs: [0.02]\n",
      "12, Dates: [-3, 351], Probs: [0.11, 0.12]\n",
      "The original max value is 27774\n",
      "Original 20 meter bands size: (8, 16, 16, 6), using 0.015625 PU\n",
      "The original L2A image size is: (8, 32, 32, 4)\n",
      "The original max value is 29274\n",
      "Shadows (8, 96, 96), clouds (8, 96, 96), S2, (8, 32, 32, 10), S2d, (8,)\n",
      "(8, 32, 32, 10)\n",
      "uploading ../data/train-raw/220037.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/220037.npy\n",
      "\n",
      "\n",
      "Downloading 4/24, 220036\n",
      "Shadows ((25, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [15, 366, 371], Probs: [0.17, 0.05, 0.17]\n",
      "2, Dates: [32, 37, 57, 399], Probs: [0.1, 0.15, 0.07, 0.14]\n",
      "3, Dates: [84, 89], Probs: [0.08, 0.14]\n",
      "4, Dates: [94, 99, 109], Probs: [0.21, 0.21, 0.12]\n",
      "5, Dates: [129, 134], Probs: [0.14, 0.22]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [259], Probs: [0.11]\n",
      "10, Dates: [281, 286], Probs: [0.06, 0.07]\n",
      "11, Dates: [-33, 306, 311, 316], Probs: [0.13, 0.12, 0.03, 0.03]\n",
      "12, Dates: [-3, 334, 351, 361], Probs: [0.1, 0.16, 0.15, 0.13]\n",
      "1, Dates: [], Probs: []\n",
      "2, Dates: [32], Probs: [0.1]\n",
      "3, Dates: [84], Probs: [0.08]\n",
      "4, Dates: [109], Probs: [0.12]\n",
      "5, Dates: [129], Probs: [0.14]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [259], Probs: [0.11]\n",
      "10, Dates: [286], Probs: [0.07]\n",
      "11, Dates: [316], Probs: [0.03]\n",
      "12, Dates: [-3, 351], Probs: [0.1, 0.15]\n",
      "The original max value is 32951\n",
      "Original 20 meter bands size: (9, 16, 16, 6), using 0.017578125 PU\n",
      "The original L2A image size is: (9, 32, 32, 4)\n",
      "The original max value is 32315\n",
      "Shadows (9, 96, 96), clouds (9, 96, 96), S2, (9, 32, 32, 10), S2d, (9,)\n",
      "(9, 32, 32, 10)\n",
      "uploading ../data/train-raw/220036.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/220036.npy\n",
      "400 %\n",
      "\n",
      "\n",
      "Downloading 5/24, 220035\n",
      "Shadows ((25, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [15, 366, 371], Probs: [0.18, 0.05, 0.19]\n",
      "2, Dates: [32, 37, 57, 399], Probs: [0.11, 0.15, 0.1, 0.2]\n",
      "3, Dates: [84, 89], Probs: [0.1, 0.17]\n",
      "4, Dates: [94, 99, 109], Probs: [0.23, 0.23, 0.08]\n",
      "5, Dates: [129, 134], Probs: [0.16, 0.25]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [281, 286, 294], Probs: [0.21, 0.12, 0.09]\n",
      "11, Dates: [-33, 306, 311, 316], Probs: [0.15, 0.11, 0.02, 0.03]\n",
      "12, Dates: [-3, 334, 351, 361], Probs: [0.1, 0.16, 0.18, 0.14]\n",
      "1, Dates: [], Probs: []\n",
      "2, Dates: [32], Probs: [0.11]\n",
      "3, Dates: [84], Probs: [0.1]\n",
      "4, Dates: [109], Probs: [0.08]\n",
      "5, Dates: [129], Probs: [0.16]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [294], Probs: [0.09]\n",
      "11, Dates: [316], Probs: [0.03]\n",
      "12, Dates: [-3, 334], Probs: [0.1, 0.16]\n",
      "The original max value is 29825\n",
      "Original 20 meter bands size: (8, 16, 16, 6), using 0.015625 PU\n",
      "The original L2A image size is: (8, 32, 32, 4)\n",
      "The original max value is 31732\n",
      "Shadows (8, 96, 96), clouds (8, 96, 96), S2, (8, 32, 32, 10), S2d, (8,)\n",
      "(8, 32, 32, 10)\n",
      "uploading ../data/train-raw/220035.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/220035.npy\n",
      "\n",
      "\n",
      "Downloading 6/24, 220034\n",
      "Shadows ((27, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [15, 366, 371], Probs: [0.17, 0.04, 0.17]\n",
      "2, Dates: [32, 37, 57, 399], Probs: [0.1, 0.15, 0.08, 0.2]\n",
      "3, Dates: [84, 89], Probs: [0.1, 0.16]\n",
      "4, Dates: [94, 99, 109], Probs: [0.23, 0.23, 0.08]\n",
      "5, Dates: [129, 134], Probs: [0.16, 0.25]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [206], Probs: [0.14]\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [259], Probs: [0.14]\n",
      "10, Dates: [281, 286, 294], Probs: [0.12, 0.1, 0.13]\n",
      "11, Dates: [-33, 306, 311, 316], Probs: [0.14, 0.1, 0.02, 0.03]\n",
      "12, Dates: [-3, 334, 351, 361], Probs: [0.1, 0.15, 0.15, 0.12]\n",
      "1, Dates: [], Probs: []\n",
      "2, Dates: [32], Probs: [0.1]\n",
      "3, Dates: [84], Probs: [0.1]\n",
      "4, Dates: [109], Probs: [0.08]\n",
      "5, Dates: [], Probs: []\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [206], Probs: [0.14]\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [259], Probs: [0.14]\n",
      "10, Dates: [286], Probs: [0.1]\n",
      "11, Dates: [316], Probs: [0.03]\n",
      "12, Dates: [-3, 351], Probs: [0.1, 0.15]\n",
      "The original max value is 30008\n",
      "Original 20 meter bands size: (9, 16, 16, 6), using 0.017578125 PU\n",
      "The original L2A image size is: (9, 32, 32, 4)\n",
      "The original max value is 29825\n",
      "Shadows (9, 96, 96), clouds (9, 96, 96), S2, (9, 32, 32, 10), S2d, (9,)\n",
      "(9, 32, 32, 10)\n",
      "uploading ../data/train-raw/220034.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/220034.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Downloading 7/24, 220033\n",
      "Shadows ((25, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [366, 371], Probs: [0.05, 0.17]\n",
      "2, Dates: [32, 37, 57, 399], Probs: [0.09, 0.14, 0.06, 0.14]\n",
      "3, Dates: [84, 89], Probs: [0.06, 0.12]\n",
      "4, Dates: [94, 99, 109], Probs: [0.18, 0.19, 0.1]\n",
      "5, Dates: [129, 134], Probs: [0.13, 0.2]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [206], Probs: [0.18]\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [259], Probs: [0.12]\n",
      "10, Dates: [281, 286], Probs: [0.06, 0.07]\n",
      "11, Dates: [-33, 306, 311, 316], Probs: [0.13, 0.1, 0.02, 0.03]\n",
      "12, Dates: [-3, 334, 351, 361], Probs: [0.11, 0.14, 0.15, 0.13]\n",
      "1, Dates: [], Probs: []\n",
      "2, Dates: [32], Probs: [0.09]\n",
      "3, Dates: [84], Probs: [0.06]\n",
      "4, Dates: [109], Probs: [0.1]\n",
      "5, Dates: [129], Probs: [0.13]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [259], Probs: [0.12]\n",
      "10, Dates: [286], Probs: [0.07]\n",
      "11, Dates: [316], Probs: [0.03]\n",
      "12, Dates: [-3, 334], Probs: [0.11, 0.14]\n",
      "The original max value is 31057\n",
      "Original 20 meter bands size: (9, 16, 16, 6), using 0.017578125 PU\n",
      "The original L2A image size is: (9, 32, 32, 4)\n",
      "The original max value is 31050\n",
      "Shadows (9, 96, 96), clouds (9, 96, 96), S2, (9, 32, 32, 10), S2d, (9,)\n",
      "(9, 32, 32, 10)\n",
      "uploading ../data/train-raw/220033.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/220033.npy\n",
      "\n",
      "\n",
      "Downloading 8/24, 220032\n",
      "Shadows ((23, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [15, 366, 371], Probs: [0.14, 0.07, 0.19]\n",
      "2, Dates: [32, 37, 57, 399], Probs: [0.09, 0.17, 0.12, 0.18]\n",
      "3, Dates: [79, 84, 89], Probs: [0.21, 0.11, 0.16]\n",
      "4, Dates: [94, 109], Probs: [0.22, 0.07]\n",
      "5, Dates: [129], Probs: [0.17]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [246], Probs: [0.05]\n",
      "10, Dates: [281, 286], Probs: [0.12, 0.08]\n",
      "11, Dates: [-33, 306, 311, 316], Probs: [0.08, 0.09, 0.02, 0.02]\n",
      "12, Dates: [-3, 334, 361], Probs: [0.09, 0.14, 0.14]\n",
      "1, Dates: [], Probs: []\n",
      "2, Dates: [32], Probs: [0.09]\n",
      "3, Dates: [84], Probs: [0.11]\n",
      "4, Dates: [109], Probs: [0.07]\n",
      "5, Dates: [129], Probs: [0.17]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [246], Probs: [0.05]\n",
      "10, Dates: [286], Probs: [0.08]\n",
      "11, Dates: [316], Probs: [0.02]\n",
      "12, Dates: [-3, 334], Probs: [0.09, 0.14]\n",
      "The original max value is 32761\n",
      "Original 20 meter bands size: (9, 16, 16, 6), using 0.017578125 PU\n",
      "The original L2A image size is: (9, 32, 32, 4)\n",
      "The original max value is 37532\n",
      "Shadows (9, 96, 96), clouds (9, 96, 96), S2, (9, 32, 32, 10), S2d, (9,)\n",
      "(9, 32, 32, 10)\n",
      "uploading ../data/train-raw/220032.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/220032.npy\n",
      "\n",
      "\n",
      "Downloading 9/24, 220031\n",
      "Shadows ((27, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [15, 366, 371], Probs: [0.18, 0.06, 0.21]\n",
      "2, Dates: [32, 37, 42, 57, 399], Probs: [0.1, 0.16, 0.23, 0.14, 0.21]\n",
      "3, Dates: [79, 84, 89], Probs: [0.24, 0.1, 0.16]\n",
      "4, Dates: [94, 109], Probs: [0.23, 0.05]\n",
      "5, Dates: [129, 134], Probs: [0.13, 0.17]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [206], Probs: [0.12]\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [246], Probs: [0.05]\n",
      "10, Dates: [274, 281, 286], Probs: [0.18, 0.07, 0.08]\n",
      "11, Dates: [-33, 306, 311, 316], Probs: [0.12, 0.12, 0.02, 0.02]\n",
      "12, Dates: [-3, 334, 361], Probs: [0.1, 0.15, 0.14]\n",
      "1, Dates: [], Probs: []\n",
      "2, Dates: [32], Probs: [0.1]\n",
      "3, Dates: [84], Probs: [0.1]\n",
      "4, Dates: [109], Probs: [0.05]\n",
      "5, Dates: [129], Probs: [0.13]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [206], Probs: [0.12]\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [246], Probs: [0.05]\n",
      "10, Dates: [286], Probs: [0.08]\n",
      "11, Dates: [316], Probs: [0.02]\n",
      "12, Dates: [-3, 361], Probs: [0.1, 0.14]\n",
      "The original max value is 30448\n",
      "Original 20 meter bands size: (10, 16, 16, 6), using 0.01953125 PU\n",
      "The original L2A image size is: (10, 32, 32, 4)\n",
      "The original max value is 31640\n",
      "Shadows (10, 96, 96), clouds (10, 96, 96), S2, (10, 32, 32, 10), S2d, (10,)\n",
      "(10, 32, 32, 10)\n",
      "uploading ../data/train-raw/220031.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/220031.npy\n",
      "810 %\n",
      "\n",
      "\n",
      "Downloading 10/24, 220030\n",
      "Shadows ((26, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [15, 366, 371], Probs: [0.15, 0.05, 0.19]\n",
      "2, Dates: [32, 37, 57, 399], Probs: [0.09, 0.13, 0.1, 0.15]\n",
      "3, Dates: [79, 84, 89], Probs: [0.23, 0.1, 0.16]\n",
      "4, Dates: [94, 109], Probs: [0.22, 0.07]\n",
      "5, Dates: [129, 134], Probs: [0.12, 0.18]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [206], Probs: [0.15]\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [246], Probs: [0.05]\n",
      "10, Dates: [281, 286], Probs: [0.06, 0.07]\n",
      "11, Dates: [-33, 306, 311, 316], Probs: [0.1, 0.1, 0.02, 0.02]\n",
      "12, Dates: [-28, -3, 334, 361], Probs: [0.18, 0.1, 0.15, 0.11]\n",
      "1, Dates: [], Probs: []\n",
      "2, Dates: [32], Probs: [0.09]\n",
      "3, Dates: [84], Probs: [0.1]\n",
      "4, Dates: [109], Probs: [0.07]\n",
      "5, Dates: [129], Probs: [0.12]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [246], Probs: [0.05]\n",
      "10, Dates: [286], Probs: [0.07]\n",
      "11, Dates: [316], Probs: [0.02]\n",
      "12, Dates: [-3, 361], Probs: [0.1, 0.11]\n",
      "The original max value is 32866\n",
      "Original 20 meter bands size: (9, 16, 16, 6), using 0.017578125 PU\n",
      "The original L2A image size is: (9, 32, 32, 4)\n",
      "The original max value is 33659\n",
      "Shadows (9, 96, 96), clouds (9, 96, 96), S2, (9, 32, 32, 10), S2d, (9,)\n",
      "(9, 32, 32, 10)\n",
      "uploading ../data/train-raw/220030.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/220030.npy\n",
      "1000 %\n",
      "\n",
      "\n",
      "Downloading 11/24, 220026\n",
      "Shadows ((10, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [26, 395], Probs: [0.09, 0.16]\n",
      "2, Dates: [36, 46], Probs: [0.11, 0.09]\n",
      "3, Dates: [60], Probs: [0.1]\n",
      "4, Dates: [90], Probs: [0.14]\n",
      "5, Dates: [150], Probs: [0.04]\n",
      "6, Dates: [160], Probs: [0.1]\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [], Probs: []\n",
      "11, Dates: [-34, 320], Probs: [0.15, 0.08]\n",
      "12, Dates: [], Probs: []\n",
      "1, Dates: [26, 395], Probs: [0.09, 0.16]\n",
      "2, Dates: [46], Probs: [0.09]\n",
      "3, Dates: [60], Probs: [0.1]\n",
      "4, Dates: [90], Probs: [0.14]\n",
      "5, Dates: [150], Probs: [0.04]\n",
      "6, Dates: [160], Probs: [0.1]\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [], Probs: []\n",
      "11, Dates: [320], Probs: [0.08]\n",
      "12, Dates: [], Probs: []\n",
      "The original max value is 27479\n",
      "Original 20 meter bands size: (8, 16, 16, 6), using 0.015625 PU\n",
      "The original L2A image size is: (8, 32, 32, 4)\n",
      "The original max value is 27184\n",
      "Shadows (8, 96, 96), clouds (8, 96, 96), S2, (8, 32, 32, 10), S2d, (8,)\n",
      "(8, 32, 32, 10)\n",
      "uploading ../data/train-raw/220026.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/220026.npy\n",
      "\n",
      "\n",
      "Downloading 12/24, 220021\n",
      "Shadows ((1, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [], Probs: []\n",
      "2, Dates: [], Probs: []\n",
      "3, Dates: [60], Probs: [0.21]\n",
      "4, Dates: [], Probs: []\n",
      "5, Dates: [], Probs: []\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [], Probs: []\n",
      "11, Dates: [], Probs: []\n",
      "12, Dates: [], Probs: []\n",
      "1, Dates: [], Probs: []\n",
      "2, Dates: [], Probs: []\n",
      "3, Dates: [], Probs: []\n",
      "4, Dates: [], Probs: []\n",
      "5, Dates: [], Probs: []\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [], Probs: []\n",
      "11, Dates: [], Probs: []\n",
      "12, Dates: [], Probs: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:need at least one array to stack\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-8-cdb85e55c715>\", line 43, in download_layer\n",
      "    img_20 = np.stack(img_bands)\n",
      "  File \"<__array_function__ internals>\", line 6, in stack\n",
      "  File \"/Users/jbrandt.terminal/opt/anaconda3/envs/tf/lib/python3.7/site-packages/numpy/core/shape_base.py\", line 422, in stack\n",
      "    raise ValueError('need at least one array to stack')\n",
      "ValueError: need at least one array to stack\n",
      "CRITICAL:root:cannot unpack non-iterable NoneType object\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-11-c86e3e63af6b>\", line 65, in download_raw_data\n",
      "    s2, s2_dates = download_layer(sentinel2_bbx, clean_steps = clean_dates, epsg = epsg)\n",
      "TypeError: cannot unpack non-iterable NoneType object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot unpack non-iterable NoneType object\n",
      "Downloading 13/24, 220017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:tuple index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-11-c86e3e63af6b>\", line 48, in download_raw_data\n",
      "    cloud_probs, shadows, _, clean_dates = identify_clouds(cloud_bbx, epsg = epsg)\n",
      "  File \"<ipython-input-6-ded57245c76e>\", line 55, in identify_clouds\n",
      "    shadow_pus = (shadow_img.shape[1]*shadow_img.shape[2])/(512*512) * shadow_img.shape[0]\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuple index out of range\n",
      "Downloading 14/24, 220016\n",
      "Shadows ((6, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [6], Probs: [0.15]\n",
      "2, Dates: [36, 46], Probs: [0.18, 0.17]\n",
      "3, Dates: [60], Probs: [0.18]\n",
      "4, Dates: [], Probs: []\n",
      "5, Dates: [], Probs: []\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [300], Probs: [0.16]\n",
      "11, Dates: [], Probs: []\n",
      "12, Dates: [-4], Probs: [0.17]\n",
      "1, Dates: [6], Probs: [0.15]\n",
      "2, Dates: [36, 46], Probs: [0.18, 0.17]\n",
      "3, Dates: [60], Probs: [0.18]\n",
      "4, Dates: [], Probs: []\n",
      "5, Dates: [], Probs: []\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [300], Probs: [0.16]\n",
      "11, Dates: [], Probs: []\n",
      "12, Dates: [-4], Probs: [0.17]\n",
      "The original max value is 31870\n",
      "Original 20 meter bands size: (6, 16, 16, 6), using 0.01171875 PU\n",
      "The original L2A image size is: (6, 32, 32, 4)\n",
      "The original max value is 34786\n",
      "Shadows (6, 96, 96), clouds (6, 96, 96), S2, (6, 32, 32, 10), S2d, (6,)\n",
      "(6, 32, 32, 10)\n",
      "uploading ../data/train-raw/220016.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/220016.npy\n",
      "\n",
      "\n",
      "Downloading 15/24, 220015\n",
      "Shadows ((15, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [1, 6, 26], Probs: [0.2, 0.21, 0.24]\n",
      "2, Dates: [46, 56, 400], Probs: [0.17, 0.19, 0.2]\n",
      "3, Dates: [60], Probs: [0.17]\n",
      "4, Dates: [90, 100, 110], Probs: [0.22, 0.16, 0.11]\n",
      "5, Dates: [145, 150], Probs: [0.16, 0.12]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [], Probs: []\n",
      "11, Dates: [320], Probs: [0.17]\n",
      "12, Dates: [-19, -4], Probs: [0.15, 0.12]\n",
      "1, Dates: [], Probs: []\n",
      "2, Dates: [46, 400], Probs: [0.17, 0.2]\n",
      "3, Dates: [60], Probs: [0.17]\n",
      "4, Dates: [110], Probs: [0.11]\n",
      "5, Dates: [150], Probs: [0.12]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [], Probs: []\n",
      "11, Dates: [320], Probs: [0.17]\n",
      "12, Dates: [-4], Probs: [0.12]\n",
      "The original max value is 39242\n",
      "Original 20 meter bands size: (7, 16, 16, 6), using 0.013671875 PU\n",
      "The original L2A image size is: (7, 32, 32, 4)\n",
      "The original max value is 32302\n",
      "Shadows (7, 96, 96), clouds (7, 96, 96), S2, (7, 32, 32, 10), S2d, (7,)\n",
      "(7, 32, 32, 10)\n",
      "uploading ../data/train-raw/220015.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/220015.npy\n",
      "1585 %\n",
      "\n",
      "\n",
      "Downloading 16/24, 220014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:tuple index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-11-c86e3e63af6b>\", line 48, in download_raw_data\n",
      "    cloud_probs, shadows, _, clean_dates = identify_clouds(cloud_bbx, epsg = epsg)\n",
      "  File \"<ipython-input-6-ded57245c76e>\", line 55, in identify_clouds\n",
      "    shadow_pus = (shadow_img.shape[1]*shadow_img.shape[2])/(512*512) * shadow_img.shape[0]\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuple index out of range\n",
      "Downloading 17/24, 220013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:tuple index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-11-c86e3e63af6b>\", line 48, in download_raw_data\n",
      "    cloud_probs, shadows, _, clean_dates = identify_clouds(cloud_bbx, epsg = epsg)\n",
      "  File \"<ipython-input-6-ded57245c76e>\", line 55, in identify_clouds\n",
      "    shadow_pus = (shadow_img.shape[1]*shadow_img.shape[2])/(512*512) * shadow_img.shape[0]\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuple index out of range\n",
      "Downloading 18/24, 220010\n",
      "Shadows ((1, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [], Probs: []\n",
      "2, Dates: [], Probs: []\n",
      "3, Dates: [], Probs: []\n",
      "4, Dates: [], Probs: []\n",
      "5, Dates: [150], Probs: [0.18]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [], Probs: []\n",
      "11, Dates: [], Probs: []\n",
      "12, Dates: [], Probs: []\n",
      "1, Dates: [], Probs: []\n",
      "2, Dates: [], Probs: []\n",
      "3, Dates: [], Probs: []\n",
      "4, Dates: [], Probs: []\n",
      "5, Dates: [150], Probs: [0.18]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [], Probs: []\n",
      "11, Dates: [], Probs: []\n",
      "12, Dates: [], Probs: []\n",
      "The original max value is 38469\n",
      "Original 20 meter bands size: (1, 16, 16, 6), using 0.001953125 PU\n",
      "The original L2A image size is: (1, 32, 32, 4)\n",
      "The original max value is 35572\n",
      "Shadows (1, 96, 96), clouds (1, 96, 96), S2, (1, 32, 32, 10), S2d, (1,)\n",
      "(1, 32, 32, 10)\n",
      "uploading ../data/train-raw/220010.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/220010.npy\n",
      "\n",
      "\n",
      "Downloading 19/24, 22006\n",
      "Shadows ((2, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [], Probs: []\n",
      "2, Dates: [], Probs: []\n",
      "3, Dates: [60], Probs: [0.19]\n",
      "4, Dates: [], Probs: []\n",
      "5, Dates: [], Probs: []\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [], Probs: []\n",
      "11, Dates: [], Probs: []\n",
      "12, Dates: [360], Probs: [0.19]\n",
      "1, Dates: [], Probs: []\n",
      "2, Dates: [], Probs: []\n",
      "3, Dates: [60], Probs: [0.19]\n",
      "4, Dates: [], Probs: []\n",
      "5, Dates: [], Probs: []\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [], Probs: []\n",
      "8, Dates: [], Probs: []\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [], Probs: []\n",
      "11, Dates: [], Probs: []\n",
      "12, Dates: [360], Probs: [0.19]\n",
      "The original max value is 27767\n",
      "Original 20 meter bands size: (2, 16, 16, 6), using 0.00390625 PU\n",
      "The original L2A image size is: (2, 32, 32, 4)\n",
      "The original max value is 26070\n",
      "Shadows (2, 96, 96), clouds (2, 96, 96), S2, (2, 32, 32, 10), S2d, (2,)\n",
      "(2, 32, 32, 10)\n",
      "uploading ../data/train-raw/22006.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/22006.npy\n",
      "\n",
      "\n",
      "Downloading 20/24, 22005\n",
      "Shadows ((20, 96, 96)) used 0.0 processing units\n",
      "1, Dates: [1, 6, 26, 380, 395], Probs: [0.01, 0.0, 0.05, 0.0, 0.06]\n",
      "2, Dates: [36, 41], Probs: [0.0, 0.05]\n",
      "3, Dates: [60, 75], Probs: [0.01, 0.03]\n",
      "4, Dates: [100, 105], Probs: [0.0, 0.01]\n",
      "5, Dates: [125, 145, 150], Probs: [0.05, 0.05, 0.0]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [185], Probs: [0.03]\n",
      "8, Dates: [240], Probs: [0.09]\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [275], Probs: [0.1]\n",
      "11, Dates: [-44, 320], Probs: [0.03, 0.01]\n",
      "12, Dates: [-4], Probs: [0.0]\n",
      "1, Dates: [6, 395], Probs: [0.0, 0.06]\n",
      "2, Dates: [41], Probs: [0.05]\n",
      "3, Dates: [75], Probs: [0.03]\n",
      "4, Dates: [105], Probs: [0.01]\n",
      "5, Dates: [145], Probs: [0.05]\n",
      "6, Dates: [], Probs: []\n",
      "7, Dates: [185], Probs: [0.03]\n",
      "8, Dates: [240], Probs: [0.09]\n",
      "9, Dates: [], Probs: []\n",
      "10, Dates: [275], Probs: [0.1]\n",
      "11, Dates: [320], Probs: [0.01]\n",
      "12, Dates: [], Probs: []\n",
      "The original max value is 13900\n",
      "Original 20 meter bands size: (10, 16, 16, 6), using 0.01953125 PU\n",
      "The original L2A image size is: (10, 32, 32, 4)\n",
      "The original max value is 25952\n",
      "Shadows (10, 96, 96), clouds (10, 96, 96), S2, (10, 32, 32, 10), S2d, (10,)\n",
      "(10, 32, 32, 10)\n",
      "uploading ../data/train-raw/22005.npy to restoration-monitoring as restoration-mapper/model-data/train/raw/22005.npy\n",
      "\n",
      "\n",
      "Downloading 21/24, 22004\n"
     ]
    }
   ],
   "source": [
    "for i in (os.listdir(\"../data/train-csv/\")):\n",
    "    if \"ceo-asia\" in i:\n",
    "        download_raw_data(\"../data/train-csv/\" + i,\n",
    "                          \"../data/train-raw/\", \n",
    "                          fmt = 'train',\n",
    "                          image_format = MimeType.TIFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process train / test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "plots = [str(x[:-4]) for x in os.listdir(\"../data/train-raw/\") if \".npy\" in x]\n",
    "for plot in plots:\n",
    "    i += 1\n",
    "    if not os.path.exists(\"../data/train-s2/\" + plot + \".hkl\"):\n",
    "        print(plot)\n",
    "        try:\n",
    "            tiles = process_raw(plot, path = 'train')\n",
    "            print(i, plot)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
