{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process sentinel 2 data\n",
    "\n",
    "## John Brandt\n",
    "## December 2, 2020\n",
    "\n",
    "## Package imports, API import, source scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from collections import Counter\n",
    "from osgeo import ogr, osr\n",
    "from random import shuffle\n",
    "from scipy.sparse.linalg import splu\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType\n",
    "from sentinelhub import CRS, BBox, constants, DataSource, CustomUrlParam\n",
    "from skimage.transform import resize\n",
    "from typing import Tuple, List\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key'] \n",
    "        \n",
    "%matplotlib inline\n",
    "%run ../src/preprocessing/slope.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/downloading/utils.py\n",
    "%run ../src/preprocessing/cloud_removal.py\n",
    "%run ../src/preprocessing/whittaker_smoother.py\n",
    "%run ../src/downloading/io.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../config.yaml\"):\n",
    "    with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key']\n",
    "        AWSKEY = key['awskey']\n",
    "        AWSSECRET = key['awssecret']\n",
    "else:\n",
    "    API_KEY = \"none\"\n",
    "    \n",
    "\n",
    "uploader = FileUploader(awskey = AWSKEY, awssecret = AWSSECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "YEAR = 2020\n",
    "TIME = ('{}-11-15'.format(str(YEAR - 1)), '{}-02-15'.format(str(YEAR + 1)))\n",
    "EPSG = CRS.WGS84\n",
    "IMSIZE = http://localhost:8888/?token=714d2fba8626932ef5e82d86654f2875f05f66ad664d9b3c\n",
    "\n",
    "# Constants\n",
    "starting_days = np.cumsum([0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding boxes\n",
    "\n",
    "These cell blocks calculate the min_x, min_y, max_x, max_y of the area of interest (AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bbox(plot_id: int, df: \"DataFrame\") -> List:\n",
    "    \"\"\" Calculates the corners of a bounding box from an input\n",
    "        pandas dataframe as output by Collect Earth Online\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (int): plot_id of associated plot\n",
    "         df (pandas.DataFrame): dataframe of associated CEO survey\n",
    "    \n",
    "        Returns:\n",
    "         bounding_box (list): [(min(x), min(y)),\n",
    "                              (max(x), max_y))]\n",
    "    \"\"\"\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])),\n",
    "            (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "\n",
    "def bounding_box(points: List[Tuple[float, float]], \n",
    "                 expansion: int = 160) -> ((Tuple, Tuple), str):\n",
    "    \"\"\" Calculates the corners of a bounding box with an\n",
    "        input expansion in meters from a given bounding_box\n",
    "        \n",
    "        Subcalls:\n",
    "         calculate_epsg, convertCoords\n",
    "\n",
    "        Parameters:\n",
    "         points (list): output of calc_bbox\n",
    "         expansion (float): number of meters to expand or shrink the\n",
    "                            points edges to be\n",
    "    \n",
    "        Returns:\n",
    "         bl (tuple): x, y of bottom left corner with edges of expansion meters\n",
    "         tr (tuple): x, y of top right corner with edges of expansion meters\n",
    "    \"\"\"\n",
    "    bl = list(points[0])\n",
    "    tr = list(points[1])\n",
    "    inproj = Proj('epsg:4326')\n",
    "    outproj_code = calculate_epsg(bl)\n",
    "    outproj = Proj('epsg:' + str(outproj_code))\n",
    "    bl_utm =  transform(inproj, outproj, bl[1], bl[0])\n",
    "    tr_utm =  transform(inproj, outproj, tr[1], tr[0])\n",
    "\n",
    "    distance1 = tr_utm[0] - bl_utm[0]\n",
    "    distance2 = tr_utm[1] - bl_utm[1]\n",
    "    expansion1 = (expansion - distance1)/2\n",
    "    expansion2 = (expansion - distance2)/2\n",
    "        \n",
    "    bl_utm = [bl_utm[0] - expansion1, bl_utm[1] - expansion2]\n",
    "    tr_utm = [tr_utm[0] + expansion1, tr_utm[1] + expansion2]\n",
    "\n",
    "    zone = str(outproj_code)[3:]\n",
    "    zone = zone[1:] if zone[0] == \"0\" else zone\n",
    "    direction = 'N' if tr[1] >= 0 else 'S'\n",
    "    utm_epsg = \"UTM_\" + zone + direction\n",
    "    return (bl_utm, tr_utm), CRS[utm_epsg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(date_dict: dict, year: int) -> List:\n",
    "    \"\"\" Transforms a SentinelHub date dictionary to a\n",
    "         list of integer calendar dates\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "    starting_days = np.cumsum(days_per_month)\n",
    "    for date in date_dict:\n",
    "        if date.year == year - 1:\n",
    "            dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    return dates\n",
    "\n",
    "def to_float32(array: np.array) -> np.array:\n",
    "    \"\"\"Converts an int_x array to float32\"\"\"\n",
    "    print(f'The original max value is {np.max(array)}')\n",
    "    if not isinstance(array.flat[0], np.floating):\n",
    "        assert np.max(array) > 1\n",
    "        array = np.float32(array) / 65535.\n",
    "    assert np.max(array) <= 1\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud and cloud shadow\n",
    "\n",
    "This cell block identifies clouds using s2Cloudless, and identifies clouds (and shadows) using Candra et al. 2019.\n",
    "\n",
    "The output is per-px cloud/shadow masks, and a list of sentinel 2 dates to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_clouds(bbox: List[Tuple[float, float]], epsg: 'CRS', time: dict = TIME):\n",
    "    \"\"\" Downloads and calculates cloud cover and shadow\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         cloud_img (np.array): (X, 96, 96) array of cloud probs\n",
    "         shadows (np.array):  (X, 96, 96) array of shadow binary\n",
    "         clean_steps (np.array): (N,) array of clean idx\n",
    "         cloud_dates (np.array): (N,) array of clean cloud datets\n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    cloud_request = WcsRequest(\n",
    "        layer='CLOUD_NEW',\n",
    "        bbox=box, time=time,\n",
    "        resx='160m', resy='160m',\n",
    "        image_format = MimeType.TIFF_d8,\n",
    "        maxcc=0.75, instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=96))\n",
    "\n",
    "    shadow_request = WcsRequest(\n",
    "        layer='SHADOW',\n",
    "        bbox=box, time=time,\n",
    "        resx='60m', resy='60m',\n",
    "        image_format =  MimeType.TIFF_d16,\n",
    "        maxcc=0.75, instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=96))\n",
    "\n",
    "    cloud_img = np.array(cloud_request.get_data())\n",
    "    if not isinstance(cloud_img.flat[0], np.floating):\n",
    "        assert np.max(cloud_img) > 1\n",
    "        cloud_img = cloud_img / 255.\n",
    "    assert np.max(cloud_img) <= 1\n",
    "\n",
    "    cloud_img = resize(cloud_img, (cloud_img.shape[0], 96, 96), order = 0)\n",
    "    n_cloud_px = np.sum(cloud_img > 0.33, axis = (1, 2))\n",
    "    cloud_steps = np.argwhere(n_cloud_px > (96**2 * 0.15))\n",
    "    clean_steps = [x for x in range(cloud_img.shape[0]) if x not in cloud_steps]\n",
    "    \n",
    "    \n",
    "    cloud_dates_dict = [x for x in cloud_request.get_dates()]\n",
    "    cloud_dates = extract_dates(cloud_dates_dict, YEAR)\n",
    "    cloud_dates = [val for idx, val in enumerate(cloud_dates) if idx in clean_steps]\n",
    "    \n",
    "    shadow_dates_dict = [x for x in shadow_request.get_dates()]\n",
    "    shadow_dates = extract_dates(shadow_dates_dict, YEAR)\n",
    "    shadow_steps = [idx for idx, val in enumerate(shadow_dates) if val in cloud_dates]    \n",
    "    \n",
    "    shadow_img = np.array(shadow_request.get_data(data_filter = shadow_steps))\n",
    "    shadow_pus = (shadow_img.shape[1]*shadow_img.shape[2])/(512*512) * shadow_img.shape[0]\n",
    "    shadow_img = resize(shadow_img, (shadow_img.shape[0], 96, 96, shadow_img.shape[-1]), order = 0,\n",
    "                        anti_aliasing = False, preserve_range = True).astype(np.uint16)\n",
    "\n",
    "    cloud_img = np.delete(cloud_img, cloud_steps, 0)\n",
    "    assert shadow_img.shape[0] == cloud_img.shape[0], (shadow_img.shape, cloud_img.shape)\n",
    "    shadows = mcm_shadow_mask(shadow_img, cloud_img) # Make usre this makes sense??\n",
    "    print(f\"Shadows ({shadows.shape}) used {round(shadow_pus, 1)} processing units\")\n",
    "    return cloud_img, shadows, clean_steps, np.array(cloud_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEM and slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dem(plot_id: int, df: 'DataFrame', epsg: 'CRS') -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads MapZen digital elevation model and return slope\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (tuple): plot id from collect earth online (CEO)\n",
    "         df (pandas.DataFrame): data associated with plot_id from CEO\n",
    "         epsg (int): UTM EPSG associated with plot_id\n",
    "    \n",
    "        Returns:\n",
    "         slope (arr): (X, Y, 1) array of per-pixel slope from [0, 1]\n",
    "    \"\"\"\n",
    "    location = calc_bbox(plot_id, df = df)\n",
    "    bbox, epsg = bounding_box(location, expansion = (32+2)*10)\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_request = WcsRequest(\n",
    "                         layer='DEM', bbox=box,\n",
    "                         resx = \"10m\", resy = \"10m\",\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format= MimeType.TIFF_d32f,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image_init = dem_request.get_data()[0]\n",
    "    dem_image = np.copy(dem_image_init)\n",
    "    dem_image = median_filter(dem_image_init, size = 5)\n",
    "    slope = calcSlope(dem_image.reshape((1, 32+2, 32+2)),\n",
    "                      np.full((32+2, 32+2), 10),\n",
    "                      np.full((32+2, 32+2), 10), \n",
    "                      zScale = 1, minSlope = 0.02)\n",
    "    slope = slope / 90\n",
    "    slope = slope.reshape((32+2, 32+2, 1))\n",
    "    slope = slope[1:32+1, 1:32+1, :]\n",
    "    return slope, dem_image_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 and 20 meter L2A bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_layer(bbox: List[Tuple[float, float]],\n",
    "                   clean_steps: np.ndarray, epsg: 'CRS',\n",
    "                   dates: dict = TIME, year: int = YEAR) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads the L2A sentinel layer with 10 and 20 meter bands\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         img (arr):\n",
    "         img_request (obj): \n",
    "    \"\"\"\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A20',\n",
    "                bbox=box, time=dates,\n",
    "                image_format = MimeType.TIFF_d16,\n",
    "                data_source = DataSource.SENTINEL2_L2A,\n",
    "                maxcc=0.75,\n",
    "                resx='20m', resy='20m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=96),\n",
    "            )\n",
    "        \n",
    "        image_dates = []\n",
    "        for date in image_request.get_dates():\n",
    "            if date.year == YEAR - 1:\n",
    "                image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "            if date.year == YEAR:\n",
    "                image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "            if date.year == YEAR + 1:\n",
    "                image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "        \n",
    "        steps_to_download = [i for i, val in enumerate(image_dates) if val in clean_steps]\n",
    "        dates_to_download = [val for i, val in enumerate(image_dates) if val in clean_steps]\n",
    "              \n",
    "        img_bands = image_request.get_data(data_filter = steps_to_download)\n",
    "        img_20 = np.stack(img_bands)\n",
    "        img_20 = to_float32(img_20)\n",
    "\n",
    "        s2_20_usage = (img_20.shape[1]*img_20.shape[2])/(512*512) * (6/3) * img_20.shape[0]\n",
    "        if (img_20.shape[1] * img_20.shape[2]) != 14*14:\n",
    "            print(f\"Original 20 meter bands size: {img_20.shape}, using {s2_20_usage} PU\")\n",
    "        img_20 = resize(img_20, (img_20.shape[0], IMSIZE, IMSIZE, img_20.shape[-1]), order = 0)\n",
    "        \n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A10',\n",
    "                bbox=box, time=dates,\n",
    "                image_format = MimeType.TIFF_d16,\n",
    "                data_source = DataSource.SENTINEL2_L2A,\n",
    "                maxcc=0.75,\n",
    "                resx='10m', resy='10m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=96),\n",
    "        )\n",
    "        \n",
    "        img_bands = image_request.get_data(data_filter = steps_to_download)\n",
    "        img_10 = np.stack(img_bands)\n",
    "        if (img_10.shape[1] * img_10.shape[2]) != 28*28:\n",
    "            print(f\"The original L2A image size is: {img_10.shape}\")\n",
    "        img_10 = to_float32(img_10)\n",
    "            \n",
    "        img_10 = resize(img_10, (img_10.shape[0], IMSIZE, IMSIZE, img_10.shape[-1]), order = 0)\n",
    "        img = np.concatenate([img_10, img_20], axis = -1)\n",
    "\n",
    "        \n",
    "        return img, np.array(dates_to_download)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super resolution\n",
    "\n",
    "Super-resolve the 20 meter bands to 10 meters using DSen2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "MDL_PATH = \"../models/supres/\"\n",
    "\n",
    "model = tf.train.import_meta_graph(MDL_PATH + 'model.meta')\n",
    "model.restore(sess, tf.train.latest_checkpoint(MDL_PATH))\n",
    "\n",
    "logits = tf.get_default_graph().get_tensor_by_name(\"Add_6:0\")\n",
    "inp = tf.get_default_graph().get_tensor_by_name(\"Placeholder:0\")\n",
    "inp_bilinear = tf.get_default_graph().get_tensor_by_name(\"Placeholder_1:0\")\n",
    "\n",
    "def superresolve(input_data):\n",
    "    bilinear_upsample = input_data[..., 4:]\n",
    "    x = sess.run([logits], \n",
    "                 feed_dict={inp: input_data,\n",
    "                            inp_bilinear: bilinear_upsample})\n",
    "    return x[0]\n",
    "\n",
    "def superresolve_tile(x):\n",
    "    twentym = x[..., 4:]\n",
    "    imsize = x.shape[1]\n",
    "    twentym = np.reshape(twentym, (x.shape[0], imsize // 2, 2, imsize // 2, 2, 6))\n",
    "    twentym = np.mean(twentym, (2, 4))\n",
    "    twentym = resize(twentym, (x.shape[0], imsize, imsize, 6), 1)\n",
    "    x[..., 4:] = twentym\n",
    "    x[..., 4:] = superresolve(x)\n",
    "    crop_amt = (imsize - 24) // 2\n",
    "    x = x[:, crop_amt:-crop_amt, crop_amt:-crop_amt, :]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_new_dem(data_location: 'os.Path',\n",
    "                     output_folder: 'os.Path',\n",
    "                     image_format: 'MimeType' = MimeType.TIFF_d16):\n",
    "    \"\"\" Downloads and saves DEM and slope files\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path): \n",
    "         output_folder (os.path): \n",
    "         image_format (MimeType): \n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(data_location)\n",
    "    df.columns = [x.upper() for x in df.columns]\n",
    "    for column in ['IMAGERY_TITLE', 'STACKINGPROFILEDG', 'PL_PLOTID', 'IMAGERYYEARDG',\n",
    "                  'IMAGERYMONTHPLANET', 'IMAGERYYEARPLANET', 'IMAGERYDATESECUREWATCH',\n",
    "                  'IMAGERYENDDATESECUREWATCH', 'IMAGERYFEATUREPROFILESECUREWATCH',\n",
    "                  'IMAGERYSTARTDATESECUREWATCH','IMAGERY_ATTRIBUTIONS',\n",
    "                  'SAMPLE_GEOM']:\n",
    "        if column in df.columns:\n",
    "            df = df.drop(column, axis = 1)\n",
    "            \n",
    "    df = df.dropna(axis = 0)\n",
    "    plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "    existing = [int(x[:-4]) for x in os.listdir(output_folder) if \".DS\" not in x]\n",
    "    existing = existing + [139089844]\n",
    "    to_download = [x for x in plot_ids if x not in existing]\n",
    "    print(f\"Starting download of {len(to_download)}\"\n",
    "          f\" plots from {data_location} to {output_folder}\")\n",
    "    errors = []\n",
    "    for i, val in enumerate(to_download):\n",
    "        print(f\"Downloading {i + 1}/{len(to_download)}, {val}\")\n",
    "        initial_bbx = calc_bbox(val, df = df)\n",
    "        dem_bbx, epsg = bounding_box(initial_bbx, expansion = 32*10)\n",
    "        slope, dem = download_dem(val, epsg = epsg, df = df)\n",
    "        np.save(output_folder + str(val), dem)\n",
    "        np.save(\"../data/train-slope/\" + str(val), slope)\n",
    "        \n",
    "\n",
    "\n",
    "def concatenate_dem(x, dem):\n",
    "    dem = np.tile(dem.reshape((1, 32, 32, 1)), (x.shape[0], 1, 1, 1))\n",
    "    dem = dem[:, 4:-4, 4:-4, :]\n",
    "    x = np.concatenate([x, dem], axis = -1)\n",
    "    assert x.shape[1] == x.shape[2] == 24\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def id_missing_px(sentinel2):\n",
    "    missing_images = np.sum(sentinel2[..., :10] == 0.0, axis = (1, 2, 3))\n",
    "    missing_images_p = np.sum(sentinel2[..., :10] >= 1., axis = (1, 2, 3))\n",
    "    missing_images = missing_images + missing_images_p\n",
    "    if np.sum(missing_images) > 0:\n",
    "        print(\"Missing px\", missing_images)\n",
    "        print(np.max(sentinel2[..., :10]), np.min(sentinel2[..., :10]))\n",
    "    missing_images = np.argwhere(missing_images >= ((sentinel2.shape[1]**2) / 11))\n",
    "    if len(missing_images) > 0:\n",
    "        print(\"Missing images\", missing_images)\n",
    "    return missing_images\n",
    "\n",
    "def download_raw_data(data_location, output_folder, fmt = \"train\", image_format = MimeType.TIFF_d16):\n",
    "    \"\"\" Downloads slope and sentinel-2 data for all plots associated\n",
    "        with an input CSV from a collect earth online survey\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path)\n",
    "         output_folder (os.path)\n",
    "        \n",
    "        Creates:\n",
    "         output_folder/{plot_id}.npy\n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_location)\n",
    "    df.columns = [x.upper() for x in df.columns]\n",
    "    for column in ['IMAGERY_TITLE', 'STACKINGPROFILEDG', 'PL_PLOTID', 'IMAGERYYEARDG',\n",
    "                  'IMAGERYMONTHPLANET', 'IMAGERYYEARPLANET', 'IMAGERYDATESECUREWATCH',\n",
    "                  'IMAGERYENDDATESECUREWATCH', 'IMAGERYFEATUREPROFILESECUREWATCH',\n",
    "                  'IMAGERYSTARTDATESECUREWATCH','IMAGERY_ATTRIBUTIONS',\n",
    "                  'SAMPLE_GEOM']:\n",
    "        if column in df.columns:\n",
    "            df = df.drop(column, axis = 1)\n",
    "\n",
    "    df = df.dropna(axis = 0)\n",
    "    plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "    existing = [int(x[:-4]) for x in os.listdir(f\"../data/{fmt}-dates/\") if \".DS\" not in x]\n",
    "    existing = existing + [139190271, 139187199, 139319876, 139319877]\n",
    "    to_download = [x for x in plot_ids if x not in existing]\n",
    "    print(f\"Starting download of {len(to_download)}\"\n",
    "          f\" plots from {data_location} to {output_folder}\")\n",
    "    for i, val in enumerate(to_download):\n",
    "        print(f\"Downloading {i + 1}/{len(to_download)}, {val}\")\n",
    "        initial_bbx = calc_bbox(val, df = df)\n",
    "        sentinel2_bbx, epsg = bounding_box(initial_bbx, expansion = IMSIZE*10)\n",
    "        cloud_bbx, _ = bounding_box(initial_bbx, expansion = 96*10)\n",
    "        try:\n",
    "            # Identify cloud steps, download DEM, and download L2A series\n",
    "            cloud_probs, shadows, _, clean_dates = identify_clouds(cloud_bbx, epsg = epsg)\n",
    "            dem, _ = download_dem(val, epsg = epsg, df = df)\n",
    "            to_remove, _ = calculate_cloud_steps(cloud_probs, clean_dates)\n",
    "            \n",
    "            if len(to_remove) > 0:\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "                clean_dates = np.delete(clean_dates, to_remove)\n",
    "                shadows = np.delete(shadows, to_remove, 0)\n",
    "                \n",
    "            to_remove = subset_contiguous_sunny_dates(clean_dates)\n",
    "            if len(to_remove) > 0:\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "                clean_dates = np.delete(clean_dates, to_remove)\n",
    "                shadows = np.delete(shadows, to_remove, 0)\n",
    "        \n",
    "            s2, s2_dates = download_layer(sentinel2_bbx, clean_steps = clean_dates, epsg = epsg)    \n",
    "            \n",
    "            # Step to ensure that shadows, clouds, sentinel l2a have aligned dates\n",
    "            to_remove_clouds = [i for i, val in enumerate(clean_dates) if val not in s2_dates]\n",
    "            to_remove_dates = [val for i, val in enumerate(clean_dates) if val not in s2_dates]\n",
    "            if len(to_remove_clouds) > 0:\n",
    "                print(f\"Removing {to_remove_dates} from clouds because not in S2\")\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove_clouds, 0)\n",
    "                shadows = np.delete(shadows, to_remove_clouds, 0)\n",
    "            print(f\"Shadows {shadows.shape}, clouds {cloud_probs.shape},\"\n",
    "                  f\" S2, {s2.shape}, S2d, {s2_dates.shape}\")\n",
    "            \n",
    "            to_remove = remove_missed_clouds(s2)\n",
    "            if len(to_remove) > 0:\n",
    "                print(f\"Removing {len(to_remove)} steps based on ratio\")\n",
    "                s2 = np.delete(s2, to_remove, 0)\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "                s2_dates = np.delete(s2_dates, to_remove)\n",
    "                shadows = np.delete(shadows, to_remove, 0)\n",
    "            \n",
    "            cloud_probs = cloud_probs[:, 24:-24, 24:-24]\n",
    "            shadows = shadows[:, 24:-24, 24:-24]\n",
    "            x, interp = remove_cloud_and_shadows(s2, cloud_probs, shadows, s2_dates)\n",
    "            to_remove = np.argwhere(np.mean(interp, axis = (1, 2)) > 0.5)\n",
    "            if len(to_remove) > 0:\n",
    "                print(f\"Removing {len(to_remove)} steps with >50% interpolation: {to_remove}\")\n",
    "                x = np.delete(x, to_remove, 0)\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "                s2_dates = np.delete(s2_dates, to_remove)\n",
    "                shadows = np.delete(shadows, to_remove, 0)\n",
    "                print(np.sum(shadows, axis = (1, 2)))\n",
    "            \n",
    "            x_to_save = np.copy(x)\n",
    "            x_to_save = np.clip(x_to_save, 0, 1)\n",
    "            x_to_save = np.trunc(x_to_save * 65535).astype(np.uint16)\n",
    "            np.save(f\"../data/{fmt}-dates/{str(val)}\", s2_dates)\n",
    "            np.save(f\"../data/{fmt}-raw/{str(val)}\", x_to_save)\n",
    "            file = f\"../data/{fmt}-raw/{str(val)}.npy\"\n",
    "            key = f'restoration-mapper/model-data/{fmt}/raw/{str(val)}.npy'\n",
    "            uploader.upload(bucket = 'restoration-monitoring', key = key, file = file)\n",
    "            print(\"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "def select_dates(dates):\n",
    "    \"\"\"For imagery that was downloaded prior to capping the number \n",
    "       of monthly images to be 3, it is necessary to enforce that cap\n",
    "       on the training / testing data.\n",
    "       \n",
    "       This function identifies the indices of the imagery to deletet\n",
    "       such that there is a maximum of three images per month.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    before = len(dates)\n",
    "    selected_indices = np.arange(len(dates))\n",
    "    begin = [-60, 0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n",
    "    end = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334, 390]\n",
    "    indices_to_remove = []\n",
    "    for x, y in zip(begin, end):\n",
    "        indices_month = np.argwhere(np.logical_and(dates >= x, dates < y)).flatten()\n",
    "        if len(indices_month) > 3:\n",
    "            to_delete = np.empty((0,))\n",
    "            if begin == -60:\n",
    "                to_delete = indices_month[:-3]\n",
    "            elif begin == 334:\n",
    "                to_delete = indices_month[3:]\n",
    "            elif len(indices_month) == 4:\n",
    "                to_delete = indices_month[1]\n",
    "            elif len(indices_month) == 5:\n",
    "                to_delete = np.array([indices_month[1],\n",
    "                                      indices_month[3]])\n",
    "            elif len(indices_month) == 6:\n",
    "                to_delete = np.array([indices_month[1],\n",
    "                                      indices_month[3],\n",
    "                                      indices_month[4]])\n",
    "                \n",
    "            to_delete = np.array(to_delete)\n",
    "            if to_delete.size > 0:\n",
    "                indices_to_remove.append(to_delete.flatten())\n",
    "                \n",
    "    if len(indices_to_remove) > 0:\n",
    "        indices_to_remove = np.concatenate(indices_to_remove)\n",
    "        after = before - len(indices_to_remove)\n",
    "        print(f\"Keeping {after}/{before}\")\n",
    "        return indices_to_remove\n",
    "    \n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "\n",
    "def subset_contiguous_sunny_dates(dates):\n",
    "    begin = [-60, 0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n",
    "    end = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334, 390]\n",
    "    n_per_month = []\n",
    "    months_to_adjust = []\n",
    "    indices_to_rm = []\n",
    "    \n",
    "    if len(dates) >= 20:\n",
    "        for x, y in zip(begin, end):\n",
    "            indices_month = np.argwhere(np.logical_and(\n",
    "                dates >= x, dates < y)).flatten()\n",
    "            n_per_month.append(len(indices_month))\n",
    "\n",
    "        for x in range(11):\n",
    "            three_m_sum = np.sum(n_per_month[x:x+3])\n",
    "            if three_m_sum >= 7:\n",
    "                months_to_adjust.append([x, x+1, x+2])\n",
    "\n",
    "        months_to_adjust = [item for sublist in months_to_adjust for item in sublist]\n",
    "        months_to_adjust = list(set(months_to_adjust))\n",
    "\n",
    "\n",
    "        if len(months_to_adjust) > 0:\n",
    "            for month in months_to_adjust:\n",
    "                indices_month = np.argwhere(np.logical_and(\n",
    "                    dates >= begin[month], dates < end[month])).flatten()\n",
    "                if len(indices_month) == 3:\n",
    "                    indices_to_rm.append(indices_month[1])\n",
    "    print(f\"Removing {len(indices_to_rm)}/{len(dates)} sunny dates\")\n",
    "    return indices_to_rm\n",
    "\n",
    "\n",
    "def to_int16(array: np.array) -> np.array:\n",
    "    '''Converts a float32 array to uint16, reducing storage costs by three-fold'''\n",
    "    array = np.clip(array, 0, 1)\n",
    "    array = np.trunc(array * 65535)\n",
    "    assert np.min(array >= 0)\n",
    "    assert np.max(array <= 65535)\n",
    "    \n",
    "    return array.astype(np.uint16)\n",
    "\n",
    "\n",
    "def process_raw(plot_id, path = 'train'):\n",
    "    \"\"\" Downloads slope and sentinel-2 data for all plots associated\n",
    "        with an input CSV from a collect earth online survey\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path)\n",
    "         output_folder (os.path)\n",
    "        \n",
    "        Creates:\n",
    "         output_folder/{plot_id}.npy\n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"         \n",
    "\n",
    "    x = np.load(f\"../data/{path}-raw/{plot_id}.npy\")\n",
    "    x = np.float32(x) / 65535\n",
    "    s2_dates = np.load(f\"../data/{path}-dates/{plot_id}.npy\")\n",
    "    dem = np.load(f\"../data/{path}-slope/{plot_id}.npy\")\n",
    "    \n",
    "    assert x.shape[0] == s2_dates.shape[0]\n",
    "\n",
    "    missing_px = id_missing_px(x)\n",
    "    if len(missing_px) > 0:\n",
    "        print(f\"Deleting {missing_px} because of missing data\")\n",
    "        x = np.delete(x, missing_px, 0)\n",
    "        s2_dates = np.delete(s2_dates, missing_px)\n",
    "\n",
    "\n",
    "    n_images = x.shape[0]\n",
    "    to_remove = remove_missed_clouds(x)\n",
    "    if len(to_remove) > 0:\n",
    "        x = np.delete(x, to_remove, 0)\n",
    "        s2_dates = np.delete(s2_dates, to_remove)\n",
    "        print(f\"Removing {len(to_remove)} steps based on MCM mask\")\n",
    "        \n",
    "    to_remove = select_dates(s2_dates)\n",
    "    if len(to_remove) > 0:\n",
    "        x = np.delete(x, to_remove, 0)\n",
    "        s2_dates = np.delete(s2_dates, to_remove)\n",
    "        \n",
    "        \n",
    "    to_remove = subset_contiguous_sunny_dates(s2_dates)\n",
    "    if len(to_remove) > 0:\n",
    "        x = np.delete(x, to_remove, 0)\n",
    "        s2_dates = np.delete(s2_dates, to_remove)\n",
    "        \n",
    "    for band in range(0, 10):\n",
    "        for time in range(0, x.shape[0]):\n",
    "            x_i = x[time, :, :, band]\n",
    "            x_i[np.argwhere(np.isnan(x_i))] = np.mean(x_i)\n",
    "            x[time, :, :, band] = x_i\n",
    "\n",
    "    # Interpolate linearly to 5 day frequency\n",
    "    tiles, max_distance = calculate_and_save_best_images(x, s2_dates)\n",
    "    sm = Smoother(lmbd = 800, size = tiles.shape[0],\n",
    "                  nbands = 10, dim = tiles.shape[1])\n",
    "    x = sm.interpolate_array(tiles)\n",
    "\n",
    "    x = superresolve_tile(x)\n",
    "    tiles = concatenate_dem(x, dem)\n",
    "    dates = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n",
    "    dates = np.array(dates) + 15\n",
    "    closest_date = []\n",
    "    for date in dates:\n",
    "        date_diff = s2_dates[np.argmin(abs(s2_dates - date))]\n",
    "        closest_date.append(date_diff)\n",
    "    \n",
    "    closest_date = np.array(closest_date)\n",
    "    print(closest_date)\n",
    "    closest_date = closest_date[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "    closest_date = np.broadcast_to(closest_date, (12, 24, 24, 1))\n",
    "    # -45 is the minimum, so adding 45 ensures that it is [0, ]\n",
    "    # 411 (365 + 46) si the maximum, so dividing by 416 ensures that it is [0, 1]\n",
    "    closest_date = (closest_date + 45)  / 411\n",
    "    \n",
    "    tiles = np.concatenate([tiles, closest_date], axis = -1)\n",
    "    \n",
    "    \n",
    "\n",
    "    if np.sum(np.isnan(tiles)) == 0:\n",
    "        print(f\"There are {np.sum(np.isnan(tiles))} NA values\")\n",
    "        if max_distance <= 300 and n_images >= 5:\n",
    "            tiles = to_int16(tiles)\n",
    "            #np.save(f\"../data/{path}-s2-new/{plot_id}\", tiles)\n",
    "            tile_path = f\"../data/{path}-s2-new/{plot_id}\"\n",
    "            tile_path = tile_path + \".hkl\"\n",
    "            hkl.dump(tiles, tile_path, mode='w', compression='gzip')\n",
    "            print(f\"Saved {tiles.shape} shape, {n_images} img,\"\n",
    "                  f\" to {tile_path} \\n\")\n",
    "        else:\n",
    "            print(f\"Skipping {plot_id} because {max_distance} distance, and {n_images} img \\n\")\n",
    "\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function execution\n",
    "## 1. Download DEM and Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (os.listdir(\"../data/train-csv/\")):\n",
    "    if \"dr-\" in i:\n",
    "        tile = download_new_dem(\"../data/train-csv/\" + i,\n",
    "                                \"../data/train-dem/\",\n",
    "                                image_format = MimeType.TIFF_d16)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Raw data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/preprocessing/cloud_removal.py\n",
    "\n",
    "for i in (os.listdir(\"../data/train-csv/\")):\n",
    "    if \"dr-\" in i:\n",
    "        download_raw_data(\"../data/train-csv/\" + i,\n",
    "                          \"../data/train-raw/\", \n",
    "                          fmt = 'train',\n",
    "                          image_format = MimeType.TIFF_d16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mins = []\n",
    "maxs = []\n",
    "for file in os.listdir(\"../data/train-dates\"):\n",
    "    if '.npy' in file:\n",
    "        f = np.load(\"../data/train-dates/\" + file)\n",
    "        mins.append(np.min(f))\n",
    "        maxs.append(np.max(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process train / test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "plots = [str(x[:-4]) for x in os.listdir(\"../data/train-raw/\") if \".npy\" in x]\n",
    "for plot in plots:\n",
    "    i += 1\n",
    "    if not os.path.exists(\"../data/train-s2-new/\" + plot + \".hkl\"):\n",
    "        if plot in\n",
    "            tiles = process_raw(plot, path = 'train')\n",
    "            print(i, plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
