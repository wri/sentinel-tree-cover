{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process sentinel 2 data\n",
    "\n",
    "This notebook downloads and processes one year of the training and validation plots as labelled on Collect Earth Online. \n",
    "\n",
    "## John Brandt\n",
    "## Last edit: Sept 20, 2021\n",
    "\n",
    "## Package imports, API import, source scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import hickle as hkl\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "from scipy.sparse.linalg import splu\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType\n",
    "from sentinelhub import CRS, BBox, constants, DataSource, CustomUrlParam\n",
    "from skimage.transform import resize\n",
    "from typing import Tuple, List\n",
    "from scipy.ndimage import median_filter\n",
    "from sentinelhub.config import SHConfig\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key'] \n",
    "        \n",
    "%matplotlib inline\n",
    "%run ../src/preprocessing/slope.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/downloading/utils.py\n",
    "%run ../src/preprocessing/cloud_removal.py\n",
    "%run ../src/preprocessing/whittaker_smoother.py\n",
    "%run ../src/downloading/io.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "    key = (yaml.safe_load(stream))\n",
    "    API_KEY = key['key']\n",
    "    SHUB_SECRET = key['shub_secret']\n",
    "    SHUB_KEY = key['shub_id']\n",
    "    AWSKEY = key['awskey']\n",
    "    AWSSECRET = key['awssecret']\n",
    "            \n",
    "shconfig = SHConfig()\n",
    "shconfig.instance_id = API_KEY\n",
    "shconfig.sh_client_id = SHUB_KEY\n",
    "shconfig.sh_client_secret = SHUB_SECRET\n",
    "    \n",
    "\n",
    "uploader = FileUploader(awskey = AWSKEY, awssecret = AWSSECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "YEAR = 2020\n",
    "TIME = ('{}-11-15'.format(str(YEAR - 1)), '{}-02-15'.format(str(YEAR + 1)))\n",
    "EPSG = CRS.WGS84\n",
    "IMSIZE = 32\n",
    "\n",
    "# Constants\n",
    "starting_days = np.cumsum([0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geographic utility functions\n",
    "\n",
    "These cell blocks calculate the min_x, min_y, max_x, max_y of the area of interest (AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bbox(plot_id: int, df: pd.DataFrame) -> List:\n",
    "    \"\"\" Calculates the corners of a bounding box from an input\n",
    "        pandas dataframe as output by Collect Earth Online\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (int): plot_id of associated plot\n",
    "         df (pandas.DataFrame): dataframe of associated CEO survey\n",
    "    \n",
    "        Returns:\n",
    "         bounding_box (list): [(min(x), min(y)),\n",
    "                              (max(x), max_y))]\n",
    "    \"\"\"\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])),\n",
    "            (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "\n",
    "def bounding_box(points: List[Tuple[float, float]], \n",
    "                 expansion: int = 160) -> ((Tuple, Tuple), str):\n",
    "    \"\"\" Calculates the corners of a bounding box with an\n",
    "        input expansion in meters from a given bounding_box\n",
    "        \n",
    "        Subcalls:\n",
    "         calculate_epsg, convertCoords\n",
    "\n",
    "        Parameters:\n",
    "         points (list): output of calc_bbox\n",
    "         expansion (float): number of meters to expand or shrink the\n",
    "                            points edges to be\n",
    "    \n",
    "        Returns:\n",
    "         bl (tuple): x, y of bottom left corner with edges of expansion meters\n",
    "         tr (tuple): x, y of top right corner with edges of expansion meters\n",
    "    \"\"\"\n",
    "    bl = list(points[0])\n",
    "    tr = list(points[1])\n",
    "    inproj = Proj('epsg:4326')\n",
    "    outproj_code = calculate_epsg(bl)\n",
    "    outproj = Proj('epsg:' + str(outproj_code))\n",
    "    bl_utm =  transform(inproj, outproj, bl[1], bl[0])\n",
    "    tr_utm =  transform(inproj, outproj, tr[1], tr[0])\n",
    "\n",
    "    distance1 = tr_utm[0] - bl_utm[0]\n",
    "    distance2 = tr_utm[1] - bl_utm[1]\n",
    "    expansion1 = (expansion - distance1)/2\n",
    "    expansion2 = (expansion - distance2)/2\n",
    "        \n",
    "    bl_utm = [bl_utm[0] - expansion1, bl_utm[1] - expansion2]\n",
    "    tr_utm = [tr_utm[0] + expansion1, tr_utm[1] + expansion2]\n",
    "\n",
    "    zone = str(outproj_code)[3:]\n",
    "    zone = zone[1:] if zone[0] == \"0\" else zone\n",
    "    direction = 'N' if tr[1] >= 0 else 'S'\n",
    "    utm_epsg = \"UTM_\" + zone + direction\n",
    "    return (bl_utm, tr_utm), CRS[utm_epsg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(date_dict: dict, year: int) -> List:\n",
    "    \"\"\" Transforms a SentinelHub date dictionary to a\n",
    "         list of integer calendar dates\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "    starting_days = np.cumsum(days_per_month)\n",
    "    for date in date_dict:\n",
    "        if date.year == year - 1:\n",
    "            dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    return dates\n",
    "\n",
    "def to_float32(array: np.array) -> np.array:\n",
    "    \"\"\"Converts an int_x array to float32\"\"\"\n",
    "    print(f'The original max value is {np.max(array)}')\n",
    "    if not isinstance(array.flat[0], np.floating):\n",
    "        assert np.max(array) > 1\n",
    "        array = np.float32(array) / 65535.\n",
    "    assert np.max(array) <= 1\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud and cloud shadow calculation functions\n",
    "\n",
    "This cell block identifies clouds using s2Cloudless, and identifies clouds (and shadows) using Candra et al. 2019.\n",
    "\n",
    "The output is per-px cloud/shadow masks, and a list of sentinel 2 dates to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_clouds(bbox: List[Tuple[float, float]], epsg: 'CRS', time: dict = TIME):\n",
    "    \"\"\" Downloads and calculates cloud cover and shadow\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         cloud_img (np.array): (X, 96, 96) array of cloud probs\n",
    "         shadows (np.array):  (X, 96, 96) array of shadow binary\n",
    "         clean_steps (np.array): (N,) array of clean idx\n",
    "         cloud_dates (np.array): (N,) array of clean cloud datets\n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    cloud_request = WcsRequest(\n",
    "        layer='CLOUD_NEW',\n",
    "        bbox=box, time=time,\n",
    "        resx='160m', resy='160m',\n",
    "        image_format = MimeType.TIFF,\n",
    "        maxcc=0.75, config=shconfig,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=96))\n",
    "\n",
    "    shadow_request = WcsRequest(\n",
    "        layer='SHADOW',\n",
    "        bbox=box, time=time,\n",
    "        resx='60m', resy='60m',\n",
    "        image_format =  MimeType.TIFF,\n",
    "        maxcc=0.75, config=shconfig,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=96))\n",
    "\n",
    "    cloud_img = np.array(cloud_request.get_data())\n",
    "    if not isinstance(cloud_img.flat[0], np.floating):\n",
    "        assert np.max(cloud_img) > 1\n",
    "        cloud_img = cloud_img / 255.\n",
    "    assert np.max(cloud_img) <= 1\n",
    "\n",
    "    cloud_img = resize(cloud_img, (cloud_img.shape[0], 96, 96), order = 0)\n",
    "    n_cloud_px = np.sum(cloud_img > 0.33, axis = (1, 2))\n",
    "    cloud_steps = np.argwhere(n_cloud_px > (96**2 * 0.15))\n",
    "    clean_steps = [x for x in range(cloud_img.shape[0]) if x not in cloud_steps]\n",
    "    \n",
    "    \n",
    "    cloud_dates_dict = [x for x in cloud_request.get_dates()]\n",
    "    cloud_dates = extract_dates(cloud_dates_dict, YEAR)\n",
    "    cloud_dates = [val for idx, val in enumerate(cloud_dates) if idx in clean_steps]\n",
    "    \n",
    "    shadow_dates_dict = [x for x in shadow_request.get_dates()]\n",
    "    shadow_dates = extract_dates(shadow_dates_dict, YEAR)\n",
    "    shadow_steps = [idx for idx, val in enumerate(shadow_dates) if val in cloud_dates]    \n",
    "    \n",
    "    shadow_img = np.array(shadow_request.get_data(data_filter = shadow_steps))\n",
    "    shadow_pus = (shadow_img.shape[1]*shadow_img.shape[2])/(512*512) * shadow_img.shape[0]\n",
    "    shadow_img = resize(shadow_img, (shadow_img.shape[0], 96, 96, shadow_img.shape[-1]), order = 0,\n",
    "                        anti_aliasing = False, preserve_range = True).astype(np.uint16)\n",
    "\n",
    "    cloud_img = np.delete(cloud_img, cloud_steps, 0)\n",
    "    assert shadow_img.shape[0] == cloud_img.shape[0], (shadow_img.shape, cloud_img.shape)\n",
    "    shadows = mcm_shadow_mask(shadow_img, cloud_img) # Make usre this makes sense??\n",
    "    print(f\"Shadows ({shadows.shape}) used {round(shadow_pus, 1)} processing units\")\n",
    "    return cloud_img, shadows, clean_steps, np.array(cloud_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEM and slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dem(plot_id: int, df: 'DataFrame', epsg: 'CRS') -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads MapZen digital elevation model and return slope\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (tuple): plot id from collect earth online (CEO)\n",
    "         df (pandas.DataFrame): data associated with plot_id from CEO\n",
    "         epsg (int): UTM EPSG associated with plot_id\n",
    "    \n",
    "        Returns:\n",
    "         slope (arr): (X, Y, 1) array of per-pixel slope from [0, 1]\n",
    "    \"\"\"\n",
    "    location = calc_bbox(plot_id, df = df)\n",
    "    bbox, epsg = bounding_box(location, expansion = (32+2)*10)\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_request = WcsRequest(\n",
    "                         layer='DEM', bbox=box,\n",
    "                         resx = \"10m\", resy = \"10m\",\n",
    "                         config=shconfig,\n",
    "                         image_format= MimeType.TIFF,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image_init = dem_request.get_data()[0]\n",
    "    dem_image_init = dem_image_init - 12000\n",
    "    dem_image_init = dem_image_init.astype(np.float32)\n",
    "    dem_image = np.copy(dem_image_init)\n",
    "    dem_image = median_filter(dem_image_init, size = 5)\n",
    "    slope = calcSlope(dem_image.reshape((1, 32+2, 32+2)),\n",
    "                      np.full((32+2, 32+2), 10),\n",
    "                      np.full((32+2, 32+2), 10), \n",
    "                      zScale = 1, minSlope = 0.02)\n",
    "    slope = slope / 90\n",
    "    slope = slope.reshape((32+2, 32+2, 1))\n",
    "    slope = slope[1:32+1, 1:32+1, :]\n",
    "    return slope, dem_image_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 and 20 meter L2A bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_layer(bbox: List[Tuple[float, float]],\n",
    "                   clean_steps: np.ndarray, epsg: 'CRS',\n",
    "                   dates: dict = TIME, year: int = YEAR) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads the L2A sentinel layer with 10 and 20 meter bands\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         img (arr):\n",
    "         img_request (obj): \n",
    "    \"\"\"\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A20',\n",
    "                bbox=box, time=dates,\n",
    "                image_format = MimeType.TIFF,\n",
    "                data_source = DataSource.SENTINEL2_L2A,\n",
    "                maxcc=0.75,\n",
    "                resx='20m', resy='20m',\n",
    "                config=shconfig,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=96),\n",
    "            )\n",
    "        \n",
    "        image_dates = []\n",
    "        for date in image_request.get_dates():\n",
    "            if date.year == YEAR - 1:\n",
    "                image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "            if date.year == YEAR:\n",
    "                image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "            if date.year == YEAR + 1:\n",
    "                image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "        \n",
    "        steps_to_download = [i for i, val in enumerate(image_dates) if val in clean_steps]\n",
    "        dates_to_download = [val for i, val in enumerate(image_dates) if val in clean_steps]\n",
    "              \n",
    "        img_bands = image_request.get_data(data_filter = steps_to_download)\n",
    "        img_20 = np.stack(img_bands)\n",
    "        img_20 = to_float32(img_20)\n",
    "\n",
    "        s2_20_usage = (img_20.shape[1]*img_20.shape[2])/(512*512) * (6/3) * img_20.shape[0]\n",
    "        if (img_20.shape[1] * img_20.shape[2]) != 14*14:\n",
    "            print(f\"Original 20 meter bands size: {img_20.shape}, using {s2_20_usage} PU\")\n",
    "        img_20 = resize(img_20, (img_20.shape[0], IMSIZE, IMSIZE, img_20.shape[-1]), order = 0)\n",
    "        \n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A10',\n",
    "                bbox=box, time=dates,\n",
    "                image_format = MimeType.TIFF,\n",
    "                data_source = DataSource.SENTINEL2_L2A,\n",
    "                maxcc=0.75,\n",
    "                resx='10m', resy='10m',\n",
    "                config=shconfig,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=96),\n",
    "        )\n",
    "        \n",
    "        img_bands = image_request.get_data(data_filter = steps_to_download)\n",
    "        img_10 = np.stack(img_bands)\n",
    "        if (img_10.shape[1] * img_10.shape[2]) != 28*28:\n",
    "            print(f\"The original L2A image size is: {img_10.shape}\")\n",
    "        img_10 = to_float32(img_10)\n",
    "            \n",
    "        img_10 = resize(img_10, (img_10.shape[0], IMSIZE, IMSIZE, img_10.shape[-1]), order = 0)\n",
    "        img = np.concatenate([img_10, img_20], axis = -1)\n",
    "\n",
    "        \n",
    "        return img, np.array(dates_to_download)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super resolution\n",
    "\n",
    "Super-resolve the 20 meter bands to 10 meters using DSen2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/supres/model\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "MDL_PATH = \"../models/supres/\"\n",
    "\n",
    "model = tf.train.import_meta_graph(MDL_PATH + 'model.meta')\n",
    "model.restore(sess, tf.train.latest_checkpoint(MDL_PATH))\n",
    "\n",
    "logits = tf.get_default_graph().get_tensor_by_name(\"Add_6:0\")\n",
    "inp = tf.get_default_graph().get_tensor_by_name(\"Placeholder:0\")\n",
    "inp_bilinear = tf.get_default_graph().get_tensor_by_name(\"Placeholder_1:0\")\n",
    "\n",
    "def superresolve(input_data):\n",
    "    bilinear_upsample = input_data[..., 4:]\n",
    "    x = sess.run([logits], \n",
    "                 feed_dict={inp: input_data,\n",
    "                            inp_bilinear: bilinear_upsample})\n",
    "    return x[0]\n",
    "\n",
    "def superresolve_tile(x):\n",
    "    twentym = x[..., 4:]\n",
    "    imsize = x.shape[1]\n",
    "    twentym = np.reshape(twentym, (x.shape[0], imsize // 2, 2, imsize // 2, 2, 6))\n",
    "    twentym = np.mean(twentym, (2, 4))\n",
    "    twentym = resize(twentym, (x.shape[0], imsize, imsize, 6), 1)\n",
    "    x[..., 4:] = twentym\n",
    "    x[..., 4:] = superresolve(x)\n",
    "    if imsize > 28:\n",
    "        crop_amt = (imsize - 28) // 2\n",
    "        x = x[:, crop_amt:-crop_amt, crop_amt:-crop_amt, :]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_new_dem(data_location: 'os.Path',\n",
    "                     output_folder: 'os.Path',\n",
    "                     image_format: 'MimeType' = MimeType.TIFF):\n",
    "    \"\"\" Downloads and saves DEM and slope files\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path): \n",
    "         output_folder (os.path): \n",
    "         image_format (MimeType): \n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(data_location)\n",
    "    df.columns = [x.upper() for x in df.columns]\n",
    "    for column in ['IMAGERY_TITLE', 'STACKINGPROFILEDG', 'PL_PLOTID', 'IMAGERYYEARDG',\n",
    "                  'IMAGERYMONTHPLANET', 'IMAGERYYEARPLANET', 'IMAGERYDATESECUREWATCH',\n",
    "                  'IMAGERYENDDATESECUREWATCH', 'IMAGERYFEATUREPROFILESECUREWATCH',\n",
    "                  'IMAGERYSTARTDATESECUREWATCH','IMAGERY_ATTRIBUTIONS',\n",
    "                  'SAMPLE_GEOM']:\n",
    "        if column in df.columns:\n",
    "            df = df.drop(column, axis = 1)\n",
    "            \n",
    "    df = df.dropna(axis = 0)\n",
    "    plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "    existing = [int(x[:-4]) for x in os.listdir(output_folder) if \".DS\" not in x]\n",
    "    existing = existing + [139089844]\n",
    "    to_download = [x for x in plot_ids if x not in existing]\n",
    "    print(f\"Starting download of {len(to_download)}\"\n",
    "          f\" plots from {data_location} to {output_folder}\")\n",
    "    errors = []\n",
    "    for i, val in enumerate(to_download):\n",
    "        print(f\"Downloading {i + 1}/{len(to_download)}, {val}\")\n",
    "        initial_bbx = calc_bbox(val, df = df)\n",
    "        dem_bbx, epsg = bounding_box(initial_bbx, expansion = 32*10)\n",
    "        slope, dem = download_dem(val, epsg = epsg, df = df)\n",
    "        np.save(output_folder + str(val), dem)\n",
    "        np.save(\"../data/train-slope/\" + str(val), slope)\n",
    "        \n",
    "\n",
    "\n",
    "def concatenate_dem(x, dem):\n",
    "    dem = np.tile(dem.reshape((1, 32, 32, 1)), (x.shape[0], 1, 1, 1))\n",
    "    dem = dem[:, 2:-2, 2:-2, :]\n",
    "    x = np.concatenate([x, dem], axis = -1)\n",
    "    assert x.shape[1] == x.shape[2] == 28\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_missing_px(sentinel2: np.ndarray, thresh: int = 100) -> np.ndarray:\n",
    "    missing_images_0 = np.sum(sentinel2[..., :10] == 0.0, axis = (1, 2, 3))\n",
    "    missing_images_p = np.sum(sentinel2[..., :10] >= 1., axis = (1, 2, 3))\n",
    "    missing_images = missing_images_0 + missing_images_p\n",
    "    \n",
    "    missing_images = np.argwhere(missing_images >= (sentinel2.shape[1]**2) / thresh).flatten()\n",
    "    return missing_images\n",
    "\n",
    "\n",
    "def download_raw_data(data_location, output_folder, fmt = \"train\", image_format = MimeType.TIFF):\n",
    "    \"\"\" Downloads slope and sentinel-2 data for all plots associated\n",
    "        with an input CSV from a collect earth online survey\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path)\n",
    "         output_folder (os.path)\n",
    "        \n",
    "        Creates:\n",
    "         output_folder/{plot_id}.npy\n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_location)\n",
    "    df.columns = [x.upper() for x in df.columns]\n",
    "    for column in ['IMAGERY_TITLE', 'STACKINGPROFILEDG', 'PL_PLOTID', 'IMAGERYYEARDG',\n",
    "                  'IMAGERYMONTHPLANET', 'IMAGERYYEARPLANET', 'IMAGERYDATESECUREWATCH',\n",
    "                  'IMAGERYENDDATESECUREWATCH', 'IMAGERYFEATUREPROFILESECUREWATCH',\n",
    "                  'IMAGERYSTARTDATESECUREWATCH','IMAGERY_ATTRIBUTIONS',\n",
    "                  'SAMPLE_GEOM']:\n",
    "        if column in df.columns:\n",
    "            df = df.drop(column, axis = 1)\n",
    "\n",
    "    df = df.dropna(axis = 0)\n",
    "    plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "    existing = [int(x[:-4]) for x in os.listdir(f\"../data/{fmt}-dates/\") if \".DS\" not in x]\n",
    "    existing = existing + [139190271, 139187199, 139319876, 139319877]\n",
    "    to_download = [x for x in plot_ids if x not in existing]\n",
    "    print(f\"Starting download of {len(to_download)}\"\n",
    "          f\" plots from {data_location} to {output_folder}\")\n",
    "    for i, val in enumerate(reversed(to_download)):\n",
    "        print(f\"Downloading {i + 1}/{len(to_download)}, {val}\")\n",
    "        initial_bbx = calc_bbox(val, df = df)\n",
    "        sentinel2_bbx, epsg = bounding_box(initial_bbx, expansion = IMSIZE*10)\n",
    "        cloud_bbx, _ = bounding_box(initial_bbx, expansion = 96*10)\n",
    "        try:\n",
    "            # Identify cloud steps, download DEM, and download L2A series\n",
    "            cloud_probs, shadows, _, clean_dates = identify_clouds(cloud_bbx, epsg = epsg)\n",
    "            dem, _ = download_dem(val, epsg = epsg, df = df)\n",
    "            #to_remove, _ = calculate_cloud_steps(cloud_probs, clean_dates)\n",
    "            \n",
    "            #if len(to_remove) > 0:\n",
    "            #    cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "            #    clean_dates = np.delete(clean_dates, to_remove)\n",
    "            #    shadows = np.delete(shadows, to_remove, 0)\n",
    "                \n",
    "            to_remove = subset_contiguous_sunny_dates(clean_dates, cloud_probs)\n",
    "            if len(to_remove) > 0:\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "                clean_dates = np.delete(clean_dates, to_remove)\n",
    "                shadows = np.delete(shadows, to_remove, 0)\n",
    "                \n",
    "            _ = print_dates(clean_dates, np.mean(cloud_probs, axis = (1, 2)))\n",
    "        \n",
    "            s2, s2_dates = download_layer(sentinel2_bbx, clean_steps = clean_dates, epsg = epsg)    \n",
    "            \n",
    "            # Step to ensure that shadows, clouds, sentinel l2a have aligned dates\n",
    "            to_remove_clouds = [i for i, val in enumerate(clean_dates) if val not in s2_dates]\n",
    "            to_remove_dates = [val for i, val in enumerate(clean_dates) if val not in s2_dates]\n",
    "            if len(to_remove_clouds) > 0:\n",
    "                print(f\"Removing {to_remove_dates} from clouds because not in S2\")\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove_clouds, 0)\n",
    "                shadows = np.delete(shadows, to_remove_clouds, 0)\n",
    "            print(f\"Shadows {shadows.shape}, clouds {cloud_probs.shape},\"\n",
    "                  f\" S2, {s2.shape}, S2d, {s2_dates.shape}\")\n",
    "            \n",
    "            print(s2.shape)\n",
    "            \n",
    "            cloud_probs = cloud_probs[:, 24:-24, 24:-24]\n",
    "            shadows = shadows[:, 24:-24, 24:-24]\n",
    "            x, interp = remove_cloud_and_shadows(s2, cloud_probs, shadows, s2_dates)\n",
    "            to_remove = np.argwhere(np.mean(interp, axis = (1, 2)) > 0.5)\n",
    "            if len(to_remove) > 0:\n",
    "                print(f\"Removing {len(to_remove)} steps with >50% interpolation: {to_remove}\")\n",
    "                x = np.delete(x, to_remove, 0)\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "                s2_dates = np.delete(s2_dates, to_remove)\n",
    "                shadows = np.delete(shadows, to_remove, 0)\n",
    "                print(np.sum(shadows, axis = (1, 2)))\n",
    "            \n",
    "            x_to_save = np.copy(x)\n",
    "            x_to_save = np.clip(x_to_save, 0, 1)\n",
    "            x_to_save = np.trunc(x_to_save * 65535).astype(np.uint16)\n",
    "            np.save(f\"../data/{fmt}-dates/{str(val)}\", s2_dates)\n",
    "            np.save(f\"../data/{fmt}-raw/{str(val)}\", x_to_save)\n",
    "            file = f\"../data/{fmt}-raw/{str(val)}.npy\"\n",
    "            key = f'restoration-mapper/model-data/{fmt}/raw/{str(val)}.npy'\n",
    "            uploader.upload(bucket = 'restoration-monitoring', key = key, file = file)\n",
    "            print(\"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dates(dates):\n",
    "    \"\"\"For imagery that was downloaded prior to capping the number \n",
    "       of monthly images to be 3, it is necessary to enforce that cap\n",
    "       on the training / testing data.\n",
    "       \n",
    "       This function identifies the indices of the imagery to deletet\n",
    "       such that there is a maximum of three images per month.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    before = len(dates)\n",
    "    selected_indices = np.arange(len(dates))\n",
    "    begin = [-60, 0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n",
    "    end = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334, 390]\n",
    "    indices_to_remove = []\n",
    "    for x, y in zip(begin, end):\n",
    "        indices_month = np.argwhere(np.logical_and(dates >= x, dates < y)).flatten()\n",
    "        if len(indices_month) > 3:\n",
    "            to_delete = np.empty((0,))\n",
    "            if begin == -60:\n",
    "                to_delete = indices_month[:-3]\n",
    "            elif begin == 334:\n",
    "                to_delete = indices_month[3:]\n",
    "            elif len(indices_month) == 4:\n",
    "                to_delete = indices_month[1]\n",
    "            elif len(indices_month) == 5:\n",
    "                to_delete = np.array([indices_month[1],\n",
    "                                      indices_month[3]])\n",
    "            elif len(indices_month) == 6:\n",
    "                to_delete = np.array([indices_month[1],\n",
    "                                      indices_month[3],\n",
    "                                      indices_month[4]])\n",
    "                \n",
    "            to_delete = np.array(to_delete)\n",
    "            if to_delete.size > 0:\n",
    "                indices_to_remove.append(to_delete.flatten())\n",
    "                \n",
    "    if len(indices_to_remove) > 0:\n",
    "        indices_to_remove = np.concatenate(indices_to_remove)\n",
    "        after = before - len(indices_to_remove)\n",
    "        print(f\"Keeping {after}/{before}\")\n",
    "        return indices_to_remove\n",
    "    \n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "\n",
    "def subset_contiguous_sunny_dates(dates, probs):\n",
    "    \"\"\"\n",
    "    The general imagery subsetting strategy is as below:\n",
    "        - Select all images with < 30% cloud cover\n",
    "        - For each month, select up to 2 images that are <30% CC and are the closest to\n",
    "          the beginning and the midde of the month\n",
    "        - Select only one image per month for each month if the following criteria are met\n",
    "              - Within Q1 and Q4, apply if at least 3 images in quarter\n",
    "              - Otherwise, apply if at least 8 total images for year\n",
    "              - Select the second image if max CC < 15%, otherwise select least-cloudy image\n",
    "        - If more than 10 images remain, remove any images for April and September\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    probs = np.mean(probs, axis = (1, 2))\n",
    "    begin = [-60, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n",
    "    end = [31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334, 410]\n",
    "    n_per_month = []\n",
    "    months_to_adjust = []\n",
    "    months_to_adjust_again = []\n",
    "    indices_to_rm = []\n",
    "    indices = [x for x in range(len(dates))]\n",
    "    \n",
    "    def _indices_month(dates, x, y):\n",
    "        indices_month = np.argwhere(np.logical_and(\n",
    "                    dates >= x, dates < y)).flatten()\n",
    "        return indices_month\n",
    "\n",
    "    \n",
    "    _ = print_dates(dates, probs)\n",
    "    # Select the best 2 images per month to start with\n",
    "    best_two_per_month = []\n",
    "    for x, y in zip(begin, end):\n",
    "        indices_month = np.argwhere(np.logical_and(\n",
    "            dates >= x, dates < y)).flatten()\n",
    "\n",
    "        month_dates = dates[indices_month]\n",
    "        month_clouds = probs[indices_month]\n",
    "        month_good_dates = month_dates[month_clouds < 0.20]\n",
    "        indices_month = indices_month[month_clouds < 0.20]\n",
    "\n",
    "        if len(month_good_dates) >= 2:\n",
    "            if x > 0:\n",
    "                ideal_dates = [x, x + 15]\n",
    "            else:\n",
    "                ideal_dates = [0, 15]\n",
    "\n",
    "            # We first pick the 2 images with <30% cloud cover that are the closest\n",
    "            # to the 1st and 15th of the month\n",
    "            # todo: if both these images are above 15%, and one below 15% is available, include it\n",
    "            closest_to_first_img = np.argmin(abs(month_good_dates - ideal_dates[0]))\n",
    "            closest_to_second_img = np.argmin(abs(month_good_dates - ideal_dates[1]))\n",
    "            if closest_to_second_img == closest_to_first_img:\n",
    "                distances = abs(month_good_dates - ideal_dates[1])\n",
    "                closest_to_second_img = np.argsort(distances)[1]\n",
    "\n",
    "            first_image = indices_month[closest_to_first_img]\n",
    "            second_image = indices_month[closest_to_second_img]\n",
    "            best_two_per_month.append(first_image)\n",
    "            best_two_per_month.append(second_image)\n",
    "                    \n",
    "        elif len(month_good_dates) >= 1:\n",
    "            if x > 0:\n",
    "                ideal_dates = [x, x + 15]\n",
    "            else:\n",
    "                ideal_dates = [0, 15]\n",
    "\n",
    "            closest_to_second_img = np.argmin(abs(month_good_dates - ideal_dates[1]))\n",
    "            second_image = indices_month[closest_to_second_img]\n",
    "            best_two_per_month.append(second_image)\n",
    "                \n",
    "    dates_round_2 = dates[best_two_per_month]\n",
    "    probs_round_2 = probs[best_two_per_month]\n",
    "    \n",
    "    # We then select between those two images to keep a max of one per month\n",
    "    # We select the least cloudy image if the most cloudy has >15% cloud cover\n",
    "    # Otherwise we select the second image\n",
    "\n",
    "    # If there are more than 8 images, subset so only 1 image per month,\n",
    "    # To bring down to a min of 8 images\n",
    "    if len(dates_round_2) >= 8:\n",
    "        n_to_rm = len(dates_round_2) - 8\n",
    "        monthly_dates = []\n",
    "        monthly_probs = []\n",
    "        monthly_dates_date = []\n",
    "        removed = 0\n",
    "        for x, y in zip(begin, end):\n",
    "            indices_month = np.argwhere(np.logical_and(\n",
    "                dates >= x, dates < y)).flatten()\n",
    "            dates_month = dates[indices_month]\n",
    "            indices_month = [val for i, val in enumerate(indices_month) if dates_month[i] in dates_round_2]\n",
    "            if len(indices_month) > 1:\n",
    "                month_dates = dates[indices_month]\n",
    "                month_clouds = probs[indices_month]\n",
    "\n",
    "                subset_month = True\n",
    "                if x == -60:\n",
    "                    feb_mar = np.argwhere(np.logical_and(\n",
    "                        dates >= 31, dates < 90)).flatten()\n",
    "                    subset_month = False if len(feb_mar) < 2 else True\n",
    "                if x == 334:\n",
    "                    oct_nov = np.argwhere(np.logical_and(\n",
    "                        dates >= 273, dates < 334)).flatten()\n",
    "                    subset_month = False if len(oct_nov) < 2 else True\n",
    "\n",
    "                if subset_month:\n",
    "                    subset_month = True if removed <= n_to_rm else False\n",
    "                if subset_month:\n",
    "                    if np.max(month_clouds) >= 0.10:\n",
    "                        month_best_date = [indices_month[np.argmin(month_clouds)]]\n",
    "                    else:\n",
    "                        month_best_date = [indices_month[1]]\n",
    "                else:\n",
    "                    month_best_date = indices_month\n",
    "                monthly_dates.extend(month_best_date)\n",
    "                monthly_probs.extend(probs[month_best_date])\n",
    "                monthly_dates_date.extend(dates[month_best_date])\n",
    "                removed += 1\n",
    "            elif len(indices_month) == 1:\n",
    "                monthly_dates.append(indices_month[0])\n",
    "                monthly_probs.append(probs[indices_month[0]])\n",
    "                monthly_dates_date.append(dates[indices_month[0]])\n",
    "    else:\n",
    "        monthly_dates = best_two_per_month\n",
    "        \n",
    "    indices_to_rm = [x for x in indices if x not in monthly_dates]\n",
    "\n",
    "\n",
    "    dates_round_3 = dates[monthly_dates]\n",
    "    probs_round_3 = probs[monthly_dates]\n",
    "\n",
    "    if len(dates_round_3) >= 10:\n",
    "        delete_max = False\n",
    "        if np.max(probs_round_3) >= 0.15:\n",
    "            delete_max = True\n",
    "            indices_to_rm.append(monthly_dates[np.argmax(probs_round_3)])\n",
    "        for x, y in zip(begin, end):\n",
    "            indices_month = np.argwhere(np.logical_and(\n",
    "                dates >= x, dates < y)).flatten()\n",
    "            dates_month = dates[indices_month]\n",
    "            indices_month = [x for x in indices_month if x in monthly_dates]\n",
    "\n",
    "            n_removed = 0\n",
    "            if len(indices_month) >= 1:\n",
    "                if len(monthly_dates) == 11 and delete_max:\n",
    "                    continue\n",
    "                elif len(monthly_dates) >= 11:\n",
    "                    if x in [90, 243]:\n",
    "                        indices_to_rm.append(indices_month[0])\n",
    "\n",
    "    return indices_to_rm\n",
    "\n",
    "\n",
    "def to_int16(array: np.array) -> np.array:\n",
    "    '''Converts a float32 array to uint16, reducing storage costs by three-fold'''\n",
    "    array = np.clip(array, 0, 1)\n",
    "    array = np.trunc(array * 65535)\n",
    "    assert np.min(array >= 0)\n",
    "    assert np.max(array <= 65535)\n",
    "    \n",
    "    return array.astype(np.uint16)\n",
    "\n",
    "\n",
    "def process_raw(plot_id, path = 'train'):\n",
    "    \"\"\" Downloads slope and sentinel-2 data for all plots associated\n",
    "        with an input CSV from a collect earth online survey\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path)\n",
    "         output_folder (os.path)\n",
    "        \n",
    "        Creates:\n",
    "         output_folder/{plot_id}.npy\n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"         \n",
    "\n",
    "    x = np.load(f\"../data/{path}-raw/{plot_id}.npy\")\n",
    "    x = np.float32(x) / 65535\n",
    "    if x.shape[-1] == 10:\n",
    "        s2_dates = np.load(f\"../data/{path}-dates/{plot_id}.npy\")\n",
    "        dem = np.load(f\"../data/{path}-slope/{plot_id}.npy\")\n",
    "\n",
    "        assert x.shape[0] == s2_dates.shape[0]\n",
    "\n",
    "        missing_px = id_missing_px(x)\n",
    "        if len(missing_px) > 0:\n",
    "            print(f\"Deleting {missing_px} because of missing data\")\n",
    "            x = np.delete(x, missing_px, 0)\n",
    "            s2_dates = np.delete(s2_dates, missing_px)\n",
    "\n",
    "\n",
    "        n_images = x.shape[0]\n",
    "\n",
    "        to_remove = select_dates(s2_dates)\n",
    "        if len(to_remove) > 0:\n",
    "            x = np.delete(x, to_remove, 0)\n",
    "            s2_dates = np.delete(s2_dates, to_remove)\n",
    "\n",
    "        print(x.shape)\n",
    "        #to_remove = subset_contiguous_sunny_dates(s2_dates)\n",
    "        #if len(to_remove) > 0:\n",
    "        #    x = np.delete(x, to_remove, 0)\n",
    "        #    s2_dates = np.delete(s2_dates, to_remove)\n",
    "\n",
    "        for band in range(0, 10):\n",
    "            for time in range(0, x.shape[0]):\n",
    "                x_i = x[time, :, :, band]\n",
    "                x_i[np.argwhere(np.isnan(x_i))] = np.mean(x_i)\n",
    "                x[time, :, :, band] = x_i\n",
    "\n",
    "        # Interpolate linearly to 5 day frequency\n",
    "        tiles, max_distance = calculate_and_save_best_images(x, s2_dates)\n",
    "        sm = Smoother(lmbd = 150, size = tiles.shape[0],\n",
    "                      nbands = 10, dimx = tiles.shape[1], dimy = tiles.shape[2])\n",
    "        x = sm.interpolate_array(tiles)\n",
    "        x = superresolve_tile(x)\n",
    "        tiles = concatenate_dem(x, dem)\n",
    "        dates = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n",
    "        dates = np.array(dates) + 15\n",
    "        closest_date = []\n",
    "        for date in dates:\n",
    "            date_diff = s2_dates[np.argmin(abs(s2_dates - date))]\n",
    "            closest_date.append(date_diff)\n",
    "\n",
    "        closest_date = np.array(closest_date)\n",
    "        closest_date = closest_date[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "        closest_date = np.broadcast_to(closest_date, (12, 28, 28, 1))\n",
    "        closest_date = (closest_date + 45)  / 411\n",
    "\n",
    "        tiles = np.concatenate([tiles, closest_date], axis = -1)\n",
    "\n",
    "        if np.sum(np.isnan(tiles)) == 0:\n",
    "            print(f\"There are {np.sum(np.isnan(tiles))} NA values\")\n",
    "            if max_distance <= 300 and n_images >= 5:\n",
    "                tiles = to_int16(tiles)\n",
    "                #np.save(f\"../data/{path}-s2-new/{plot_id}\", tiles)\n",
    "                tile_path = f\"../data/{path}-s2/{plot_id}\"\n",
    "                tile_path = tile_path + \".hkl\"\n",
    "                hkl.dump(tiles, tile_path, mode='w', compression='gzip')\n",
    "                print(f\"Saved {tiles.shape} shape, {n_images} img,\"\n",
    "                      f\" to {tile_path} \\n\")\n",
    "            else:\n",
    "                print(f\"Skipping {plot_id} because {max_distance} distance, and {n_images} img \\n\")\n",
    "\n",
    "        return tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function execution\n",
    "## 1. Download DEM and Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "for i in (os.listdir(\"../data/train-csv/\")):\n",
    "    if \"chaco\" in i:\n",
    "        tile = download_new_dem(\"../data/train-csv/\" + i,\n",
    "                                \"../data/train-dem/\",\n",
    "                                image_format = MimeType.TIFF)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Raw data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (os.listdir(\"../data/train-csv/\")):\n",
    "    if \"chaco\" in i:\n",
    "        download_raw_data(\"../data/train-csv/\" + i,\n",
    "                          \"../data/train-raw/\", \n",
    "                          fmt = 'train',\n",
    "                          image_format = MimeType.TIFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process train / test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "plots = [str(x[:-4]) for x in os.listdir(\"../data/train-raw/\") if \".npy\" in x]\n",
    "for plot in plots:\n",
    "    i += 1\n",
    "    if not os.path.exists(\"../data/train-s2/\" + plot + \".hkl\"):\n",
    "        print(plot)\n",
    "        try:\n",
    "            tiles = process_raw(plot, path = 'train')\n",
    "            print(i, plot)\n",
    "        except:\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
