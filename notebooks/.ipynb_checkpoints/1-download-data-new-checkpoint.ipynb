{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data download pipeline\n",
    "\n",
    "Downloads 16x16 training data plots from Sentinel Hub, with the following steps:\n",
    "\n",
    "*  Convert coordinates to UTM, identify bounding boxes of 160 and 180 meter borders\n",
    "*  Download all L1C steps, correct missing bands, and calculate cloud cover\n",
    "*  Select L2A imagery corresponding to the best imagery per 15 days, with missing imagery calculated as the weighted average of the nearest time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants, DataSource, CustomUrlParam\n",
    "from s2cloudless import S2PixelCloudDetector, CloudMaskRequest\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import splu\n",
    "\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOCATION = '../data/train-csv/malawi-train.csv'\n",
    "OUTPUT_FOLDER = '../data/train-new-shadow/'\n",
    "EPSG = CRS.WGS84\n",
    "IMSIZE = 48\n",
    "existing = [int(x[:-4]) for x in os.listdir(\"../data/train-new-shadow/\") if \".DS\" not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/slope.py\n",
    "%run ../src/utils-bilinear.py\n",
    "%run ../src/dsen2/utils/DSen2Net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup function to reproject coordinates\n",
    "def convertCoords(xy, src='', targ=''):\n",
    "\n",
    "    srcproj = osr.SpatialReference()\n",
    "    srcproj.ImportFromEPSG(src)\n",
    "    targproj = osr.SpatialReference()\n",
    "    if isinstance(targ, str):\n",
    "        targproj.ImportFromProj4(targ)\n",
    "    else:\n",
    "        targproj.ImportFromEPSG(targ)\n",
    "    transform = osr.CoordinateTransformation(srcproj, targproj)\n",
    "\n",
    "    pt = ogr.Geometry(ogr.wkbPoint)\n",
    "    pt.AddPoint(xy[0], xy[1])\n",
    "    pt.Transform(transform)\n",
    "    return([pt.GetX(), pt.GetY()])\n",
    "\n",
    "def bounding_box(points, expansion = 160):\n",
    "    # LONG, LAT FOR SOME REASON\n",
    "    bl = list(points[0])\n",
    "    tr = list(points[1])\n",
    "    \n",
    "    if 48 <= bl[0] <= 54:\n",
    "        epsg = 32639 if bl[1] > 0 else 32739\n",
    "    if 42 <= bl[0] <= 48:\n",
    "        epsg = 32638 if bl[1] > 0 else 32738\n",
    "    if 36 <= bl[0] <= 42:\n",
    "        epsg = 32637 if bl[1] > 0 else 32737\n",
    "    if 30 <= bl[0] <= 36:\n",
    "        epsg = 32636 if bl[1] > 0 else 32736\n",
    "    if 24 <= bl[0] <= 30:\n",
    "        epsg = 32635 if bl[1] > 0 else 32735\n",
    "    if 18 <= bl[0] <= 24:\n",
    "        epsg = 32634 if bl[1] > 0 else 32734\n",
    "    if 12 <= bl[0] <= 18:\n",
    "        epsg = 32633 if bl[1] > 0 else 32733\n",
    "    if 6 <= bl[0] <= 12:\n",
    "        epsg = 32632 if bl[1] > 0 else 32732\n",
    "    if 0 <= bl[0] <= 6:\n",
    "        epsg = 32631 if bl[1] > 0 else 32731\n",
    "    if -90 <= bl[0] <= -84:\n",
    "        epsg = 32616 if bl[1] > 0 else 32716\n",
    "    if -96 <= bl[0] <= -90:\n",
    "        epsg = 32615 if bl[1] > 0 else 32715\n",
    "\n",
    "    bl = convertCoords(bl, 4326, epsg)\n",
    "    tr = convertCoords(tr, 4326, epsg)\n",
    "    init = [b - a for a,b in zip(bl, tr)]\n",
    "    distance1 = tr[0] - bl[0]\n",
    "    distance2 = tr[1] - bl[1]\n",
    "    expansion1 = (expansion - distance1)/2\n",
    "    expansion2 = (expansion - distance2)/2\n",
    "   # EXPANSION = (expansion - np.mean([distance1, distance2]))/2 # should this be 155 or 160?\n",
    "    bl = [bl[0] - expansion1, bl[1] - expansion2]\n",
    "    tr = [tr[0] + expansion1, tr[1] + expansion2]\n",
    "    #bl = [a - expansion1 for a in bl]\n",
    "    #tr = [a + expansion2 for a in tr]\n",
    "    \n",
    "    after = [b - a for a,b in zip(bl, tr)]   \n",
    "    print(after)\n",
    "    if max(init) > 130:\n",
    "        print(\"ERROR: Initial field greater than 130m\")\n",
    "    if min(init) < 120:\n",
    "        print(\"ERROR: Initial field less than 130m\")\n",
    "        \n",
    "    if min(after) < (expansion - 4.5):\n",
    "        print(\"ERROR\")\n",
    "    if max(after) > (expansion + 5):\n",
    "        print(\"ERROR\")\n",
    "    diffs = [b - a for b, a in zip(after, init)]\n",
    "\n",
    "    bl = convertCoords(bl, epsg, 4326)\n",
    "    tr = convertCoords(tr, epsg, 4326)\n",
    "    return bl, tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bbox(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # TOP, LEFT, BOTTOM, RIGHT\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])),\n",
    "            (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATA_LOCATION)\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1)\n",
    "df = df.dropna(axis = 0)\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[160.0, 160.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([29.667053997839567, -11.76100649111097],\n",
       " [29.668506970328114, -11.759547375727022])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_box(calc_bbox(plot_ids[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_detector = S2PixelCloudDetector(threshold=0.4, average_over=4, dilation_size=2)\n",
    "\n",
    "def threshold_shadows(arr):\n",
    "    arr = np.copy(arr)\n",
    "    iqr = np.percentile(arr.flatten(), 75) - np.percentile(arr.flatten(), 25)\n",
    "    low = np.percentile(arr.flatten(), 25)\n",
    "    #high = np.percentile(arr.flatten(), 75)\n",
    "    thresh_low = low - 1.5*iqr\n",
    "    #thresh_high = high + 2*iqr\n",
    "    #arr[np.where(arr > thresh_high)] = 1.\n",
    "    arr[np.where(arr < thresh_low)] = 1.\n",
    "    arr[np.where(arr < 1)] = 0.\n",
    "    arr = np.reshape(arr, (arr.shape[0], 6, 8, 6, 8))\n",
    "    arr = np.sum(arr, axis = (2, 4))\n",
    "    arr = resize(arr, (arr.shape[0], 48, 48), 0)\n",
    "    fake_shadows = np.zeros((arr.shape[0], arr.shape[1], arr.shape[2]))\n",
    "    for step in range(arr.shape[0]):\n",
    "        if step > 0:\n",
    "            for x in range(arr.shape[1]):\n",
    "                for y in range(arr.shape[2]):\n",
    "                    if arr[step, x, y] > 0:\n",
    "                        before = arr[step - 1, x, y]\n",
    "                        if abs(before - arr[step, x, y]) <= 16:\n",
    "                            fake_shadows[step, x, y] = 1\n",
    "    arr[np.where(arr > 2)] = 1.\n",
    "    before = np.sum(arr)\n",
    "    arr[np.where(fake_shadows == 1)] = 0.\n",
    "    after = np.sum(arr)\n",
    "    print(\"Removed {} fake shadows\".format(before - after))\n",
    "    print(\"There are now {} shadows to fix\".format(np.sum(arr)))\n",
    "    return arr\n",
    "\n",
    "def calculate_proximal_steps(uniques, date, clean_steps):\n",
    "    arg_before = None\n",
    "    arg_after = None\n",
    "    uniques = np.array(uniques)\n",
    "    satisfactory = np.argwhere(uniques > 2)\n",
    "    satisfactory = np.array([x for x in satisfactory if x in clean_steps])\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    print(arg_before, date, arg_after)\n",
    "    return arg_before, arg_after\n",
    "\n",
    "def calculate_proximal_steps_index(date, satisfactory):\n",
    "    arg_before = None\n",
    "    arg_after = None\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    return arg_before, arg_after\n",
    "\n",
    "\n",
    "def identify_clouds(bbox, epsg = EPSG, time = ('2018-12-15', '2019-12-15')):\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        cloud_request = WmsRequest(\n",
    "            layer='CLOUD_DETECTION',\n",
    "            bbox=box,\n",
    "            time=time,\n",
    "            width=IMSIZE,\n",
    "            height=IMSIZE,\n",
    "            image_format = MimeType.TIFF_d32f,\n",
    "            maxcc=0.5,\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=24),\n",
    "        )\n",
    "        \n",
    "        cloud_img = cloud_request.get_data()\n",
    "        cloud_probs = cloud_detector.get_cloud_probability_maps(np.array(cloud_img))\n",
    "        means = np.mean(cloud_probs, (1, 2))\n",
    "        clean_steps = [i for i, val in enumerate(means) if val < 0.20]\n",
    "        return clean_steps, means, cloud_probs\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "    \n",
    "    \n",
    "def download_dem(val, epsg = EPSG):\n",
    "    location = calc_bbox(val)\n",
    "    bbox = bounding_box(location, expansion = (IMSIZE+2)*10)\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM',\n",
    "                         bbox=box,\n",
    "                         width=IMSIZE+2,\n",
    "                         height=IMSIZE+2,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format=MimeType.TIFF_d32f,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    dem_image = calcSlope(dem_image.reshape((1, IMSIZE+2, IMSIZE+2)),\n",
    "                  np.full((IMSIZE+2, IMSIZE+2), 10), np.full((IMSIZE+2, IMSIZE+2), 10), zScale = 1, minSlope = 0.02)\n",
    "    dem_image = dem_image.reshape((IMSIZE+2, IMSIZE+2, 1))\n",
    "    dem_image = dem_image[1:IMSIZE+1, 1:IMSIZE+1, :]\n",
    "    return dem_image\n",
    "\n",
    "def calculate_shadows(bbox, epsg = EPSG, time = ('2018-12-15', '2019-12-15')):\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    image_request = WcsRequest(\n",
    "            layer='SCENE',\n",
    "            bbox=box,\n",
    "            time=time,\n",
    "            image_format = MimeType.TIFF_d32f,\n",
    "            maxcc=0.5,\n",
    "            resx='10m', resy='10m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=24),\n",
    "        )\n",
    "    img_bands = image_request.get_data()\n",
    "    img = np.stack(img_bands)\n",
    "    img[np.where(img != 3)] = 0\n",
    "    print(\"Shadows: {}\".format(img.shape))\n",
    "    shadow_sum = np.sum(img, axis = (1, 2))\n",
    "    shadow_steps = np.argwhere(shadow_sum) > (IMSIZE*IMSIZE) / 5\n",
    "    return img, shadow_steps\n",
    "\n",
    "\n",
    "def check_zenith(bbox, epsg = EPSG, time = ('2018-12-15', '2019-12-15')):\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        zenith = WmsRequest(\n",
    "            layer='ZENITH',\n",
    "            bbox=box,\n",
    "            time=time,\n",
    "            width=IMSIZE,\n",
    "            height=IMSIZE,\n",
    "            image_format = MimeType.TIFF_d32f,\n",
    "            maxcc=0.5,\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=24),\n",
    "        )\n",
    "        \n",
    "        zenith = zenith.get_data()\n",
    "        return zenith\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "    \n",
    "        \n",
    "        \n",
    "def calculate_and_save_best_images(img_bands, image_dates):\n",
    "    # This function interpolates data to 5 day windows linearly\n",
    "\n",
    "    biweekly_dates = [day for day in range(0, 360, 5)] # ideal imagery dates are every 15 days\n",
    "    \n",
    "    # Clouds have been removed at this step, so all steps are satisfactory\n",
    "    satisfactory_ids = [x for x in range(0, img_bands.shape[0])]\n",
    "    satisfactory_dates = [value for idx, value in enumerate(image_dates) if idx in satisfactory_ids]\n",
    "    \n",
    "    \n",
    "    selected_images = {}\n",
    "    for i in biweekly_dates:\n",
    "        distances = [abs(date - i) for date in satisfactory_dates]\n",
    "        closest = np.min(distances)\n",
    "        closest_id = np.argmin(distances)\n",
    "        # If there is imagery within 5 days, select it\n",
    "        if closest < 5:\n",
    "            date = satisfactory_dates[closest_id]\n",
    "            image_idx = int(np.argwhere(np.array(image_dates) == date)[0])\n",
    "            selected_images[i] = {'image_date': [date], 'image_ratio': [1], 'image_idx': [image_idx]}\n",
    "        # If there is not imagery within 5 days, look for the closest above and below imagery\n",
    "        else:\n",
    "            distances = np.array([(date - i) for date in satisfactory_dates])\n",
    "            # Number of days above and below the selected date of the nearest clean imagery\n",
    "            above = distances[np.where(distances < 0, distances, -np.inf).argmax()]\n",
    "            below = distances[np.where(distances > 0, distances, np.inf).argmin()]\n",
    "            if abs(above) > 100: # If date is the last date, occassionally argmax would set above to - number\n",
    "                above = below\n",
    "            if abs(below) > 100:\n",
    "                below = above\n",
    "            if above != below:\n",
    "                below_ratio = above / (above - below)\n",
    "                above_ratio = 1 - below_ratio\n",
    "            else:\n",
    "                above_ratio = below_ratio = 0.5\n",
    "                \n",
    "            # Extract the image date and imagery index for the above and below values\n",
    "            above_date = i + above\n",
    "            above_image_idx = int(np.argwhere(np.array(image_dates) == above_date)[0])\n",
    "            \n",
    "            below_date = i + below\n",
    "            below_image_idx = int(np.argwhere(np.array(image_dates) == below_date)[0])\n",
    "            \n",
    "            selected_images[i] = {'image_date': [above_date, below_date], 'image_ratio': [above_ratio, below_ratio],\n",
    "                                 'image_idx': [above_image_idx, below_image_idx]}\n",
    "                               \n",
    "    max_distance = 0\n",
    "    \n",
    "    for i in selected_images.keys():\n",
    "        #print(i, selected_images[i])\n",
    "        if len(selected_images[i]['image_date']) == 2:\n",
    "            dist = selected_images[i]['image_date'][1] - selected_images[i]['image_date'][0]\n",
    "            if dist > max_distance:\n",
    "                max_distance = dist\n",
    "    \n",
    "    print(\"Maximum time distance: {}\".format(max_distance))\n",
    "        \n",
    "    # Compute the weighted average of the selected imagery for each time step\n",
    "    keep_steps = []\n",
    "    for i in selected_images.keys():\n",
    "        info = selected_images[i]\n",
    "        if len(info['image_idx']) == 1:\n",
    "            step = img_bands[info['image_idx'][0]]\n",
    "        if len(info['image_idx']) == 2:\n",
    "            step1 = img_bands[info['image_idx'][0]] * info['image_ratio'][0]\n",
    "            step2 = img_bands[info['image_idx'][1]] * info['image_ratio'][1]\n",
    "            step = step1 + step2\n",
    "        keep_steps.append(step)\n",
    "        \n",
    "    keep_steps = np.stack(keep_steps)\n",
    "    return keep_steps, max_distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speyediff(N, d, format = 'csc'):\n",
    "    shape = (N-d, N)\n",
    "    diagonals = np.zeros(2*d + 1)\n",
    "    diagonals[d] = 1.\n",
    "    for i in range(d):\n",
    "        diff = diagonals[:-1] - diagonals[1:]\n",
    "        diagonals = diff\n",
    "    offsets = np.arange(d+1)\n",
    "    spmat = sparse.diags(diagonals, offsets, shape, format = format)\n",
    "    return spmat\n",
    "\n",
    "def smooth(y, lmbd, d = 2):\n",
    "    m = len(y)\n",
    "    E = sparse.eye(m, format = 'csc')\n",
    "    D = speyediff(m, d, format = 'csc')\n",
    "    coefmat = E + lmbd * D.conj().T.dot(D)\n",
    "    z = splu(coefmat).solve(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic Model Created.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "MDL_PATH = \"../src/dsen2/models/\"\n",
    "\n",
    "input_shape = ((4, None, None), (6, None, None))\n",
    "model = s2model(input_shape, num_layers=6, feature_size=128)\n",
    "predict_file = MDL_PATH+'s2_032_lr_1e-04.hdf5'\n",
    "print('Symbolic Model Created.')\n",
    "\n",
    "model.load_weights(predict_file)\n",
    "\n",
    "def DSen2(d10, d20):\n",
    "    test = [d10, d20]\n",
    "    input_shape = ((4, None, None), (6, None, None))\n",
    "    prediction = _predict(test, input_shape, deep=False)\n",
    "    #prediction *= 5\n",
    "    return prediction\n",
    "\n",
    "def _predict(test, input_shape, model = model, deep=False, run_60=False):\n",
    "    \n",
    "    print(\"Predicting using file: {}\".format(predict_file))\n",
    "    prediction = model.predict(test, verbose=1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "c_arr = np.array([[1, 1, 1, 1, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 2, 3, 2, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 1, 1, 1, 1,],])\n",
    "                  \n",
    "c_arr = c_arr / 3\n",
    "o_arr = 1 - c_arr\n",
    "c_arr = np.tile(c_arr[:, :, np.newaxis], (1, 1, 11))\n",
    "o_arr = np.tile(o_arr[:, :, np.newaxis], (1, 1, 11))\n",
    "\n",
    "def download_layer(bbox, epsg = EPSG, time = ('2018-12-15', '2019-12-15')):\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A20',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                image_format = MimeType.TIFF_d32f,\n",
    "                maxcc=0.5,\n",
    "                resx='10m', resy='10m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=24),\n",
    "            )\n",
    "        img_bands = image_request.get_data()\n",
    "        img_20 = np.stack(img_bands)\n",
    "        img_20 = resize(img_20, (img_20.shape[0], IMSIZE, IMSIZE, img_20.shape[-1]), order = 0)\n",
    "        \n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A10',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                image_format = MimeType.TIFF_d32f,\n",
    "                maxcc=0.5,\n",
    "                resx='10m', resy='10m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=24),\n",
    "        )\n",
    "        \n",
    "        img_bands = image_request.get_data()\n",
    "        img_10 = np.stack(img_bands)\n",
    "        img_10 = resize(img_10, (img_10.shape[0], IMSIZE, IMSIZE, img_10.shape[-1]), order = 0)\n",
    "        shadows = img_10[:, :, :, -1]\n",
    "        img_10 = img_10[:, :, :, :-1]\n",
    "        \n",
    "        shadows[np.where(shadows != 3)] = 0\n",
    "        shadows[np.where(shadows == 3)] = 1\n",
    "        shadows_sums = np.sum(shadows, axis = 0)\n",
    "        before = np.sum(shadows)\n",
    "        #shadows[np.where(shadows_sums > shadows.shape[0]/2)] = 0.\n",
    "        print(\"Difference: {}\".format(np.sum(shadows) - before))\n",
    "        print(\"Shadows: {}\".format(shadows.shape))\n",
    "        shadow_sum = np.sum(shadows, axis = (1, 2))\n",
    "        shadow_steps = np.argwhere(shadow_sum > (IMSIZE*IMSIZE) / 5)\n",
    "        \n",
    "        img = np.concatenate([img_10, img_20], axis = -1)\n",
    "        return img, image_request, shadows, shadow_steps\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "        \n",
    "def remove_cloud_and_shadows(tiles, probs, shadows, wsize = 5):\n",
    "    c_probs = np.copy(probs)\n",
    "    c_probs = c_probs - np.min(c_probs, axis = 0)\n",
    "    c_probs = np.reshape(c_probs, [c_probs.shape[0], int(IMSIZE/8), 8, int(IMSIZE/8), 8])\n",
    "    c_probs = np.max(c_probs, axis = (2, 4))\n",
    "    c_probs = resize(c_probs, (c_probs.shape[0], IMSIZE, IMSIZE), 0)\n",
    "    c_probs[np.where(c_probs > 0.3)] = 1.\n",
    "    c_probs += shadows\n",
    "    \n",
    "    for cval in range(0, IMSIZE - 4, 1):\n",
    "        for rval in range(0, IMSIZE - 4, 1):\n",
    "            subs = c_probs[:, cval:cval + wsize, rval:rval+wsize]\n",
    "            satisfactory = [x for x in range(c_probs.shape[0]) if np.sum(subs[x, :, :]) < 5]\n",
    "            satisfactory = np.array(satisfactory)\n",
    "            for date in range(0, tiles.shape[0]):\n",
    "                if np.sum(subs[date, :, :]) > 10:\n",
    "                    before, after = calculate_proximal_steps_index(date, satisfactory)\n",
    "                    before = date + before\n",
    "                    after = date + after\n",
    "                    bef = tiles[before, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    aft = tiles[after, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    bef_wt = 1 - (abs(before) / (abs(before) + abs(after)))\n",
    "                    aft_wt = 1 - bef_wt\n",
    "                    candidate = bef_wt*bef + aft_wt*aft\n",
    "                    candidate = candidate*c_arr + tiles[date, cval:cval+wsize, rval:rval+wsize, : ]*o_arr\n",
    "                    tiles[date, cval:cval+wsize, rval:rval+wsize, : ] = candidate                    \n",
    "    return tiles\n",
    "\n",
    "def remove_missed_clouds(img):\n",
    "    iqr = np.percentile(img[:, :, :, 3].flatten(), 75) - np.percentile(img[:, :, :, 3].flatten(), 25)\n",
    "    thresh_t = np.percentile(img[:, :, :, 3].flatten(), 75) + iqr*2\n",
    "    thresh_b = np.percentile(img[:, :, :, 3].flatten(), 25) - iqr*2\n",
    "    diffs_fw = np.diff(img, 1, axis = 0)\n",
    "    diffs_fw = np.mean(diffs_fw, axis = (1, 2, 3))\n",
    "    diffs_fw = np.array([0] + list(diffs_fw))\n",
    "    diffs_bw = np.diff(np.flip(img, 0), 1, axis = 0)\n",
    "    diffs_bw = np.flip(np.mean(diffs_bw, axis = (1, 2, 3)))\n",
    "    diffs_bw = np.array(list(diffs_bw) + [0])\n",
    "    diffs = abs(diffs_fw - diffs_bw) * 100 # 3, -3 -> 6, -3, 3 -> 6, -3, -3\n",
    "    #diffs = [int(x) for x in diffs]\n",
    "    outlier_percs = []\n",
    "    for step in range(img.shape[0]):\n",
    "        bottom = len(np.argwhere(img[step, :, :, 3].flatten() > thresh_t))\n",
    "        top = len(np.argwhere(img[step, :, :, 3].flatten() < thresh_b))\n",
    "        p = 100* ((bottom + top) / (IMSIZE*IMSIZE))\n",
    "        outlier_percs.append(p)\n",
    "    to_remove = np.argwhere(np.array(outlier_percs) > 15)\n",
    "    print([int(x) for x in outlier_percs])\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING DOWNLOAD OF 2 plots from ../data/train-csv/malawi-train.csv to ../data/train-new-shadow/\n",
      "Downloading 1/2, 135672755\n",
      "[480.0, 480.0]\n",
      "[500.0, 500.0]\n",
      "Difference: 0.0\n",
      "Shadows: (54, 48, 48)\n",
      "There are 16/54 dirty steps: 11 cloud, 1 missing, 0 zenith, 5 shadows\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Removing 0 steps based on ratio\n",
      "Removed 0.0 fake shadows\n",
      "There are now 0.0 shadows to fix\n",
      "Clean: (38, 16, 16, 15)\n",
      "Maximum time distance: 47\n",
      "(24, 16, 16, 15)\n",
      "\n",
      "\n",
      "Downloading 2/2, 135672799\n",
      "[480.0, 480.0]\n",
      "[500.0, 500.0]\n",
      "Difference: 0.0\n",
      "Shadows: (55, 48, 48)\n",
      "There are 25/55 dirty steps: 14 cloud, 3 missing, 0 zenith, 12 shadows\n",
      "[1, 3, 11, 8, 11, 7, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Removing 1 steps based on ratio\n",
      "Removed 1536.0 fake shadows\n",
      "There are now 192.0 shadows to fix\n",
      "Clean: (29, 16, 16, 15)\n",
      "Maximum time distance: 94\n",
      "(24, 16, 16, 15)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "super_resolve = False\n",
    "to_download = [x for x in plot_ids if x not in existing]\n",
    "#to_download = [135542409]\n",
    "errors = []\n",
    "year = 2019\n",
    "print(\"STARTING DOWNLOAD OF {} plots from {} to {}\".format(len(to_download), DATA_LOCATION, OUTPUT_FOLDER))\n",
    "for i, val in enumerate(to_download):\n",
    "    print(\"Downloading {}/{}, {}\".format(i+1, len(to_download), val))\n",
    "    location = calc_bbox(val)\n",
    "    location = bounding_box(location, expansion = IMSIZE*10)\n",
    "    try:\n",
    "        # Identify cloud steps, download DEM, and download L2A series\n",
    "        clean_steps, means, probs = identify_clouds(location)\n",
    "        dem = download_dem(val)\n",
    "        img, image_request, shadows, shadow_steps = download_layer(location)\n",
    "\n",
    "        # Subset zenith < 70\n",
    "        zenith = check_zenith(location)\n",
    "        zenith = np.mean(np.stack(zenith), axis = (1, 2))\n",
    "        zenith_outliers = np.argwhere(zenith > 70)\n",
    "        if len(zenith_outliers) > 0:\n",
    "            print(\"Zenith outlier: {}\".format(zenith_outliers))\n",
    "\n",
    "        # Calculate imagery dates\n",
    "        image_dates = []\n",
    "        for date in image_request.get_dates():\n",
    "            if date.year == year - 1:\n",
    "                image_dates.append(-360 + (date.month-1)*30 + date.day)\n",
    "            if date.year == year:\n",
    "                image_dates.append((date.month-1)*30 + date.day)\n",
    "            if date.year == year + 1:\n",
    "                image_dates.append(365 + (date.month-1)*30+date.day)\n",
    "        image_dates = np.array(image_dates)\n",
    "\n",
    "        # Remove imagery where >4% is clouds, and where there is null data\n",
    "        args = np.array([len(np.argwhere(probs[x].flatten() > 0.3)) for x in range(probs.shape[0])])\n",
    "        dirty_steps = np.argwhere(args > (IMSIZE)*(IMSIZE) / 10)\n",
    "        missing_images = [np.argwhere(img[x, :, : :].flatten() == 0.0) for x in range(img.shape[0])]\n",
    "        missing_images = np.array([len(x) for x in missing_images])\n",
    "        missing_images_p = [np.argwhere(img[x, :, : :].flatten() >= 1) for x in range(img.shape[0])]\n",
    "        missing_images_p = np.array([len(x) for x in missing_images_p])\n",
    "        missing_images += missing_images_p\n",
    "        missing_images = list(np.argwhere(missing_images >= 25))\n",
    "        to_remove = np.unique(np.array(list(dirty_steps) + list(missing_images) + list(zenith_outliers) + list(shadow_steps)))\n",
    "\n",
    "        # Remove null steps\n",
    "        print(\"There are {}/{} dirty steps: {} cloud, {} missing, {} zenith, {} shadows\".format(len(to_remove),\n",
    "                                                                                    len(img), len(dirty_steps),\n",
    "                                                                                    len(missing_images),\n",
    "                                                                                    len(zenith_outliers),\n",
    "                                                                                    len(shadow_steps)))\n",
    "\n",
    "        img = np.delete(img, to_remove, 0)\n",
    "        probs = np.delete(probs, to_remove, 0)\n",
    "        shadows = np.delete(shadows, to_remove, 0)\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        \n",
    "        to_remove = remove_missed_clouds(img)\n",
    "        img = np.delete(img, to_remove, 0)\n",
    "        shadows = np.delete(shadows, to_remove, 0)\n",
    "        probs = np.delete(probs, to_remove, 0)\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        print(\"Removing {} steps based on ratio\".format(len(to_remove)))\n",
    "\n",
    "\n",
    "        # Concatenate DEM\n",
    "        dem = np.tile(dem.reshape((1, IMSIZE, IMSIZE, 1)), (img.shape[0], 1, 1, 1))\n",
    "        tiles = np.concatenate([img, dem], axis = -1)\n",
    "        tiles[:, :, :, -1] /= 90\n",
    "        \n",
    "        new_shadows = threshold_shadows(tiles[:, :, :, 3])\n",
    "        x = remove_cloud_and_shadows(tiles, probs, new_shadows)\n",
    "        if super_resolve:\n",
    "            x = x[:, 8:40, 8:40, :]\n",
    "            print(\"Before super: {}\".format(x.shape))\n",
    "\n",
    "            d10 = x[:, :, :, 0:4]\n",
    "            d20 = x[:, :, :, 4:10]\n",
    "\n",
    "            d10 = np.swapaxes(d10, 1, -1)\n",
    "            d10 = np.swapaxes(d10, 2, 3)\n",
    "            d20 = np.swapaxes(d20, 1, -1)\n",
    "            d20 = np.swapaxes(d20, 2, 3)\n",
    "            superresolved = DSen2(d10, d20)\n",
    "            print(superresolved.shape)\n",
    "            superresolved = np.swapaxes(superresolved, 1, -1)\n",
    "            print(superresolved.shape)\n",
    "            superresolved = np.swapaxes(superresolved, 1, 2)\n",
    "            print(superresolved.shape)\n",
    "            print(x.shape)\n",
    "\n",
    "            # returns band IDXs 3, 4, 5, 7, 8, 9\n",
    "            x[:, :, :, 4:10] = superresolved\n",
    "            x = x[:, 8:24, 8:24, :]\n",
    "            print(\"After super shape: {}\".format(x.shape))\n",
    "        else:\n",
    "            bottom = int(IMSIZE/2 - 8)\n",
    "            top = int(IMSIZE/2 + 8)\n",
    "            x = x[:, bottom:top, bottom:top, :]\n",
    "\n",
    "        # Calculate indices\n",
    "        tiles, amin = evi(x, True)\n",
    "        # Where evi is OOB, remove (likely cloud cover missed)\n",
    "        if len(amin) > 0:\n",
    "            satisfactory = [x for x in range(tiles.shape[0]) if x not in amin]\n",
    "            for i in amin:\n",
    "                before, after = calculate_proximal_steps_index(i, satisfactory)\n",
    "                print(\"Interpolating {} with {} and {}\".format(i, before, after))\n",
    "                bef = tiles[before, :, :, :]\n",
    "                aft = tiles[after, :, :, :]\n",
    "                tiles[i, :, :, :] = (bef + aft) / 2\n",
    "\n",
    "        tiles = bi(tiles, True)\n",
    "        tiles = msavi2(tiles, True)\n",
    "        x = si(tiles, True)\n",
    "\n",
    "        print(\"Clean: {}\".format(x.shape))\n",
    "\n",
    "        # Interpolate linearly to 5 day frequency\n",
    "        tiles, max_distance = calculate_and_save_best_images(x, image_dates) # 22, 16, 16, 10\n",
    "\n",
    "        # Smooth linear interpolation\n",
    "        for row in range(0, 16):\n",
    "            for column in range(0, 16):\n",
    "                for band in [x for x in range(0, 15) if x != 10]:\n",
    "                    sm = smooth(tiles[:, row, column, band], 1.5, d = 3)\n",
    "                    tiles[:, row, column, band] = sm\n",
    "\n",
    "        # Retain only iamgery every 15 days\n",
    "        biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "        to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "        tiles = np.delete(tiles, to_remove, 0)\n",
    "        print(tiles.shape)\n",
    "\n",
    "        if max_distance <= 160:\n",
    "            np.save(OUTPUT_FOLDER + str(val), tiles)\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print(\"Skipping {} because there is a {} distance\".format(val, max_distance))\n",
    "            print(\"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        logging.fatal(e, exc_info=True)\n",
    "        errors.append(img)\n",
    "        #continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
