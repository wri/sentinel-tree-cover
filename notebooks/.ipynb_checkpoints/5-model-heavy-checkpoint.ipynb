{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master model development\n",
    "\n",
    "## John Brandt\n",
    "\n",
    "### Last updated: Feb 4 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Package loading\n",
    "*  Hyperparameter definitions\n",
    "*  Additional layer definitions\n",
    "*  Model definition\n",
    "*  Data loading\n",
    "*  Data preprocessing\n",
    "*  K means clustering\n",
    "*  Augment training data\n",
    "*  Loss definition\n",
    "*  Equibatch creation\n",
    "*  Model training\n",
    "*  Model validation and sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment log\n",
    "*  Baseline - Bilinear up/downsample separate for 10, 20m bands, no superresolution\n",
    "*  Bilinear up/downsample for 10m bands, nearest upsample for 20m bands to 10m\n",
    "*  Nearest up/downsample for all bands, no superresolution\n",
    "*  Bicubic up/downsample for all bands, no superresolution\n",
    "*  Bicubic up/downsample separate for 10, 20m bands, no superresolution\n",
    "*  Above w/ superresolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Remove imports that aren't needed to save RAM\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras.layers import Conv2D, Lambda, Dense, Multiply, Add, Bidirectional, ConvLSTM2D\n",
    "from tensorflow.python.keras.activations import selu\n",
    "from tensorflow.initializers import glorot_normal, lecun_normal\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim import conv2d\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from tflearn.layers.conv import global_avg_pool\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.layers import batch_normalization\n",
    "from tensorflow.python.util import deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/zoneout.py\n",
    "%run ../src/convgru.py\n",
    "%run ../src/lovasz.py\n",
    "%run ../src/utils.py\n",
    "%run ../src/adabound.py\n",
    "%run ../src/slope.py\n",
    "%run ../src/dropblock.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONE_OUT_PROB = 0.35\n",
    "L2_REG = 0.0\n",
    "INITIAL_LR = 2e-4\n",
    "FINAL_LR = 1e-3\n",
    "BN_MOMENTUM = 0.9\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_RATIO = 0.8\n",
    "TEST_RATIO = 0.2\n",
    "MAX_DROPBLOCK = 0.85\n",
    "\n",
    "gru_flt = 32\n",
    "fpa_flt = 32\n",
    "out_conv_flt = 32\n",
    "\n",
    "\n",
    "AUGMENTATION_RATIO = 4\n",
    "IMAGE_SIZE = 16\n",
    "LABEL_SIZE = 14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional layer definitions\n",
    "\n",
    "Some of these may be able to be removed, or moved to a src/*.py\n",
    "\n",
    "*  Conv SELU\n",
    "*  Conv BN ELU\n",
    "*  Feature pyramid attention (with downsample / upsample)\n",
    "*  Feature pyramid attention (w/o downsample / upsample)\n",
    "*  Temporal attention\n",
    "*  CSE, SSE, cCSE\n",
    "*  Reflection padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_elu(inp, is_training, kernel_size, scope,\n",
    "                filter_count = 16, pad = True, padding = 'valid', dilated = False,\n",
    "                activation = True):\n",
    "    if activation:\n",
    "        act = selu\n",
    "    else:\n",
    "        act = None\n",
    "    print(act)\n",
    "    if not dilated:\n",
    "        padded = ReflectionPadding2D((1, 1,))(inp)\n",
    "        conv = Conv2D(filters = filter_count, kernel_size = (kernel_size, kernel_size), activation = act,\n",
    "                        padding = padding, kernel_initializer = lecun_normal())(padded)\n",
    "    if not dilated and not pad:\n",
    "        conv = Conv2D(filters = filter_count, kernel_size = (kernel_size, kernel_size), activation = act,\n",
    "                        padding = padding, kernel_initializer = lecun_normal())(inp)\n",
    "    if dilated:\n",
    "        padded = ReflectionPadding2D((2, 2,))(inp)\n",
    "        conv = Conv2D(filters = filter_count, kernel_size = (3, 3), activation = act, dilation_rate = (2, 2),\n",
    "                        padding = padding, kernel_initializer = lecun_normal())(padded)\n",
    "    return conv\n",
    "\n",
    "def conv_bn_relu(inp, is_training, kernel_size, scope, filters, activation = True, use_bias = False):\n",
    "    \n",
    "    conv = Conv2D(filters = filters, kernel_size = (kernel_size, kernel_size), activation = None,\n",
    "                  padding = 'valid', use_bias = use_bias,\n",
    "                  kernel_initializer = tf.keras.initializers.he_normal())(inp)\n",
    "    \n",
    "    bn = Batch_Normalization(conv, is_training, scope)\n",
    "    if activation:\n",
    "        bn = tf.nn.relu(bn)\n",
    "\n",
    "    return bn\n",
    "    \n",
    "def fpa(inp, is_training, filter_count):\n",
    "    #one = conv_bn_elu(inp, is_training, 1, 'forward1', filter_count, False, 'valid', False, False)\n",
    "    #five = conv_bn_elu(inp, is_training, 5, 'down1', filter_count, False, 'valid', False, True)\n",
    "    #five_f = conv_bn_elu(five, is_training, 5, 'down1_f', filter_count, False, 'valid', False, False)\n",
    "    #three = conv_bn_elu(five, is_training, 3, 'down2', filter_count, False, 'valid', False, True)\n",
    "    #three_f = conv_bn_elu(three, is_training, 3, 'down2_f', filter_count, False, 'valid', False, False)\n",
    "    \n",
    "    one = conv_bn_relu(inp, is_training, 1, 'forward1', filter_count, False)\n",
    "    five = conv_bn_relu(inp, is_training, 5, 'down1', filter_count, True)\n",
    "    five_f = conv_bn_relu(five, is_training, 5, 'down1_f', filter_count, False)\n",
    "    three = conv_bn_relu(five, is_training, 3, 'down2', filter_count, True)\n",
    "    three_f = conv_bn_relu(three, is_training, 3, 'down2_f', filter_count, False)\n",
    "    \n",
    "    \n",
    "    three_up = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'bilinear')(three_f)\n",
    "    #three_up = get_deconv2d(three_f, filter_count, filter_count, 'three_up', is_training)\n",
    "    three_up = tf.nn.relu(three_up)\n",
    "    five_up = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'bilinear')(five_f)\n",
    "    #five_up = get_deconv2d(five_f, filter_count, filter_count, 'five_up', is_training)\n",
    "    five_up = tf.nn.relu(five_up)\n",
    "    \n",
    "    print(\"One: {}\".format(one.shape))\n",
    "    print(\"Five: {}\".format(five.shape))\n",
    "    print(\"Five_F: {}\".format(five_f.shape))\n",
    "    print(\"Three: {}\".format(three.shape))\n",
    "    print(\"Three_f: {}\".format(three_f.shape))\n",
    "    print(\"Three_up: {}\".format(three_up.shape))\n",
    "    print(\"Five_up: {}\".format(five_up.shape))\n",
    "    \n",
    "    # top block\n",
    "    #pooled = tf.keras.layers.GlobalAveragePooling2D()(inp)\n",
    "    #one_top = conv_bn_elu(tf.reshape(pooled, (-1, 1, 1, pooled.shape[-1])),\n",
    "    #                      is_training, 1, 'top1', filter_count, pad = False)\n",
    "    #four_top = tf.keras.layers.UpSampling2D((16, 16))(one_top)\n",
    "    #rint(\"Sixteen top: {}\".format(four_top.shape))\n",
    "    \n",
    "    \n",
    "    concat_1 = tf.nn.relu(tf.multiply(one, tf.nn.relu(tf.add(three_up, five_up))))\n",
    "    #concat_2 = tf.add(concat_1, four_top)\n",
    "    print(\"Feature pyramid attention shape {}\".format(concat_1.shape))\n",
    "    return concat_1\n",
    "    \n",
    "def create_deconv_init(filter_size, num_channels):\n",
    "    bilinear_kernel = np.zeros([filter_size, filter_size], dtype=np.float32)\n",
    "    scale_factor = (filter_size + 1) // 2\n",
    "    if filter_size % 2 == 1:\n",
    "        center = scale_factor - 1\n",
    "    else:\n",
    "        center = scale_factor - 0.5\n",
    "    for x in range(filter_size):\n",
    "        for y in range(filter_size):\n",
    "            bilinear_kernel[x,y] = (1 - abs(x - center) / scale_factor) * \\\n",
    "                                   (1 - abs(y - center) / scale_factor)\n",
    "    weights = np.zeros((filter_size, filter_size, num_channels, num_channels))\n",
    "    for i in range(num_channels):\n",
    "        weights[:, :, i, i] = bilinear_kernel\n",
    "\n",
    "    #assign numpy array to constant_initalizer and pass to get_variable\n",
    "    bilinear_init = tf.constant_initializer(value=weights, dtype=tf.float32)\n",
    "    return bilinear_init\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_deconv2d(inp, filter_count, num_channels, scope, is_training):\n",
    "    bilinear_init = create_deconv_init(4, filter_count)\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = filter_count, kernel_size = (4, 4),\n",
    "                                        strides=(2, 2), padding='same', \n",
    "                                        use_bias = False,\n",
    "                                        kernel_initializer = bilinear_init)(inp)\n",
    "    #x = ELU()(x)\n",
    "    #x = tf.nn.relu(x)\n",
    "    x = Batch_Normalization(x, training=is_training, scope = scope + \"bn\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    return batch_normalization(inputs=x, \n",
    "                               momentum = BN_MOMENTUM, \n",
    "                               training=training,\n",
    "                               renorm = True,\n",
    "                               reuse=None,\n",
    "                               name = scope)\n",
    "\n",
    "def temporal_attention(inp, units):\n",
    "    # This rescales each output\n",
    "    # Timesteps that are more important get weighted higher\n",
    "    # Timesteps that are least important get weighted lower --> B, N, H, W, C\n",
    "    conved = TimeDistributed(Conv2D(units, (1, 1), padding = 'same', kernel_initializer = 'glorot_uniform',\n",
    "                            activation = 'tanh', strides = (1, 1)))(inp)\n",
    "    \n",
    "    \n",
    "    #conved = tf.reshape(conved, (-1, units, 16, 16, STEPS))\n",
    "    print(\"Attention weight shape: {}\".format(conved.shape))\n",
    "    conved = TimeDistributed(Conv2D(1, (1, 1), padding = 'same', kernel_initializer = 'glorot_uniform',\n",
    "                            activation = 'sigmoid', use_bias = False, strides = (1, 1)))(conved)\n",
    "    print(\"Conved sigmoid shape: {}\".format(conved.shape))\n",
    "    #conved = tf.reshape(conved, (-1, 24, 1, 1, 1))\n",
    "    \n",
    "    alphas = tf.reduce_sum(conved, axis = 1, keep_dims = True)\n",
    "    print(\"Attention alphas: {}\".format(alphas.shape))\n",
    "    # We need to calculate the total sum for each pixel for each channel, so that we can combine them\n",
    "    alphas = conved / alphas\n",
    "    print(\"Attention weight shapes {}\".format(alphas.shape))\n",
    "    \n",
    "    # This actually multiplies the Conv by the input\n",
    "    multiplied = tf.reduce_sum(alphas * inp, axis = 1)\n",
    "    return multiplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cse_block(prevlayer, prefix):\n",
    "    mean = Lambda(lambda xin: K.mean(xin, axis=[1, 2]))(prevlayer)\n",
    "    lin1 = Dense(K.int_shape(prevlayer)[3] // 2, name=prefix + 'cse_lin1', activation='relu')(mean)\n",
    "    lin2 = Dense(K.int_shape(prevlayer)[3], name=prefix + 'cse_lin2', activation='sigmoid')(lin1)\n",
    "    x = Multiply()([prevlayer, lin2])\n",
    "    return x\n",
    "\n",
    "\n",
    "def sse_block(prevlayer, prefix):\n",
    "    conv = Conv2D(1, (1, 1), padding=\"same\", kernel_initializer=\"glorot_uniform\",\n",
    "                  activation='sigmoid', strides=(1, 1),\n",
    "                  name=prefix + \"_conv\")(prevlayer)\n",
    "    conv = Multiply(name=prefix + \"_mul\")([prevlayer, conv])\n",
    "    return conv\n",
    "\n",
    "\n",
    "def csse_block(x, prefix):\n",
    "    '''\n",
    "    Implementation of Concurrent Spatial and Channel ‘Squeeze & Excitation’ in Fully Convolutional Networks\n",
    "    https://arxiv.org/abs/1803.02579\n",
    "    '''\n",
    "    cse = cse_block(x, prefix)\n",
    "    sse = sse_block(x, prefix)\n",
    "    x = Add(name=prefix + \"_csse_mul\")([cse, sse])\n",
    "\n",
    "    return x\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "    \n",
    "def gru_block(inp, length, size, flt, scope, train, normalize = True):\n",
    "    with tf.variable_scope(scope):\n",
    "        print(\"GRU input shape {}, zoneout: {}\".format(inp.shape, ZONE_OUT_PROB))\n",
    "        cell_fw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID', normalize = normalize, fpa = True)\n",
    "        cell_bw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID', normalize = normalize, fpa = True)\n",
    "        cell_fw = ZoneoutWrapper(\n",
    "           cell_fw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        cell_bw = ZoneoutWrapper(\n",
    "            cell_bw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        steps, out = convGRU(inp, cell_fw, cell_bw, length)\n",
    "        gru = tf.concat(out, axis = -1)\n",
    "        steps = tf.concat(steps, axis = -1)\n",
    "        print(\"Down block output shape {}\".format(gru.shape))\n",
    "    return gru, steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bands = 14\n",
    "reg = keras.regularizers.l2(L2_REG) # for GRU\n",
    "inp = tf.placeholder(tf.float32, shape=(None, 24, IMAGE_SIZE, IMAGE_SIZE, n_bands))\n",
    "length = tf.placeholder(tf.int32, shape = (None, 1))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 14, 14))#, 1))\n",
    "keep_rate = tf.placeholder_with_default(1.0, ()) # For DropBlock\n",
    "length2 = tf.reshape(length, (-1,)) # Remove\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training') # For BN, DropBlock\n",
    "alpha = tf.placeholder(tf.float32, shape = ()) # For loss scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsure if this block is better replaced by 1x1 ConvGRU output\n",
    "#stacked = tf.reshape(inp, (-1, 16, 16, n_bands*24))\n",
    "#pixel_information = Conv2D(filters = gru_flt, kernel_size = (1, 1),\n",
    "#            padding = 'valid', activation = selu, kernel_initializer = lecun_normal())(stacked)\n",
    "    \n",
    "# 3 x 3 GRU\n",
    "\n",
    "inp_first_half = inp[:, :, :, :, :10]\n",
    "inp_second_half = inp[:, :, :, :, 11:]\n",
    "no_dem = tf.concat([inp_first_half, inp_second_half], axis = -1)\n",
    "dem = tf.reshape(tf.reduce_mean(inp[:, :, :, :, 10], axis = 1), (-1, 16, 16, 1))\n",
    "gru_out, steps = gru_block(inp = no_dem, length = length2, \n",
    "                            size = [16, 16], \n",
    "                            flt = gru_flt, \n",
    "                            scope = 'down_16', \n",
    "                            train = is_training)\n",
    "\n",
    "\n",
    "gru_out = tf.concat([gru_out, dem], axis = -1)\n",
    "csse1 = csse_block(gru_out, 'csse1')\n",
    "drop_block1 = DropBlock2D(keep_prob=keep_rate, block_size=5)\n",
    "csse1 = drop_block1(csse1, is_training)\n",
    "\n",
    "# Light FPA, CSSE, 4x4 Drop block\n",
    "fpa1 = fpa(csse1, is_training, fpa_flt)\n",
    "csse2 = csse_block(fpa1, 'csse2')\n",
    "drop_block2 = DropBlock2D(keep_prob=keep_rate, block_size=4)\n",
    "csse2 = drop_block2(csse2, is_training)\n",
    "\n",
    "\n",
    "# Skip connect\n",
    "x = tf.concat([csse2, csse1], axis = -1)\n",
    "\n",
    "# Conv SELU, 16x16 - 14x14\n",
    "\n",
    "\n",
    "\n",
    "x = conv_bn_elu(x, is_training, 3, \"out_2\", out_conv_flt, False, 'valid')\n",
    "#x = conv_bn_relu(x, is_training, 3, 'out_2', out_conv_flt, False)\n",
    "\n",
    "print(\"Initializing last sigmoid bias with -2.94 constant\")\n",
    "init = tf.constant_initializer([-np.log(0.7/0.3)]) # For focal loss\n",
    "fm = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1), \n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid',\n",
    "            bias_initializer = init,\n",
    "           )(x) # For focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(\"This model has {} parameters\".format(total_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "*  Load in CSV data from Collect Earth\n",
    "*  Reconstruct the X, Y grid for the Y data per sample\n",
    "*  Calculate NDVI, EVI, SAVI, BI, MSAVI2, and SI\n",
    "*  Stack X, Y, length data\n",
    "*  Apply median filter to DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.load(\"../tile_data/processed/data_x_l2a_processed.npy\")\n",
    "data_y = np.load(\"../tile_data/processed/data_y_l2a_processed.npy\")\n",
    "lengths = np.load(\"../tile_data/processed/length_l2a_processed.npy\")\n",
    "\n",
    "data_x = np.delete(data_x, -1, -1) \n",
    "#data_x = np.delete(data_x, 13, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data_x[65, 0, :, :, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize\n",
    "for sample in tnrange(0, len(data_x)):\n",
    "    filtered = median_filter(data_x[sample, 0, :, :, 10], size = 5)\n",
    "    filtered = np.reshape(filtered, (8, 2, 8, 2))\n",
    "    filtered = np.mean(filtered, axis = (1, 3))\n",
    "    filtered = resize(filtered, (16, 16), 0)\n",
    "    data_x[sample, :, :, :, 10] = np.stack([filtered] * 24)\n",
    "    \n",
    "#data_x = np.delete(data_x, 10, -1)\n",
    "print(data_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data_x[65, 0, :, :, 10], vmax = 0.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "*  Identify and remove samples with time steps / channels that have a 0. or 1. value, which indicates missing data\n",
    "*  Identify and remove samples with time steps / channels with no variation, which indicates missing data\n",
    "*  Identify and remove samples with values above or below the allowable values for the band\n",
    "*  Identify and remove samples with null data, or samples with extreme band 0 data (which squash all the \"clean\" samples)\n",
    "*  Smooth per-pixel temporal data with Whittaker smoother, d = 2, lambda = 0.5 to reduce sample noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "below_1 = [i for i, val in enumerate(data_x) if np.min(val) < -1.5]\n",
    "above_1 = [i for i, val in enumerate(data_x) if np.max(val) > 1.5]\n",
    "min_vals = [np.min(val) for i, val in enumerate(data_x) if np.min(val) < -1.5]\n",
    "max_vals = [np.max(val) for i, val in enumerate(data_x) if np.max(val) > 1.5]\n",
    "nans = [i for i, val in enumerate(data_x) if np.sum(np.isnan(val)) > 0]\n",
    "oob_vals = [i for i, val in enumerate(data_x) if np.max(val[:, :, :, 0]) > 0.7]\n",
    "print(oob_vals)\n",
    "\n",
    "outliers = below_1 + above_1 + nans + oob_vals\n",
    "outliers = list(set(outliers))\n",
    "print(\"The outliers are: {}, totalling {}\".format(outliers, len(outliers)))\n",
    "print(\"\\n\")\n",
    "print(min_vals, max_vals)\n",
    "data_x = data_x[[x for x in range(0, len(data_x)) if x not in outliers]]\n",
    "data_y = data_y[[x for x in range(0, len(data_y)) if x not in outliers]]\n",
    "lengths = lengths[[x for x in range(0, len(lengths)) if x not in outliers]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_treecover = np.argwhere(np.sum(data_y, axis = (1, 2)) < 100)\n",
    "high_treecover = np.delete(data_x, high_treecover, 0)\n",
    "\n",
    "low_treecover = np.argwhere(np.sum(data_y, axis = (1, 2)) != 0)\n",
    "low_treecover = np.delete(data_x, low_treecover, 0)\n",
    "\n",
    "high_scatter = np.std(high_treecover, axis = (0, 2, 3))\n",
    "low_scatter = np.std(low_treecover, axis = (0, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(y = low_scatter[:, 12], x = [x for x in range(24)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_scatter = np.mean(data_x[7, :, :, :, -4], axis = (1, 2))\n",
    "sns.scatterplot(y = high_scatter[:, 12], x = [x for x in range(24)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_all = [0.019788351394236087,\n",
    " 0.0376599433645606,\n",
    " 0.022249658703804017,\n",
    " 0.09112316101789475,\n",
    " 0.06029978767037392,\n",
    " 0.08587831154465678,\n",
    " 0.09322187490761281,\n",
    " 0.10038416087627411,\n",
    " 0.09066671133041382,\n",
    " 0.04962744191288948,\n",
    " 0.0,\n",
    " 0.010814227256923915,\n",
    " -0.020943850725889203,\n",
    " 0.008634393354877836]\n",
    "\n",
    "max_all = [0.17124534726142882,\n",
    " 0.2569628745317458,\n",
    " 0.3763543248176573,\n",
    " 0.4688883873820304,\n",
    " 0.422776158452034,\n",
    " 0.44434353709220886,\n",
    " 0.4722869098186493,\n",
    " 0.4879453480243683,\n",
    " 0.6069133794307708,\n",
    " 0.5277097404003142,\n",
    " 0.36966854333877563,\n",
    " 0.69947146654129,\n",
    " 0.39734595388174054,\n",
    " 0.6361596417427062]\n",
    "\n",
    "\n",
    "\n",
    "#min_all = []\n",
    "#max_all = []\n",
    "\n",
    "for band in range(0, data_x.shape[-1]):\n",
    "    print(band)\n",
    "    #mins = np.percentile(data_x[:, :, :, :, band], 1)\n",
    "    #maxs = np.percentile(data_x[:, :, :, :, band], 99)\n",
    "    #mins, maxs = (np.min(data_x[:, :, :, :, band]), np.max(data_x[:, :, :, :, band]))\n",
    "    mins = min_all[band]\n",
    "    maxs = max_all[band]\n",
    "    data_x[:, :, :, :, band] = np.clip(data_x[:, :, :, :, band], mins, maxs)\n",
    "    midrange = (maxs + mins) / 2\n",
    "    rng = maxs - mins\n",
    "    standardized = (data_x[:, :, :, :, band] - midrange) / (rng / 2)\n",
    "    data_x[:, :, :, :, band] = standardized\n",
    "    \n",
    "    #$min_all.append(mins)\n",
    "    #max_all.append(maxs)\n",
    "    \n",
    "print(\"The data has been scaled to [{}, {}]\".format(np.min(data_x), np.max(data_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data_x[12, 5, :, :, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(y = data_x[105, :, 15, 14, 3], x = [x for x in range(24)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment training data\n",
    "\n",
    "Horizontal and vertical flips for 4x augmentation.\n",
    "\n",
    "**To do**\n",
    "*  Random guassian noise\n",
    "*  Brightness, contrast\n",
    "*  Region swaps (randomply position positive samples at different locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_augmented = []\n",
    "data_y_augmented = []\n",
    "lengths_augmented = []\n",
    "\n",
    "for i, val in enumerate([x for x in range(len(data_x))]):\n",
    "    data_x_augmented.append(data_x[val])\n",
    "    data_y_augmented.append(data_y[val])\n",
    "    lengths_augmented.append(data_x[val].shape[0])\n",
    "    \n",
    "    x1 = np.flip(data_x[val], 1)\n",
    "    y1 = np.flip(data_y[val], 0)\n",
    "    lengths_augmented.append(x1.shape[0])\n",
    "    data_x_augmented.append(x1)\n",
    "    data_y_augmented.append(y1)\n",
    "    \n",
    "    x1 = np.flip(data_x[val], [2, 1])\n",
    "    y1 = np.flip(data_y[val], [1, 0])\n",
    "    lengths_augmented.append(x1.shape[0])\n",
    "    data_x_augmented.append(x1)\n",
    "    data_y_augmented.append(y1)\n",
    "    \n",
    "    x1 = np.flip(data_x[val], 2)\n",
    "    y1 = np.flip(data_y[val], 1)\n",
    "    lengths_augmented.append(x1.shape[0])\n",
    "    data_x_augmented.append(x1)\n",
    "    data_y_augmented.append(y1)\n",
    "\n",
    "train_x = np.stack(data_x_augmented)\n",
    "train_y = np.stack(data_y_augmented)\n",
    "train_y = np.reshape(train_y, (train_y.shape[0], 14, 14, 1))\n",
    "train_l = np.stack(lengths_augmented)\n",
    "train_l = np.reshape(train_l, (train_y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.load(\"../tile_data/processed/test_x_l2a_processed.npy\")\n",
    "test_y = np.load(\"../tile_data/processed/test_y_l2a_processed.npy\")\n",
    "test_lengths = np.load(\"../tile_data/processed/test_length_l2a_processed.npy\")\n",
    "\n",
    "test_x = np.delete(test_x, -1, -1)\n",
    "#test_x = np.delete(test_x, 13, -1)\n",
    "\n",
    "\n",
    "below_1 = [i for i, val in enumerate(test_x) if np.min(val) < -1.5]\n",
    "above_1 = [i for i, val in enumerate(test_x) if np.max(val) > 1.5]\n",
    "min_vals = [np.min(val) for i, val in enumerate(test_x) if np.min(val) < -1.5]\n",
    "max_vals = [np.max(val) for i, val in enumerate(test_x) if np.max(val) > 1.5]\n",
    "nans = [i for i, val in enumerate(test_x) if np.sum(np.isnan(val)) > 0]\n",
    "outliers = below_1 + above_1 + nans\n",
    "outliers = list(set(outliers))\n",
    "print(\"The outliers are: {}, totalling {}\".format(outliers, len(outliers)))\n",
    "print(\"\\n\")\n",
    "print(min_vals, max_vals)\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "    mins = np.min(test_x[i, :, :, :, :])\n",
    "    maxs = np.max(test_x[i, :, :, :, :])\n",
    "    if mins < -1 or maxs > 1:\n",
    "        offender_max = np.argmax(np.max(test_x[i, :, :, :, :], (0, 1, 2)), -1)\n",
    "        offender_min = np.argmin(np.min(test_x[i, :, :, :, :], (0, 1, 2)), -1)\n",
    "        \n",
    "        print(\"{} Offender max/min: {} {}\".format(i, offender_max, offender_min))\n",
    "test_x = test_x[[x for x in range(0, len(test_x)) if x not in outliers]]\n",
    "test_y = test_y[[x for x in range(0, len(test_y)) if x not in outliers]]\n",
    "test_lengths = test_lengths[[x for x in range(0, len(test_lengths)) if x not in outliers]]\n",
    "\n",
    "for sample in tnrange(0, len(test_x)):\n",
    "    filtered = median_filter(test_x[sample, 0, :, :, 10], size = 5)\n",
    "    filtered = np.reshape(filtered, (8, 2, 8, 2))\n",
    "    filtered = np.mean(filtered, axis = (1, 3))\n",
    "    filtered = resize(filtered, (16, 16), 0)\n",
    "    test_x[sample, :, :, :, 10] = np.stack([filtered] * 24)\n",
    "\n",
    "#test_x = np.delete(test_x, 10, -1)\n",
    "    \n",
    "for band in range(0, test_x.shape[-1]):\n",
    "    mins = min_all[band]\n",
    "    maxs = max_all[band]\n",
    "    test_x[:, :, :, :, band] = np.clip(test_x[:, :, :, :, band], mins, maxs)\n",
    "    midrange = (maxs + mins) / 2\n",
    "    rng = maxs - mins\n",
    "    standardized = (test_x[:, :, :, :, band] - midrange) / (rng / 2)\n",
    "    test_x[:, :, :, :, band] = standardized\n",
    "    \n",
    "    \n",
    "print(\"The data has been scaled to [{}, {}]\".format(np.min(test_x), np.max(test_x)))\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train and test characteristics:\")\n",
    "print(\"Train mean Y {}\".format(np.mean([np.sum(x) for x in test_y])))\n",
    "#print(\"Test mean Y {}\".format(np.mean([np.sum(x) for x in data_y[test_ids]])))\n",
    "#print(\"Train STD Y {}\".format(np.std([np.sum(x) for x in data_y[train_ids]])))\n",
    "print(\"Test STD Y {}\".format(np.std([np.sum(x) for x in test_y])))\n",
    "#print(\"Train number with zero trees {}\".format(0.2*len([x for x in data_y[train_ids] if np.sum(x) == 0])))\n",
    "#print(\"Test number with zero trees {}\".format(0.8*len([x for x in data_y[test_ids] if np.sum(x) == 0])))\n",
    "print(\"Train mean NDVI\")\n",
    "print(\"Test mean NDVI\")\n",
    "#print(\"There are {} train and {} test samples\".format(len(train_ids), len(test_ids)))\n",
    "#print(\"There is {} overlap between train and test\".format(len([x for x in train_ids if x in test_ids])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loss definition\n",
    "\n",
    "The current best loss is a combination of weighted binary cross entropy and per-image Lovasz-Softmax, with a loss schedule with the latter becoming more important each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "import math\n",
    "from scipy.ndimage import distance_transform_edt as distance\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight, smooth = 0.025):\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    y_true = K.clip(y_true, smooth, 1. - smooth)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "        y_true,\n",
    "        logit_y_pred,\n",
    "        weight,\n",
    "    )\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def calc_dist_map(seg):\n",
    "    res = np.zeros_like(seg)\n",
    "    posmask = seg.astype(np.bool)\n",
    "\n",
    "    if posmask.any():\n",
    "        negmask = ~posmask\n",
    "        res = distance(negmask) * negmask - (distance(posmask) - 1) * posmask\n",
    "\n",
    "    return res\n",
    "\n",
    "def calc_dist_map_batch(y_true):\n",
    "    y_true_numpy = y_true.numpy()\n",
    "    return np.array([calc_dist_map(y)\n",
    "                     for y in y_true_numpy]).astype(np.float32)\n",
    "\n",
    "def surface_loss(y_true, y_pred):\n",
    "    y_true_dist_map = tf.py_function(func=calc_dist_map_batch,\n",
    "                                     inp=[y_true],\n",
    "                                     Tout=tf.float32)\n",
    "    multipled = y_pred * y_true_dist_map\n",
    "    return K.mean(multipled)\n",
    "\n",
    "def bce_loss(y_true, y_pred, smooth = 0.05, mask = None):\n",
    "    bce = weighted_bce_loss(y_true, y_pred, 1.4, smooth)\n",
    "    return bce\n",
    "\n",
    "def finetune_loss(y_true, y_pred):\n",
    "    lv = lovasz_softmax(y_pred, tf.reshape(y_true, (-1, 14, 14)), classes=[1], per_image=True)\n",
    "    return lv\n",
    "\n",
    "def bce_lv(y_true, y_pred, alpha, smooth = 0.025, mask = None, ):\n",
    "    bce =  weighted_bce_loss(y_true, y_pred, 1.4, smooth = smooth)\n",
    "    #surf = surface_loss(y_true, y_pred)\n",
    "    lv = lovasz_softmax(y_pred, tf.reshape(y_true, (-1, 14, 14)), classes=[1], per_image=True)\n",
    "    #global_loss = (1 - alpha) * ((1-alpha)*bce + alpha*lv)\n",
    "    #regional_loss = alpha * surf\n",
    "    #return lv\n",
    "    return ( (1-alpha)*bce + (alpha * lv) )#global_loss #+ regional_loss\n",
    "\n",
    "countries = {'ethiopia': [0, 112],\n",
    " 'kenya': [113, 201],\n",
    " 'ghana': [202, 271],\n",
    " 'africa': [272, 306],\n",
    " 'india': [307, 357],\n",
    " 'lac': [358, 383],\n",
    " 'all': [0, 383]}\n",
    "\n",
    "#! Move to a src.py\n",
    "def calculate_metrics(country):\n",
    "    start_idx = countries[country][0]\n",
    "    stop_idx = countries[country][1]\n",
    "    best_f1 = 0\n",
    "    best_thresh = 0\n",
    "    p = 0\n",
    "    r = 0\n",
    "    error = 0\n",
    "    ys = []\n",
    "    vls = []\n",
    "    t_alls = []\n",
    "    test_ids = [x for x in range(len(test_x))]\n",
    "    for test_sample in test_ids[start_idx:stop_idx]:\n",
    "        y, vl = sess.run([fm, test_loss], feed_dict={inp: test_x[test_sample].reshape(1, 24, 16, 16, n_bands),\n",
    "                                          length: test_lengths[test_sample].reshape(1, 1),\n",
    "                                          is_training: False,\n",
    "                                          labels: test_y[test_sample, :, :].reshape(1, 14, 14),\n",
    "                                          })\n",
    "        ys.append(y.reshape((14, 14)))\n",
    "        vls.append(vl)\n",
    "        t = test_y[test_sample].reshape((14, 14))\n",
    "        t_alls.append(t)\n",
    "    to_remove = np.argwhere(vls > np.percentile(vls, 96))\n",
    "    #print(to_remove)\n",
    "    ys = list(np.delete(np.array(ys), to_remove, 0))\n",
    "    t_alls = list(np.delete(np.array(t_alls), to_remove, 0))\n",
    "    for thresh in range(7, 13):\n",
    "        tps = []\n",
    "        fps = []\n",
    "        fns = []\n",
    "        perc_error_20 = []\n",
    "        perc_error_100 = []\n",
    "        perc_error_196 = []\n",
    "        trues = []\n",
    "        preds = []\n",
    "        val_loss = []\n",
    "        for sample in range(len(ys)):\n",
    "            pred = np.copy(ys[sample])\n",
    "            true = t_alls[sample]\n",
    "            vl = vls[sample]\n",
    "            pred[np.where(pred > thresh*0.05)] = 1\n",
    "            pred[np.where(pred < thresh*0.05)] = 0\n",
    "            true_s = np.sum(true)\n",
    "            pred_s = np.sum(pred)\n",
    "            p_error = np.around(((abs(pred_s - true_s))/196), 3)\n",
    "            if true_s < 20:\n",
    "                perc_error_20.append(p_error)\n",
    "            elif true_s < 100:\n",
    "                perc_error_100.append(p_error)\n",
    "            else:\n",
    "                perc_error_196.append(p_error)\n",
    "\n",
    "            #perc_error.append(abs(pred_s - true_s))\n",
    "            tp, fp, fn = thirty_meter(true, pred)\n",
    "            tps.append(tp)\n",
    "            fps.append(fp)\n",
    "            fns.append(fn)\n",
    "            trues.append(true_s)\n",
    "            preds.append(pred_s)\n",
    "            val_loss.append(np.mean(vl))\n",
    "        oa_error = abs(np.sum(preds) - np.sum(trues)) / np.sum(trues)\n",
    "        precision = np.sum(tps) / (np.sum(tps) + np.sum(fps))\n",
    "        recall = np.sum(tps) / (np.sum(tps) + np.sum(fns))\n",
    "        f1 = 2*((precision* recall) / (precision + recall))\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            p = precision\n",
    "            r = recall\n",
    "            error = oa_error\n",
    "            best_thresh = thresh*0.05\n",
    "    print(\"{}: Val loss: {} Thresh: {} F1: {} Recall: {} Precision: {} Error: {}\".format(country, np.around(np.mean(val_loss), 3), np.around(best_thresh, 2),\n",
    "                                                                                     np.around(best_f1, 3), np.around(p, 3), np.around(r, 3), \n",
    "                                                                                     np.around(error, 3)))\n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = np.sum(train_y, axis = (1, 2))\n",
    "percents = [np.percentile(sums, x) for x in range(3*10, 100, 10)]\n",
    "print(percents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equibatch creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [x for x in range(0, len(train_y))]\n",
    "print(len(train_ids))\n",
    "\n",
    "def multiplot(matrices):\n",
    "    '''Plot multiple heatmaps with subplots'''\n",
    "    fig, axs = plt.subplots(ncols=4)\n",
    "    fig.set_size_inches(20, 4)\n",
    "    for i, matrix in enumerate(matrices):\n",
    "        sns.heatmap(data = matrix, ax = axs[i], vmin = 0, vmax = 0.9)\n",
    "        axs[i].set_xlabel(\"\")\n",
    "        axs[i].set_ylabel(\"\")\n",
    "        axs[i].set_yticks([])\n",
    "        axs[i].set_xticks([])\n",
    "    plt.show()\n",
    "\n",
    "print(percents)\n",
    "def equibatch(train_ids, p = percents, lovasz = False):\n",
    "    percents = [8.0, 14.0, 23.0, 33.0, 50.0, 78.0, 145.0]\n",
    "    #percents = [18.0, 25.0, 35.0, 63.0, 149.0, 192.0]\n",
    "    #percents = [12, 25.0, 54.5, 81.0, 117.0, 185.0] # latin and south america\n",
    "    #percents = [21.0, 33.0, 52.0, 84.0, 165.0, 189.0] # cocoa ghana\n",
    "    print(percents)\n",
    "    np.random.shuffle(train_ids)\n",
    "    ix = train_ids\n",
    "    percs = [np.sum(x) for x in train_y[ix]]\n",
    "    ids0 = [x for x, z in zip(ix, percs) if z == 0]\n",
    "    ids30 = [x for x, z in zip(ix, percs) if 0 < z < percents[0]]\n",
    "    ids40 = [x for x, z in zip(ix, percs) if percents[0] < z < percents[1]]\n",
    "    ids50 = [x for x, z in zip(ix, percs) if percents[1] < z < percents[2]]\n",
    "    ids60 = [x for x, z in zip(ix, percs) if percents[2] < z < percents[3]]\n",
    "    ids70 = [x for x, z in zip(ix, percs) if percents[3] < z < percents[4]]\n",
    "    ids80 = [x for x, z in zip(ix, percs) if percents[4] < z < percents[5]]\n",
    "    ids90 = [x for x, z in zip(ix, percs) if percents[5] < z < percents[6]]\n",
    "    ids100 = [x for x, z in zip(ix, percs) if percents[6] < z]\n",
    "    \n",
    "\n",
    "    new_batches = []\n",
    "    maxes = [len(ids0), len(ids30), len(ids40), len(ids50), len(ids60), len(ids70),\n",
    "             len(ids80), len(ids90), len(ids100)]\n",
    "    print(maxes)\n",
    "    cur_ids = [0] * len(maxes)\n",
    "    iter_len = len(train_ids)//(len(maxes)+1)\n",
    "    for i in range(0, iter_len):\n",
    "        for i, val in enumerate(cur_ids):\n",
    "            if val > maxes[i] - 1:\n",
    "                cur_ids[i] = 0\n",
    "        if cur_ids[0] >= (maxes[0] - 2):\n",
    "            cur_ids[0] = 0\n",
    "        to_append = [ids0[cur_ids[0]], ids0[cur_ids[0] + 1], ids30[cur_ids[1]], ids40[cur_ids[2]],\n",
    "                    ids50[cur_ids[3]],\n",
    "                    ids60[cur_ids[4]], ids70[cur_ids[5]], ids80[cur_ids[6]],\n",
    "                    ids90[cur_ids[7]], ids100[cur_ids[8]]]\n",
    "        \n",
    "        np.random.shuffle(to_append)\n",
    "        new_batches.append(to_append)\n",
    "        cur_ids = [x + 1 for x in cur_ids]\n",
    "        cur_ids[0] += 1\n",
    "        \n",
    "    new_batches = [item for sublist in new_batches for item in sublist]\n",
    "    #overlap = [x for x in new_batches if x in test_ids]\n",
    "    #print(\"There is {} overlap. Error if > 0\".format(len(overlap)))\n",
    "    return new_batches\n",
    "\n",
    "batch = equibatch(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[4:8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[8:12]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[0:4]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRESH_START = True\n",
    "best_val = 0.65\n",
    "\n",
    "START_EPOCH = 1\n",
    "END_EPOCH = 150\n",
    "LEARNING_RATE = 3e-3\n",
    "test_ids = [x for x in range(0, len(test_x))]\n",
    "\n",
    "print(\"The model does not overfit after 100 epochs from 5e-4 to 1e-1\")\n",
    "print(\"We still need to figure out whether or not to use lovasz for finetune\")\n",
    "print(\"Starting model with: \\n {} zone out \\n {} l2 \\n {} initial LR \\n {} final LR \\n {} parameters\"\n",
    "     .format(ZONE_OUT_PROB, L2_REG, INITIAL_LR, FINAL_LR, total_parameters))\n",
    "\n",
    "if not FRESH_START:\n",
    "    print(\"Resuming training with a best validation score of {}\".format(best_val))\n",
    "    \n",
    "if FRESH_START:\n",
    "    print(\"Restarting training from scratch on {} \"\n",
    "          \"train and {} test samples, total {}\".format(len(train_ids), len(test_ids), len(train_ids)/4))\n",
    "\n",
    "    #optimizer = tf.train.AdamOptimizer(LEARNING_RATE, epsilon = 1e-8)\n",
    "    optimizer = AdaBoundOptimizer(1e-3, 1e-1)\n",
    "    train_loss = bce_lv(tf.reshape(labels, (-1, 14, 14, 1)), fm, alpha = alpha)\n",
    "    #l2_loss = tf.losses.get_regularization_l05oss()\n",
    "    #train_loss += l2_loss\n",
    "\n",
    "    ft_optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "    ft_loss = finetune_loss(tf.reshape(labels, (-1, 14, 14, 1)), fm)\n",
    "    #wu_loss = weighted_bce_loss(tf.reshape(labels, (-1, 14, 14, 1)), fm, weight = 1.5)\n",
    "    \n",
    "    test_loss = weighted_bce_loss(tf.reshape(labels, (-1, 14, 14, 1)), fm, weight = 1.)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(train_loss)   \n",
    "        #ft_op = ft_optimizer.minimize(ft_loss)\n",
    "        ft_op = ft_optimizer.minimize(train_loss)\n",
    "        \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver(max_to_keep = 2)\n",
    "    \n",
    "print(\"The graph has been finalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should be run to do fine-tuning, if commented - train from scratch\n",
    "\n",
    "new_saver = tf.train.import_meta_graph('../models/january-super-sgd-100epochs-82/model.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint('../models/january-super-sgd-100epochs-82'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "best_val = 0.826\n",
    "\n",
    "train_ids = [x for x in range(len(train_y))]\n",
    "\n",
    "for i in range(41, 100):\n",
    "    #al = np.min([i*0.01, 0.20])\n",
    "    #al = 0.\n",
    "    al = min(i*0.02, 0.05)\n",
    "    #randomize = train_ids\n",
    "    #np.random.shuffle(randomize)\n",
    "    randomize = equibatch(train_ids)\n",
    "    #print(\"Loss: {} 1.5 weighted BCE + {} Image Lovasz\".format( (0.95-al), (0.05 + al)))\n",
    "    #op = train_op\n",
    "    #loss = train_loss\n",
    "    op = ft_op\n",
    "    loss = train_loss\n",
    "    BATCH_SIZE = 20\n",
    "    test_ids = [x for x in range(0, len(test_x))]\n",
    "    losses = []\n",
    "    \n",
    "    for k in tnrange(int(len(randomize) // BATCH_SIZE)):\n",
    "        if k % 25 == 0:\n",
    "            sleep(3)\n",
    "        batch_ids = randomize[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "        batch_y = train_y[batch_ids, :, :].reshape(len(batch_ids), 14, 14)\n",
    "        opt, tr = sess.run([op, loss],\n",
    "                              feed_dict={inp: train_x[batch_ids, :, :, :],\n",
    "                                         length: train_l[batch_ids].reshape((-1, 1)),\n",
    "                                         labels: batch_y,\n",
    "                                         is_training: True,\n",
    "                                         keep_rate: np.max((1 - (i*0.006), 0.90)),\n",
    "                                         alpha: al\n",
    "                                         })\n",
    "        losses.append(tr)\n",
    "    \n",
    "    print(\"Epoch {}: Loss {}\".format(i, np.around(np.mean(losses[:-1]), 3)))\n",
    "    calculate_metrics('ethiopia')\n",
    "    calculate_metrics('ghana')\n",
    "    calculate_metrics('kenya')\n",
    "    calculate_metrics('lac')\n",
    "    calculate_metrics('india')\n",
    "    calculate_metrics('africa')\n",
    "    f1 = calculate_metrics('all')\n",
    "    if f1 > (best_val - 0.01):\n",
    "        #best_val = f1\n",
    "        print(\"Saving model with {}\".format(f1))\n",
    "        save_path = saver.save(sess, \"../models/february-sgd-finetune/model\")\n",
    "        if f1 > best_val:\n",
    "            best_val = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics('ethiopia')\n",
    "calculate_metrics('ghana')\n",
    "calculate_metrics('kenya')\n",
    "calculate_metrics('lac')\n",
    "calculate_metrics('india')\n",
    "calculate_metrics('africa')\n",
    "calculate_metrics('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential problem test IDS: 135505813, 135505835, \n",
    "save_path = saver.save(sess, \"../models/february-sgd-finetune/model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation and sanity checks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "test_ids = [x for x in range(0, len(test_x))]\n",
    "\n",
    "def multiplot(matrices, nrows = 2, ncols = 4):\n",
    "    ''\n",
    "    'Plot multipleheatmaps with subplots'''\n",
    "    fig, axs = plt.subplots(ncols=4, nrows = nrows)\n",
    "    fig.set_size_inches(20, 4*nrows)\n",
    "    to_iter = [[x for x in range(i, i + ncols + 1)] for i in range(0, nrows*ncols, ncols)]\n",
    "    for r in range(1, nrows + 1):\n",
    "        min_i = min(to_iter[r-1])\n",
    "        max_i = max(to_iter[r-1])\n",
    "        for i, matrix in enumerate(matrices[min_i:max_i]):\n",
    "            sns.heatmap(data = matrix, ax = axs[r - 1, i], vmin = 0, vmax = 0.9)\n",
    "            axs[r - 1, i].set_xlabel(\"\")\n",
    "            axs[r - 1, i].set_ylabel(\"\")\n",
    "            axs[r - 1, i].set_yticks([])\n",
    "            axs[r - 1, i].set_xticks([])\n",
    "    plt.show\n",
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_alignment(true, pred, wsize = 3):\n",
    "    n_single_trees = 0\n",
    "    for x in range(1, true.shape[0] - 1, 1):\n",
    "        for y in range(1, true.shape[1] - 1, 1):\n",
    "            wind_true = true[x-1:x+2, y-1:y+2]\n",
    "            wind_pred = pred[x-1:x+2, y-1:y+2]\n",
    "            if wind_true[1, 1] == 1:\n",
    "                if np.sum(wind_true) == 1:\n",
    "                    n_single_trees += 1\n",
    "                    pred_place = np.argmax(wind_pred.flatten())\n",
    "                    diff = wind_pred.flatten()[pred_place] - wind_pred.flatten()[4]\n",
    "                    if pred_place != 4:\n",
    "                        if diff > 0.2:\n",
    "                            x_lv = pred_place // 3\n",
    "                            y_lv = pred_place % 3\n",
    "                            print(x_lv, y_lv)\n",
    "                            proposed = wind_true[x_lv - 1:x_lv+2, y_lv-1:y_lv+2]\n",
    "                            if np.sum(proposed) == 0:\n",
    "                                print(\"There is a missed position at {} x, {} y: {}\".format(x, y, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "print(start/len(test_ids))\n",
    "test_ids = sorted(test_ids)\n",
    "matrix_ids = [test_ids[start], test_ids[start + 1], test_ids[start + 2], test_ids[start + 3],\n",
    "              test_ids[start + 4], test_ids[start + 5], test_ids[start + 6], test_ids[start + 7]]\n",
    "#matrix_ids = random.sample(test_ids, 4)z\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    print(i)\n",
    "    y = sess.run([fm], feed_dict={inp: test_x[idx].reshape(1, 24, IMAGE_SIZE, IMAGE_SIZE, n_bands),\n",
    "                                  length: test_lengths[idx].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })\n",
    "    y = np.array(y).reshape(14, 14)\n",
    "    #y[np.where(y > 0.7)] = 0.85\n",
    "    preds.append(y)\n",
    "    true = test_y[idx].reshape(14, 14)\n",
    "    identify_alignment(true, y)\n",
    "    trues.append(true)\n",
    "    \n",
    "    \n",
    "\"\"\n",
    "\n",
    "\n",
    "to_plot = trues[0:4] + preds[0:4] + trues[4:] + preds[4:]\n",
    "\n",
    "multiplot(to_plot, nrows = 4, ncols = 4)\n",
    "\n",
    "start = start + 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [x for x in range(train_x.shape[0])]\n",
    "start = 0*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 127, 143, 238, 241, 242, 252, 264, 282, 288, 318\n",
    "test_losses = []\n",
    "train_ids = sorted(train_ids)\n",
    "#matrix_ids = random.sample(755, 4)\n",
    "matrix_ids = [train_ids[start], train_ids[start + 4], train_ids[start + 8], train_ids[start + 12], train_ids[start + 16],\n",
    "             train_ids[start + 20], train_ids[start + 24], train_ids[start + 28], train_ids[start + 32]]\n",
    "\n",
    "#matrix_ids = [722*4, 751*4, 761*4, 762*4]\n",
    "#matrix_ids = [len(train_x)-28]\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "print(start//4)\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    y = sess.run([fm], feed_dict={inp: train_x[idx].reshape(1, 24, IMAGE_SIZE, IMAGE_SIZE, n_bands),\n",
    "                                  length: train_l[idx].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })\n",
    "    y = np.array(y).reshape(14, 14)\n",
    "    #y[np.where(y > 0.3)] = 0.85\n",
    "    preds.append(y)\n",
    "    true = train_y[idx].reshape(14, 14)\n",
    "    #identify_alignment(true, y)\n",
    "    trues.append(true)\n",
    "    \n",
    "start += 36\n",
    "# 20, \n",
    "    \n",
    "\n",
    "to_plot = trues[0:4] + preds[0:4] + trues[5:] + preds[5:]\n",
    "multiplot(to_plot, nrows = 4, ncols = 4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO @jombrandt top 10 worst training, test samples by IOU \n",
    "\n",
    "These should be written to a tmp/ .txt file and indexed by validate-data.ipynb to ensure that original classifications were correct, and to identify regions that need more training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions (N, k) ndarray\n",
    "           targets (N, k) ndarray        \n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.sum(targets*np.log(predictions+1e-9))/N\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy(np.array([[.01], [0.01]]), np.array([[.04], [0.04]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy-toolkit",
   "language": "python",
   "name": "policy-toolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
