{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants\n",
    "from s2cloudless import S2PixelCloudDetector, CloudMaskRequest\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "from sentinelhub import DataSource\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import splu\n",
    "\n",
    "\n",
    "from sentinelhub import CustomUrlParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/slope.py\n",
    "%run ../src/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSG = CRS.WGS84\n",
    "GRID_SIZE_X = 1\n",
    "GRID_SIZE_Y = 1\n",
    "\n",
    "IMAGE_X = 14*GRID_SIZE_X\n",
    "IMAGE_Y = 14*GRID_SIZE_Y\n",
    "\n",
    "TEST_X = 5\n",
    "TEST_Y = 5\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "    key = (yaml.safe_load(stream))\n",
    "    API_KEY = key['key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2018\n",
    "#time = (str(year - 1) +'-12-15', str(year+1) +'-1-15')\n",
    "time = ('2018-12-15', '2019-11-25')\n",
    "SIZE = 9\n",
    "print(time)\n",
    "\n",
    "def convertCoords(xy, src='', targ=''):\n",
    "\n",
    "    srcproj = osr.SpatialReference()\n",
    "    srcproj.ImportFromEPSG(src)\n",
    "    targproj = osr.SpatialReference()\n",
    "    if isinstance(targ, str):\n",
    "        targproj.ImportFromProj4(targ)\n",
    "    else:\n",
    "        targproj.ImportFromEPSG(targ)\n",
    "    transform = osr.CoordinateTransformation(srcproj, targproj)\n",
    "\n",
    "    pt = ogr.Geometry(ogr.wkbPoint)\n",
    "    pt.AddPoint(xy[0], xy[1])\n",
    "    pt.Transform(transform)\n",
    "    return([pt.GetX(), pt.GetY()])\n",
    "\n",
    "def bounding_box(point, x_offset_max = 140, y_offset_max = 140, expansion = 10):\n",
    "    # LONG, LAT FOR SOME REASON\n",
    "    tl = point\n",
    "    \n",
    "    if 48 <= tl[0] <= 54:\n",
    "        epsg = 32639 if tl[1] > 0 else 32739\n",
    "    if 42 <= tl[0] <= 48:\n",
    "        epsg = 32638 if tl[1] > 0 else 32738\n",
    "    if 36 <= tl[0] <= 42:\n",
    "        epsg = 32637 if tl[1] > 0 else 32737\n",
    "    if 30 <= tl[0] <= 36:\n",
    "        epsg = 32636 if tl[1] > 0 else 32736\n",
    "    if 24 <= tl[0] <= 30:\n",
    "        epsg = 32635 if tl[1] > 0 else 32735\n",
    "    if 18 <= tl[0] <= 24:\n",
    "        epsg = 32634 if tl[1] > 0 else 32734\n",
    "\n",
    "    tl = convertCoords(tl, 4326, epsg)\n",
    "    \n",
    "    br = (tl[0], tl[1])\n",
    "    tl = ((tl[0] + (x_offset_max)), (tl[1] + (y_offset_max )))\n",
    "    distance1 = tl[0] - br[0]\n",
    "    distance2 = tl[1] - br[1]\n",
    "    #EXPANSION = (expansion - np.mean([distance1, distance2]))/2 # should this be 155 or 160?\n",
    "    \n",
    "    br = [a - expansion for a in br]\n",
    "    tl = [a + expansion for a in tl]\n",
    "    \n",
    "    after = [b - a for a,b in zip(br, tl)]\n",
    "    #br = (br[0] + 20, br[1] + 20)\n",
    "    #tl = (tl[0] - 20, tl[1] - 20)\n",
    "    br = convertCoords(br, epsg, 4326)\n",
    "    tl = convertCoords(tl, epsg, 4326)\n",
    "    \n",
    "    min_x = tl[0] # original X offset - 10 meters\n",
    "    max_x = br[0] # original X offset + 10*GRID_SIZE meters\n",
    "    \n",
    "    min_y = tl[1] # original Y offset - 10 meters\n",
    "    max_y = br[1] # original Y offset + 10 meters + 140 meters\n",
    "    # (min_x, min_y), (max_x, max_y)\n",
    "    # (bl, tr)\n",
    "    return [(min_x, min_y), (max_x, max_y)]\n",
    " \n",
    "cloud_detector = S2PixelCloudDetector(threshold=0.4, average_over=4, dilation_size=2)\n",
    "\n",
    "def identify_clouds(bbox, epsg = EPSG, time = time):\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        cloud_request = WmsRequest(\n",
    "            layer='CLOUD_DETECTION',\n",
    "            bbox=box,\n",
    "            time=time,\n",
    "            width=(SIZE*14)+2,\n",
    "            height=(SIZE*14)+2,\n",
    "            image_format = MimeType.TIFF_d32f,\n",
    "            maxcc=0.5,\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=24),\n",
    "        )\n",
    "        \n",
    "        cloud_img = cloud_request.get_data()\n",
    "        cloud_probs = cloud_detector.get_cloud_probability_maps(np.array(cloud_img))\n",
    "        means = np.mean(cloud_probs, (1, 2))\n",
    "        cloud_steps = [i for i, val in enumerate(means) if val > 0.20]\n",
    "        return cloud_steps, means, cloud_probs\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "    \n",
    "    \n",
    "def download_dem(bbox, epsg = EPSG):\n",
    "    #bbox = modify_bbox(bbox, expansion = 10)\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_s = (SIZE*14)+4\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM',\n",
    "                         bbox=box,\n",
    "                         width=dem_s,\n",
    "                         height=dem_s,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format=MimeType.TIFF_d32f,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    dem_image = calcSlope(dem_image.reshape((1, dem_s, dem_s)),\n",
    "                  np.full((dem_s, dem_s), 10), np.full((dem_s, dem_s), 10), zScale = 1, minSlope = 0.02)\n",
    "    dem_image = dem_image.reshape((dem_s,dem_s, 1))\n",
    "    dem_image = dem_image[1:dem_s-1, 1:dem_s-1, :]\n",
    "    return dem_image #/ np.max(dem_image)\n",
    "\n",
    "\n",
    "    \n",
    "def download_tiles(bbox, clean_steps, epsg = EPSG, time = time):\n",
    "    try:\n",
    "        clean_steps = np.argwhere(clean_steps <= 0.2)\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WmsRequest(\n",
    "                layer='L2A',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                width=(SIZE*14)+2,\n",
    "                height=(SIZE*14)+2,\n",
    "                image_format = MimeType.TIFF_d32f,\n",
    "                maxcc=0.50,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=24),\n",
    "            )\n",
    "        img_bands = image_request.get_data()\n",
    "        img_bands = np.array(img_bands)\n",
    "        print(\"There are {}/{} clean steps\".format(len(clean_steps), len(img_bands)))\n",
    "        return img_bands, image_request\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "        \n",
    "    \n",
    "def calculate_and_save_best_images(img_bands, image_dates):\n",
    "\n",
    "    biweekly_dates = [day for day in range(0, 360, 5)] # ideal imagery dates are every 15 days\n",
    "    \n",
    "    # Identify the dates where there is < 20% cloud cover\n",
    "    #satisfactory_ids = list(np.argwhere(np.array(means) < 4.).reshape(-1, )) \n",
    "    satisfactory_ids = [x for x in range(0, img_bands.shape[0])]\n",
    "    satisfactory_dates = [value for idx, value in enumerate(image_dates) if idx in satisfactory_ids]\n",
    "    \n",
    "    \n",
    "    selected_images = {}\n",
    "    for i in biweekly_dates:\n",
    "        distances = [abs(date - i) for date in satisfactory_dates]\n",
    "        closest = np.min(distances)\n",
    "        closest_id = np.argmin(distances)\n",
    "        # If there is imagery within 8 days, select it\n",
    "        if closest < 5:\n",
    "            date = satisfactory_dates[closest_id]\n",
    "            image_idx = int(np.argwhere(np.array(image_dates) == date)[0])\n",
    "            selected_images[i] = {'image_date': [date], 'image_ratio': [1], 'image_idx': [image_idx]}\n",
    "        # If there is not imagery within 8 days, look for the closest above and below imagery\n",
    "        else:\n",
    "            distances = np.array([(date - i) for date in satisfactory_dates])\n",
    "            # Number of days above and below the selected date of the nearest clean imagery\n",
    "            above = distances[np.where(distances < 0, distances, -np.inf).argmax()]\n",
    "            below = distances[np.where(distances > 0, distances, np.inf).argmin()]\n",
    "            if abs(above) > 100: # If date is the last date, occassionally argmax would set above to - number\n",
    "                above = below\n",
    "            if abs(below) > 100:\n",
    "                below = above\n",
    "            if above != below:\n",
    "                below_ratio = above / (above - below)\n",
    "                above_ratio = 1 - below_ratio\n",
    "            else:\n",
    "                above_ratio = below_ratio = 0.5\n",
    "                \n",
    "            # Extract the image date and imagery index for the above and below values\n",
    "            above_date = i + above\n",
    "            above_image_idx = int(np.argwhere(np.array(image_dates) == above_date)[0])\n",
    "            \n",
    "            below_date = i + below\n",
    "            below_image_idx = int(np.argwhere(np.array(image_dates) == below_date)[0])\n",
    "            \n",
    "            selected_images[i] = {'image_date': [above_date, below_date], 'image_ratio': [above_ratio, below_ratio],\n",
    "                                 'image_idx': [above_image_idx, below_image_idx]}\n",
    "                            \n",
    "    #for i in selected_images.keys():\n",
    "    #    print(i, selected_images[i])\n",
    "        \n",
    "    keep_steps = []\n",
    "    for i in selected_images.keys():\n",
    "        info = selected_images[i]\n",
    "        if len(info['image_idx']) == 1:\n",
    "            step = img_bands[info['image_idx'][0]]\n",
    "        if len(info['image_idx']) == 2:\n",
    "            step1 = img_bands[info['image_idx'][0]] * info['image_ratio'][0]\n",
    "            step2 = img_bands[info['image_idx'][1]] * info['image_ratio'][1]\n",
    "            step = step1 + step2\n",
    "        keep_steps.append(step)\n",
    "        \n",
    "    keep_steps = np.stack(keep_steps)\n",
    "    return keep_steps\n",
    "\n",
    "def calc_best(tiles, cloud_probs, request, offset_x, offset_y):\n",
    "    c_probs = cloud_probs[:, offset_x:offset_x+16, offset_y:offset_y+16]\n",
    "    images = np.stack(tiles)[:, offset_x:offset_x+16, offset_y:offset_y+16]\n",
    "    mins = np.min(c_probs, axis = 0)\n",
    "    c_probs = c_probs - mins\n",
    "    # Calculating the number of pixels in the 16x16 window that are 0.3 above min cloud probability\n",
    "    args = np.array([len(np.argwhere(c_probs[x, :, :].reshape(16*16) > 0.3)) for x in range(c_probs.shape[0])])\n",
    "    print(len([x for x in args if x > 3]))\n",
    "    best = calculate_and_save_best_images(images, request, args)\n",
    "    return best\n",
    "\n",
    "def speyediff(N, d, format = 'csc'):\n",
    "    shape = (N-d, N)\n",
    "    diagonals = np.zeros(2*d + 1)\n",
    "    diagonals[d] = 1.\n",
    "    for i in range(d):\n",
    "        diff = diagonals[:-1] - diagonals[1:]\n",
    "        diagonals = diff\n",
    "    offsets = np.arange(d+1)\n",
    "    spmat = sparse.diags(diagonals, offsets, shape, format = format)\n",
    "    return spmat\n",
    "\n",
    "def smooth(y, lmbd, d = 2):\n",
    "    m = len(y)\n",
    "    E = sparse.eye(m, format = 'csc')\n",
    "    D = speyediff(m, d, format = 'csc')\n",
    "    coefmat = E + lmbd * D.conj().T.dot(D)\n",
    "    z = splu(coefmat).solve(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = (13.540810, 38.177220) # tigray\n",
    "#coords = (-1.726374, 37.440204) # makueni\n",
    "#coords = (-1.515869, 29.952997) # rwanda\n",
    "coords = (coords[1], coords[0])\n",
    "OUTPUT_FOLDER = '../data/tigray-l2a/'\n",
    "#13.567962754335872\n",
    "\n",
    "borders = bounding_box(coords, 10*(SIZE*14), 10*(SIZE*14), expansion = 0)\n",
    "print(borders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this needs the first coordinate to not be [coords] for different y steps\n",
    "\n",
    "def calculate_offset_coords(coords, x_step, y_step, number):\n",
    "    offset_coords = []\n",
    "    y_coord = bounding_box(coords, x_step, y_step, expansion = 0)\n",
    "    y_coord = y_coord[0][1]\n",
    "    for i in range(number):\n",
    "        bbx = bounding_box(coords, (i+1)*x_step, y_step, expansion = 10)\n",
    "        coord_x = bbx[0][0]\n",
    "        coord_y = y_coord\n",
    "        offset_coords.append((coord_x, coord_y))\n",
    "    coords = [(coords[0], y_coord)]\n",
    "    return coords + offset_coords\n",
    "\n",
    "corner_coordinates = []\n",
    "for row in range(0, 30):\n",
    "    temp = calculate_offset_coords(coords, x_step = (SIZE*140), \n",
    "                                            y_step = row*(SIZE*140), \n",
    "                                            number = 30)\n",
    "    corner_coordinates.append([[x, [y, row]] for x, y in zip(temp, [col for col in range(0, 30)])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outliers(sample):\n",
    "    for date in range(24):\n",
    "        for band in range(10):\n",
    "            maxs = np.max(sample[date, :, :, band])\n",
    "            mins = np.min(sample[date, :, :, band])\n",
    "            if maxs == 1.0 or mins == 0.0:\n",
    "                print(\"Found null outlier\")\n",
    "                return True\n",
    "            if maxs == mins:\n",
    "                print(\"Found missing outlier\")\n",
    "                return True\n",
    "            if maxs >= 1.05 or mins <= -1.05:\n",
    "                print(\"Found range outlier\")\n",
    "                return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def tile_images(arr, output_folder):\n",
    "    # Normal\n",
    "    for x_offset, cval in enumerate([x for x in range(0, 70, 14)]):\n",
    "        for y_offset, rval in enumerate([x for x in range(0, 70, 14)]):\n",
    "            base_id = 0\n",
    "            subs = arr[:, cval:cval+16, rval:rval+16]\n",
    "            np.save(output_folder + str(base_id+x_offset*5+y_offset+1), subs)\n",
    "            \n",
    "    # Upright        \n",
    "    for x_offset, cval in enumerate([x for x in range(7, 70, 14)]):\n",
    "        for y_offset, rval in enumerate([x for x in range(7, 70, 14)]):\n",
    "            base_id = 25\n",
    "            subs = arr[:, cval:cval+16, rval:rval+16]\n",
    "            np.save(output_folder + str(base_id+x_offset*4+y_offset+1), subs)\n",
    "            \n",
    "    # Right\n",
    "    for x_offset, cval in enumerate([x for x in range(7, 63, 14)]):\n",
    "        for y_offset, rval in enumerate([x for x in range(0, 70, 14)]):\n",
    "            base_id = 25+16\n",
    "            subs = arr[:, cval:cval+16, rval:rval+16]\n",
    "            np.save(output_folder + str(base_id+x_offset*5+y_offset+1), subs)\n",
    "            \n",
    "    # Up\n",
    "    for x_offset, cval in enumerate([x for x in range(0, 70, 14)]):\n",
    "        for y_offset, rval in enumerate([x for x in range(7, 63, 14)]):\n",
    "            base_id = 25+16+20\n",
    "            subs = arr[:, cval:cval+16, rval:rval+16]\n",
    "            np.save(output_folder + str(base_id+x_offset*4+y_offset+1), subs)\n",
    "            \n",
    "            \n",
    "def calculate_proximal_steps(date, satisfactory):\n",
    "    arg_before = None\n",
    "    arg_after = None\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    #print(arg_before, date, arg_after)\n",
    "    return arg_before, arg_after\n",
    "\n",
    "def worker_download(coord, folder = OUTPUT_FOLDER, year = 2019):\n",
    "    idx_x = coord[1][0]\n",
    "    idx_y = coord[1][1]\n",
    "    print(\"Starting: {} {}\".format(idx_x, idx_y))\n",
    "    coord = coord[0]\n",
    "    output_folder = OUTPUT_FOLDER + \"{}/{}/\".format(str(idx_y), str(idx_x))\n",
    "    if not os.path.exists(os.path.realpath(output_folder)):\n",
    "        os.makedirs(os.path.realpath(output_folder))\n",
    "    existing = len([x for x in os.listdir(os.path.realpath(output_folder))])\n",
    "    if existing > 80:\n",
    "        tiled_bbx = bounding_box(coord, y_offset_max = SIZE*140, x_offset_max = SIZE*140, expansion = 10)\n",
    "        dem_bbx = bounding_box(coord, y_offset_max = SIZE*140, x_offset_max = SIZE*140, expansion = 20)\n",
    "        cloud_steps, means, cloud_probs = identify_clouds(tiled_bbx)\n",
    "        tiles, request = download_tiles(tiled_bbx, means)\n",
    "        \n",
    "        image_dates = []\n",
    "        for date in request.get_dates():\n",
    "            if date.year == year - 1:\n",
    "                image_dates.append(-360 + (date.month-1)*30 + date.day)\n",
    "            if date.year == year:\n",
    "                image_dates.append((date.month-1)*30 + date.day)\n",
    "            if date.year == year + 1:\n",
    "                image_dates.append(365 + (date.month-1)*30+date.day)\n",
    "        image_dates = np.array(image_dates)\n",
    "\n",
    "        args = np.array([len(np.argwhere(cloud_probs[x, :, :].reshape((SIZE*14+2)*(SIZE*14+2)) > 0.3)) for x in range(cloud_probs.shape[0])])\n",
    "        dirty_steps = np.argwhere(args > (SIZE*14+2)*(SIZE*14+2) / 10)\n",
    "        #print(dirty_steps)\n",
    "        missing_images = [np.argwhere(tiles[x, :, : :].flatten() == 0.0) for x in range(tiles.shape[0])]\n",
    "        missing_images = [len(x) for x in missing_images]\n",
    "        tiles = np.delete(tiles, dirty_steps, 0)\n",
    "        cloud_probs = np.delete(cloud_probs, dirty_steps, 0)\n",
    "        image_dates = np.delete(image_dates, dirty_steps)\n",
    "\n",
    "        dem = download_dem(dem_bbx)\n",
    "        dem = np.tile(dem.reshape((1, (SIZE*14)+2, (SIZE*14)+2, 1)), (tiles.shape[0], 1, 1, 1))\n",
    "        tiles = np.concatenate([tiles, dem], axis = -1)\n",
    "\n",
    "        for x_offset, cval in enumerate([x for x in range(0, SIZE*14, 14)]):\n",
    "            for y_offset, rval in enumerate([x for x in range(0, SIZE*14, 14)]):\n",
    "                subs = tiles[:, cval:cval+16, rval:rval+16, : ]\n",
    "                c_probs = cloud_probs[:, cval:cval+16, rval:rval+16]\n",
    "                args = np.array([len(np.argwhere(c_probs[x, :, :].reshape(16*16) > 0.3)) for x in range(c_probs.shape[0])])\n",
    "                to_fix = list(np.argwhere(args > 10))\n",
    "                to_fix = np.unique(np.array(to_fix))\n",
    "\n",
    "                satisfactory_steps = np.array([x for x in range(tiles.shape[0]) if x not in to_fix])\n",
    "                if len(to_fix) > 0:\n",
    "                    for i in to_fix:\n",
    "                        before, after = calculate_proximal_steps(i, satisfactory_steps)\n",
    "                        bef = tiles[before, cval:cval+16, rval:rval+16, : ]\n",
    "                        aft = tiles[after, cval:cval+16, rval:rval+16, : ]\n",
    "                        tiles[i, cval:cval+16, rval:rval+16, : ] = (bef + aft) / 2\n",
    "        \n",
    "        \n",
    "        tiles[:, :, :, 10] /= 90\n",
    "        x, amin = evi(tiles, True)\n",
    "        # Where evi is OOB, remove (likely cloud cover missed)\n",
    "        if len(amin) > 0:\n",
    "            satisfactory = [x for x in range(x.shape[0]) if x not in amin]\n",
    "            for i in amin:\n",
    "                before, after = calculate_proximal_steps(i, satisfactory)\n",
    "                print(\"Interpolating {} with {} and {}\".format(i, before, after))\n",
    "                bef = x[before, :, :, :]\n",
    "                aft = x[after, :, :, :]\n",
    "                x[i, :, :, :] = (bef + aft) / 2\n",
    "        x = bi(x, True)\n",
    "        x = msavi2(x, True)\n",
    "        x = si(x, True)\n",
    "        \n",
    "        # check for and remove null values\n",
    "        missing = np.argwhere(np.isnan(np.mean(x, axis = (1, 2, 3))))\n",
    "        if len(missing) > 0:\n",
    "            x = np.delete(x, missing, 0)\n",
    "            image_dates = np.delete(image_dates, missing)\n",
    "        \n",
    "        x = calculate_and_save_best_images(x, image_dates)\n",
    "    \n",
    "        for row in range(0, 72):\n",
    "            for column in range(0, 72):\n",
    "                for band in [x for x in range(0, 15) if x != 10]:\n",
    "                    sm = smooth(x[:, row, column, band], 1.0, d = 2)\n",
    "                    x[:, row, column, band] = sm\n",
    "\n",
    "        biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "        to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "        x = np.delete(x, to_remove, 0)\n",
    "        #tile_images(x, output_folder)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(0, 1):\n",
    "    for c in range(0, 1):\n",
    "        x = worker_download(corner_coordinates[r][c])\n",
    "\n",
    "# 0 24 has an invalid value\n",
    "# we are at 1, 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d10 = np.stack([x[:, :, :, 0], x[:, :, :, 1], x[:, :, :, 2], x[:, :, :, 6]])\n",
    "d20 = np.stack([x[:, :, :, 3], x[:, :, :, 4], x[:, :, :, 5], x[:, :, :, 7], \n",
    "                x[:, :, :, 8], x[:, :, :, 9]])\n",
    "\n",
    "#3, 4, 5, 7, 8, 9\n",
    "\n",
    "d10 = d10.reshape(24, 4, 128, 128)\n",
    "d20 = d20.reshape(24, 6, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12, 10))\n",
    "#sns.heatmap(x[0, :, :, 3] * ( (pan[0, :, :] - np.mean(x[0, :, :, 3]))))\n",
    "sns.heatmap(d20[0, 1, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/dsen2/utils/DSen2Net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need utils.Dsen2Net, s2model\n",
    "\n",
    "MDL_PATH = \"../src/dsen2/models/\"\n",
    "def DSen2(d10, d20):\n",
    "    d10 = np.reshape(d10, (24, 4, 128, 128))\n",
    "    d20 = np.reshape(d20, (24, 6, 128, 128))\n",
    "    print(np.max(d10))\n",
    "    test = [d10, d20]\n",
    "    input_shape = ((4, None, None), (6, None, None))\n",
    "    prediction = _predict(test, input_shape, deep=False)\n",
    "    #prediction *= 5\n",
    "    return prediction\n",
    "\n",
    "def _predict(test, input_shape, deep=False, run_60=False):\n",
    "    model = s2model(input_shape, num_layers=6, feature_size=128)\n",
    "    predict_file = MDL_PATH+'s2_032_lr_1e-04.hdf5'\n",
    "    print('Symbolic Model Created.')\n",
    "\n",
    "    model.load_weights(predict_file)\n",
    "    print(\"Predicting using file: {}\".format(predict_file))\n",
    "    prediction = model.predict(test, verbose=1)\n",
    "    return prediction\n",
    "\n",
    "pred = DSen2(d10, d20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "\n",
    "sns.heatmap(pred[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
