{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, preprocess, and save train and test data\n",
    "# John Brandt\n",
    "# April 1, 2020\n",
    "\n",
    "- Fuse Sentinel 1/2 data\n",
    "- Reconstruct 2D-array from CEO output CSV by plot\n",
    "- Match sentinel data to CEO labels\n",
    "- Stack data_x, data_y, length\n",
    "- Save numpy arrays for data_x, data_y, length\n",
    "\n",
    "The notebook additionally contains some development code for:\n",
    "- Parameter selection in whittaker smoothing\n",
    "- Graphing plot locations on map\n",
    "\n",
    "# Package imports and source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "%run ../src/preprocessing/slope.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_images(plot_id):\n",
    "    '''Takes a plot ID and subsets the input pd.DataFrame to that plot ID\n",
    "       returns a (14, 14) array-like list with binary labels\n",
    "       \n",
    "        Parameters:\n",
    "          batch_ids (list):\n",
    "          batch_size (int):\n",
    "          \n",
    "         Returns:\n",
    "          x_batch (arr):\n",
    "          y_batch (arr):\n",
    "    '''\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    rows = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        rows.append(list(subs_lat['TREE']))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'train'\n",
    "sentinel_1 = True\n",
    "s2_path = \"../data/{}-s2-new/\".format(source)\n",
    "s1_path = \"../data/{}-s1-radiometric/\".format(source)\n",
    "csv_path = \"../data/{}-csv/\".format(source)\n",
    "output_path = \"../data/{}-processed/\".format(source)\n",
    "dem_path = \"../data/{}-dem/\".format(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025\n"
     ]
    }
   ],
   "source": [
    "# Load and edit bad plot ids if needed\n",
    "verified_lu_change = np.load(\"bad_plot_ids.npy\")\n",
    "len(verified_lu_change)\n",
    "\n",
    "to_add = []\n",
    "to_add = [x for x in to_add if x not in verified_lu_change]\n",
    "verified_lu_change = np.concatenate([verified_lu_change, \n",
    "                     np.array(to_add).flatten()])\n",
    "\n",
    "to_remove = []\n",
    "\n",
    "verified_lu_change = [x for x in verified_lu_change if x not in to_remove]\n",
    "np.save(\"bad_plot_ids.npy\", np.array(verified_lu_change))\n",
    "print(len(verified_lu_change))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ceo-c-asia-train-sample-data-2021-02-16.csv\n",
      "There are 179 plots\n"
     ]
    }
   ],
   "source": [
    "# For either train or test data, loop through each plot and determine whether there is\n",
    "# labelled Y data for it -- returning one dataframe for the entire data set\n",
    "\n",
    "cols_to_keep = ['PLOT_ID', 'SAMPLE_ID', 'LON', 'LAT', 'FLAGGED', 'ANALYSES', 'USER_ID',\n",
    "       'COLLECTION_TIME', 'ANALYSIS_DURATION', 'TREE']\n",
    "csvs = [x for x in os.listdir(csv_path) if 'c-asia' in x]\n",
    "\n",
    "dfs = []\n",
    "for i in csvs:\n",
    "    print(i)\n",
    "    df = pd.read_csv(csv_path + i, encoding = \"ISO-8859-1\")\n",
    "    df.columns = [x.upper() for x in df.columns]\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column not in cols_to_keep:\n",
    "            df = df.drop(column, axis = 1)\n",
    "    df['country'] = i.split(\".\")[0]\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index = True)\n",
    "df = df[~pd.isna(df['TREE'])]\n",
    "\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "plot_ids_loaded = plot_ids\n",
    "\n",
    "print(f\"There are {len(plot_ids)} plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int16(array: np.array) -> np.array:\n",
    "    '''Converts a float32 array to int16, reducing storage costs by three-fold'''\n",
    "    assert np.min(array) >= 0, np.min(array)\n",
    "    assert np.max(array) <= 1, np.max(array)\n",
    "    \n",
    "    array = np.clip(array, 0, 1)\n",
    "    array = np.trunc(array * 65535)\n",
    "    assert np.min(array >= 0)\n",
    "    assert np.max(array <= 65535)\n",
    "    \n",
    "    return array.astype(np.uint16)\n",
    "\n",
    "def process_dem(dem):\n",
    "    dem =  median_filter(dem, size = 5)\n",
    "    dem = calcSlope(dem.reshape((1, 32+2, 32+2)),\n",
    "                      np.full((32+2, 32+2), 10),\n",
    "                      np.full((32+2, 32+2), 10), \n",
    "                      zScale = 1, minSlope = 0.02)\n",
    "    dem = dem / 90\n",
    "    dem = dem.reshape((32+2, 32+2, 1))\n",
    "    dem = dem[1:-1, 1:-1]\n",
    "    dem = median_filter(dem, 5)[4:-4, 4:-4]\n",
    "    return dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3411742cbf7041229a1d0c104e65ca10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=179), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 170 plots\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efd07bfc11e432dbb9d3c1d875d67ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=170), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished loading: (170, 13, 24, 24, 17) of uint16 type\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "dataframe = pd.DataFrame({'plot_id': [''], 'lat': [0.325], 'long': [0.325]})\n",
    "\n",
    "# Identify shape of data to load\n",
    "plot_ids_to_load = []\n",
    "for i in tnrange(len(plot_ids)):\n",
    "    s1_i = f'{s1_path}{str(plot_ids[i])}.npy'\n",
    "    s2_i = f'{s2_path}{str(plot_ids[i])}.npy'\n",
    "    dem_i = f'{dem_path}{str(plot_ids[i])}.npy'\n",
    "    s1_old_i = f'../data/{source}-s1-new/{str(plot_ids[i])}.npy'\n",
    "    if os.path.isfile(s2_i) and os.path.isfile(s1_i):\n",
    "        if plot_ids[i] not in verified_lu_change:\n",
    "            plot_ids_to_load.append(plot_ids[i])\n",
    "\n",
    "print(f\"There are {len(plot_ids_to_load)} plots\")\n",
    "plot_ids_to_load = [x for x in plot_ids_to_load if x != 139077414]\n",
    "data_x = np.zeros((len(plot_ids_to_load), 13, 24, 24, 17)).astype(np.uint16)\n",
    "data_y = np.zeros((len(plot_ids_to_load), 14, 14))\n",
    "            \n",
    "    \n",
    "\n",
    "# Iterate over each plot\n",
    "to_remove = []\n",
    "for i in tnrange(len(plot_ids_to_load)):\n",
    "    s1_i = f'{s1_path}{str(plot_ids_to_load[i])}.npy'\n",
    "    s2_i = f'{s2_path}{str(plot_ids_to_load[i])}.npy'\n",
    "    dem_i = f'{dem_path}{str(plot_ids_to_load[i])}.npy'\n",
    "\n",
    "    x = np.load(s2_i)\n",
    "    s1 = np.load(s1_i)\n",
    "    s1_median = np.median(s1, axis = 0)\n",
    "    s1 = np.concatenate([s1, s1_median[np.newaxis]], axis = 0)\n",
    "    dem = np.load(dem_i)\n",
    "    dem = process_dem(dem)\n",
    "    dem = np.tile(dem.reshape((1, 24, 24)), (x.shape[0], 1, 1))\n",
    "    x[..., 10] = dem\n",
    "    x = np.concatenate([x, s1], axis = -1)\n",
    "    count += 1\n",
    "    y = reconstruct_images(plot_ids_to_load[i])\n",
    "    long = np.mean(df[df['PLOT_ID'] == plot_ids_to_load[i]]['LON'])\n",
    "    lat = np.mean(df[df['PLOT_ID'] == plot_ids_to_load[i]]['LAT'])\n",
    "    dataframe = dataframe.append({'plot_id': str(plot_ids_to_load[i]), 'lat': lat, 'long': long}, ignore_index = True)\n",
    "    dataframe.append([plot_ids_to_load[i], lat, long])\n",
    "    # The indices can range from -1 to 1, clip to 0-1\n",
    "    x[..., 11:15] = np.clip(x[..., 11:15], -1, 1)\n",
    "    x[..., 11:15] = (x[..., 11:15] + 1) / 2\n",
    "    if np.sum(np.isnan(x)) > 0:\n",
    "        to_remove.append(i)\n",
    "    else:\n",
    "        x = np.clip(x, 0, 1)\n",
    "        x = to_int16(x)\n",
    "        data_x[i] = x\n",
    "        data_y[i] = np.array(y)\n",
    "            \n",
    "# Remove any data samples that had missing values\n",
    "if len(to_remove) > 0:\n",
    "    print(f\"Removing {to_remove}\")\n",
    "    data_x = np.delete(data_x, to_remove, 0)\n",
    "    data_y = np.delete(data_y, to_remove, 0)\n",
    "            \n",
    "print(f\"Finished loading: {data_x.shape} of {data_x.dtype} type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train data\n"
     ]
    }
   ],
   "source": [
    "import hickle as hkl\n",
    "dataframe = dataframe.drop(0, 0)\n",
    "dataframe.reset_index(inplace = True, drop = True)\n",
    "if len(to_remove) > 0:\n",
    "    dataframe = dataframe.drop(to_remove, 0)\n",
    "    dataframe.reset_index(inplace = True, drop = True)\n",
    "\n",
    "print(f\"Writing {source} data\")\n",
    "hkl.dump(data_x, f\"../tile_data/{source}/{source}_x.hkl\", mode='w', compression='gzip')\n",
    "hkl.dump(data_y, f\"../tile_data/{source}/{source}_y.hkl\", mode='w', compression='gzip')\n",
    "dataframe.to_csv(f\"../tile_data/{source}/{source}_plot_ids.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
