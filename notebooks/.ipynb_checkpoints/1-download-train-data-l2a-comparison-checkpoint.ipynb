{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data download pipeline\n",
    "\n",
    "Downloads 16x16 training data plots from Sentinel Hub, with the following steps:\n",
    "\n",
    "*  Convert coordinates to UTM, identify bounding boxes of 160 and 180 meter borders\n",
    "*  Download all L1C steps, correct missing bands, and calculate cloud cover\n",
    "*  Select L2A imagery corresponding to the best imagery per 15 days, with missing imagery calculated as the weighted average of the nearest time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants, DataSource, CustomUrlParam\n",
    "from s2cloudless import S2PixelCloudDetector, CloudMaskRequest\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import splu\n",
    "\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOCATION = '../data/subplot4.csv'\n",
    "OUTPUT_FOLDER = '../data/train-l2a-superresolve/'\n",
    "EPSG = CRS.WGS84\n",
    "IMSIZE = 32\n",
    "existing = [int(x[:-4]) for x in os.listdir(OUTPUT_FOLDER) if \".DS\" not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing = [x for x in existing if x in [135345883]]\n",
    "print(existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/slope.py\n",
    "%run ../src/utils.py\n",
    "%run ../src/dsen2/utils/DSen2Net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup function to reproject coordinates\n",
    "def convertCoords(xy, src='', targ=''):\n",
    "\n",
    "    srcproj = osr.SpatialReference()\n",
    "    srcproj.ImportFromEPSG(src)\n",
    "    targproj = osr.SpatialReference()\n",
    "    if isinstance(targ, str):\n",
    "        targproj.ImportFromProj4(targ)\n",
    "    else:\n",
    "        targproj.ImportFromEPSG(targ)\n",
    "    transform = osr.CoordinateTransformation(srcproj, targproj)\n",
    "\n",
    "    pt = ogr.Geometry(ogr.wkbPoint)\n",
    "    pt.AddPoint(xy[0], xy[1])\n",
    "    pt.Transform(transform)\n",
    "    return([pt.GetX(), pt.GetY()])\n",
    "\n",
    "def bounding_box(points, expansion = 160):\n",
    "    # LONG, LAT FOR SOME REASON\n",
    "    bl = list(points[0])\n",
    "    tr = list(points[1])\n",
    "    \n",
    "    if 48 <= bl[0] <= 54:\n",
    "        epsg = 32639 if bl[1] > 0 else 32739\n",
    "    if 42 <= bl[0] <= 48:\n",
    "        epsg = 32638 if bl[1] > 0 else 32738\n",
    "    if 36 <= bl[0] <= 42:\n",
    "        epsg = 32637 if bl[1] > 0 else 32737\n",
    "    if 30 <= bl[0] <= 36:\n",
    "        epsg = 32636 if bl[1] > 0 else 32736\n",
    "    if 24 <= bl[0] <= 30:\n",
    "        epsg = 32635 if bl[1] > 0 else 32735\n",
    "    if 18 <= bl[0] <= 24:\n",
    "        epsg = 32634 if bl[1] > 0 else 32734\n",
    "\n",
    "    bl = convertCoords(bl, 4326, epsg)\n",
    "    tr = convertCoords(tr, 4326, epsg)\n",
    "    init = [b - a for a,b in zip(bl, tr)]\n",
    "    distance1 = tr[0] - bl[0]\n",
    "    distance2 = tr[1] - bl[1]\n",
    "    EXPANSION = (expansion - np.mean([distance1, distance2]))/2 # should this be 155 or 160?\n",
    "    \n",
    "    bl = [a - EXPANSION for a in bl]\n",
    "    tr = [a + EXPANSION for a in tr]\n",
    "    \n",
    "    after = [b - a for a,b in zip(bl, tr)]    \n",
    "    if max(init) > 130:\n",
    "        print(\"ERROR: Initial field greater than 130m\")\n",
    "    if min(init) < 120:\n",
    "        print(\"ERROR: Initial field less than 130m\")\n",
    "        \n",
    "    if min(after) < (expansion - 4.5):\n",
    "        print(\"ERROR\")\n",
    "    if max(after) > (expansion + 5):\n",
    "        print(\"ERROR\")\n",
    "    diffs = [b - a for b, a in zip(after, init)]\n",
    "\n",
    "    bl = convertCoords(bl, epsg, 4326)\n",
    "    tr = convertCoords(tr, epsg, 4326)\n",
    "    return bl, tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "location = calc_bbox(val)\n",
    "location = bounding_box(location)\n",
    "box = BBox(location, crs = EPSG)\n",
    "\n",
    "image_request = WcsRequest(\n",
    "                layer='ALL_BANDS_NDVI',\n",
    "                bbox=box,\n",
    "                time = ('2018-01-01', '2018-12-31'),\n",
    "                image_format = MimeType.TIFF_d32f,\n",
    "                maxcc=1,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=24),\n",
    "            )\n",
    "img_bands = image_request.get_data()\n",
    "img_bands[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bbox(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # TOP, LEFT, BOTTOM, RIGHT\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])),\n",
    "            (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATA_LOCATION)\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1)\n",
    "df = df.dropna(axis = 0)\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box(calc_bbox(plot_ids[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_detector = S2PixelCloudDetector(threshold=0.4, average_over=4, dilation_size=2)\n",
    "\n",
    "def calculate_proximal_steps(uniques, date, clean_steps):\n",
    "    arg_before = None\n",
    "    arg_after = None\n",
    "    uniques = np.array(uniques)\n",
    "    satisfactory = np.argwhere(uniques > 2)\n",
    "    satisfactory = np.array([x for x in satisfactory if x in clean_steps])\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    print(arg_before, date, arg_after)\n",
    "    return arg_before, arg_after\n",
    "\n",
    "def calculate_proximal_steps_index(date, satisfactory):\n",
    "    arg_before = None\n",
    "    arg_after = None\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    return arg_before, arg_after\n",
    "\n",
    "\n",
    "def identify_clouds(bbox, epsg = EPSG, time = ('2018-12-15', '2019-11-25')):\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        cloud_request = WmsRequest(\n",
    "            layer='CLOUD_DETECTION',\n",
    "            bbox=box,\n",
    "            time=time,\n",
    "            width=16,\n",
    "            height=16,\n",
    "            image_format = MimeType.TIFF_d32f,\n",
    "            maxcc=0.33,\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "            time_difference=datetime.timedelta(hours=24),\n",
    "        )\n",
    "        \n",
    "        cloud_img = cloud_request.get_data()\n",
    "        cloud_probs = cloud_detector.get_cloud_probability_maps(np.array(cloud_img))\n",
    "        means = np.mean(cloud_probs, (1, 2))\n",
    "        clean_steps = [i for i, val in enumerate(means) if val < 0.20]\n",
    "        return clean_steps, means, cloud_probs\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "    \n",
    "    \n",
    "def download_dem(val, epsg = EPSG):\n",
    "    location = calc_bbox(val)\n",
    "    bbox = bounding_box(location, expansion = 340)\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM',\n",
    "                         bbox=box,\n",
    "                         width=IMSIZE+2,\n",
    "                         height=IMSIZE+2,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format=MimeType.TIFF_d32f,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    dem_image = calcSlope(dem_image.reshape((1, IMSIZE+2, IMSIZE+2)),\n",
    "                  np.full((IMSIZE+2, IMSIZE+2), 10), np.full((IMSIZE+2, IMSIZE+2), 10), zScale = 1, minSlope = 0.02)\n",
    "    dem_image = dem_image.reshape((IMSIZE+2, IMSIZE+2, 1))\n",
    "    dem_image = dem_image[1:IMSIZE+1, 1:IMSIZE+1, :]\n",
    "    return dem_image\n",
    "\n",
    "\n",
    "def check_zenith(bbox, epsg = EPSG, time = ('2018-12-15', '2019-11-25')):\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        zenith = WmsRequest(\n",
    "            layer='ZENITH',\n",
    "            bbox=box,\n",
    "            time=time,\n",
    "            width=IMSIZE,\n",
    "            height=IMSIZE,\n",
    "            image_format = MimeType.TIFF_d32f,\n",
    "            maxcc=0.33,\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=24),\n",
    "        )\n",
    "        \n",
    "        zenith = zenith.get_data()\n",
    "        return zenith\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "    \n",
    "        \n",
    "    \n",
    "def download_tiles(bbox, clean_steps, sz, epsg = EPSG, time = ('2018-12-15', '2019-11-25')):\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WmsRequest(\n",
    "                layer='L2A',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                width=sz,\n",
    "                height=sz,\n",
    "                image_format = MimeType.TIFF_d32f,\n",
    "                maxcc=0.33,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=24),\n",
    "            )\n",
    "        img_bands = image_request.get_data()\n",
    "        img_bands = np.array(img_bands)\n",
    "        return img_bands, image_request\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "    \n",
    "\n",
    "        \n",
    "def calculate_and_save_best_images(img_bands, image_dates):\n",
    "    # This function interpolates data to 5 day windows linearly\n",
    "\n",
    "    biweekly_dates = [day for day in range(0, 360, 5)] # ideal imagery dates are every 15 days\n",
    "    \n",
    "    # Clouds have been removed at this step, so all steps are satisfactory\n",
    "    satisfactory_ids = [x for x in range(0, img_bands.shape[0])]\n",
    "    satisfactory_dates = [value for idx, value in enumerate(image_dates) if idx in satisfactory_ids]\n",
    "    \n",
    "    \n",
    "    selected_images = {}\n",
    "    for i in biweekly_dates:\n",
    "        distances = [abs(date - i) for date in satisfactory_dates]\n",
    "        closest = np.min(distances)\n",
    "        closest_id = np.argmin(distances)\n",
    "        # If there is imagery within 5 days, select it\n",
    "        if closest < 5:\n",
    "            date = satisfactory_dates[closest_id]\n",
    "            image_idx = int(np.argwhere(np.array(image_dates) == date)[0])\n",
    "            selected_images[i] = {'image_date': [date], 'image_ratio': [1], 'image_idx': [image_idx]}\n",
    "        # If there is not imagery within 5 days, look for the closest above and below imagery\n",
    "        else:\n",
    "            distances = np.array([(date - i) for date in satisfactory_dates])\n",
    "            # Number of days above and below the selected date of the nearest clean imagery\n",
    "            above = distances[np.where(distances < 0, distances, -np.inf).argmax()]\n",
    "            below = distances[np.where(distances > 0, distances, np.inf).argmin()]\n",
    "            if abs(above) > 100: # If date is the last date, occassionally argmax would set above to - number\n",
    "                above = below\n",
    "            if abs(below) > 100:\n",
    "                below = above\n",
    "            if above != below:\n",
    "                below_ratio = above / (above - below)\n",
    "                above_ratio = 1 - below_ratio\n",
    "            else:\n",
    "                above_ratio = below_ratio = 0.5\n",
    "                \n",
    "            # Extract the image date and imagery index for the above and below values\n",
    "            above_date = i + above\n",
    "            above_image_idx = int(np.argwhere(np.array(image_dates) == above_date)[0])\n",
    "            \n",
    "            below_date = i + below\n",
    "            below_image_idx = int(np.argwhere(np.array(image_dates) == below_date)[0])\n",
    "            \n",
    "            selected_images[i] = {'image_date': [above_date, below_date], 'image_ratio': [above_ratio, below_ratio],\n",
    "                                 'image_idx': [above_image_idx, below_image_idx]}\n",
    "                               \n",
    "    max_distance = 0\n",
    "    \n",
    "    for i in selected_images.keys():\n",
    "        #print(i, selected_images[i])\n",
    "        if len(selected_images[i]['image_date']) == 2:\n",
    "            dist = selected_images[i]['image_date'][1] - selected_images[i]['image_date'][0]\n",
    "            if dist > max_distance:\n",
    "                max_distance = dist\n",
    "    \n",
    "    print(\"Maximum time distance: {}\".format(max_distance))\n",
    "        \n",
    "    # Compute the weighted average of the selected imagery for each time step\n",
    "    keep_steps = []\n",
    "    for i in selected_images.keys():\n",
    "        info = selected_images[i]\n",
    "        if len(info['image_idx']) == 1:\n",
    "            step = img_bands[info['image_idx'][0]]\n",
    "        if len(info['image_idx']) == 2:\n",
    "            step1 = img_bands[info['image_idx'][0]] * info['image_ratio'][0]\n",
    "            step2 = img_bands[info['image_idx'][1]] * info['image_ratio'][1]\n",
    "            step = step1 + step2\n",
    "        keep_steps.append(step)\n",
    "        \n",
    "    keep_steps = np.stack(keep_steps)\n",
    "    return keep_steps, max_distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speyediff(N, d, format = 'csc'):\n",
    "    shape = (N-d, N)\n",
    "    diagonals = np.zeros(2*d + 1)\n",
    "    diagonals[d] = 1.\n",
    "    for i in range(d):\n",
    "        diff = diagonals[:-1] - diagonals[1:]\n",
    "        diagonals = diff\n",
    "    offsets = np.arange(d+1)\n",
    "    spmat = sparse.diags(diagonals, offsets, shape, format = format)\n",
    "    return spmat\n",
    "\n",
    "def smooth(y, lmbd, d = 2):\n",
    "    m = len(y)\n",
    "    E = sparse.eye(m, format = 'csc')\n",
    "    D = speyediff(m, d, format = 'csc')\n",
    "    coefmat = E + lmbd * D.conj().T.dot(D)\n",
    "    z = splu(coefmat).solve(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDL_PATH = \"../src/dsen2/models/\"\n",
    "\n",
    "input_shape = ((4, None, None), (6, None, None))\n",
    "model = s2model(input_shape, num_layers=6, feature_size=128)\n",
    "predict_file = MDL_PATH+'s2_032_lr_1e-04.hdf5'\n",
    "print('Symbolic Model Created.')\n",
    "\n",
    "model.load_weights(predict_file)\n",
    "\n",
    "def DSen2(d10, d20):\n",
    "    test = [d10, d20]\n",
    "    input_shape = ((4, None, None), (6, None, None))\n",
    "    prediction = _predict(test, input_shape, deep=False)\n",
    "    #prediction *= 5\n",
    "    return prediction\n",
    "\n",
    "def _predict(test, input_shape, model = model, deep=False, run_60=False):\n",
    "    \n",
    "    print(\"Predicting using file: {}\".format(predict_file))\n",
    "    prediction = model.predict(test, verbose=1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ids = [135345883]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_download = [x for x in plot_ids if x not in existing]\n",
    "errors = []\n",
    "year = 2019\n",
    "print(\"STARTING DOWNLOAD OF {} plots from {} to {}\".format(len(to_download), DATA_LOCATION, OUTPUT_FOLDER))\n",
    "for i, val in enumerate(plot_ids):\n",
    "    if val in existing:\n",
    "        print(\"Downloading {}/{}, {}\".format(i, len(plot_ids), val))\n",
    "        location = calc_bbox(val)\n",
    "        location1 = bounding_box(location, expansion = 320)\n",
    "        location2 = bounding_box(location, expansion = 160)\n",
    "        try:\n",
    "            # Identify cloud steps, download DEM, and download L2A series\n",
    "            clean_steps, means, probs = identify_clouds(location2)\n",
    "            dem = download_dem(val)\n",
    "            print(\"Downloading 128x128 img\")\n",
    "            img, image_request = download_tiles(location1, means, 32)\n",
    "            print(\"Downloading 16x16 img\")\n",
    "            small, small_request = download_tiles(location2, means, 16)\n",
    "            \n",
    "            # Replace the middle of the 128x128 img with the AOI\n",
    "            #middle = img[:, 56:72, 56:72, :]\n",
    "            middle = img[:, 8:24, 8:24, :]\n",
    "            difference = np.sum(abs(middle - small))\n",
    "            print(\"Small/large diff: {}\".format(difference))\n",
    "            img[:, 8:24, 8:24, :] = small\n",
    "            \n",
    "            \n",
    "            # Subset zenith < 70\n",
    "            zenith = check_zenith(location2)\n",
    "            zenith = np.mean(np.stack(zenith), axis = (1, 2))\n",
    "            zenith_outliers = np.argwhere(zenith > 70)\n",
    "            if len(zenith_outliers) > 0:\n",
    "                print(\"Zenith outlier: {}\".format(zenith_outliers))\n",
    "            \n",
    "            # Calculate imagery dates\n",
    "            image_dates = []\n",
    "            for date in small_request.get_dates():\n",
    "                if date.year == year - 1:\n",
    "                    image_dates.append(-360 + (date.month-1)*30 + date.day)\n",
    "                if date.year == year:\n",
    "                    image_dates.append((date.month-1)*30 + date.day)\n",
    "                if date.year == year + 1:\n",
    "                    image_dates.append(365 + (date.month-1)*30+date.day)\n",
    "            image_dates = np.array(image_dates)\n",
    "            \n",
    "            # Remove imagery where >4% is clouds, and where there is null data\n",
    "            args = np.array([len(np.argwhere(probs[x, :, :].reshape((1*14+2)*(1*14+2)) > 0.3)) for x in range(probs.shape[0])])\n",
    "            dirty_steps = np.argwhere(args > (1*14+2)*(1*14+2) / 25)\n",
    "            missing_images = [np.argwhere(img[x, :, : :].flatten() == 0.0) for x in range(img.shape[0])]\n",
    "            missing_images = np.array([len(x) for x in missing_images])\n",
    "            missing_images_p = [np.argwhere(img[x, :, : :].flatten() >= 1) for x in range(img.shape[0])]\n",
    "            missing_images_p = np.array([len(x) for x in missing_images_p])\n",
    "            missing_images += missing_images_p\n",
    "            missing_images = list(np.argwhere(missing_images >= 1))\n",
    "            to_remove = np.unique(np.array(list(dirty_steps) + list(missing_images) + list(zenith_outliers)))\n",
    "            \n",
    "            # Remove null steps\n",
    "            print(\"There are {}/{} dirty steps: {} cloud, {} missing, {} zenith\".format(len(to_remove),\n",
    "                                                                                        len(img), len(dirty_steps),\n",
    "                                                                                        len(missing_images), len(zenith_outliers)))\n",
    "            \n",
    "            \n",
    "            img = np.delete(img, to_remove, 0)\n",
    "            small = np.delete(small, to_remove, 0)\n",
    "            probs = np.delete(probs, to_remove, 0)\n",
    "            image_dates = np.delete(image_dates, to_remove)\n",
    "            img = remove_blank_steps(img)\n",
    "            \n",
    "            # Concatenate DEM\n",
    "            dem = np.tile(dem.reshape((1, 32, 32, 1)), (img.shape[0], 1, 1, 1))\n",
    "            dem = dem[:, 8:24, 8:24, :]\n",
    "            small = np.concatenate([small, dem], axis = -1)\n",
    "            small[:, :, :, -1] /= 90\n",
    "            \n",
    "            # Superresolve the imagery\n",
    "            x = img\n",
    "            d10 = np.stack([x[:, :, :, 0], x[:, :, :, 1], x[:, :, :, 2], x[:, :, :, 6]], axis = -1)\n",
    "            d20 = np.stack([x[:, :, :, 3], x[:, :, :, 4], x[:, :, :, 5], x[:, :, :, 7], \n",
    "                            x[:, :, :, 8], x[:, :, :, 9]], axis = -1)\n",
    "            \n",
    "            d10 = np.swapaxes(d10, 1, -1)\n",
    "            d10 = np.swapaxes(d10, 2, 3)\n",
    "            d20 = np.swapaxes(d20, 1, -1)\n",
    "            d20 = np.swapaxes(d20, 2, 3)\n",
    "            superresolved = DSen2(d10, d20)\n",
    "            \n",
    "            # returns band IDXs 3, 4, 5, 7, 8, 9\n",
    "            #x[:, :, :, 3] = superresolved[:, 0, :, :]\n",
    "            #x[:, :, :, 4] = superresolved[:, 1, :, :]\n",
    "            #x[:, :, :, 5] = superresolved[:, 2, :, :]\n",
    "            #x[:, :, :, 7] = superresolved[:, 3, :, :]\n",
    "            #x[:, :, :, 8] = superresolved[:, 4, :, :]\n",
    "            #x[:, :, :, 9] = superresolved[:, 5, :, :]\n",
    "            #to_save = x\n",
    "            # Re-subset the AOI\n",
    "            #x = x[:, 8:24, 8:24, :]\n",
    "            \n",
    "            # Calculate indices\n",
    "            tiles, amin = evi(small, True)\n",
    "            # Where evi is OOB, remove (likely cloud cover missed)\n",
    "            if len(amin) > 0:\n",
    "                satisfactory = [x for x in range(tiles.shape[0]) if x not in amin]\n",
    "                for i in amin:\n",
    "                    before, after = calculate_proximal_steps_index(i, satisfactory)\n",
    "                    print(\"Interpolating {} with {} and {}\".format(i, before, after))\n",
    "                    bef = tiles[before, :, :, :]\n",
    "                    aft = tiles[after, :, :, :]\n",
    "                    tiles[i, :, :, :] = (bef + aft) / 2\n",
    "                \n",
    "            tiles = bi(tiles, True)\n",
    "            tiles = msavi2(tiles, True)\n",
    "            x = si(tiles, True)\n",
    "            \n",
    "            print(\"Clean: {}\".format(x.shape))\n",
    "            \n",
    "            # Interpolate linearly to 5 day frequency\n",
    "            tiles, max_distance = calculate_and_save_best_images(x, image_dates) # 22, 16, 16, 10\n",
    "            \n",
    "            # Smooth linear interpolation\n",
    "            for row in range(0, 16):\n",
    "                for column in range(0, 16):\n",
    "                    for band in [x for x in range(0, 15) if x != 10]:\n",
    "                        sm = smooth(tiles[:, row, column, band], 1.0, d = 2)\n",
    "                        tiles[:, row, column, band] = sm\n",
    "\n",
    "            # Retain only iamgery every 15 days\n",
    "            biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "            to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "            tiles = np.delete(tiles, to_remove, 0)\n",
    "            print(tiles.shape)\n",
    "\n",
    "            #if max_distance <= 90:\n",
    "                #np.save(OUTPUT_FOLDER + str(val), tiles)\n",
    "                #print(\"\\n\")\n",
    "            #else:\n",
    "            #    print(\"Skipping {} because there is a {} distance\".format(val, max_distance))\n",
    "            #    print(\"\\n\")\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.fatal(e, exc_info=True)\n",
    "            errors.append(img)\n",
    "            #continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compare = np.load(\"../data/train-l2a-dec/135345883.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(np.sum(abs(to_compare - tiles), axis = (1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = calc_bbox(val)\n",
    "location1 = bounding_box(location, expansion = 322)\n",
    "location2 = bounding_box(location, expansion = 162)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_download(bbox, epsg = EPSG, time = ('2019-01-30', '2019-7-30')):\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WcsRequest(\n",
    "                layer='TRUE_COLOR',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                image_format = MimeType.TIFF_d32f,\n",
    "                maxcc=0.2,\n",
    "                #resx='10m', resy='10m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=24),\n",
    "            )\n",
    "        img_bands = image_request.get_data()\n",
    "        return img_bands, image_request\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "        \n",
    "x = check_download(location1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.stack(x1[0])\n",
    "x = np.stack(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(np.sum(abs(x - x1), axis = (1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap((x1[22, :, :, 0] - x[22, :, :, 0]).reshape(16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1[1].get_dates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
