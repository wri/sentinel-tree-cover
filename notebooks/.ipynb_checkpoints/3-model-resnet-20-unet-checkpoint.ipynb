{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree segmentation with Sentinel 1/2 imagery - UNET Baseline\n",
    "\n",
    "## John Brandt\n",
    "## April 20, 2020\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook contains the TensorFlow model training and prediction used to segment trees for [Restoration Mapper](https://restorationmapper.org). The notebook uses tensorflow 1.13.1 and additionally relies on Keras and tflearn. \n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Package loading\n",
    "- Utility scripts\n",
    "- Hyperparameter definitions\n",
    "- Custom tensorflow layer functions\n",
    "- Tensorflow graph creation\n",
    "- Data loading\n",
    "- Data preprocessing\n",
    "- Equibatch creation\n",
    "- Loss definition\n",
    "- Tensorflow graph initialization\n",
    "- Training\n",
    "- Model validation\n",
    "- Sanity Checks\n",
    "\n",
    "## Package Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras.layers import Conv2D, Lambda, Dense, Multiply, Add\n",
    "from tensorflow.python.keras.activations import selu\n",
    "from tensorflow.initializers import glorot_normal, lecun_normal\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim import conv2d\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from tflearn.layers.conv import global_avg_pool\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.layers import batch_normalization\n",
    "from tensorflow.python.util import deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/zoneout.py\n",
    "%run ../src/convgru.py\n",
    "%run ../src/lovasz.py\n",
    "%run ../src/utils.py\n",
    "%run ../src/adabound.py\n",
    "%run ../src/slope.py\n",
    "%run ../src/dropblock.py\n",
    "%run ../src/metrics_new.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONE_OUT_PROB = 0.20\n",
    "N_FILTERS = 32\n",
    "AVERAGE_GRU_FILTERS = True\n",
    "ACTIVATION_FUNCTION = 'relu'\n",
    "\n",
    "INITIAL_LR = 1e-4\n",
    "FINAL_LR = 2e-2\n",
    "DROPBLOCK_MAXSIZE = 5\n",
    "DECONV = 'nearest'\n",
    "N_CONV_BLOCKS = 1\n",
    "FINAL_ALPHA = 0.7\n",
    "LABEL_SMOOTHING = 0.075\n",
    "BATCH_RENORM = 'norm'\n",
    "\n",
    "L2_REG = 0.0\n",
    "BN_MOMENTUM = 0.9\n",
    "BATCH_SIZE = 16\n",
    "MAX_DROPBLOCK = 0.85\n",
    "\n",
    "gru_flt = 32\n",
    "fpa_flt = 28\n",
    "out_conv_flt = 32\n",
    "\n",
    "IMAGE_SIZE = 16\n",
    "LABEL_SIZE = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter grid search definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 3.8 million combinations of hyperparameters\n",
    "params = {\n",
    "    'out_conv_filter': range(16, 64, 16), # 5\n",
    "    'activation_function': ['relu', 'selu', 'elu'], # 3\n",
    "    'final_lr': [5e-3, 1e-2, 2e-2, 5e-2], # 4\n",
    "    'dropblock_maxsize': [7, 6, 5, 4, 3], # 5\n",
    "    'label_smoothing': [0., 0.025, 0.05, 0.075, 0.10, 0.125],\n",
    "    'batch_renorm': ['renorm', 'norm']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom layer definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility blocks (Batch norm, cSSE, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Batch_Normalization(x, training, scope, clipping_params):\n",
    "    '''Batch renormalization layer from tensorflow.layers\n",
    "\n",
    "         Parameters:\n",
    "          x (tf.Variable): input layer\n",
    "          training (tf.Variable): flag to differentiate between train/test ops\n",
    "          scope (str): tensorflow scope\n",
    "          clipping_params (dict): specifies clipping of \n",
    "                                  rmax, dmax, rmin for renormalization\n",
    "\n",
    "         Returns:\n",
    "          x (tf.Variable): output of batch renormalization\n",
    "          \n",
    "         References:\n",
    "          https://github.com/tensorflow/docs/blob/r1.13/site/en/\n",
    "          api_docs/python/tf/layers/batch_normalization.md\n",
    "    '''\n",
    "    return batch_normalization(inputs=x, \n",
    "                               momentum = BN_MOMENTUM, \n",
    "                               training=training,\n",
    "                               renorm = True,\n",
    "                               reuse=None,\n",
    "                               renorm_clipping = clipping_params,\n",
    "                               name = scope)\n",
    "\n",
    "\n",
    "def calc_renorm_params(epoch, n_samples, batch_size, k = 0):\n",
    "    '''Calculates the clipping parameters for renormalization\n",
    "       based on the learning schedule outlined in the original paper\n",
    "       where rmax is initialized to 1, dmax to 0, rmin to 0,\n",
    "       (effectively batch normalization), and then rmax and dmax\n",
    "       are gradually reaxed to 3 and 5 over 40k and 25k steps,\n",
    "       respectively. In this case, 40k has been reduced to 30k, and 25k\n",
    "       has been reduced to 20k.\n",
    "\n",
    "         Parameters:\n",
    "          epoch (int): number of current training epoch\n",
    "                       if testing, epoch number of model used\n",
    "          n_samples (int): total number of training samples\n",
    "          batch_size (int): training batch size \n",
    "\n",
    "         Returns:\n",
    "          rmax (float)\n",
    "          dmax (float)\n",
    "          rmin (float)\n",
    "    '''\n",
    "    step = epoch * (n_samples // batch_size)\n",
    "    step += k\n",
    "    if step < 2500:\n",
    "        rmax = 1.\n",
    "        dmax = 0.\n",
    "        rmin = 0.\n",
    "    if step >= 2500:\n",
    "        rmax = np.min([1 + 2*((step-2500)/25000), 3])\n",
    "        dmax = np.min([1 + 5*((step-2500)/15000), 5])\n",
    "        rmin = 0.\n",
    "    return rmax, dmax, rmin\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_selu(inp, is_training, kernel_size, scope,\n",
    "                filter_count = 16, pad = True, padding = 'valid', dilated = False,\n",
    "                activation = True):\n",
    "    '''Convolutional 2D layer with SELU activation and Lecun normal initialization\n",
    "       with no batch norm. Only used if params['activation'] = 'selu'\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): (B, H, W, C) input layer\n",
    "          is_training (str): flag to differentiate between train/test ops\n",
    "          kernel_size (int): kernel size of convolution\n",
    "          scope (str): tensorflow variable scope\n",
    "          filter_count (int): number of convolution filters\n",
    "          pad (bool): whether or not to reflect pad input\n",
    "          padding (str): one of ['valid', 'same']\n",
    "          dilated (bool): whether to perform atruous convolution\n",
    "          activation (bool): whether to activate output\n",
    "\n",
    "         Returns:\n",
    "          conv (tf.Variable): output of Conv2D -> SELU\n",
    "          \n",
    "         References:\n",
    "          https://arxiv.org/abs/1706.02515\n",
    "    '''\n",
    "    if activation:\n",
    "        act = selu\n",
    "    else:\n",
    "        act = None\n",
    "    if not dilated:\n",
    "        padded = ReflectionPadding2D((1, 1,))(inp)\n",
    "        conv = Conv2D(filters = filter_count, kernel_size = (kernel_size, kernel_size), activation = act,\n",
    "                        padding = padding, kernel_initializer = lecun_normal())(padded)\n",
    "    if not dilated and not pad:\n",
    "        conv = Conv2D(filters = filter_count, kernel_size = (kernel_size, kernel_size), activation = act,\n",
    "                        padding = padding, kernel_initializer = lecun_normal())(inp)\n",
    "    if dilated:\n",
    "        padded = ReflectionPadding2D((2, 2,))(inp)\n",
    "        conv = Conv2D(filters = filter_count, kernel_size = (3, 3), activation = act, dilation_rate = (2, 2),\n",
    "                        padding = padding, kernel_initializer = lecun_normal())(padded)\n",
    "    return conv\n",
    "\n",
    "def conv_bn_relu(inp, \n",
    "                 is_training, \n",
    "                 kernel_size,\n",
    "                 scope,\n",
    "                 filters, \n",
    "                 clipping_params,\n",
    "                 keep_rate,\n",
    "                 activation = True,\n",
    "                 use_bias = False,\n",
    "                 batch_norm = True):\n",
    "    \n",
    "    '''2D convolution, batch renorm, relu block, 3x3 drop block. \n",
    "       Use_bias must be set to False for batch normalization to work. \n",
    "       He normal initialization is used with batch normalization.\n",
    "       RELU is better applied after the batch norm.\n",
    "       DropBlock performs best when applied last, according to original paper.\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): input layer\n",
    "          is_training (str): flag to differentiate between train/test ops\n",
    "          kernel_size (int): size of convolution\n",
    "          scope (str): tensorflow variable scope\n",
    "          filters (int): number of filters for convolution\n",
    "          clipping_params (dict): specifies clipping of \n",
    "                                  rmax, dmax, rmin for renormalization\n",
    "          activation (bool): whether to apply RELU\n",
    "          use_bias (str): whether to use bias. Should always be false\n",
    "\n",
    "         Returns:\n",
    "          bn (tf.Variable): output of Conv2D -> Batch Norm -> RELU\n",
    "        \n",
    "         References:\n",
    "          http://papers.nips.cc/paper/8271-dropblock-a-regularization-\n",
    "              method-for-convolutional-networks.pdf\n",
    "          https://arxiv.org/abs/1702.03275\n",
    "          \n",
    "    '''\n",
    "    \n",
    "    conv = Conv2D(filters = filters, kernel_size = (kernel_size, kernel_size), \n",
    "                  activation = None, padding = 'valid', use_bias = use_bias,\n",
    "                  kernel_initializer = tf.keras.initializers.he_normal())(inp)\n",
    "    if batch_norm:\n",
    "        conv = Batch_Normalization(conv, is_training, scope, clipping_params)\n",
    "    if activation:\n",
    "        conv = tf.nn.relu(conv)\n",
    "    drop_block = DropBlock2D(keep_prob=keep_rate, block_size=3)\n",
    "    conv = drop_block(conv, is_training)\n",
    "\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bands = 16\n",
    "reg = keras.regularizers.l2(L2_REG) # for GRU\n",
    "inp = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, n_bands))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 14, 14))#, 1))\n",
    "keep_rate = tf.placeholder_with_default(1.0, ()) # For DropBlock\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training') # For BN, DropBlock\n",
    "ft_lr = tf.placeholder_with_default(0.001, shape = ()) # For loss scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmax = tf.placeholder(tf.float32, shape = ())\n",
    "rmin = tf.placeholder(tf.float32, shape = ())\n",
    "dmax = tf.placeholder(tf.float32, shape = ())\n",
    "\n",
    "clipping_params = {\n",
    "    'rmax': rmax,\n",
    "    'rmin': rmin,\n",
    "    'dmax': dmax\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWN 1\n",
    "print(\"Input: {}\".format(inp.shape))\n",
    "x1 = ReflectionPadding2D((1, 1,))(inp)\n",
    "conv1 = conv_bn_relu(inp = x1, \n",
    "                 is_training = is_training, kernel_size = 3,\n",
    "                 scope = \"conv1\", filters = N_FILTERS, \n",
    "                 clipping_params = clipping_params,\n",
    "                 activation = True, keep_rate = keep_rate, \n",
    "                 use_bias = False, batch_norm = True)\n",
    "print(\"Conv1: {}\".format(conv1.shape))\n",
    "\n",
    "\n",
    "\n",
    "x1 = ReflectionPadding2D((1, 1,))(conv1)\n",
    "x = conv_bn_relu(inp = x1, \n",
    "                 is_training = is_training, kernel_size = 3,\n",
    "                 scope = \"conv2\", filters = N_FILTERS, \n",
    "                 clipping_params = clipping_params,\n",
    "                 activation = True, keep_rate = keep_rate, \n",
    "                 use_bias = False, batch_norm = True)\n",
    "print(\"Conv2: {}\".format(x.shape))\n",
    "\n",
    "\n",
    "down16 = tf.concat([x, conv1], axis = -1)\n",
    "print(down16.shape)\n",
    "\n",
    "maxpool1 = MaxPool2D(pool_size=(2, 2), strides=None)(down16)\n",
    "print(\"Maxpool: {}\".format(maxpool1.shape))\n",
    "\n",
    "\n",
    "# DOWN 2\n",
    "x1 = ReflectionPadding2D((1, 1,))(maxpool1)\n",
    "x1 = conv_bn_relu(inp = x1, \n",
    "                 is_training = is_training, kernel_size = 3,\n",
    "                 scope = \"conv9\", filters = N_FILTERS*2, \n",
    "                 clipping_params = clipping_params,\n",
    "                 activation = True, keep_rate = keep_rate, \n",
    "                 use_bias = False, batch_norm = True)\n",
    "\n",
    "x1 = ReflectionPadding2D((1, 1,))(x1)\n",
    "x = conv_bn_relu(inp = x1, \n",
    "                 is_training = is_training, kernel_size = 3,\n",
    "                 scope = \"conv10\", filters = N_FILTERS*2, \n",
    "                 clipping_params = clipping_params,\n",
    "                 activation = True, keep_rate = keep_rate, \n",
    "                 use_bias = False, batch_norm = True)\n",
    "\n",
    "down8 = tf.concat([x, maxpool1], axis = -1)\n",
    "maxpool2 = MaxPool2D(pool_size=(2, 2), strides=None)(down8)\n",
    "print(\"Maxpool 2: {}\".format(maxpool2.shape))\n",
    "\n",
    "# 4x4 Block\n",
    "x1 = ReflectionPadding2D((1, 1,))(maxpool2)\n",
    "x1 = conv_bn_relu(inp = x1, \n",
    "                 is_training = is_training, kernel_size = 3,\n",
    "                 scope = \"conv13\", filters = N_FILTERS*4, \n",
    "                 clipping_params = clipping_params,\n",
    "                 activation = True, keep_rate = keep_rate, \n",
    "                 use_bias = False, batch_norm = True)\n",
    "x1 = ReflectionPadding2D((1, 1,))(x1)\n",
    "print(\"Down mid: {}\".format(x1.shape))\n",
    "\n",
    "x = conv_bn_relu(inp = x1, \n",
    "                 is_training = is_training, kernel_size = 3,\n",
    "                 scope = \"conv14\", filters = N_FILTERS*4, \n",
    "                 clipping_params = clipping_params,\n",
    "                 activation = True, keep_rate = keep_rate, \n",
    "                 use_bias = False, batch_norm = True)\n",
    "print(\"Down out: {}\".format(x.shape))\n",
    "\n",
    "# UPSAMPLING 4 - 8\n",
    "three_up = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(x)\n",
    "three_up = tf.concat([three_up, down8], axis = -1)\n",
    "\n",
    "three_up = ReflectionPadding2D((1, 1,))(three_up)\n",
    "three_up = conv_bn_relu(inp = three_up, is_training = is_training, \n",
    "           kernel_size = 3, scope =  'upconv1',\n",
    "           filters = N_FILTERS*2, clipping_params = clipping_params,\n",
    "           keep_rate = keep_rate, activation = True,\n",
    "           use_bias = False, batch_norm = True)\n",
    "print(three_up.shape)\n",
    "\n",
    "x1 = ReflectionPadding2D((1, 1,))(three_up)\n",
    "x = conv_bn_relu(inp = x1, \n",
    "                 is_training = is_training, kernel_size = 3,\n",
    "                 scope = \"convup1\", filters = N_FILTERS*2, \n",
    "                 clipping_params = clipping_params,\n",
    "                 activation = True, keep_rate = keep_rate, \n",
    "                 use_bias = False, batch_norm = True)\n",
    "\n",
    "# Upsampling 8 - 16\n",
    "three_up = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(three_up)\n",
    "three_up = tf.concat([three_up, down16], axis = -1)\n",
    "three_up = ReflectionPadding2D((1, 1,))(three_up)\n",
    "three_up = conv_bn_relu(inp = three_up, is_training = is_training, \n",
    "           kernel_size = 3, scope =  'upconv2',\n",
    "           filters = N_FILTERS, clipping_params = clipping_params,\n",
    "           keep_rate = keep_rate, activation = True,\n",
    "           use_bias = False, batch_norm = True)\n",
    "print(three_up.shape)\n",
    "\n",
    "x1 = ReflectionPadding2D((1, 1,))(three_up)\n",
    "x = conv_bn_relu(inp = x1, \n",
    "                 is_training = is_training, kernel_size = 3,\n",
    "                 scope = \"convup2\", filters = N_FILTERS, \n",
    "                 clipping_params = clipping_params,\n",
    "                 activation = True, keep_rate = keep_rate, \n",
    "                 use_bias = False, batch_norm = True)\n",
    "\n",
    "# Out block\n",
    "x = conv_bn_relu(inp = three_up, \n",
    "                 is_training = is_training, kernel_size = 3,\n",
    "                 scope = \"convout\", filters = N_FILTERS, \n",
    "                 clipping_params = clipping_params,\n",
    "                 activation = True, keep_rate = keep_rate, \n",
    "                 use_bias = False, batch_norm = True)\n",
    "print(x.shape)\n",
    "\n",
    "# \n",
    "\n",
    "print(\"Initializing last sigmoid bias with -2.94 constant\")\n",
    "init = tf.constant_initializer([-np.log(0.7/0.3)]) # For focal loss\n",
    "fm = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1), \n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid',\n",
    "            bias_initializer = init,\n",
    "           )(x) # For focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(\"This model has {} parameters\".format(total_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "*  Load in CSV data from Collect Earth\n",
    "*  Reconstruct the X, Y grid for the Y data per sample\n",
    "*  Calculate NDVI, EVI, SAVI, BI, MSAVI2, and SI\n",
    "*  Stack X, Y, length data\n",
    "*  Apply median filter to DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.load(\"../tile_data/processed/data_x_l2a_processed.npy\")\n",
    "train_y = np.load(\"../tile_data/processed/data_y_l2a_processed.npy\")\n",
    "\n",
    "train_x = np.delete(train_x, 14, -1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "*  Identify and remove samples with time steps / channels that have a 0. or 1. value, which indicates missing data\n",
    "*  Identify and remove samples with time steps / channels with no variation, which indicates missing data\n",
    "*  Identify and remove samples with values above or below the allowable values for the band\n",
    "*  Identify and remove samples with null data, or samples with extreme band 0 data (which squash all the \"clean\" samples)\n",
    "*  Smooth per-pixel temporal data with Whittaker smoother, d = 2, lambda = 0.5 to reduce sample noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "below_1 = [i for i, val in enumerate(train_x[:, :, :, :, :-2]) if np.min(val) < -1.5]\n",
    "above_1 = [i for i, val in enumerate(train_x[:, :, :, :, :-2]) if np.max(val) > 1.5]\n",
    "min_vals = [np.min(val) for i, val in enumerate(train_x[:, :, :, :, :-2]) if np.min(val) < -1.5]\n",
    "max_vals = [np.max(val) for i, val in enumerate(train_x[:, :, :, :, :-2]) if np.max(val) > 1.5]\n",
    "nans = [i for i, val in enumerate(train_x) if np.sum(np.isnan(val)) > 0]\n",
    "oob_vals = [i for i, val in enumerate(train_x) if np.max(val[:, :, :, 0]) > 0.7]\n",
    "\n",
    "outliers = below_1 + above_1 + nans + oob_vals\n",
    "outliers = list(set(outliers))\n",
    "print(\"Removing {} outlying training data points\".format(len(outliers)))\n",
    "print(\"\\n\")\n",
    "\n",
    "train_x = train_x[[x for x in range(0, len(train_x)) if x not in outliers]]\n",
    "train_y = train_y[[x for x in range(0, len(train_y)) if x not in outliers]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min, Max scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_all = []\n",
    "max_all = []\n",
    "\n",
    "for band in range(0, train_x.shape[-1]):\n",
    "    mins = np.percentile(train_x[:, :, :, :, band], 1)\n",
    "    maxs = np.percentile(train_x[:, :, :, :, band], 99)\n",
    "\n",
    "    train_x[:, :, :, :, band] = np.clip(train_x[:, :, :, :, band], mins, maxs)\n",
    "    midrange = (maxs + mins) / 2\n",
    "    rng = maxs - mins\n",
    "    standardized = (train_x[:, :, :, :, band] - midrange) / (rng / 2)\n",
    "    train_x[:, :, :, :, band] = standardized\n",
    "    \n",
    "    min_all.append(mins)\n",
    "    max_all.append(maxs)\n",
    "    \n",
    "print(\"The data has been scaled to [{}, {}]\".format(np.min(train_x), np.max(train_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment training data\n",
    "\n",
    "Horizontal and vertical flips for 4x augmentation.\n",
    "\n",
    "**To do**\n",
    "*  Random guassian noise\n",
    "*  Brightness, contrast\n",
    "*  Region swaps (randomply position positive samples at different locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.load(\"../tile_data/processed/test_x_l2a_processed.npy\")\n",
    "test_y = np.load(\"../tile_data/processed/test_y_l2a_processed.npy\")\n",
    "\n",
    "test_x = np.delete(test_x, 14, -1)\n",
    "\n",
    "below_1 = [i for i, val in enumerate(test_x[:, :, :, :, :-2]) if np.min(val) < -1.67]\n",
    "above_1 = [i for i, val in enumerate(test_x[:, :, :, :, :-2]) if np.max(val) > 1.67]\n",
    "min_vals = [np.min(val) for i, val in enumerate(test_x[:, :, :, :, :-2]) if np.min(val) < -1.5]\n",
    "max_vals = [np.max(val) for i, val in enumerate(test_x[:, :, :, :, :-2]) if np.max(val) > 1.5]\n",
    "nans = [i for i, val in enumerate(test_x) if np.sum(np.isnan(val)) > 0]\n",
    "outliers = below_1 + above_1 + nans\n",
    "outliers = list(set(outliers))\n",
    "print(\"There are {} outliers\".format(len(outliers)))\n",
    "print(\"\\n\")\n",
    "print(min_vals, max_vals)\n",
    "        \n",
    "test_x = test_x[[x for x in range(0, len(test_x)) if x not in outliers]]\n",
    "test_y = test_y[[x for x in range(0, len(test_y)) if x not in outliers]]\n",
    "\n",
    "for sample in tnrange(0, len(test_x)):\n",
    "    filtered = median_filter(test_x[sample, 0, :, :, 10], size = 5)\n",
    "    filtered = np.reshape(filtered, (8, 2, 8, 2))\n",
    "    filtered = np.mean(filtered, axis = (1, 3))\n",
    "    filtered = resize(filtered, (16, 16), 0)\n",
    "    test_x[sample, :, :, :, 10] = np.stack([filtered] * 24)\n",
    "    \n",
    "for band in range(0, test_x.shape[-1]):\n",
    "    mins = min_all[band]\n",
    "    maxs = max_all[band]\n",
    "    test_x[:, :, :, :, band] = np.clip(test_x[:, :, :, :, band], mins, maxs)\n",
    "    midrange = (maxs + mins) / 2\n",
    "    rng = maxs - mins\n",
    "    standardized = (test_x[:, :, :, :, band] - midrange) / (rng / 2)\n",
    "    test_x[:, :, :, :, band] = standardized\n",
    "    \n",
    "    \n",
    "print(\"The data has been scaled to [{}, {}]\".format(np.min(test_x), np.max(test_x)))\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train and test characteristics:\")\n",
    "print(\"Train mean Y {}\".format(np.mean([np.sum(x) for x in test_y])))\n",
    "print(\"Test STD Y {}\".format(np.std([np.sum(x) for x in test_y])))\n",
    "#print(\"Train number with zero trees {}\".format(0.2*len([x for x in data_y[train_ids] if np.sum(x) == 0])))\n",
    "#print(\"Test number with zero trees {}\".format(0.8*len([x for x in data_y[test_ids] if np.sum(x) == 0])))\n",
    "print(\"Train mean NDVI\")\n",
    "print(\"Test mean NDVI\")\n",
    "#print(\"There are {} train and {} test samples\".format(len(train_ids), len(test_ids)))\n",
    "#print(\"There is {} overlap between train and test\".format(len([x for x in train_ids if x in test_ids])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equibatch creation\n",
    "\n",
    "The modelling approach uses equibatch sampling to ensure that there is a near constant standard deviation of the percent tree cover in the output labels for each batch. This helps ensure that the model performs equally well across gradients of tree cover, by mitigating the random possibility that many batches in a row near the end of sampling may be randomly biased towards a tree cover range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = np.sum(train_y, axis = (1, 2))\n",
    "percents = [np.percentile(sums, x) for x in range(3*10, 100, 10)]\n",
    "print(percents)\n",
    "print(\"There are {} zeros\".format(len(np.argwhere(sums == 0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [x for x in range(0, len(train_y))]\n",
    "\n",
    "def multiplot(matrices):\n",
    "    '''Plot multiple heatmaps with subplots\n",
    "    \n",
    "         Parameters:\n",
    "          matrices (list of arrays):\n",
    "\n",
    "         Returns:\n",
    "          None\n",
    "    '''\n",
    "    fig, axs = plt.subplots(ncols=4)\n",
    "    fig.set_size_inches(20, 4)\n",
    "    for i, matrix in enumerate(matrices):\n",
    "        sns.heatmap(data = matrix, ax = axs[i], vmin = 0, vmax = 0.9)\n",
    "        axs[i].set_xlabel(\"\")\n",
    "        axs[i].set_ylabel(\"\")\n",
    "        axs[i].set_yticks([])\n",
    "        axs[i].set_xticks([])\n",
    "\n",
    "def equibatch(train_ids, p = percents, lovasz = False):\n",
    "    '''Docstring\n",
    "    \n",
    "         Parameters:\n",
    "          train_ids (list):\n",
    "          p (list):\n",
    "          lovasz (bool):\n",
    "\n",
    "         Returns:\n",
    "          equibatches (list):\n",
    "    '''\n",
    "    percents = [7.0, 13.0, 21.0, 33.0, 50.0, 80.0, 130.0] # overall\n",
    "    np.random.shuffle(train_ids)\n",
    "    ix = train_ids\n",
    "    percs = [np.sum(x) for x in train_y[ix]]\n",
    "    ids0 = [x for x, z in zip(ix, percs) if z <= 2]\n",
    "    ids30 = [x for x, z in zip(ix, percs) if 2 < z < percents[0]]\n",
    "    ids40 = [x for x, z in zip(ix, percs) if percents[0] < z < percents[1]]\n",
    "    ids50 = [x for x, z in zip(ix, percs) if percents[1] < z < percents[2]]\n",
    "    ids60 = [x for x, z in zip(ix, percs) if percents[2] < z < percents[3]]\n",
    "    ids70 = [x for x, z in zip(ix, percs) if percents[3] < z < percents[4]]\n",
    "    ids80 = [x for x, z in zip(ix, percs) if percents[4] < z < percents[5]]\n",
    "    ids90 = [x for x, z in zip(ix, percs) if percents[5] < z < percents[6]]\n",
    "    ids100 = [x for x, z in zip(ix, percs) if percents[6] < z]\n",
    "    \n",
    "\n",
    "    new_batches = []\n",
    "    maxes = [len(ids0), len(ids30), len(ids40), len(ids50), len(ids60), len(ids70),\n",
    "             len(ids80), len(ids90), len(ids100)]\n",
    "    print(maxes)\n",
    "    cur_ids = [0] * len(maxes)\n",
    "    iter_len = len(train_ids)//(len(maxes))\n",
    "    for i in range(0, iter_len):\n",
    "        for i, val in enumerate(cur_ids):\n",
    "            if val > maxes[i] - 1:\n",
    "                cur_ids[i] = 0\n",
    "        if cur_ids[0] >= (maxes[0] - 2):\n",
    "            cur_ids[0] = 0\n",
    "        to_append = [ids0[cur_ids[0]],\n",
    "                    ids30[cur_ids[1]], ids40[cur_ids[2]],\n",
    "                    ids50[cur_ids[3]],\n",
    "                    ids60[cur_ids[4]], ids70[cur_ids[5]], ids80[cur_ids[6]],\n",
    "                    ids90[cur_ids[7]], ids100[cur_ids[8]]]\n",
    "        \n",
    "        np.random.shuffle(to_append)\n",
    "        new_batches.append(to_append)\n",
    "        cur_ids = [x + 1 for x in cur_ids]\n",
    "        \n",
    "    new_batches = [item for sublist in new_batches for item in sublist]\n",
    "    return new_batches\n",
    "\n",
    "batch = equibatch(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "f.set_size_inches(15, 6)\n",
    "sns.distplot(np.sum(train_y, axis = (1, 2)), bins = 100, kde = False, ax = ax1)\n",
    "ax1.set_title('Original distribution')\n",
    "ax2.set_title('Equibatch distribution')\n",
    "sns.distplot(np.sum(train_y[batch], axis = (1, 2)),\n",
    "             bins = 100, kde = False, ax = ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example equibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[4:8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[8:12]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[0:4]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight cross entropy by effective number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_pos = np.sum(train_y[batch])\n",
    "sum_neg = len(train_y[batch]) * 196 - sum_pos\n",
    "beta = (sum_pos + sum_neg - 1) / (sum_pos + sum_neg)\n",
    "print(\"Beta: {}\".format(beta))\n",
    "samples_per_cls = np.array([sum_neg, sum_pos])\n",
    "effective_num = 1.0 - np.power(beta, samples_per_cls)\n",
    "print(effective_num)\n",
    "weights = (1.0 - beta) / np.array(effective_num)\n",
    "weights = weights / np.sum(weights)\n",
    "print(\"Neg and pos weights: {}\".format(weights))\n",
    "weight = weights[1] / weights[0]\n",
    "print(weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline: The positive is: {}\".format(weights[0]))\n",
    "print(\"Baseline: The negative is: {}\".format(weights[1]))\n",
    "print(\"\\n\")\n",
    "print(\"Balanced: The positive is: {}\".format(weight*weights[0]))\n",
    "print(\"Balanced: The negative is: {}\".format(weights[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loss definition\n",
    "\n",
    "The current best loss is a combination of weighted binary cross entropy and per-image Lovasz-Softmax, with a loss schedule with the latter becoming more important each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight, mask = None, smooth = 0.025):\n",
    "    '''Calculates the weighted binary cross entropy loss between y_true and\n",
    "       y_pred with optional masking and smoothing for regularization\n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          y_pred (arr):\n",
    "          weight (float):\n",
    "          mask (arr):\n",
    "          smooth (float):\n",
    "\n",
    "         Returns:\n",
    "          loss (float):\n",
    "    '''\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    y_true = K.clip(y_true, smooth, 1. - smooth)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "        y_true,\n",
    "        logit_y_pred,\n",
    "        weight,\n",
    "    )\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = {'africaeast-test': [0, 34],\n",
    " 'africawest-test': [35, 96],\n",
    " 'cameroon-test': [97, 135],\n",
    " 'ethiopia-test': [136, 248],\n",
    " 'ghana-test': [249, 282],\n",
    " 'ghana-test-large': [283, 318],\n",
    " 'global-test': [319, 460],\n",
    " 'india-test': [461, 511],\n",
    " 'kenya-test': [512, 600],\n",
    " 'lac-north-test': [601, 641],\n",
    " 'lac-south-test': [642, 683], 'all': [0, 683]}\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    '''Calculates the dice score of ground truth and prediction arrays\n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          y_pred (arr):\n",
    "          \n",
    "         Returns:\n",
    "          metric (arr):\n",
    "    '''\n",
    "    numerator = 2 * np.sum(y_true * y_pred, axis=-1)\n",
    "    denominator = np.sum(y_true + y_pred, axis=-1)\n",
    "    return (numerator + 1) / (denominator + 1)\n",
    "\n",
    "def compute_f1_score_at_tolerance(true, pred, tolerance = 1):\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    tp = np.zeros_like(true)\n",
    "    fp = np.zeros_like(true)\n",
    "    fn = np.zeros_like(true)\n",
    "    \n",
    "    \n",
    "    for x in range(true.shape[0]):\n",
    "        for y in range(true.shape[1]):\n",
    "            min_x = np.max([0, x-1])\n",
    "            min_y = np.max([0, y-1])\n",
    "            max_y = np.min([true.shape[0], y+2])\n",
    "            max_x = np.min([true.shape[0], x+2])\n",
    "            if true[x, y] == 1:\n",
    "                if np.sum(pred[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    if pred[x, y] == 1:\n",
    "                        tp[x, y] = 1\n",
    "                else:\n",
    "                    fn[x, y] = 1\n",
    "            if pred[x, y] == 1:\n",
    "                if np.sum(true[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    if true[x, y] == 1:\n",
    "                        tp[x, y] = 1\n",
    "                else:\n",
    "                    fp[x, y] = 1                \n",
    "                \n",
    "    return np.sum(tp), np.sum(fp), np.sum(fn)\n",
    "\n",
    "def calculate_metrics(country, al = 0.4, canopy_thresh = 100):\n",
    "    '''Calculates the following metrics for an input country, based on\n",
    "       indexing of the country dictionary:\n",
    "       \n",
    "         - Loss\n",
    "         - F1\n",
    "         - Precision\n",
    "         - Recall\n",
    "         - Dice\n",
    "         - Mean surface distance\n",
    "         - Average error\n",
    "    \n",
    "         Parameters:\n",
    "          country (str):\n",
    "          al (float):\n",
    "          \n",
    "         Returns:\n",
    "          val_loss (float):\n",
    "          best_dice (float):\n",
    "          error (float):\n",
    "    '''\n",
    "    print(canopy_thresh)\n",
    "    start_idx = countries[country][0]\n",
    "    stop_idx = countries[country][1]\n",
    "    best_f1 = 0\n",
    "    best_dice = 0\n",
    "    best_thresh = 0\n",
    "    p = 0\n",
    "    r = 0\n",
    "    error = 0\n",
    "    d = 0\n",
    "    ys = []\n",
    "    vls = []\n",
    "    t_alls = []\n",
    "    test_ids = [x for x in range(len(test_x))]\n",
    "    for test_sample in test_ids[start_idx:stop_idx]:\n",
    "        if np.sum(test_y[test_sample]) < ((canopy_thresh/100) * 197) :\n",
    "            y, vl = sess.run([fm, test_loss], feed_dict={inp: test_x[test_sample].reshape(1, 16, 16, n_bands),\n",
    "                                              is_training: False,\n",
    "                                              clipping_params['rmax']: rmax_epoch,\n",
    "                                              clipping_params['rmin']: rmin_epoch,\n",
    "                                              clipping_params['dmax']: dmax_epoch,\n",
    "                                              labels: test_y[test_sample, :, :].reshape(1, 14, 14),\n",
    "                                              })\n",
    "            ys.append(y.reshape((14, 14)))\n",
    "            vls.append(vl)\n",
    "            t = test_y[test_sample].reshape((14, 14))\n",
    "            t_alls.append(t)\n",
    "\n",
    "            \n",
    "    for thresh in range(7, 13):\n",
    "        all_preds, all_trues = [], []\n",
    "        tps, fps, fns = [], [], []\n",
    "        abs_error, haus = [], []\n",
    "        trues, preds = [], []\n",
    "        dice_losses = []\n",
    "        val_loss = []\n",
    "        for sample in range(len(ys)):\n",
    "            pred = np.copy(ys[sample])\n",
    "            true = t_alls[sample]\n",
    "            vl = vls[sample]\n",
    "            pred[np.where(pred >= thresh*0.05)] = 1\n",
    "            pred[np.where(pred < thresh*0.05)] = 0\n",
    "            \n",
    "            true_s = np.sum(true[1:-1])\n",
    "            pred_s = np.sum(pred[1:-1])\n",
    "            abs_error.append(abs(pred_s - true_s))\n",
    "            tp, fp, fn = compute_f1_score_at_tolerance(true, pred)\n",
    "            tps.append(tp)\n",
    "            fps.append(fp)\n",
    "            fns.append(fn)\n",
    "            trues.append(true_s)\n",
    "            preds.append(pred_s)\n",
    "            all_trues.append(true.flatten())\n",
    "            all_preds.append(pred.flatten())\n",
    "            val_loss.append(np.mean(vl))\n",
    "            \n",
    "        oa_error = np.mean(abs_error)\n",
    "        precision = np.sum(tps) / (np.sum(tps) + np.sum(fps))\n",
    "        recall = np.sum(tps) / (np.sum(tps) + np.sum(fns))\n",
    "        f1 = 2*((precision* recall) / (precision + recall))\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            p = precision\n",
    "            r = recall\n",
    "            error = oa_error\n",
    "            best_thresh = thresh*0.05\n",
    "            preds_return = all_preds\n",
    "            trues_return = all_trues\n",
    "    print(\"{}: Val loss: {} Thresh: {} F1: {}\"\n",
    "          \" R: {} P: {} Error: {}\".format(country, \n",
    "                                         np.around(np.mean(val_loss), 3),\n",
    "                                         np.around(best_thresh, 2),\n",
    "                                         np.around(best_f1, 3), np.around(p, 3), np.around(r, 3), \n",
    "                                         np.around(error, 3)))\n",
    "    return np.mean(val_loss), best_f1, error, preds_return, trues_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_batch(batch_ids, batch_size):\n",
    "    '''Performs random flips and rotations of the X and Y\n",
    "       data for a total of 4 x augmentation\n",
    "    \n",
    "         Parameters:\n",
    "          batch_ids (list):\n",
    "          batch_size (int):\n",
    "          \n",
    "         Returns:\n",
    "          x_batch (arr):\n",
    "          y_batch (arr):\n",
    "    '''\n",
    "    x = train_x[batch_ids, :, :, :]\n",
    "    y = train_y[batch_ids, :, :]\n",
    "    x_batch = np.zeros_like(x)\n",
    "    y_batch = np.zeros_like(y)\n",
    "    flips = np.random.choice(np.array([0, 1, 2, 3]), batch_size, replace = True)\n",
    "    for i in range(x.shape[0]):\n",
    "        current_flip = flips[i]\n",
    "        if current_flip == 0:\n",
    "            x_batch[i, :, :, :] = x[i]\n",
    "            y_batch[i, :, :] = y[i]\n",
    "        if current_flip == 1:\n",
    "            x_batch[i, :, :, :] = np.flip(x[i], 1)\n",
    "            y_batch[i, :, :] = np.flip(y[i], 0)\n",
    "        if current_flip == 2:\n",
    "            x_batch[i, :, :, :] = np.flip(x[i], [2, 1])\n",
    "            y_batch[i, :, :] = np.flip(y[i], [1, 0])\n",
    "        if current_flip == 3:\n",
    "            x_batch[i, :, :, :] = np.flip(x[i], 2)\n",
    "            y_batch[i, :, :] = np.flip(y[i], 1)\n",
    "\n",
    "    y_batch = y_batch.reshape((batch_size, 14, 14))\n",
    "    return x_batch, y_batch\n",
    "\n",
    "x_batch_test, y_batch_test = augment_batch([0, 1], 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRESH_START = True\n",
    "best_val = 0.2\n",
    "\n",
    "START_EPOCH = 1\n",
    "END_EPOCH = 150\n",
    "LEARNING_RATE = 3e-3\n",
    "test_ids = [x for x in range(0, len(test_x))]\n",
    "\n",
    "print(\"Starting model with: \\n {} zone out \\n {} l2 \\n {} initial LR \\n {} final LR \\n {} parameters\"\n",
    "     .format(ZONE_OUT_PROB, L2_REG, INITIAL_LR, FINAL_LR, total_parameters))\n",
    "\n",
    "if not FRESH_START:\n",
    "    print(\"Resuming training with a best validation score of {}\".format(best_val))\n",
    "    \n",
    "if FRESH_START:\n",
    "    print(\"Restarting training from scratch on {} \"\n",
    "          \"train and {} test samples, total {}\".format(len(train_ids), len(test_ids), len(train_ids)/4))\n",
    "\n",
    "    # current best 1e-4 5e-3\n",
    "    optimizer = AdaBoundOptimizer(1e-3, 1e-1)\n",
    "    train_loss = weighted_bce_loss(tf.reshape(labels, (-1, 14, 14, 1)), fm, weight = weight, smooth = 0.)\n",
    "    #l2_loss = tf.losses.get_regularization_l05oss()\n",
    "    #train_loss += l2_loss\n",
    "\n",
    "    ft_optimizer = tf.train.GradientDescentOptimizer(ft_lr)\n",
    "    \n",
    "    test_loss = weighted_bce_loss(tf.reshape(labels, (-1, 14, 14, 1)), fm, weight = 2.36, smooth = 0.)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(train_loss)   \n",
    "        ft_op = ft_optimizer.minimize(train_loss)\n",
    "        \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver(max_to_keep = 75)\n",
    "    \n",
    "print(\"The graph has been finalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should be run to do fine-tuning, if commented - train from scratch\n",
    "path = \"../models/unet-resnet/new/34-65-9/\"\n",
    "new_saver = tf.train.import_meta_graph(path + 'model.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "f1s = []\n",
    "errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ((c1r1, c1r2), (c2r1, c2r2)) = plt.subplots(2, 2, sharey=False)\n",
    "f.set_size_inches(15, 12)\n",
    "\n",
    "c1r1.set_title(\"Train losses\")\n",
    "sns.scatterplot(y = np.array(train_losses[:]),\n",
    "                x = np.array([x for x in range(len(train_losses[:]))]),\n",
    "               ax = c1r1)\n",
    "\n",
    "c1r2.set_title(\"F1 score\")\n",
    "sns.scatterplot(y = np.array(dices[:]),\n",
    "                x = np.array([x for x in range(len(f1s[:]))]),\n",
    "               ax = c1r2)\n",
    "\n",
    "c2r1.set_title(\"Test losses\")\n",
    "sns.scatterplot(y = np.array(test_losses[:]),\n",
    "                x = np.array([x for x in range(len(test_losses[:]))]),\n",
    "               ax = c2r1)\n",
    "\n",
    "c2r2.set_title(\"Errors\")\n",
    "sns.scatterplot(y = np.array(errors[:]),\n",
    "                x = np.array([x for x in range(len(errors[:]))]),\n",
    "               ax = c2r2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the equibatch is working with the augmentation\n",
    "randomize = equibatch(train_ids)\n",
    "sum_no_equibatch = []\n",
    "sum_equibatch = []\n",
    "for k in tnrange(int(len(randomize) // 20)):\n",
    "    rand = [x for x in range(len(randomize))]\n",
    "    batch_ids = rand[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "    _, y_batch = augment_batch(batch_ids, BATCH_SIZE)\n",
    "    sum_no_equibatch.append(np.sum(y_batch))\n",
    "    \n",
    "for k in tnrange(int(len(randomize) // 20)):\n",
    "    batch_ids = randomize[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "    _, y_batch = augment_batch(batch_ids, BATCH_SIZE)\n",
    "    sum_equibatch.append(np.sum(y_batch))\n",
    "    \n",
    "print(\"No equibatch SD: {}\".format(np.std(np.array(sum_no_equibatch))))\n",
    "print(\"Equibatch SD: {}\".format(np.std(np.array(sum_equibatch))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.mean(train_x, axis = 1)\n",
    "test_x = np.mean(test_x, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "best_val = 0.5\n",
    "fine_tune = False\n",
    "countries['all'] = [0, len(test_x)]\n",
    "ft_epochs = 0\n",
    "ft_learning_rate = 1e-2\n",
    "\n",
    "train_ids = [x for x in range(len(train_y))]\n",
    "for i in range(34, 100):\n",
    "    if fine_tune == True:\n",
    "        op = ft_op\n",
    "        ft_epochs += 1\n",
    "        if ft_epochs % 15 == 0:\n",
    "            ft_learning_rate /= 2\n",
    "        print(\"FINE TUNING WITH {} LR\".format(ft_learning_rate))\n",
    "        \n",
    "    else:\n",
    "        op = op = train_op\n",
    "    randomize = equibatch(train_ids)\n",
    "    print(\"starting epoch {}, drop: {}\".format(i, np.max((1. - (i*0.003), 0.75))))\n",
    "    \n",
    "    loss = train_loss\n",
    "    BATCH_SIZE = 20\n",
    "    test_ids = [x for x in range(0, len(test_x))]\n",
    "    losses = []\n",
    "    \n",
    "    for k in tnrange(int(len(randomize) // BATCH_SIZE)):\n",
    "        rmax_epoch, dmax_epoch, rmin_epoch = calc_renorm_params(i, len(train_y), 20, k)\n",
    "        if k % 40 == 0:\n",
    "            sleep(1)\n",
    "        batch_ids = randomize[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "        x_batch, y_batch = augment_batch(batch_ids, BATCH_SIZE)\n",
    "        opt, tr = sess.run([op, loss],\n",
    "                          feed_dict={inp: x_batch,\n",
    "                                     labels: y_batch,\n",
    "                                     is_training: True,\n",
    "                                     clipping_params['rmax']: rmax_epoch,\n",
    "                                     clipping_params['rmin']: rmin_epoch,\n",
    "                                     clipping_params['dmax']: dmax_epoch,\n",
    "                                     keep_rate: np.max((1. - (i*0.003), 0.9)),\n",
    "                                     ft_lr: ft_learning_rate,\n",
    "                                     })\n",
    "        losses.append(tr)\n",
    "    \n",
    "    print(\"Epoch {}: Loss {}\".format(i, np.around(np.mean(losses[:-1]), 3)))\n",
    "    #_, _, _, _ = calculate_metrics(\"global-test\", al = al)\n",
    "    val_loss, f1, error, _, _ = calculate_metrics('all')\n",
    "    train_losses.append(np.mean(losses[:-1]))\n",
    "    test_losses.append(val_loss)\n",
    "    f1s.append(f1)\n",
    "    errors.append(error)\n",
    "    if f1 > (best_val - 0.005):\n",
    "        print(\"Saving model with {}\".format(dice))\n",
    "        os.mkdir(\"../models/unet-resnet/new/{}-{}-{}/\".format(str(i), str(dice*100)[:2], str(dice*100)[3]))\n",
    "        save_path = saver.save(sess, \"../models/unet-resnet/new/{}-{}-{}/model\".format(str(i), str(dice*100)[:2], str(dice*100)[3]))\n",
    "        if f1 > best_val:\n",
    "            best_val = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, f1, error, _, _ = calculate_metrics('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation and sanity checks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries['all'] = [0, len(test_x)]\n",
    "_, _, error, _, preds, trues = calculate_metrics('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = np.concatenate(preds).flatten()\n",
    "trues2 = np.concatenate(trues).flatten()\n",
    "tp = preds2 * trues2\n",
    "fn = [1 if x > y else 0 for (x, y) in zip(trues2, preds2)]\n",
    "fp = [1 if y > x else 0 for (x, y) in zip(trues2, preds2)]\n",
    "tn = (len(test_y) * 196) - np.sum(tp) - np.sum(fn) - np.sum(fp)\n",
    "print(\"TP {}, FN {}, FP {}, TN {}\".format(np.sum(tp), sum(fn), np.sum(fp), tn))\n",
    "print(len(preds2))\n",
    "\n",
    "tps = []\n",
    "fns = []\n",
    "fps = []\n",
    "for i in range(0, len(test_y)*196, 196):\n",
    "    tps.append(np.sum(tp[i:i+196]))\n",
    "    fns.append(np.sum(fn[i:i+196]))\n",
    "    fps.append(np.sum(fp[i:i+196]))\n",
    "    \n",
    "    \n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "true_canopy = []\n",
    "error_canopy = []\n",
    "pred_canopy = []\n",
    "f1_hard = []\n",
    "tp_softs = []\n",
    "fp_softs = []\n",
    "fn_softs = []\n",
    "for i in range(len(trues)):\n",
    "    true_canopy.append(np.sum(trues[i]) / 1.96)\n",
    "    error_canopy.append(abs(np.sum(preds[i]) - np.sum(trues[i])) / 1.96)\n",
    "    pred_canopy.append(np.sum(preds[i]) / 1.96)\n",
    "    f1_hard.append(f1_score(trues[i], preds[i]))\n",
    "    tp_soft, fp_soft, fn_soft = compute_f1_score_at_tolerance(np.array(trues[i].reshape((14, 14))),\n",
    "                                                 np.array(preds[i].reshape((14, 14))))\n",
    "    tp_softs.append(tp_soft)\n",
    "    fp_softs.append(fp_soft)\n",
    "    fn_softs.append(fn_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame({'true': true_canopy,\n",
    "                        'pred': pred_canopy,\n",
    "                        'f1_hard': f1_hard,\n",
    "                        'error': error_canopy,\n",
    "                        'tp': tps,\n",
    "                        'fp': fps,\n",
    "                        'fn': fns,\n",
    "                        'tp_soft': tp_softs,\n",
    "                        'fp_soft': fp_softs,\n",
    "                        'fn_soft': fn_softs,\n",
    "                       })\n",
    "\n",
    "res = map(lambda x: int(math.floor(np.min([x, 90]) / 10.0)) * 10, true_canopy)\n",
    "res = [x for x in res]\n",
    "metrics['group'] = res\n",
    "metrics['model'] = 'U-net'\n",
    "\n",
    "metrics.to_csv(\"../data/metrics/unet-sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_precisions = metrics.groupby('group').apply(lambda x: (np.sum(x.tp) / np.sum(x.tp + x.fp)))\n",
    "hard_recalls = metrics.groupby('group').apply(lambda x: (np.sum(x.tp) / np.sum(x.tp + x.fn)))\n",
    "errors = metrics.groupby('group').apply(lambda x: np.mean(x.error))\n",
    "hard_f1 = 2 *  ((hard_precisions * hard_recalls) / (hard_precisions + hard_recalls))\n",
    "\n",
    "precisions = metrics.groupby('group').apply(lambda x: (np.sum(x.tp_soft) / np.sum(x.tp_soft + x.fp_soft)))\n",
    "recalls = metrics.groupby('group').apply(lambda x: (np.sum(x.tp_soft) / np.sum(x.tp_soft + x.fn_soft)))\n",
    "soft_f1 = 2 *  ((precisions * recalls) / (precisions + recalls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metrics = pd.DataFrame({'group': [x for x in range(0, 100, 10)],\n",
    "                            'hard_rec': hard_recalls,\n",
    "                            'soft_rec': recalls,\n",
    "                            'hard_prec': hard_precisions,\n",
    "                            'soft_prec': precisions,\n",
    "                            'hard_f1': hard_f1,\n",
    "                            'soft_f1': soft_f1,\n",
    "                            'error': errors,\n",
    "                            'model': 'U-net'\n",
    "                           })\n",
    "\n",
    "new_metrics.to_csv(\"../data/metrics/unet.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model results plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 400\n",
    "\n",
    "test_ids = [x for x in range(0, len(test_x))]\n",
    "\n",
    "def multiplot(matrices, nrows = 2, ncols = 4):\n",
    "    '''Docstring\n",
    "    \n",
    "         Parameters:\n",
    "          matrices (list):\n",
    "          nrows (int):\n",
    "          \n",
    "         Returns:\n",
    "          None\n",
    "    '''\n",
    "    fig, axs = plt.subplots(ncols=4, nrows = nrows)\n",
    "    fig.set_size_inches(20, 4*nrows)\n",
    "    to_iter = [[x for x in range(i, i + ncols + 1)] for i in range(0, nrows*ncols, ncols)]\n",
    "    for r in range(1, nrows + 1):\n",
    "        min_i = min(to_iter[r-1])\n",
    "        max_i = max(to_iter[r-1])\n",
    "        for i, matrix in enumerate(matrices[min_i:max_i]):\n",
    "            sns.heatmap(data = matrix, ax = axs[r - 1, i], vmin = 0, vmax = 0.9)\n",
    "            axs[r - 1, i].set_xlabel(\"\")\n",
    "            axs[r - 1, i].set_ylabel(\"\")\n",
    "            axs[r - 1, i].set_yticks([])\n",
    "            axs[r - 1, i].set_xticks([])\n",
    "    plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_alignment(true, pred, wsize = 3, difference = 0.2):\n",
    "    '''Docstring\n",
    "    \n",
    "         Parameters:\n",
    "          true (arr):\n",
    "          pred (arr):\n",
    "          wsize (int):\n",
    "          difference (float):\n",
    "          \n",
    "         Returns:\n",
    "          None\n",
    "    '''\n",
    "    n_single_trees = 0\n",
    "    for x in range(1, true.shape[0] - 1, 1):\n",
    "        for y in range(1, true.shape[1] - 1, 1):\n",
    "            wind_true = true[x-1:x+2, y-1:y+2]\n",
    "            wind_pred = pred[x-1:x+2, y-1:y+2]\n",
    "            if wind_true[1, 1] == 1:\n",
    "                if np.sum(wind_true) == 1:\n",
    "                    n_single_trees += 1\n",
    "                    pred_place = np.argmax(wind_pred.flatten())\n",
    "                    diff = wind_pred.flatten()[pred_place] - wind_pred.flatten()[4]\n",
    "                    if pred_place != 4:\n",
    "                        if diff > difference:\n",
    "                            x_lv = pred_place // 3\n",
    "                            y_lv = pred_place % 3\n",
    "                            print(x_lv, y_lv)\n",
    "                            proposed = wind_true[x_lv - 1:x_lv+2, y_lv-1:y_lv+2]\n",
    "                            if np.sum(proposed) == 0:\n",
    "                                print(\"There is a missed position at {} x, {} y: {}\".format(x, y, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(start/len(test_ids))\n",
    "test_ids = sorted(test_ids)\n",
    "matrix_ids = [test_ids[start], test_ids[start + 1], test_ids[start + 2], test_ids[start + 3],\n",
    "              test_ids[start + 4], test_ids[start + 5], test_ids[start + 6], test_ids[start + 7]]\n",
    "preds = []\n",
    "trues = []\n",
    "print(matrix_ids)\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    y = sess.run([fm], feed_dict={inp: test_x[idx].reshape(1, IMAGE_SIZE, IMAGE_SIZE, n_bands),\n",
    "                                  length: test_lengths[idx].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  clipping_params['rmax']: rmax_epoch,\n",
    "                                  clipping_params['rmin']: rmin_epoch,\n",
    "                                  clipping_params['dmax']: dmax_epoch,\n",
    "                                  })\n",
    "    y = np.array(y).reshape(14, 14)\n",
    "    preds.append(y)\n",
    "    y2 = np.copy(y)\n",
    "    #y[np.where(y > 0.4)] = 1.0\n",
    "    #y[np.where(y < 0.4)] = 0.\n",
    "    #y[np.where(y >= 0.4)] = 1.\n",
    "    #dc = np.around(dice_loss(test_y[idx].flatten(), y2.flatten()), 3)\n",
    "    true = test_y[idx].reshape(14, 14)\n",
    "    #y[np.where(y > 0.4)] = 1.\n",
    "    identify_alignment(true, y)\n",
    "    #f1 = f1_score(true.flatten(), y.flatten())\n",
    "    #print(i, f1, (1-f1)*np.sum(true))\n",
    "    #if np.sum(true[1:-1]) > 0 and np.sum(y[1:-1]) > 0:\n",
    "    #    print(i, assd(true[1:-1], y[1:-1]))\n",
    "    #print(i, np.sum(abs(true - y)))\n",
    "    trues.append(true)\n",
    "    \n",
    "    \n",
    "\"\"\n",
    "\n",
    "\n",
    "to_plot = trues[0:4] + preds[0:4] + trues[4:] + preds[4:]\n",
    "\n",
    "multiplot(to_plot, nrows = 4, ncols = 4)\n",
    "# 448\"109, \"\"\n",
    "start = start + 8 \n",
    "# 50, 64, 66, 83, 94\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [x for x in range(train_x.shape[0])]\n",
    "train_ids = sorted(train_ids)\n",
    "start = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atrix_ids = random.sample(755, 4)\n",
    "matrix_ids = [train_ids[start], train_ids[start + 1], train_ids[start + 2], train_ids[start + 3], train_ids[start + 4],\n",
    "             train_ids[start + 5], train_ids[start + 6], train_ids[start + 7], train_ids[start + 8]]\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "print(start//4)\n",
    "print(matrix_ids)\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    y = sess.run([fm], feed_dict={inp: train_x[idx].reshape(1, IMAGE_SIZE, IMAGE_SIZE, n_bands),\n",
    "                                  length: train_l[idx].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  clipping_params['rmax']: rmax_epoch,\n",
    "                                  clipping_params['rmin']: rmin_epoch,\n",
    "                                  clipping_params['dmax']: dmax_epoch,\n",
    "                                  })\n",
    "    y = np.array(y).reshape(14, 14)\n",
    "    #y[np.where(y > 0.3)] = 0.85\n",
    "    preds.append(y)\n",
    "    true = train_y[idx].reshape(14, 14)\n",
    "    #identify_alignment(true, y)\n",
    "    trues.append(true)\n",
    "    \n",
    "start += 8\n",
    "\n",
    "to_plot = trues[0:4] + preds[0:4] + trues[5:] + preds[5:]\n",
    "multiplot(to_plot, nrows = 4, ncols = 4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the top 10% of training and testing samples by loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ls = []\n",
    "for i in range(len(test_x)):\n",
    "    #if i % 4 == 0:\n",
    "    print(i)\n",
    "    y, losses = sess.run([fm, train_loss], feed_dict={inp: test_x[i].reshape((1, 24, 16, 16, n_bands)),\n",
    "                                         length: test_lengths[i].reshape((-1, 1)),\n",
    "                                         labels: test_y[i].reshape((1, 14, 14)),\n",
    "                                         is_training: False,\n",
    "                                         keep_rate: 0.8,\n",
    "                                         #keep_rate: np.max((0.97 - (i*0.01), 0.85)),\n",
    "                                         alpha: 0.5,\n",
    "                                         ft_lr: 0.004,\n",
    "                                         })\n",
    "    y = np.array(y).reshape(14, 14)\n",
    "    y[np.where(y > 0.4)] = 1.0\n",
    "    y[np.where(y < 0.4)] = 0.\n",
    "    preds.append(y)\n",
    "    true = test_y[i].reshape(14, 14)\n",
    "    print(i, np.sum(y) - np.sum(true))\n",
    "    identify_alignment(true, y, difference = 0.4)\n",
    "    tr_ls.append(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_id = np.argwhere(np.array(tr_ls) > np.percentile(tr_ls, 90))\n",
    "to_id.flatten()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
