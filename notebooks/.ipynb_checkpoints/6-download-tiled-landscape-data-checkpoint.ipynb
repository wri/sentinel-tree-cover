{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants\n",
    "from s2cloudless import S2PixelCloudDetector, CloudMaskRequest\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "from sentinelhub import DataSource\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import splu\n",
    "\n",
    "\n",
    "from sentinelhub import CustomUrlParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "%run ../src/slope.py\n",
    "%run ../src/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSG = CRS.WGS84\n",
    "GRID_SIZE_X = 1\n",
    "GRID_SIZE_Y = 1\n",
    "\n",
    "IMAGE_X = 14*GRID_SIZE_X\n",
    "IMAGE_Y = 14*GRID_SIZE_Y\n",
    "\n",
    "TEST_X = 5\n",
    "TEST_Y = 5\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "    key = (yaml.safe_load(stream))\n",
    "    API_KEY = key['key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCoords(xy, src='', targ=''):\n",
    "\n",
    "    srcproj = osr.SpatialReference()\n",
    "    srcproj.ImportFromEPSG(src)\n",
    "    targproj = osr.SpatialReference()\n",
    "    if isinstance(targ, str):\n",
    "        targproj.ImportFromProj4(targ)\n",
    "    else:\n",
    "        targproj.ImportFromEPSG(targ)\n",
    "    transform = osr.CoordinateTransformation(srcproj, targproj)\n",
    "\n",
    "    pt = ogr.Geometry(ogr.wkbPoint)\n",
    "    pt.AddPoint(xy[0], xy[1])\n",
    "    pt.Transform(transform)\n",
    "    return([pt.GetX(), pt.GetY()])\n",
    "\n",
    "def bounding_box(point, x_offset_max = 140, y_offset_max = 140, expansion = 10):\n",
    "    # LONG, LAT FOR SOME REASON\n",
    "    tl = point\n",
    "    \n",
    "    if 48 <= tl[0] <= 54:\n",
    "        epsg = 32639 if tl[1] > 0 else 32739\n",
    "    if 42 <= tl[0] <= 48:\n",
    "        epsg = 32638 if tl[1] > 0 else 32738\n",
    "    if 36 <= tl[0] <= 42:\n",
    "        epsg = 32637 if tl[1] > 0 else 32737\n",
    "    if 30 <= tl[0] <= 36:\n",
    "        epsg = 32636 if tl[1] > 0 else 32736\n",
    "    if 24 <= tl[0] <= 30:\n",
    "        epsg = 32635 if tl[1] > 0 else 32735\n",
    "    if 18 <= tl[0] <= 24:\n",
    "        epsg = 32634 if tl[1] > 0 else 32734\n",
    "\n",
    "    tl = convertCoords(tl, 4326, epsg)\n",
    "    \n",
    "    br = (tl[0], tl[1])\n",
    "    tl = ((tl[0] + (x_offset_max)), (tl[1] + (y_offset_max )))\n",
    "    distance1 = tl[0] - br[0]\n",
    "    distance2 = tl[1] - br[1]\n",
    "    #EXPANSION = (expansion - np.mean([distance1, distance2]))/2 # should this be 155 or 160?\n",
    "    \n",
    "    br = [a - expansion for a in br]\n",
    "    tl = [a + expansion for a in tl]\n",
    "    \n",
    "    after = [b - a for a,b in zip(br, tl)]\n",
    "    #br = (br[0] + 20, br[1] + 20)\n",
    "    #tl = (tl[0] - 20, tl[1] - 20)\n",
    "    br = convertCoords(br, epsg, 4326)\n",
    "    tl = convertCoords(tl, epsg, 4326)\n",
    "    \n",
    "    min_x = tl[0] # original X offset - 10 meters\n",
    "    max_x = br[0] # original X offset + 10*GRID_SIZE meters\n",
    "    \n",
    "    min_y = tl[1] # original Y offset - 10 meters\n",
    "    max_y = br[1] # original Y offset + 10 meters + 140 meters\n",
    "    # (min_x, min_y), (max_x, max_y)\n",
    "    # (bl, tr)\n",
    "    return [(min_x, min_y), (max_x, max_y)]\n",
    " \n",
    "cloud_detector = S2PixelCloudDetector(threshold=0.4, average_over=4, dilation_size=2)\n",
    "\n",
    "def identify_clouds(bbox, epsg = EPSG, time = ('2018-01-01', '2018-12-31')):\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        cloud_request = WmsRequest(\n",
    "            layer='CLOUD_DETECTION',\n",
    "            bbox=box,\n",
    "            time=time,\n",
    "            width=72,\n",
    "            height=72,\n",
    "            image_format = MimeType.TIFF_d32f,\n",
    "            maxcc=0.5,\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "            time_difference=datetime.timedelta(hours=24),\n",
    "        )\n",
    "        \n",
    "        cloud_img = cloud_request.get_data()\n",
    "        cloud_probs = cloud_detector.get_cloud_probability_maps(np.array(cloud_img))\n",
    "        means = np.mean(cloud_probs, (1, 2))\n",
    "        cloud_steps = [i for i, val in enumerate(means) if val > 0.20]\n",
    "        return cloud_steps, means, cloud_probs\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "    \n",
    "    \n",
    "def download_dem(bbox, epsg = EPSG):\n",
    "    #bbox = modify_bbox(bbox, expansion = 10)\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM',\n",
    "                         bbox=box,\n",
    "                         width=74,\n",
    "                         height=74,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format=MimeType.TIFF_d32f,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    dem_image = calcSlope(dem_image.reshape((1, 74, 74)),\n",
    "                  np.full((74, 74), 10), np.full((74, 74), 10), zScale = 1, minSlope = 0.02)\n",
    "    dem_image = dem_image.reshape((74,74, 1))\n",
    "    dem_image = dem_image[1:73, 1:73, :]\n",
    "    return dem_image #/ np.max(dem_image)\n",
    "\n",
    "\n",
    "    \n",
    "def download_tiles(bbox, clean_steps, epsg = EPSG, time = ('2018-01-01', '2018-12-31')):\n",
    "    try:\n",
    "        clean_steps = np.argwhere(clean_steps <= 0.2)\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WmsRequest(\n",
    "                layer='ALL_BANDS_NDVI',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                width=72,\n",
    "                height=72,\n",
    "                image_format = MimeType.TIFF_d32f,\n",
    "                maxcc=0.50,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=24),\n",
    "            )\n",
    "        img_bands = image_request.get_data()\n",
    "        img_bands = np.array(img_bands)\n",
    "        print(\"There are {}/{} clean steps\".format(len(clean_steps), len(img_bands)))\n",
    "        num_broken_steps = 0\n",
    "        for date in range(img_bands.shape[0]):\n",
    "            if date in clean_steps:\n",
    "                for band in range(10):\n",
    "                    uniques = [len(np.unique(img_bands[i, :, :, band])) for i in range(img_bands.shape[0])]\n",
    "                    maxs = np.max(img_bands[date, :, :, band])\n",
    "                    mins = np.min(img_bands[date, :, :, band])\n",
    "                    if maxs >= 1.0 or mins <= 0.0:\n",
    "                        num_broken_steps += 1\n",
    "                        before, after = calculate_proximal_steps(uniques, date, clean_steps)\n",
    "                        before = img_bands[date + int(before), :, :, band]\n",
    "                        after = img_bands[date + int(after), :, :, band]\n",
    "                        img_bands[date, :, :, band] = (before + after) / 2\n",
    "                    if len(np.unique(img_bands[date, :, :, band])) <= 3:\n",
    "                        num_broken_steps += 1\n",
    "                        before, after = calculate_proximal_steps(uniques, date, clean_steps)\n",
    "                        before = img_bands[date + int(before), :, :, band]\n",
    "                        after = img_bands[date + int(after), :, :, band]\n",
    "                        img_bands[date, :, :, band] = (before + after) / 2\n",
    "        print(\"{} broken normal steps\".format(num_broken_steps))\n",
    "        return img_bands, image_request\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "        \n",
    "    \n",
    "def calculate_and_save_best_images(cloud_steps, img_bands, image_request, means):\n",
    "    # Identify the date of the imagery\n",
    "    image_dates = []\n",
    "    for date in image_request.get_dates():\n",
    "        image_dates.append(date.month*30 + date.day)\n",
    "\n",
    "    biweekly_dates = [day for day in range(0, 360, 15)] # ideal imagery dates are every 15 days\n",
    "    \n",
    "    # Identify the dates where there is < 20% cloud cover\n",
    "    satisfactory_ids = list(np.argwhere(np.array(means) < 0.2).reshape(-1, )) \n",
    "    satisfactory_dates = [value for idx, value in enumerate(image_dates) if idx in satisfactory_ids]\n",
    "    \n",
    "    \n",
    "    selected_images = {}\n",
    "    for i in biweekly_dates:\n",
    "        distances = [abs(date - i) for date in satisfactory_dates]\n",
    "        closest = np.min(distances)\n",
    "        closest_id = np.argmin(distances)\n",
    "        # If there is imagery within 8 days, select it\n",
    "        if closest < 8:\n",
    "            date = satisfactory_dates[closest_id]\n",
    "            image_idx = int(np.argwhere(np.array(image_dates) == date)[0])\n",
    "            selected_images[i] = {'image_date': [date], 'image_ratio': [1], 'image_idx': [image_idx]}\n",
    "        # If there is not imagery within 8 days, look for the closest above and below imagery\n",
    "        else:\n",
    "            distances = np.array([(date - i) for date in satisfactory_dates])\n",
    "            # Number of days above and below the selected date of the nearest clean imagery\n",
    "            above = distances[np.where(distances < 0, distances, -np.inf).argmax()]\n",
    "            below = distances[np.where(distances > 0, distances, np.inf).argmin()]\n",
    "            if above != below:\n",
    "                below_ratio = above / (above - below)\n",
    "                above_ratio = 1 - below_ratio\n",
    "            else:\n",
    "                above_ratio = below_ratio = 0.5\n",
    "                \n",
    "            # Extract the image date and imagery index for the above and below values\n",
    "            above_date = i + above\n",
    "            above_image_idx = int(np.argwhere(np.array(image_dates) == above_date)[0])\n",
    "            \n",
    "            below_date = i + below\n",
    "            below_image_idx = int(np.argwhere(np.array(image_dates) == below_date)[0])\n",
    "            \n",
    "            selected_images[i] = {'image_date': [above_date, below_date], 'image_ratio': [above_ratio, below_ratio],\n",
    "                                 'image_idx': [above_image_idx, below_image_idx]}\n",
    "                            \n",
    "    #for i in selected_images.keys():\n",
    "    #    print(i, selected_images[i])\n",
    "        \n",
    "    keep_steps = []\n",
    "    for i in selected_images.keys():\n",
    "        info = selected_images[i]\n",
    "        if len(info['image_idx']) == 1:\n",
    "            step = img_bands[info['image_idx'][0]]\n",
    "        if len(info['image_idx']) == 2:\n",
    "            step1 = img_bands[info['image_idx'][0]] * info['image_ratio'][0]\n",
    "            step2 = img_bands[info['image_idx'][1]] * info['image_ratio'][1]\n",
    "            step = step1 + step2\n",
    "        keep_steps.append(step)\n",
    "        \n",
    "    keep_steps = np.stack(keep_steps)\n",
    "    return keep_steps\n",
    "\n",
    "def calc_best(tiles, cloud_probs, request, offset_x, offset_y):\n",
    "    c_probs = cloud_probs[:, offset_x:offset_x+16, offset_y:offset_y+16]\n",
    "    images = np.stack(tiles)[:, offset_x:offset_x+16, offset_y:offset_y+16]\n",
    "    means = np.mean(c_probs, (1, 2))\n",
    "    cloud_steps = [i for i, val in enumerate(means) if val > 0.20]\n",
    "    best = calculate_and_save_best_images(cloud_steps, images, request, means)\n",
    "    return best\n",
    "\n",
    "def speyediff(N, d, format = 'csc'):\n",
    "    shape = (N-d, N)\n",
    "    diagonals = np.zeros(2*d + 1)\n",
    "    diagonals[d] = 1.\n",
    "    for i in range(d):\n",
    "        diff = diagonals[:-1] - diagonals[1:]\n",
    "        diagonals = diff\n",
    "    offsets = np.arange(d+1)\n",
    "    spmat = sparse.diags(diagonals, offsets, shape, format = format)\n",
    "    return spmat\n",
    "\n",
    "def smooth(y, lmbd, d = 2):\n",
    "    m = len(y)\n",
    "    E = sparse.eye(m, format = 'csc')\n",
    "    D = speyediff(m, d, format = 'csc')\n",
    "    coefmat = E + lmbd * D.conj().T.dot(D)\n",
    "    z = splu(coefmat).solve(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(38.306228099454614, 13.667784864997554), (38.17721999999999, 13.540809999999999)]\n"
     ]
    }
   ],
   "source": [
    "coords = (13.540810, 38.177220) # tigray\n",
    "#coords = (-1.726374, 37.440204) # makueni\n",
    "#coords = (-1.515869, 29.952997) # rwanda\n",
    "coords = (coords[1], coords[0])\n",
    "OUTPUT_FOLDER = '../data/tigray_test/'\n",
    "#13.567962754335872\n",
    "\n",
    "borders = bounding_box(coords, 20*700, 20*700, expansion = 0)\n",
    "print(borders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this needs the first coordinate to not be [coords] for different y steps\n",
    "\n",
    "def calculate_offset_coords(coords, x_step, y_step, number):\n",
    "    offset_coords = []\n",
    "    y_coord = bounding_box(coords, x_step, y_step, expansion = 0)\n",
    "    y_coord = y_coord[0][1]\n",
    "    for i in range(number):\n",
    "        bbx = bounding_box(coords, (i+1)*x_step, y_step, expansion = 10)\n",
    "        coord_x = bbx[0][0]\n",
    "        coord_y = y_coord\n",
    "        offset_coords.append((coord_x, coord_y))\n",
    "    coords = [(coords[0], y_coord)]\n",
    "    return coords + offset_coords\n",
    "\n",
    "corner_coordinates = []\n",
    "for row in range(0, 20):\n",
    "    temp = calculate_offset_coords(coords, x_step = 700, \n",
    "                                            y_step = row*700, \n",
    "                                            number = 20)\n",
    "    corner_coordinates.append([[x, [y, row]] for x, y in zip(temp, [col for col in range(0, 20)])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outliers(sample):\n",
    "    for date in range(24):\n",
    "        for band in range(10):\n",
    "            maxs = np.max(sample[date, :, :, band])\n",
    "            mins = np.min(sample[date, :, :, band])\n",
    "            if maxs == 1.0 or mins == 0.0:\n",
    "                print(\"Found null outlier\")\n",
    "                return True\n",
    "            if maxs == mins:\n",
    "                print(\"Found missing outlier\")\n",
    "                return True\n",
    "            if maxs >= 1.05 or mins <= -1.05:\n",
    "                print(\"Found range outlier\")\n",
    "                return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def calculate_proximal_steps(uniques, date, clean_steps):\n",
    "    arg_before = None\n",
    "    arg_after = None\n",
    "    uniques = np.array(uniques)\n",
    "    satisfactory = np.argwhere(uniques > 2)\n",
    "    satisfactory = np.array([x for x in satisfactory if x in clean_steps])\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    print(arg_before, date, arg_after)\n",
    "    return arg_before, arg_after\n",
    "\n",
    "def worker_download(coord, folder = OUTPUT_FOLDER):\n",
    "    idx_x = coord[1][0]\n",
    "    idx_y = coord[1][1]\n",
    "    print(\"Starting: {} {}\".format(idx_x, idx_y))\n",
    "    #coord = [x[0] for x in coord]\n",
    "    coord = coord[0]\n",
    "    output_folder = OUTPUT_FOLDER + \"{}/{}/\".format(str(idx_y), str(idx_x))\n",
    "    if not os.path.exists(os.path.realpath(output_folder)):\n",
    "        os.makedirs(os.path.realpath(output_folder))\n",
    "    tiled_bbx = bounding_box(coord, y_offset_max = 700, x_offset_max = 700, expansion = 10)\n",
    "    dem_bbx = bounding_box(coord, y_offset_max = 700, x_offset_max = 700, expansion = 20)\n",
    "    cloud_steps, means, cloud_probs = identify_clouds(tiled_bbx)\n",
    "    tiles, request = download_tiles(tiled_bbx, means)\n",
    "    dem = download_dem(dem_bbx)\n",
    "    #tiles = np.stack(tiles)\n",
    "    dem = np.tile(dem.reshape((1, 72, 72, 1)), (24, 1, 1, 1))\n",
    "    #tiles = np.concatenate([tiles, dem], axis = -1)\n",
    "    for x_offset, cval in enumerate([x for x in range(0, 70, 14)]):\n",
    "        for y_offset, rval in enumerate([x for x in range(0, 70, 14)]):\n",
    "            # Calculate the best 24-snapshot sequence for the 16x16 tile\n",
    "            x = calc_best(tiles, cloud_probs, request, cval, rval)\n",
    "            # Download and extract the DEM\n",
    "            dem_i = dem[:, cval:cval+16, rval:rval+16, 0]\n",
    "            x = np.concatenate([x, dem_i[:, :, :, np.newaxis]], axis = -1)\n",
    "            # Remove and copy blank time steps\n",
    "            x = remove_blank_steps(x)\n",
    "            if x.shape[1] != 16:\n",
    "                print(\"Image size error on {} {}\".format(idx_x, idx_y))\n",
    "            # Check for missing bands and copy the prior timestep forward if missing\n",
    "            for date in range(24):\n",
    "                for band in range(10):\n",
    "                    uniques = [len(np.unique(x[i, :, :, band])) for i in range(24)]\n",
    "                    clean_steps = [x for x in range(0, len(uniques))]\n",
    "                    maxs = np.max(x[date, :, :, band])\n",
    "                    mins = np.min(x[date, :, :, band])\n",
    "                    if maxs >= 1.0 or mins <= 0.0:\n",
    "                        print(\"Found null outlier at {} {}\".format(date, band))\n",
    "                        before, after = calculate_proximal_steps(uniques, date, clean_steps)\n",
    "                        before = x[date + int(before), :, :, band]\n",
    "                        after = x[date + int(after), :, :, band]\n",
    "                        x[date, :, :, band] = (before + after) / 2\n",
    "                    if len(np.unique(x[date, :, :, band])) <= 3:\n",
    "                        print(\"Found missing outlier at {} {}, {}\".format(date, band, maxs))\n",
    "                        before, after = calculate_proximal_steps(uniques, date, clean_steps)\n",
    "                        before = x[date + int(before), :, :, band]\n",
    "                        after = x[date + int(after), :, :, band]\n",
    "                        x[date, :, :, band] = (before + after) / 2\n",
    "            # Calculate spectral indices after dealing with missing data issues\n",
    "            x = ndvi(x, True)\n",
    "            x = evi(x, True)\n",
    "            x = savi(x, True)\n",
    "            x = bi(x, True)\n",
    "            x = msavi2(x, True)\n",
    "            x = si(x, True)\n",
    "            \n",
    "            # Smooth all data except slope\n",
    "            x[:, :, :, 10] /= 90\n",
    "            for row in range(0, 16):\n",
    "                for column in range(0, 16):\n",
    "                    for band in [x for x in range(0, 17) if x != 10]:\n",
    "                        sm = smooth(x[:, row, column, band], 1.0, d = 2)\n",
    "                        x[:, row, column, band] = sm\n",
    "            #if x_offset*5+y_offset+1 % 10 == 0:\n",
    "            #    print(\"Saving: {} {} {}\".format(x_offset, y_offset, str(x_offset*5+y_offset+1)))\n",
    "            np.save(output_folder + str(x_offset*5+y_offset+1), x)\n",
    "    \n",
    "    # Do the same for the corner offset imagery\n",
    "    for x_offset, cval in enumerate([x for x in range(7, 63, 14)]):\n",
    "        for y_offset, rval in enumerate([x for x in range(7, 63, 14)]):\n",
    "            x = calc_best(tiles, cloud_probs, request, cval, rval)\n",
    "            dem_i = dem[:, cval:cval+16, rval:rval+16, 0]\n",
    "            x = np.concatenate([x, dem_i[:, :, :, np.newaxis]], axis = -1)\n",
    "            x = remove_blank_steps(x)\n",
    "            if x.shape[1] != 16:\n",
    "                print(\"Image size error on {} {}\".format(idx_x, idx_y))\n",
    "            for date in range(24):\n",
    "                for band in range(10):\n",
    "                    maxs = np.max(x[date, :, :, band])\n",
    "                    mins = np.min(x[date, :, :, band])\n",
    "                    uniques = [len(np.unique(x[i, :, :, band])) for i in range(24)]\n",
    "                    clean_steps = [x for x in range(0, len(uniques))]\n",
    "                    if maxs >= 1.0 or mins <= 0.0:\n",
    "                        print(\"Found null outlier at {} {}\".format(date, band))\n",
    "                        before, after = calculate_proximal_steps(uniques, date, clean_steps)\n",
    "                        before = x[date + int(before), :, :, band]\n",
    "                        after = x[date + int(after), :, :, band]\n",
    "                        x[date, :, :, band] = (before + after) / 2\n",
    "                    if len(np.unique(x[date, :, :, band])) <= 5:\n",
    "                        print(\"Found missing outlier at {} {}\".format(date, band))\n",
    "                        before, after = calculate_proximal_steps(uniques, date, clean_steps)\n",
    "                        before = x[date + int(before), :, :, band]\n",
    "                        after = x[date + int(after), :, :, band]\n",
    "                        x[date, :, :, band] = (before + after) / 2\n",
    "            x = ndvi(x, True)\n",
    "            x = evi(x, True)\n",
    "            x = savi(x, True)\n",
    "            x = bi(x, True)\n",
    "            x = msavi2(x, True)\n",
    "            x = si(x, True)\n",
    "            x[:, :, :, 10] /= 90\n",
    "            \n",
    "            for row in range(0, 16):\n",
    "                for column in range(0, 16):\n",
    "                    for band in [x for x in range(0, 17) if x != 10]:\n",
    "                        sm = smooth(x[:, row, column, band], 1.0, d = 2)\n",
    "                        x[:, row, column, band] = sm\n",
    "            #if str(25+x_offset*4+y_offset+1) % 10 == 0:\n",
    "            #    print(\"Saving: {} {} {}\".format(x_offset, y_offset, str(25+x_offset*4+y_offset+1)))\n",
    "            np.save(output_folder + str(25+(x_offset*4+y_offset+1)), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 10 2\n",
      "Starting: 11 2\n",
      "Starting: 12 2\n",
      "Starting: 13 2\n",
      "Starting: 14 2\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "START_Y = 2\n",
    "START_X = 10\n",
    "\n",
    "import multiprocessing\n",
    "for i in corner_coordinates[START_Y:]:\n",
    "    try:\n",
    "        threads = 5\n",
    "        pool = multiprocessing.Pool(threads)\n",
    "        zip(*pool.map(worker_download, i[START_X:]))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    except Exception as e:\n",
    "        print(\"error\")\n",
    "        logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 6 7\n",
      "There are 23/46 clean steps\n",
      "0 broken normal steps\n",
      "Starting: 7 7\n",
      "There are 22/46 clean steps\n",
      "0 broken normal steps\n",
      "Starting: 8 7\n",
      "There are 27/46 clean steps\n",
      "0 broken normal steps\n",
      "Starting: 9 7\n",
      "There are 33/46 clean steps\n",
      "0 broken normal steps\n"
     ]
    }
   ],
   "source": [
    "for r in range(7, 8):\n",
    "    for c in range(6, 10):\n",
    "        worker_download(corner_coordinates[r][c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
