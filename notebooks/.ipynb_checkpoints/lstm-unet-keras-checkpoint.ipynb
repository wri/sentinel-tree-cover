{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras_adabound import AdaBound\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/subplot.csv\")\n",
    "df1 = pd.read_csv(\"../data/subplot2.csv\")\n",
    "\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1)\n",
    "df1 = df1.drop('IMAGERY_TITLE', axis = 1)\n",
    "df = pd.concat([df, df1], ignore_index = True)\n",
    "df = df.dropna(axis = 0)\n",
    "\n",
    "\n",
    "N_SAMPLES = int(df.shape[0]/196)\n",
    "print(N_SAMPLES)\n",
    "\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bbox(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # TOP, LEFT, BOTTOM, RIGHT\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])), (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "locations = [calc_bbox(x) for x in plot_ids]\n",
    "locations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_images(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    rows = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        rows.append(list(subs_lat['TREE']))\n",
    "    return rows\n",
    "\n",
    "data = [reconstruct_images(x) for x in plot_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_blank_steps(array):\n",
    "    to_update = {}\n",
    "    sets = []\n",
    "    for k in range(6):\n",
    "        for i in range(array.shape[0]):\n",
    "            for k in range(array.shape[-1]):\n",
    "                mean = (np.mean(array[i, :, :, k]))\n",
    "                if mean == 0:\n",
    "                    sets.append(i)\n",
    "                    if i < array.shape[0] - 1:\n",
    "                        array[i, :, :, k] = array[i + 1, :, :, k]\n",
    "                    else:\n",
    "                        array[i, :, :, k] = array[i - 1, :, :, k]\n",
    "                if mean == 1:\n",
    "                    sets.append(i)\n",
    "                    if i < array.shape[0] - 1:\n",
    "                        array[i, :, :, k] = array[i + 1, :, :, k]\n",
    "                    else:\n",
    "                        array[i, :, :, k] = array[i - 1, :, :, k]\n",
    "    for i in range(array.shape[0]):\n",
    "        for k in range(array.shape[-1]):\n",
    "            mean = (np.mean(array[i, :, :, k]))\n",
    "            if mean == 0:\n",
    "                if i < array.shape[0] - 2:\n",
    "                    array[i, :, :, k] = array[i + 2, :, :, k]\n",
    "                else:\n",
    "                    array[i, :, :, k] = array[i - 2, :, :, k]\n",
    "            if mean == 1:\n",
    "                if i < array.shape[0] - 2:\n",
    "                    array[i, :, :, k] = array[i + 2, :, :, k]\n",
    "                else:\n",
    "                    array[i, :, :, k] = array[i - 2, :, :, k]\n",
    "    print(set(sets))\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "image_size = 14\n",
    "\n",
    "def ndvi(x):\n",
    "    # (B8 - B4)/(B8 + B4)\n",
    "    ndvis = [(im[:, :, 6] - im[:, :, 2]) / (im[:, :, 6] + im[:, :, 2]) for im in x]\n",
    "    min_ndvi = min([np.mean(x) for x in ndvis])\n",
    "    max_ndvi = max([np.mean(x) for x in ndvis])\n",
    "    ndvis = [(x - min_ndvi / (max_ndvi - min_ndvi)) for x in ndvis]\n",
    "    x_padding = np.zeros((x.shape[0], image_size, image_size, 1))\n",
    "    x = np.concatenate((x, x_padding), axis = 3)\n",
    "    # Iterate over each time step and add NDVI in as the 11th channel\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i, :, :, 10] = ndvis[i]\n",
    "    return x\n",
    "\n",
    "# Initiate empty lists to store the X and Y data in\n",
    "data_x = []\n",
    "data_y = []\n",
    "binary_y = []\n",
    "data_location_x = []\n",
    "data_location_y = []\n",
    "lengths = []\n",
    "\n",
    "# Iterate over each plot\n",
    "pad = True\n",
    "flip = True\n",
    "for i in plot_ids:\n",
    "    # Load the sentinel imagery\n",
    "    x = np.load(\"../data/ids/\" + str(i) + \".npy\")\n",
    "    # Shape check\n",
    "    if x.shape[1] == image_size:\n",
    "        x = ndvi(x)                # calc NDVI\n",
    "        x = remove_blank_steps(x)\n",
    "        y = reconstruct_images(i)\n",
    "        if sum([sum(x) for x in y]) >= 1:\n",
    "            binary_y.append(1)\n",
    "        else:\n",
    "            binary_y.append(0)\n",
    "        lengths.append(x.shape[0])\n",
    "        #x = np.median(x, axis = 0) # and calculate the median over the time steps\n",
    "        if pad:\n",
    "            if x.shape[0] < 24:\n",
    "                print(x.shape[0])\n",
    "                padding = np.zeros((24 - x.shape[0], image_size, image_size, 11))\n",
    "                x = np.concatenate((x, padding), axis = 0)\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "        if flip:\n",
    "                # FLIP HORIZONTAL\n",
    "            x1 = np.flip(x, 1)\n",
    "            data_x.append(x1)\n",
    "            data_y.append(np.flip(y, 0))\n",
    "            lengths.append(x.shape[0])\n",
    "    \n",
    "                # FLIP BOTH\n",
    "            x2 = np.flip(x, 2)\n",
    "            x2 = np.flip(x2, 1)\n",
    "            data_x.append(x2)\n",
    "            data_y.append(np.flip(y, [0, 1]))\n",
    "            lengths.append(x.shape[0])\n",
    "                # FLIP VERTICAL\n",
    "            x3 = np.flip(x, 2)\n",
    "            data_x.append(x3)\n",
    "            data_y.append(np.flip(y, 1))\n",
    "            lengths.append(x.shape[0])\n",
    "\n",
    "data_x = np.stack(data_x)\n",
    "data_y = np.stack(data_y)\n",
    "data_y = np.reshape(data_y, (N_SAMPLES*4, 14, 14, 1))\n",
    "binary_y = np.stack(binary_y)\n",
    "lengths = np.stack(lengths)\n",
    "lengths = np.reshape(lengths, (lengths.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = data_x[:int(len(data_x) * 0.8)]\n",
    "train_y = data_y[:int(len(data_x) * 0.8)]\n",
    "train_lengths = lengths[:int(len(data_x) * 0.8)]\n",
    "train_binary = binary_y[:int(len(data_x) * 0.8)]\n",
    "\n",
    "test_x = data_x[int(len(data_x) * 0.8):]\n",
    "test_y = data_y[int(len(data_x) * 0.8):]\n",
    "test_lengths = lengths[int(len(data_x) * 0.8):]\n",
    "test_binary = binary_y[int(len(data_x) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    gts = tf.reduce_sum(gt_sorted)\n",
    "    intersection = gts - tf.cumsum(gt_sorted)\n",
    "    union = gts + tf.cumsum(1. - gt_sorted)\n",
    "    jaccard = 1. - intersection / union\n",
    "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def lovasz_hinge(logits, labels):\n",
    "        \"\"\"\n",
    "        Binary Lovasz hinge loss\n",
    "          logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "          labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "          per_image: compute the loss per image instead of per batch\n",
    "          ignore: void class id\n",
    "        \"\"\"\n",
    "        def treat_image(log_lab):\n",
    "            log, lab = log_lab\n",
    "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
    "            log = tf.reshape(log, (-1,))\n",
    "            lab = tf.reshape(lab, (-1,))\n",
    "            return lovasz_hinge_flat(log, lab)\n",
    "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
    "        losses.set_shape((None,))\n",
    "        loss = tf.reduce_mean(losses)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss():\n",
    "        labelsf = tf.cast(labels, logits.dtype)\n",
    "        signs = 2. * labelsf - 1.\n",
    "        errors = 1. - logits * tf.stop_gradient(signs)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
    "        gt_sorted = tf.gather(labelsf, perm)\n",
    "        grad = lovasz_grad(gt_sorted)\n",
    "        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
    "        return loss\n",
    "\n",
    "    # deal with the void prediction case (only void pixels)\n",
    "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
    "                   lambda: tf.reduce_sum(logits) * 0.,\n",
    "                   compute_loss,\n",
    "                   strict=True,\n",
    "                   name=\"loss\"\n",
    "                   )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def bin_foc(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(0.5 * K.pow(1. - pt_1, 2) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - 0.5) * K.pow(pt_0, 2) * K.log(1. - pt_0))\n",
    "\n",
    "\n",
    "def foc_lovasz(y_true, y_pred):\n",
    "    #jaccard_loss = jaccard_distance(y_true, y_pred)\n",
    "    lovasz = lovasz_hinge(y_pred, y_true)\n",
    "    #pred_reshape = tf.reshape(y_pred, (-1, 14, 14))\n",
    "    #true_reshape = tf.reshape(y_true, (-1, 14, 14))\n",
    "    focal_loss = bin_foc(y_true, y_pred)\n",
    "    summed = lovasz + focal_loss\n",
    "    return summed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def castF(x):\n",
    "    return K.cast(x, K.floatx())\n",
    "\n",
    "def castB(x):\n",
    "    return K.cast(x, bool)\n",
    "\n",
    "def iou_loss_core(true,pred):  #this can be used as a loss if you make it negative\n",
    "    intersection = true * pred\n",
    "    notTrue = 1 - true\n",
    "    union = true + (notTrue * pred)\n",
    "\n",
    "    return (K.sum(intersection, axis=-1) + K.epsilon()) / (K.sum(union, axis=-1) + K.epsilon())\n",
    "\n",
    "def competitionMetric2(true, pred): #any shape can go - can't be a loss function\n",
    "\n",
    "    tresholds = [0.5 + (i*.05)  for i in range(10)]\n",
    "\n",
    "    #flattened images (batch, pixels)\n",
    "    true = K.batch_flatten(true)\n",
    "    pred = K.batch_flatten(pred)\n",
    "    pred = castF(K.greater(pred, 0.5))\n",
    "\n",
    "    #total white pixels - (batch,)\n",
    "    trueSum = K.sum(true, axis=-1)\n",
    "    predSum = K.sum(pred, axis=-1)\n",
    "\n",
    "    #has mask or not per image - (batch,)\n",
    "    true1 = castF(K.greater(trueSum, 1))    \n",
    "    pred1 = castF(K.greater(predSum, 1))\n",
    "\n",
    "    #to get images that have mask in both true and pred\n",
    "    truePositiveMask = castB(true1 * pred1)\n",
    "\n",
    "    #separating only the possible true positives to check iou\n",
    "    testTrue = tf.boolean_mask(true, truePositiveMask)\n",
    "    testPred = tf.boolean_mask(pred, truePositiveMask)\n",
    "\n",
    "    #getting iou and threshold comparisons\n",
    "    iou = iou_loss_core(testTrue,testPred) \n",
    "    truePositives = [castF(K.greater(iou, tres)) for tres in tresholds]\n",
    "\n",
    "    #mean of thressholds for true positives and total sum\n",
    "    truePositives = K.mean(K.stack(truePositives, axis=-1), axis=-1)\n",
    "    truePositives = K.sum(truePositives)\n",
    "\n",
    "    #to get images that don't have mask in both true and pred\n",
    "    trueNegatives = (1-true1) * (1 - pred1) # = 1 -true1 - pred1 + true1*pred1\n",
    "    trueNegatives = K.sum(trueNegatives) \n",
    "\n",
    "    return (truePositives + trueNegatives) / castF(K.shape(true)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers import *\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras.layers import Lambda\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import ELU\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "\n",
    "\n",
    "\n",
    "def conv_lstm_model():\n",
    "    inp = Input(shape = (24, 14, 14, 11))\n",
    "    \n",
    "    \n",
    "    input_shape = (24, 14, 14, 11)\n",
    "    fl = TimeDistributed(Flatten(), input_shape=input_shape)(inp)\n",
    "    ms = TimeDistributed(Masking(mask_value=0.))(fl)\n",
    "    x = Lambda(lambda x: x, output_shape=lambda s:s)(ms)\n",
    "    rs = TimeDistributed(Reshape(input_shape[1:]))(x)    \n",
    "\n",
    "    conv = Bidirectional(ConvLSTM2D(filters=32, kernel_size=(3, 3),\n",
    "                  padding='same', return_sequences=True))(rs)\n",
    "    ln = LayerNormalization()(conv)\n",
    "        \n",
    "    downsampled = TimeDistributed(MaxPool2D(pool_size = (2, 2)))(ln)\n",
    "        \n",
    "    conv = Bidirectional(ConvLSTM2D(filters=48, kernel_size=(3, 3),\n",
    "                  padding='same', return_sequences=False))(downsampled)\n",
    "    ln = LayerNormalization()(conv)\n",
    "    \n",
    "    conv_block_7_u = Conv2D(filters = 96, kernel_size = (3, 3), padding = 'same')(ln)\n",
    "    elu_7_u = ELU()(conv_block_7_u)\n",
    "\n",
    "\n",
    "    # Upsampling final block (14 x 14)\n",
    "    upsampling_14 = UpSampling2D()(elu_7_u)\n",
    "    padded = ReflectionPadding2D((1, 1))(upsampling_14)\n",
    "    fm = Conv2D(filters = 64,\n",
    "                kernel_size = (3, 3), \n",
    "                padding = 'valid',\n",
    "                )(padded)\n",
    "    elu = ELU()(fm)\n",
    "    #concat = Concatenate(axis = -1)([elu, x[1]])\n",
    "    padded = ReflectionPadding2D((1, 1))(elu)\n",
    "    #padded = ReflectionPadding2D((1, 1))(concat)\n",
    "    fm = Conv2D(filters = 32,\n",
    "                kernel_size = (3, 3), \n",
    "                padding = 'valid',\n",
    "                )(padded)\n",
    "    elu = ELU()(fm)\n",
    "    \n",
    "    # Output layer\n",
    "    fm = Conv2D(filters = 1,\n",
    "                kernel_size = (1, 1), \n",
    "                padding = 'valid',\n",
    "                activation = 'sigmoid'\n",
    "                )(elu)\n",
    "\n",
    "    mod = Model(inputs = inp,  outputs = fm)\n",
    "    mod.summary()\n",
    "    return mod\n",
    "\n",
    "m = conv_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = keras.models.load_model('best_model.h5', custom_objects = {\"AdaBound\": AdaBound,\n",
    "#                                                               \"foc_lovasz\": foc_lovasz,\n",
    "#                                                               \"competitionMetric2\": competitionMetric2,\n",
    "#                                                               \"ReflectionPadding2D\": ReflectionPadding2D})\n",
    "#m.load_weights(\"232.hdf5\")\n",
    "m.compile(loss = [foc_lovasz],\n",
    "          optimizer=AdaBound(lr=0.00005, final_lr=0.0005),  metrics=[competitionMetric2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(train_x,\n",
    "      train_y, \n",
    "      validation_data = ((test_x, test_y)),\n",
    "      batch_size=32,\n",
    "      epochs = 500,\n",
    "      callbacks=[mcp_save],\n",
    "      #class_weight=[0.1, 0.9],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m.load_weights(\".mdl_wts.hdf5\")\n",
    "m.save('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 looks good, 0, 76, 90, 96, 108, 22, 56, 63, 55, 80\n",
    "\n",
    "## 21, 98, 105, 15, 210, 235\n",
    "\n",
    "true = test_y[21].reshape(14, 14)\n",
    "\n",
    "plt.figure(figsize=(8.5, 7))\n",
    "sns.heatmap(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = m.predict([test_x[21].reshape(1, 24, 14, 14, 11)])\n",
    "pred = pred.reshape(14, 14)\n",
    "pred[np.where(pred > 0.4)] = 1\n",
    "#pred[np.where(pred < 0.5)] = 0\n",
    "#pred[np.where(pred < 0.5)] = 0\n",
    "\n",
    "plt.figure(figsize=(8.5, 7))\n",
    "\n",
    "# 111\n",
    "sns.heatmap(pred, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_hectare_accuracy(true, pred):\n",
    "    eps = 1e-9\n",
    "    true_x = np.split(true, indices_or_sections = 2, axis = 0)\n",
    "    true_y = [np.split(x, 2, 1) for x in true_x]\n",
    "    true_xy = [item for sublist in true_y for item in sublist]\n",
    "    true = [np.sum(x) for x in true_xy]\n",
    "    \n",
    "    pred_x = np.split(pred, indices_or_sections = 2, axis = 0)\n",
    "    pred_y = [np.split(x, 2, 1) for x in pred_x]\n",
    "    pred_xy = [item for sublist in pred_y for item in sublist]\n",
    "    pred = [np.sum(x) for x in pred_xy]\n",
    "    \n",
    "    recall = [min(x / y, 1) for x, y in zip(pred, true) if y > 0]\n",
    "    precision = [(x - y) / x for x, y in zip(pred, true)]\n",
    "    for i, val in enumerate(precision):\n",
    "        if val < 0:\n",
    "            precision[i] = 1\n",
    "    return np.mean(recall), np.mean(precision)\n",
    "\n",
    "half_hectare_accuracy(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thirty_meter(true, pred):\n",
    "    indices_x = np.random.randint(0, 11, 3)\n",
    "    indices_y = np.random.randint(0, 11, 3)\n",
    "    indexes = ([([a, a + 2], [b, b + 2]) for a,b in zip(indices_x, indices_y)])\n",
    "    subs_true = []\n",
    "    subs_pred = []\n",
    "    for i in indexes:\n",
    "        true_i = true[i[0][0]:i[0][1], i[1][0]:i[1][1]]\n",
    "        pred_i = pred[i[0][0]:i[0][1], i[1][0]:i[1][1]]\n",
    "        subs_true.append(true_i)\n",
    "        subs_pred.append(pred_i)\n",
    "    pred = [np.sum(x) for x in subs_pred]\n",
    "    true = [np.sum(x) for x in subs_true]\n",
    "    true_positives = []\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "    for p, t in zip(pred, true):\n",
    "        if p > t:\n",
    "            tp = p - (p - t)\n",
    "            fp = p - tp\n",
    "            fn = 0\n",
    "        if t >= p:\n",
    "            tp = t\n",
    "            fp = 0\n",
    "            fn = t - p\n",
    "        true_positives.append(tp)\n",
    "        false_positives.append(fp)\n",
    "        false_negatives.append(fn)\n",
    "    prec = [x / (x + y) for x,y in zip(true_positives, false_positives) if (x+y) > 0]\n",
    "    rec = [x / (x + y) for x,y in zip(true_positives, false_negatives) if (x+y) > 0]\n",
    "    \n",
    "    #recall = [min(x / y, 1) for x, y in zip(pred, true) if y > 0]\n",
    "    #precision = [(y - x) / x for x, y in zip(pred, true)]\n",
    "    #print(precision)\n",
    "    return np.mean(rec), np.mean(prec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = []\n",
    "precisions = []\n",
    "for i in range(len(test_y)):\n",
    "    true = test_y[i].reshape((14, 14))\n",
    "    pred = m.predict([test_x[i].reshape(1, 24, 14, 14, 11)])\n",
    "    pred = pred.reshape(14, 14)\n",
    "    pred[np.where(pred > 0.45)] = 1\n",
    "    pred[np.where(pred < 0.45)] = 0\n",
    "    rec, prec = thirty_meter(true, pred)\n",
    "    #rec, prec = half_hectare_accuracy(true, pred)\n",
    "    recalls.append(rec)\n",
    "    precisions.append(prec)\n",
    "    print(prec, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([x for x in recalls if not np.isnan(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([x for x in precisions if not np.isnan(x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICT ON TEST WALL-TO-WALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = []\n",
    "pred_len = []\n",
    "pred_files = [\"../data/test/\" + str(x) + \".npy\" for x in range(0, 14)]\n",
    "SHAPE_X = 7\n",
    "SHAPE_Y = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pred_files:\n",
    "    data = np.load(i)\n",
    "    data = ndvi(data)\n",
    "    data = remove_blank_steps(data)\n",
    "    pred_len.append(data.shape[0])\n",
    "    if data.shape[0] < 24:\n",
    "        padding = np.zeros((24 - data.shape[0], 14, 14, 11))\n",
    "        data = np.concatenate((data, padding), axis = 0)\n",
    "    pred_x.append(data)\n",
    "pred_len = np.stack(pred_len).reshape((len(pred_len), 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in range(0, SHAPE_X * SHAPE_Y):\n",
    "    pr = m.predict([pred_x[i].reshape(1, 24, 14, 14, 11)])\n",
    "    pr = pr.reshape(14, 14)\n",
    "    preds.append(pr)\n",
    "\n",
    "row1 = np.concatenate(preds[:SHAPE_X], axis = 1)\n",
    "row2 = np.concatenate(preds[SHAPE_X:], axis = 1)\n",
    "row2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = np.concatenate((row2, row1), axis = 0)\n",
    "stacked.shape\n",
    "#tacked = stacked.reshape((14, 70))\n",
    "#stacked[np.where(stacked > 0.47)] = 1\n",
    "#stacked[np.where(stacked < 0.47)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 7))\n",
    "sns.heatmap(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated = []\n",
    "for i, k in zip(range(0, stacked.shape[0], 2), (range(0, stacked.shape[1], 2))):\n",
    "    print(i, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(0, stacked.shape[1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
