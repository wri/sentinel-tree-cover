{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU, LeakyReLU\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import itertools\n",
    "from tflearn.layers.conv import global_avg_pool\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.layers import batch_normalization\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/zoneout.py\n",
    "%run ../src/convgru.py\n",
    "%run ../src/lovasz.py\n",
    "%run ../src/utils.py\n",
    "%run ../src/adabound.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/subplot.csv\")\n",
    "df1 = pd.read_csv(\"../data/subplot2.csv\")\n",
    "df2 = pd.read_csv(\"../data/subplot3.csv\")\n",
    "df3 = pd.read_csv(\"../data/subplot4.csv\")\n",
    "\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df1 = df1.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df2 = df2.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df3 = df3.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "\n",
    "lens = [len(x) for x in [df, df1, df2, df3]]\n",
    "\n",
    "df = pd.concat([df, df1, df2, df3], ignore_index = True)\n",
    "df = df.dropna(axis = 0)\n",
    "\n",
    "existing = [int(x[:-4]) for x in os.listdir('../data/shifted/') if \".DS\" not in x]\n",
    "\n",
    "df = df[df['PLOT_ID'].isin(existing)]\n",
    "N_SAMPLES = int(df.shape[0]/196)\n",
    "print(N_SAMPLES)\n",
    "\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "\n",
    "def reconstruct_images(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    rows = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        rows.append(list(subs_lat['TREE']))\n",
    "    return rows\n",
    "\n",
    "data = [reconstruct_images(x) for x in plot_ids]\n",
    "\n",
    "# Initiate empty lists to store the X and Y data in\n",
    "data_x, data_y, lengths = [], [], []\n",
    "\n",
    "# Iterate over each plot\n",
    "pad = True\n",
    "flip = False\n",
    "for i in plot_ids:\n",
    "    # Load the sentinel imagery\n",
    "    x = np.load(\"../data/shifted/\" + str(i) + \".npy\")\n",
    "    x = x[0, :, :, :, :]\n",
    "    # Shape check\n",
    "    x = ndvi(x, image_size = 16)\n",
    "    x = evi(x, image_size = 16)\n",
    "    x = savi(x, image_size = 16)\n",
    "    x = remove_blank_steps(x)\n",
    "    y = reconstruct_images(i)\n",
    "    lengths.append(x.shape[0])\n",
    "    if pad:\n",
    "        if x.shape[0] < 24:\n",
    "            padding = np.zeros((24 - x.shape[0], IMAGE_SIZE, IMAGE_SIZE, 13))\n",
    "            x = np.concatenate((x, padding), axis = 0)\n",
    "    data_x.append(x)\n",
    "    data_y.append(y)\n",
    "    if flip:\n",
    "            # FLIP HORIZONTAL\n",
    "        x1 = np.flip(x, 1)\n",
    "        data_x.append(x1)\n",
    "        data_y.append(np.flip(y, 0))\n",
    "        lengths.append(x.shape[0])\n",
    "\n",
    "            # FLIP BOTH\n",
    "        x2 = np.flip(x, 2)\n",
    "        x2 = np.flip(x2, 1)\n",
    "        data_x.append(x2)\n",
    "        data_y.append(np.flip(y, [0, 1]))\n",
    "        lengths.append(x.shape[0])\n",
    "            # FLIP VERTICAL\n",
    "        x3 = np.flip(x, 2)\n",
    "        data_x.append(x3)\n",
    "        data_y.append(np.flip(y, 1))\n",
    "        lengths.append(x.shape[0])\n",
    "\n",
    "data_x = np.stack(data_x)\n",
    "data_y = np.stack(data_y)\n",
    "data_y = np.reshape(data_y, (N_SAMPLES, 14, 14, 1))\n",
    "lengths = np.stack(lengths)\n",
    "lengths = np.reshape(lengths, (lengths.shape[0], 1))\n",
    "\n",
    "print(\"Finished data loading\")\n",
    "print(data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONE_OUT_PROB = 0.1 #(0.05, 0.20, 0.05) --> 4\n",
    "L2_REG = 0.001 #(1-e6, 1-e1, x10) --> 5\n",
    "INITIAL_LR = 2e-5 #(1e-6, 1e-3, x5) --> 10\n",
    "FINAL_LR = 2e-3 # (1e - 5, 1e-2, x5) --> 10\n",
    "LOSS_WEIGHTING = 0.5 #(0.2, 1, 0.2) --> 5\n",
    "SQUEEZE_RATIO = 4 # --> 4, 8, 12, 16 --> 4\n",
    "BN_MOMENTUM = 0.9 # --> 3\n",
    "N_LAYERS = 4 # --> 3\n",
    "REG_TYPE = 'kernel' # kernel # --> 2\n",
    "SQUEEZE = True\n",
    "LAYER_NORM = True \n",
    "BATCH_SIZE = 4 # -->4\n",
    "LOSS_TYPE = 'bce-jaccard' #bce-jaccard, bce-dice, bce-lovasz, focal-jaccard, etc. --> 4\n",
    "N_CONV_PER_LAYER = 1 # --> 2\n",
    "ACTIVATION_TYPE = 'ELU' #RELU, PRELU --> 2\n",
    "MASK_LOSS = False # --> 2\n",
    "PAD_INPUT_TYPE = 'none' # zero, reflect, none # --> 2\n",
    "RENORM_CLIPPING = None # --> 5\n",
    "FRESH_START = False\n",
    "TRAIN_RATIO = 0.75\n",
    "TEST_RATIO = 0.25\n",
    "\n",
    "\n",
    "AUGMENTATION_RATIO = 4\n",
    "IMAGE_SIZE = 16\n",
    "N_SAMPLES = 679\n",
    "RESIZE_OUTPUT = False\n",
    "\n",
    "LABEL_SIZE = 14\n",
    "#if LABEL_SIZE == 16 and not RESIZE_OUTPUT:\n",
    "#    LABEL_SIZE = IMAGE_SIZE\n",
    "    \n",
    "TRAIN_SAMPLES = int((N_SAMPLES * AUGMENTATION_RATIO) * TRAIN_RATIO)\n",
    "TEST_SAMPLES = int((N_SAMPLES * AUGMENTATION_RATIO) - TRAIN_SAMPLES)\n",
    "print(TRAIN_SAMPLES // AUGMENTATION_RATIO, N_SAMPLES - (TRAIN_SAMPLES // AUGMENTATION_RATIO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.contrib.slim import conv2d\n",
    "\n",
    "def cse_block(prevlayer, prefix):\n",
    "    mean = Lambda(lambda xin: K.mean(xin, axis=[1, 2]))(prevlayer)\n",
    "    lin1 = Dense(K.int_shape(prevlayer)[3] // 2, name=prefix + 'cse_lin1', activation='relu')(mean)\n",
    "    lin2 = Dense(K.int_shape(prevlayer)[3], name=prefix + 'cse_lin2', activation='sigmoid')(lin1)\n",
    "    x = Multiply()([prevlayer, lin2])\n",
    "    return x\n",
    "\n",
    "\n",
    "def sse_block(prevlayer, prefix):\n",
    "    # Bug? Should be 1 here?\n",
    "    conv = Conv2D(K.int_shape(prevlayer)[3], (1, 1), padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                  activation='sigmoid', strides=(1, 1),\n",
    "                  name=prefix + \"_conv\")(prevlayer)\n",
    "    conv = Multiply(name=prefix + \"_mul\")([prevlayer, conv])\n",
    "    return conv\n",
    "\n",
    "\n",
    "def csse_block(x, prefix):\n",
    "    '''\n",
    "    Implementation of Concurrent Spatial and Channel ‘Squeeze & Excitation’ in Fully Convolutional Networks\n",
    "    https://arxiv.org/abs/1803.02579\n",
    "    '''\n",
    "    cse = cse_block(x, prefix)\n",
    "    sse = sse_block(x, prefix)\n",
    "    x = Add(name=prefix + \"_csse_mul\")([cse, sse])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    return batch_normalization(inputs=x, \n",
    "                               momentum = BN_MOMENTUM, \n",
    "                               training=training,\n",
    "                               renorm = True,\n",
    "                               reuse=None,\n",
    "                               name = scope)\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "\n",
    "def conv_bn_elu(inp, is_training, kernel_size, scope, filter_count = 16):\n",
    "    if kernel_size == 3:\n",
    "        padded = ReflectionPadding2D((1, 1,))(inp)\n",
    "        padding = 'valid'\n",
    "    else:\n",
    "        padded = inp\n",
    "        padding = 'same'\n",
    "    conv = Conv2D(filters = filter_count, kernel_size = (kernel_size, kernel_size),\n",
    "                      padding = padding, kernel_regularizer=reg)(padded)\n",
    "    elu = ELU()(conv)\n",
    "    bn = Batch_Normalization(elu, training=is_training, scope = scope + \"bn\")\n",
    "    return bn\n",
    "    \n",
    "def fpa(inp, filter_count):\n",
    "    one = conv_bn_elu(inp, is_training, 1, 'forward1', filter_count)\n",
    "    three = conv_bn_elu(inp, is_training, 3, 'down1', filter_count)\n",
    "    three_f = conv_bn_elu(three, is_training, 3, 'down1_f', filter_count)\n",
    "    two = conv_bn_elu(three, is_training, 2, 'down2', filter_count)\n",
    "    two_f = conv_bn_elu(two, is_training, 2, 'down2_f', filter_count)\n",
    "    \n",
    "    # top block\n",
    "    pooled = tf.keras.layers.GlobalAveragePooling2D()(inp)\n",
    "    one_top = conv_bn_elu(tf.reshape(pooled, (-1, 1, 1, pooled.shape[-1])), is_training, 1, 'top1', filter_count)\n",
    "    four_top = tf.keras.layers.UpSampling2D((4, 4))(one_top)\n",
    "    \n",
    "    \n",
    "    concat_1 = tf.multiply(one, tf.add(three_f, two_f))\n",
    "    concat_2 = tf.add(concat_1, four_top)\n",
    "    print(\"Feature pyramid attention shape {}\".format(concat_2.shape))\n",
    "    return concat_2\n",
    "    \n",
    "    \n",
    "\n",
    "def gau(x_low_level, x_high_level, scope, filter_count, size = 4):\n",
    "    \"\"\"\n",
    "    The global attention upsample to replace the up_cat_conv element\n",
    "    \"\"\"\n",
    "    low_feat = conv_bn_elu(x_low_level, is_training, 3, 'gauforward' + scope, filter_count)\n",
    "    high_gap = tf.keras.layers.GlobalAveragePooling2D()(x_high_level)\n",
    "    high_feat = tf.keras.layers.Reshape((1, 1, -1))(high_gap)\n",
    "    high_feat_gate = tf.keras.layers.UpSampling2D((size, size))(high_feat)\n",
    "    gated_low = tf.keras.layers.multiply([low_feat, high_feat_gate])\n",
    "    gated_low = conv_bn_elu(gated_low, is_training, 3, 'gauforward5' + scope, filter_count)\n",
    "    gated_high = tf.keras.layers.Conv2DTranspose(filters = filter_count, kernel_size = (3, 3),\n",
    "                                             strides=(2, 2), padding='same', kernel_regularizer = reg)(gated_low)\n",
    "    high_clamped = conv_bn_elu(x_high_level, is_training, 3, 'gauforward1' + scope, filter_count)\n",
    "    return tf.keras.layers.add([gated_high, high_clamped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = keras.regularizers.l2(L2_REG)\n",
    "inp = tf.placeholder(tf.float32, shape=(None, 24, IMAGE_SIZE, IMAGE_SIZE, 14))\n",
    "length = tf.placeholder(tf.int32, shape = (None, 1))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 14, 14))#, 1))\n",
    "length2 = tf.reshape(length, (-1,))\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training')\n",
    "\n",
    "\n",
    "down_16 = 8\n",
    "down_8 = 32\n",
    "down_4f = 48\n",
    "#down_2f = 2\n",
    "#up_4 = 30\n",
    "up_8 = 32\n",
    "up_16 = 16\n",
    "\n",
    "def down_block(inp, length, size, flt, scope, train):\n",
    "    with tf.variable_scope(scope):\n",
    "        cell_fw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [2, 2], padding = 'SAME')\n",
    "        #cell_bw = ConvGRUCell(shape = size, filters = flt,\n",
    "         #                  kernel = [2, 2], padding = 'SAME')\n",
    "        cell_fw = ZoneoutWrapper(\n",
    "            cell_fw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        #cell_bw = ZoneoutWrapper(\n",
    "        #    cell_bw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        gru = convGRU(inp, cell_fw, cell_fw, length)\n",
    "        down = TimeDistributed(MaxPool2D(pool_size = (2, 2)))(gru[0])\n",
    "        print(\"Down block shape: {}\".format(gru[1].shape))\n",
    "    return down, gru[1]\n",
    "\n",
    "def down_block_no_gru(inp, flt, scope, train):\n",
    "    with tf.variable_scope(scope):\n",
    "        padded = ReflectionPadding2D((1, 1))(inp)\n",
    "        \n",
    "        # Conv block 1\n",
    "        conv = Conv2D(filters = flt, kernel_size = (3, 3),\n",
    "                      padding = 'valid', kernel_regularizer=reg)(padded)\n",
    "        elu = ELU()(conv)\n",
    "        bn = Batch_Normalization(elu, training=is_training, scope = scope + \"bn\")\n",
    "        x = csse_block(bn, prefix='csse_block_{}'.format(scope))\n",
    "        down = MaxPool2D(pool_size = (2, 2))(x)\n",
    "        print(\"Down block shape: {}\".format(down.shape))\n",
    "    return down\n",
    "\n",
    "\n",
    "def up_block(inp, concat_inp, flt, sq, scope, concat, is_training, padding = True):\n",
    "    with tf.variable_scope(scope):\n",
    "        \n",
    "        gau_layer = gau(inp, concat_inp, scope, flt, inp.shape[-2])\n",
    "        x = csse_block(gau_layer, prefix='csse_block_{}'.format(scope))\n",
    "        print(\"Up block conv 1 shape: {}\".format(x.shape))\n",
    "        return x\n",
    "        \n",
    "        \n",
    "down_1, copy_1 = down_block(inp, length2, [16, 16], down_16, 'down_16', is_training)\n",
    "down_2 = down_block_no_gru(copy_1, down_8, 'down_8', is_training)\n",
    "down_3 = down_block_no_gru(down_2, down_4f, 'down_4', is_training)\n",
    "\n",
    "down_fpa = fpa(down_3, down_4f)\n",
    "up_3 = up_block(down_fpa, down_2, up_8, up_8, 'up_8', True, is_training, padding =  True) # 4 - 8\n",
    "up_2 = up_block(up_3, copy_1, up_16, up_16, 'up_16', True, is_training, padding = True) # 8 - 16\n",
    "\n",
    "up_2 = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'valid')(up_2)\n",
    "elu = ELU()(up_2)\n",
    "bn = Batch_Normalization(elu, training=is_training, scope = \"out1bn\")\n",
    "x = csse_block(bn, prefix='csse_block_{}'.format(\"out1\"))\n",
    "\n",
    "up_2 = Conv2D(filters = 32, kernel_size = (3, 3), padding = 'valid')(x)\n",
    "elu = ELU()(up_2)\n",
    "\n",
    "#B = tf.Variable([-np.log(0.99/0.01)]) \n",
    "init = tf.constant_initializer([-np.log(0.9/0.1)])\n",
    "fm = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1), \n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid',\n",
    "            #bias_initializer = init,\n",
    "            )(elu)\n",
    "print(fm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.full((len(data_x), 9), 1/9)\n",
    "#weight = np.zeros((len(data_x), 9))\n",
    "#for i in range(weight.shape[0]):\n",
    "#    weight[i, 0] = 1.\n",
    "    #weight[i, np.random.randint(0, 9)] = 1.\n",
    "weight = np.load(\"weights.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight[25:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_alignment(true, pred):\n",
    "    wt = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        clipped_tensor = [true[i, 1:13, 1:13],\n",
    "                         true[i, 0:12, 1:13],\n",
    "                         true[i, 2:14, 1:13],\n",
    "                         true[i, 1:13, 0:12],\n",
    "                         true[i, 1:13, 2:14],\n",
    "                         true[i, 2:14, 0:12],\n",
    "                         true[i, 0:12, 0:12],\n",
    "                         true[i, 2:14, 2:14],\n",
    "                         true[i, 0:12, 2:14]]\n",
    "        \n",
    "        pred_i = pred[i, :, :, :]\n",
    "        \n",
    "        clipped_ces = tf.stack([binary_crossentropy(tf.reshape(x, (12, 12, 1)), pred_i) for x in clipped_tensor])\n",
    "        clipped_ces = tf.reduce_mean(clipped_ces, axis = [1,2])\n",
    "\n",
    "        clipped_ces = tf.reshape(clipped_ces, (-1, 9))\n",
    "        clipped_ces = clipped_ces - tf.reduce_mean(clipped_ces)\n",
    "        wt.append(clipped_ces)\n",
    "    print(\"WT CONCAT\", tf.squeeze(tf.stack(wt), axis = 1).shape)\n",
    "    return tf.squeeze(tf.stack(wt, axis =1))\n",
    "\n",
    "def loss_fn(true, pred, weights):\n",
    "    print(weights)\n",
    "    ce = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        weights_i = weights[i, :]\n",
    "        clipped_tensor = [true[i, 1:13, 1:13],\n",
    "                         true[i, 0:12, 1:13],\n",
    "                         true[i, 2:14, 1:13],\n",
    "                         true[i, 1:13, 0:12],\n",
    "                         true[i, 1:13, 2:14],\n",
    "                         true[i, 2:14, 0:12],\n",
    "                         true[i, 0:12, 0:12],\n",
    "                         true[i, 2:14, 2:14],\n",
    "                         true[i, 0:12, 2:14]]\n",
    "    \n",
    "        clipped_ces = [binary_crossentropy(tf.reshape(x, (-1, 12, 12, 1)), tf.reshape(pred[i, :, :], (1, 12, 12, 1))) for x in clipped_tensor]\n",
    "        clipped_ces = tf.stack(clipped_ces)\n",
    "        clipped_ces = tf.reshape(clipped_ces, (1, 9, 12, 12))\n",
    "        #weights_i = tf.stack([weights_i] * 12*12)\n",
    "        loss = clipped_ces * tf.reshape(weights_i, (1, 9, 1, 1))\n",
    "        loss = tf.reduce_sum(loss, axis = 1)\n",
    "        ce.append(loss)\n",
    "    ce = tf.reshape(tf.stack(ce), (BATCH_SIZE, 12, 12, 1))\n",
    "    print(\"LOSS SHAPE\", ce.shape)\n",
    "    return ce\n",
    "\n",
    "def loss_fn(true, pred, weights):\n",
    "    losses = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        weights_i = weights[i, :]\n",
    "        true_i = tf.reshape(true[i], (1, 14, 14, 1))\n",
    "        pred_i = tf.reshape(pred[i], (1, 12, 12, 1))\n",
    "        true_p = true_i\n",
    "        #loss_o = binary_crossentropy(true_p, pred)\n",
    "        # extract out the candidate shifts\n",
    "        true_m = true_i[:, 1:13, 1:13]\n",
    "        true_l = true_i[:, 0:12, 1:13]\n",
    "        true_r = true_i[:, 2:14, 1:13]\n",
    "        true_u = true_i[:, 1:13, 0:12]\n",
    "        true_d = true_i[:, 1:13, 2:14]\n",
    "        true_dr = true_i[:, 2:14, 0:12]\n",
    "        true_dl = true_i[:, 0:12, 0:12]\n",
    "        true_ur = true_i[:, 2:14, 2:14]\n",
    "        true_ul = true_i[:, 0:12, 2:14]\n",
    "        true_shifts = [true_m, true_l, true_r, true_u, true_d, true_dr, true_dl, true_ur, true_ul]\n",
    "        bce_shifts = tf.stack([binary_crossentropy(x, pred_i) for x in true_shifts])\n",
    "        loss = tf.reshape(bce_shifts, (1, 9, 12, 12)) * tf.reshape(weights_i, (1, 9, 1, 1))\n",
    "        loss = tf.reduce_sum(loss, axis = 1)\n",
    "        losses.append(loss)\n",
    "    loss = tf.reshape(tf.stack(losses), (BATCH_SIZE, 12, 12, 1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "BATCH_SIZE = 16\n",
    "weights = tf.placeholder(tf.float32, shape=(None, 9))#, 1))\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "ids = [x for x in range(0, len(data_x))]\n",
    "\n",
    "for total_step in range(0, 20):\n",
    "    MODE = 'EXPECTATION'\n",
    "    print(\"RUNNING A {} {} STEP\".format(total_step, MODE))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(2e-5)\n",
    "    weight_upd = calc_alignment(labels, fm)\n",
    "    #loss = bce_shift(labels, fm, 1.5)\n",
    "    loss = loss_fn(labels, fm, weights)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        optimizer = optimizer.minimize(loss)  \n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    orig = np.argmax(weight, axis = 1)\n",
    "    # Run training loop\n",
    "    for i in range(20):\n",
    "        if i == 19:\n",
    "            mode = 'MAXIMIZATION'\n",
    "            print(\"RUNNING A {} {} STEP\".format(total_step, MODE))\n",
    "        losses = []\n",
    "\n",
    "        for k in tnrange(int(len(ids) // BATCH_SIZE)):\n",
    "            batch_ids = ids[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "            batch_y = data_y[batch_ids, :, :].reshape(BATCH_SIZE, 14, 14)\n",
    "            op, tr, weight_update = sess.run([optimizer, loss, weight_upd],\n",
    "                                              feed_dict={inp: data_x[batch_ids, :, :, :],\n",
    "                                                         length: lengths[batch_ids],\n",
    "                                                         labels: data_y[batch_ids, :, :].reshape(BATCH_SIZE, 14, 14),\n",
    "                                                         is_training: True,\n",
    "                                                         weights: weight[batch_ids],\n",
    "                                                         })\n",
    "            losses.append(tr)\n",
    "            if i == 19:\n",
    "                weight_update = np.clip(weight_update, -0.25, 0.25)\n",
    "                weight[batch_ids] -= weight_update\n",
    "                weight[batch_ids] = np.clip(weight[batch_ids], 0, 1)\n",
    "                div = np.sum(weight[batch_ids], axis = 1)\n",
    "                #print(div)\n",
    "                #div = np.clip(div, 1, 10)\n",
    "                weight[batch_ids] = weight[batch_ids] / div[:, np.newaxis]\n",
    "        print(\"EPOCH {}: LOSS {}\".format(i, np.mean(losses)))\n",
    "    print('MEANS', np.mean(weight, axis = 0))\n",
    "    print('MAXS', np.max(weight, axis = 0))\n",
    "    diffs = [x for x, y in zip(np.argmax(weight, axis = 1), orig) if x != y]\n",
    "    print(\"{} samples changed\".format(len(diffs)))\n",
    "    if MODE == 'maximization':\n",
    "        weight = weight_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"weights.npy\", weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO @jombrandt \n",
    "#TODO @jombrandt -- remove augmentation of val set\n",
    "import random \n",
    "\n",
    "def multiplot(matrices):\n",
    "    '''Plot multiple heatmaps with subplots'''\n",
    "    fig, axs = plt.subplots(ncols=4)\n",
    "    fig.set_size_inches(20, 4)\n",
    "    for i,matrix in enumerate(matrices):\n",
    "        sns.heatmap(data = matrix, ax = axs[i], )\n",
    "        axs[i].set_xlabel(\"\")\n",
    "        axs[i].set_ylabel(\"\")\n",
    "        axs[i].set_yticks([])\n",
    "        axs[i].set_xticks([])\n",
    "    plt.show()\n",
    "    \n",
    "test_losses = []\n",
    "matrix_ids = random.sample(ids, 4)\n",
    "#matrix_ids = [988, 900, 2055, 444]\n",
    "# 63\"\"\n",
    "preds = []\n",
    "trues = []\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    y = sess.run([fm], feed_dict={inp: data_x[idx].reshape(1, 24, IMAGE_SIZE, IMAGE_SIZE, 14),\n",
    "                                  length: lengths[idx].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  labels: data_y[idx].reshape(1, 14, 14)\n",
    "                                  })\n",
    "    y = np.array(y).reshape(12, 12)\n",
    "    if LABEL_SIZE == 16:\n",
    "        y = y[1:15, 1:15]\n",
    "    #y[np.where(y < 0.05)] = 0\n",
    "    preds.append(y)\n",
    "    true = data_y[idx].reshape(14, 14)\n",
    "\n",
    "    trues.append(true)\n",
    "    \n",
    "multiplot(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot(trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight[68:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
