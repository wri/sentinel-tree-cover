{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.load(\"../../tile_data/processed/data_x_l2a_processed.npy\")\n",
    "data_y = np.load(\"../../tile_data/processed/data_y_l2a_processed.npy\")\n",
    "lengths = np.load(\"../../tile_data/processed/length_l2a_processed.npy\")\n",
    "\n",
    "data_x = np.delete(data_x, 14, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize\n",
    "for sample in tnrange(0, len(data_x)):\n",
    "    filtered = median_filter(data_x[sample, 0, :, :, 10], size = 5)\n",
    "    filtered = np.reshape(filtered, (8, 2, 8, 2))\n",
    "    filtered = np.mean(filtered, axis = (1, 3))\n",
    "    filtered = resize(filtered, (16, 16), 0)\n",
    "    data_x[sample, :, :, :, 10] = np.stack([filtered] * 24)\n",
    "    \n",
    "#data_x = np.delete(data_x, 10, -1)\n",
    "print(data_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "below_1 = [i for i, val in enumerate(data_x[:, :, :, :, :-2]) if np.min(val) < -1.5]\n",
    "above_1 = [i for i, val in enumerate(data_x[:, :, :, :, :-2]) if np.max(val) > 1.5]\n",
    "min_vals = [np.min(val) for i, val in enumerate(data_x[:, :, :, :, :-2]) if np.min(val) < -1.5]\n",
    "max_vals = [np.max(val) for i, val in enumerate(data_x[:, :, :, :, :-2]) if np.max(val) > 1.5]\n",
    "nans = [i for i, val in enumerate(data_x) if np.sum(np.isnan(val)) > 0]\n",
    "oob_vals = [i for i, val in enumerate(data_x) if np.max(val[:, :, :, 0]) > 0.7]\n",
    "print(oob_vals)\n",
    "\n",
    "outliers = below_1 + above_1 + nans + oob_vals\n",
    "outliers = list(set(outliers))\n",
    "print(\"The outliers are: {}, totalling {}\".format(outliers, len(outliers)))\n",
    "print(\"\\n\")\n",
    "print(min_vals, max_vals)\n",
    "data_x = data_x[[x for x in range(0, len(data_x)) if x not in outliers]]\n",
    "data_y = data_y[[x for x in range(0, len(data_y)) if x not in outliers]]\n",
    "lengths = lengths[[x for x in range(0, len(lengths)) if x not in outliers]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_all = [0.020395646858960392,\n",
    " 0.03833778686821461,\n",
    " 0.022913980782032013,\n",
    " 0.08838867269456387,\n",
    " 0.05731564749032259,\n",
    " 0.0796274145692587,\n",
    " 0.08639285631477833,\n",
    " 0.08918419919908048,\n",
    " 0.07416137464344502,\n",
    " 0.04398707151412964,\n",
    " 0.0,\n",
    " 0.008659180235117674,\n",
    " -0.0134716229327023,\n",
    " 0.006902076792903247,\n",
    " 0.006362500241957605,\n",
    " 4.999999873689376e-05]\n",
    "\n",
    "max_all = [0.18219037026166907,\n",
    " 0.27367106080055237,\n",
    " 0.3996005910634992,\n",
    " 0.4859166720509528,\n",
    " 0.4492199122905731,\n",
    " 0.46700127094984045,\n",
    " 0.49433933556079857,\n",
    " 0.5116616946458816,\n",
    " 0.6353135156631469,\n",
    " 0.5636022371053693,\n",
    " 0.3590170443058014,\n",
    " 0.693415229320526,\n",
    " 0.3948741647601127,\n",
    " 0.6290205121040344,\n",
    " 0.8152220940589849,\n",
    " 0.14367499947547913]\n",
    "\n",
    "\n",
    "\n",
    "min_all = []\n",
    "max_all = []\n",
    "\n",
    "for band in range(0, data_x.shape[-1]):\n",
    "    mins = np.percentile(data_x[:, :, :, :, band], 1)\n",
    "    maxs = np.percentile(data_x[:, :, :, :, band], 99)\n",
    "    #mins, maxs = (np.min(data_x[:, :, :, :, band]), np.max(data_x[:, :, :, :, band]))\n",
    "    #mins = min_all[band]\n",
    "    #maxs = max_all[band]\n",
    "    data_x[:, :, :, :, band] = np.clip(data_x[:, :, :, :, band], mins, maxs)\n",
    "    midrange = (maxs + mins) / 2\n",
    "    rng = maxs - mins\n",
    "    standardized = (data_x[:, :, :, :, band] - midrange) / (rng / 2)\n",
    "    data_x[:, :, :, :, band] = standardized\n",
    "    \n",
    "    min_all.append(mins)\n",
    "    max_all.append(maxs)\n",
    "    \n",
    "print(\"The data has been scaled to [{}, {}]\".format(np.min(data_x), np.max(data_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.mean(data_x, axis = 1)\n",
    "data_x = data_x[:, 1:15, 1:15, :]\n",
    "data_x = np.reshape(data_x, (data_x.shape[0]*data_x.shape[1]*data_x.shape[2], data_x.shape[-1]))\n",
    "data_y = np.reshape(data_y, (data_y.shape[0]*14*14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.median(data_x, axis = 1)\n",
    "data_x = data_x[:, 1:15, 1:15, :]\n",
    "data_x = np.reshape(data_x, (data_x.shape[0]*data_x.shape[1]*data_x.shape[2], data_x.shape[-1]))\n",
    "data_y = np.reshape(data_y, (data_y.shape[0]*14*14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean + SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_x = np.mean(data_x, axis = 1)\n",
    "std_x = np.std(data_x, axis = 1)\n",
    "data_x = np.concatenate([mean_x, std_x], axis = -1)\n",
    "data_x = data_x[:, 1:15, 1:15, :]\n",
    "data_x = np.reshape(data_x, (data_x.shape[0]*data_x.shape[1]*data_x.shape[2], data_x.shape[-1]))\n",
    "data_y = np.reshape(data_y, (data_y.shape[0]*14*14))\n",
    "print(data_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quarterly means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.reshape(data_x, (data_x.shape[0], 4, 6, 16, 16, data_x.shape[-1]))\n",
    "data_x = np.mean(data_x, axis = 2)\n",
    "data_x = data_x[:, :, 1:15, 1:15, :]\n",
    "data_x = np.swapaxes(data_x, 1, 3)\n",
    "data_x = np.swapaxes(data_x, 1, 2)\n",
    "data_x = np.reshape(data_x, (data_x.shape[0]*data_x.shape[1]*data_x.shape[2], data_x.shape[-1]*data_x.shape[-2]))\n",
    "data_y = np.reshape(data_y, (data_y.shape[0]*14*14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.reshape(data_x, (data_x.shape[0], 12, 2, 16, 16, data_x.shape[-1]))\n",
    "data_x = np.mean(data_x, axis = 2)\n",
    "data_x = data_x[:, :, 1:15, 1:15, :]\n",
    "data_x = np.swapaxes(data_x, 1, 3)\n",
    "data_x = np.swapaxes(data_x, 1, 2)\n",
    "data_x = np.reshape(data_x, (data_x.shape[0]*data_x.shape[1]*data_x.shape[2], data_x.shape[-1]*data_x.shape[-2]))\n",
    "data_y = np.reshape(data_y, (data_y.shape[0]*14*14))\n",
    "print(data_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.load(\"../../tile_data/processed/test_x_l2a_processed.npy\")\n",
    "test_y = np.load(\"../../tile_data/processed/test_y_l2a_processed.npy\")\n",
    "test_lengths = np.load(\"../../tile_data/processed/test_length_l2a_processed.npy\")\n",
    "\n",
    "test_x = np.delete(test_x, 14, -1)\n",
    "#test_x = np.delete(test_x, 13, -1)\n",
    "\n",
    "\n",
    "below_1 = [i for i, val in enumerate(test_x[:, :, :, :, :-2]) if np.min(val) < -1.67]\n",
    "above_1 = [i for i, val in enumerate(test_x[:, :, :, :, :-2]) if np.max(val) > 1.67]\n",
    "min_vals = [np.min(val) for i, val in enumerate(test_x[:, :, :, :, :-2]) if np.min(val) < -1.5]\n",
    "max_vals = [np.max(val) for i, val in enumerate(test_x[:, :, :, :, :-2]) if np.max(val) > 1.5]\n",
    "nans = [i for i, val in enumerate(test_x) if np.sum(np.isnan(val)) > 0]\n",
    "outliers = below_1 + above_1 + nans\n",
    "outliers = list(set(outliers))\n",
    "print(\"The outliers are: {}, totalling {}\".format(outliers, len(outliers)))\n",
    "print(\"\\n\")\n",
    "print(min_vals, max_vals)\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "    mins = np.min(test_x[i, :, :, :, :])\n",
    "    maxs = np.max(test_x[i, :, :, :, :])\n",
    "    if mins < -1 or maxs > 1:\n",
    "        offender_max = np.argmax(np.max(test_x[i, :, :, :, :], (0, 1, 2)), -1)\n",
    "        offender_min = np.argmin(np.min(test_x[i, :, :, :, :], (0, 1, 2)), -1)\n",
    "        \n",
    "        print(\"{} Offender max/min: {} {}\".format(i, offender_max, offender_min))\n",
    "test_x = test_x[[x for x in range(0, len(test_x)) if x not in outliers]]\n",
    "test_y = test_y[[x for x in range(0, len(test_y)) if x not in outliers]]\n",
    "test_lengths = test_lengths[[x for x in range(0, len(test_lengths)) if x not in outliers]]\n",
    "\n",
    "for sample in tnrange(0, len(test_x)):\n",
    "    filtered = median_filter(test_x[sample, 0, :, :, 10], size = 5)\n",
    "    filtered = np.reshape(filtered, (8, 2, 8, 2))\n",
    "    filtered = np.mean(filtered, axis = (1, 3))\n",
    "    filtered = resize(filtered, (16, 16), 0)\n",
    "    test_x[sample, :, :, :, 10] = np.stack([filtered] * 24)\n",
    "\n",
    "#test_x = np.delete(test_x, 10, -1)\n",
    "    \n",
    "for band in range(0, test_x.shape[-1]):\n",
    "    mins = min_all[band]\n",
    "    maxs = max_all[band]\n",
    "    test_x[:, :, :, :, band] = np.clip(test_x[:, :, :, :, band], mins, maxs)\n",
    "    midrange = (maxs + mins) / 2\n",
    "    rng = maxs - mins\n",
    "    standardized = (test_x[:, :, :, :, band] - midrange) / (rng / 2)\n",
    "    test_x[:, :, :, :, band] = standardized\n",
    "    \n",
    "    \n",
    "print(\"The data has been scaled to [{}, {}]\".format(np.min(test_x), np.max(test_x)))\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.mean(test_x, axis = 1)\n",
    "test_x = test_x[:, 1:15, 1:15, :]\n",
    "test_x = np.reshape(test_x, (test_x.shape[0]*test_x.shape[1]*test_x.shape[2], test_x.shape[-1]))\n",
    "test_y = np.reshape(test_y, (test_y.shape[0]*14*14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.median(test_x, axis = 1)\n",
    "test_x = test_x[:, 1:15, 1:15, :]\n",
    "test_x = np.reshape(test_x, (test_x.shape[0]*test_x.shape[1]*test_x.shape[2], test_x.shape[-1]))\n",
    "test_y = np.reshape(test_y, (test_y.shape[0]*14*14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean + SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_x = np.mean(test_x, axis = 1)\n",
    "std_x = np.std(test_x, axis = 1)\n",
    "test_x = np.concatenate([mean_x, std_x], axis = -1)\n",
    "test_x = test_x[:, 1:15, 1:15, :]\n",
    "test_x = np.reshape(test_x, (test_x.shape[0]*test_x.shape[1]*test_x.shape[2], test_x.shape[-1]))\n",
    "test_y = np.reshape(test_y, (test_y.shape[0]*14*14))\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quarterly means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.reshape(test_x, (test_x.shape[0], 4, 6, 16, 16, test_x.shape[-1]))\n",
    "test_x = np.mean(test_x, axis = 2)\n",
    "test_x = test_x[:, :, 1:15, 1:15, :]\n",
    "test_x = np.swapaxes(test_x, 1, 3)\n",
    "test_x = np.swapaxes(test_x, 1, 2)\n",
    "test_x = np.reshape(test_x, (test_x.shape[0]*test_x.shape[1]*test_x.shape[2], test_x.shape[-1]*test_x.shape[-2]))\n",
    "test_y = np.reshape(test_y, (test_y.shape[0]*14*14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.reshape(test_x, (test_x.shape[0], 12, 2, 16, 16, test_x.shape[-1]))\n",
    "test_x = np.mean(test_x, axis = 2)\n",
    "test_x = test_x[:, :, 1:15, 1:15, :]\n",
    "test_x = np.swapaxes(test_x, 1, 3)\n",
    "test_x = np.swapaxes(test_x, 1, 2)\n",
    "print(test_x.shape)\n",
    "test_x = np.reshape(test_x, (test_x.shape[0]*test_x.shape[1]*test_x.shape[2], test_x.shape[-1]*test_x.shape[-2]))\n",
    "test_y = np.reshape(test_y, (test_y.shape[0]*14*14))\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "clf = LinearSVR(random_state=0, tol=1e-5, max_iter = 1e4)\n",
    "clf.fit(data_x, data_y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(test_x)\n",
    "preds[np.where(preds >= 0.23)] = 1.\n",
    "preds[np.where(preds < 0.23)] = 0. # 0.69 AFR, 0.57 India, 0.77 lac, \n",
    "trues = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(preds,\n",
    "                                            trues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues = test_y\n",
    "from sklearn.metrics import f1_score\n",
    "def compute_f1_score_at_tolerance(true, pred, tolerance = 1):\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    tp = np.zeros_like(true)\n",
    "    fp = np.zeros_like(true)\n",
    "    fn = np.zeros_like(true)\n",
    "    \n",
    "    \n",
    "    for x in range(true.shape[0]):\n",
    "        for y in range(true.shape[1]):\n",
    "            min_x = np.max([0, x-1])\n",
    "            min_y = np.max([0, y-1])\n",
    "            max_y = np.min([true.shape[0], y+2])\n",
    "            max_x = np.min([true.shape[0], x+2])\n",
    "            if true[x, y] == 1:\n",
    "                if np.sum(pred[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    tp[x, y] = 1\n",
    "                else:\n",
    "                    fn[x, y] = 1\n",
    "            if pred[x, y] == 1:\n",
    "                if np.sum(true[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    if true[x, y] == 1:\n",
    "                        tp[x, y] = 1\n",
    "                else:\n",
    "                    fp[x, y] = 1                \n",
    "                \n",
    "    precision =  np.sum(tp) / (np.sum(tp) + np.sum(fp))\n",
    "    recall = np.sum(tp) / (np.sum(tp) + np.sum(fn))\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    return np.sum(tp), np.sum(fp), np.sum(fn)\n",
    "\n",
    "\n",
    "\n",
    "tp = preds * trues\n",
    "fn = [1 if x > y else 0 for (x, y) in zip(trues, preds)]\n",
    "fp = [1 if y > x else 0 for (x, y) in zip(trues, preds)]\n",
    "tn = (len(test_y) * 196) - np.sum(tp) - np.sum(fn) - np.sum(fp)\n",
    "print(\"TP {}, FN {}, FP {}, TN {}\".format(np.sum(tp), sum(fn), np.sum(fp), tn))\n",
    "\n",
    "tps = []\n",
    "fns = []\n",
    "fps = []\n",
    "for i in range(0, len(test_y), 196):\n",
    "    tps.append(np.sum(tp[i:i+196]))\n",
    "    fns.append(np.sum(fn[i:i+196]))\n",
    "    fps.append(np.sum(fp[i:i+196]))\n",
    "    \n",
    "    \n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "true_canopy = []\n",
    "error_canopy = []\n",
    "pred_canopy = []\n",
    "f1_hard = []\n",
    "tp_softs = []\n",
    "fp_softs = []\n",
    "fn_softs = []\n",
    "for i in range(0, len(preds), 196):\n",
    "    true_canopy.append(np.sum(trues[i:i+196]) / 1.96)\n",
    "    error_canopy.append(abs(np.sum(preds[i:i+196]) - np.sum(trues[i:i+196])) / 1.96)\n",
    "    pred_canopy.append(np.sum(preds[i:i+196]) / 1.96)\n",
    "    f1_hard.append(f1_score(trues[i:i+196], preds[i:i+196]))\n",
    "    tp_soft, fp_soft, fn_soft = compute_f1_score_at_tolerance(np.array(trues[i:i+196].reshape((14, 14))),\n",
    "                                                 np.array(preds[i:i+196].reshape((14, 14))))\n",
    "    tp_softs.append(tp_soft)\n",
    "    fp_softs.append(fp_soft)\n",
    "    fn_softs.append(fn_soft)\n",
    "    \n",
    "precision = np.sum(tp_softs) / (np.sum(tp_softs) + np.sum(fp_softs))\n",
    "recall = np.sum(tp_softs) / (np.sum(tp_softs) + np.sum(fn_softs))\n",
    "print(precision, recall, np.mean(error_canopy), np.mean(true_canopy), np.mean(pred_canopy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../tile_data/ghana_cocoa/processed/\"\n",
    "\n",
    "def standardize_tile(arr):\n",
    "    for x in range(0, arr.shape[-1]):\n",
    "        mins = min_all[x]\n",
    "        maxs = max_all[x]\n",
    "        arr[:, :, :, x] = np.clip(arr[:, :, :, x], mins, maxs)\n",
    "        midrange = (maxs + mins) / 2\n",
    "        rng = maxs - mins\n",
    "        standardized = (arr[:, :, :, x] - midrange) / (rng / 2)\n",
    "        arr[:, :, :, x] = standardized\n",
    "        mins, maxs = (np.min(arr[:, :, :, x]), np.max(arr[:, :, :, x]))\n",
    "    return arr\n",
    "\n",
    "def predict_tile(y, x, path = path):\n",
    "    tile_x = np.load(path + str(y) + \"/\" + str(x) + \".npy\")\n",
    "    tile_x = np.delete(tile_x, 14, -1)\n",
    "    tile_x = standardize_tile(tile_x)\n",
    "    tile_x = np.mean(tile_x, axis = 0)\n",
    "    tile_x = tile_x[1:-1, 1:-1, :]\n",
    "     \n",
    "    tile_x = np.reshape(tile_x, (tile_x.shape[0]*tile_x.shape[1], tile_x.shape[-1]))\n",
    "    tile_y = clf.predict(tile_x)\n",
    "    tile_y = np.reshape(tile_y, (126, 126))\n",
    "    return tile_y\n",
    "\n",
    "tile_y = predict_tile(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "for row in tnrange(0, 5):\n",
    "    pred_i = [predict_tile(row, col) for col in range(0, 5)]\n",
    "    pred_i = np.concatenate(pred_i, axis = 1)\n",
    "    all_preds.append(pred_i)\n",
    "all_preds.reverse()\n",
    "stacked = np.concatenate(all_preds, axis = 0)\n",
    "#stacked = stacked[400:600, 400:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 17))\n",
    "#stacked[np.where(stacked >= 0.35)] = 1.\n",
    "#stacked[np.where(stacked < 0.35)] = 0.\n",
    "sns.heatmap(stacked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 17))\n",
    "stacked[np.where(stacked >= 0.4)] = 1.\n",
    "stacked[np.where(stacked < 0.4)] = 0.\n",
    "sns.heatmap(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = stacked[250:450, 350:450]\n",
    "plt.figure(figsize=(10, 15))\n",
    "sns.heatmap(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def compute_f1_score_at_tolerance(true, pred, tolerance = 1):\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    tp = np.zeros_like(true)\n",
    "    fp = np.zeros_like(true)\n",
    "    fn = np.zeros_like(true)\n",
    "    \n",
    "    \n",
    "    for x in range(true.shape[0]):\n",
    "        for y in range(true.shape[1]):\n",
    "            min_x = np.max([0, x-1])\n",
    "            min_y = np.max([0, y-1])\n",
    "            max_y = np.min([true.shape[0], y+2])\n",
    "            max_x = np.min([true.shape[0], x+2])\n",
    "            if true[x, y] == 1:\n",
    "                if np.sum(pred[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    tp[x, y] = 1\n",
    "                else:\n",
    "                    fn[x, y] = 1\n",
    "            if pred[x, y] == 1:\n",
    "                if np.sum(true[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    if true[x, y] == 1:\n",
    "                        tp[x, y] = 1\n",
    "                else:\n",
    "                    fp[x, y] = 1    \n",
    "    return np.sum(tp), np.sum(fp), np.sum(fn)\n",
    "\n",
    "tp = preds * trues\n",
    "fn = [1 if x > y else 0 for (x, y) in zip(trues, preds)]\n",
    "fp = [1 if y > x else 0 for (x, y) in zip(trues, preds)]\n",
    "tn = (len(test_y) * 196) - np.sum(tp) - np.sum(fn) - np.sum(fp)\n",
    "print(\"TP {}, FN {}, FP {}, TN {}\".format(np.sum(tp), sum(fn), np.sum(fp), tn))\n",
    "\n",
    "tps = []\n",
    "fns = []\n",
    "fps = []\n",
    "for i in range(0, len(test_y), 196):\n",
    "    tps.append(np.sum(tp[i:i+196]))\n",
    "    fns.append(np.sum(fn[i:i+196]))\n",
    "    fps.append(np.sum(fp[i:i+196]))\n",
    "    \n",
    "    \n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "true_canopy = []\n",
    "error_canopy = []\n",
    "pred_canopy = []\n",
    "f1_hard = []\n",
    "tp_softs = []\n",
    "fp_softs = []\n",
    "fn_softs = []\n",
    "for i in range(0, len(preds), 196):\n",
    "    true_canopy.append(np.sum(trues[i:i+196]) / 1.96)\n",
    "    error_canopy.append(abs(np.sum(preds[i:i+196]) - np.sum(trues[i:i+196])) / 1.96)\n",
    "    pred_canopy.append(np.sum(preds[i:i+196]) / 1.96)\n",
    "    f1_hard.append(f1_score(trues[i:i+196], preds[i:i+196]))\n",
    "    tp_soft, fp_soft, fn_soft = compute_f1_score_at_tolerance(np.array(trues[i:i+196].reshape((14, 14))),\n",
    "                                                 np.array(preds[i:i+196].reshape((14, 14))))\n",
    "    tp_softs.append(tp_soft)\n",
    "    fp_softs.append(fp_soft)\n",
    "    fn_softs.append(fn_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "metrics = pd.DataFrame({'true': true_canopy,\n",
    "                        'pred': pred_canopy,\n",
    "                        'f1_hard': f1_hard,\n",
    "                        'error': error_canopy,\n",
    "                        'tp': tps,\n",
    "                        'fp': fps,\n",
    "                        'fn': fns,\n",
    "                        'tp_soft': tp_softs,\n",
    "                        'fp_soft': fp_softs,\n",
    "                        'fn_soft': fn_softs,\n",
    "                       })\n",
    "\n",
    "res = map(lambda x: int(math.floor(np.min([x, 90]) / 10.0)) * 10, true_canopy)\n",
    "res = [x for x in res]\n",
    "metrics['group'] = res\n",
    "metrics['model'] = 'SVM'\n",
    "\n",
    "metrics.to_csv(\"../../data/metrics/SVM-sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_precisions = metrics.groupby('group').apply(lambda x: (np.sum(x.tp) / np.sum(x.tp + x.fp)))\n",
    "hard_recalls = metrics.groupby('group').apply(lambda x: (np.sum(x.tp) / np.sum(x.tp + x.fn)))\n",
    "errors = metrics.groupby('group').apply(lambda x: np.mean(x.error))\n",
    "hard_f1 = 2 *  ((hard_precisions * hard_recalls) / (hard_precisions + hard_recalls))\n",
    "\n",
    "precisions = metrics.groupby('group').apply(lambda x: (np.sum(x.tp_soft) / np.sum(x.tp_soft + x.fp_soft)))\n",
    "recalls = metrics.groupby('group').apply(lambda x: (np.sum(x.tp_soft) / np.sum(x.tp_soft + x.fn_soft)))\n",
    "soft_f1 = 2 *  ((precisions * recalls) / (precisions + recalls))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metrics = pd.DataFrame({'group': [x for x in range(0, 100, 10)],\n",
    "                            'hard_rec': hard_recalls,\n",
    "                            'soft_rec': recalls,\n",
    "                            'hard_prec': hard_precisions,\n",
    "                            'soft_prec': precisions,\n",
    "                            'hard_f1': hard_f1,\n",
    "                            'soft_f1': soft_f1,\n",
    "                            'error': errors,\n",
    "                            'model': 'SVM'\n",
    "                           })\n",
    "\n",
    "new_metrics.to_csv(\"../../data/metrics/svm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
