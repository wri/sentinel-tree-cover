{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tile test data for smooth interpolation prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "%run ../src/preprocessing/slope.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/dsen2/utils/DSen2Net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import splu\n",
    "import scipy.sparse as sparse\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDL_PATH = \"../src/dsen2/models/\"\n",
    "INPUT_SHAPE = ((4, None, None), (6, None, None))\n",
    "MODEL = s2model(INPUT_SHAPE, num_layers=6, feature_size=128)\n",
    "PREDICT_FILE = MDL_PATH+'s2_032_lr_1e-04.hdf5'\n",
    "MODEL.load_weights(PREDICT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_arr = np.array([[1, 1, 1, 1, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 2, 3, 2, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 1, 1, 1, 1,],])\n",
    "                  \n",
    "c_arr = c_arr / 3\n",
    "o_arr = 1 - c_arr\n",
    "c_arr = np.tile(c_arr[:, :, np.newaxis], (1, 1, 11))\n",
    "o_arr = np.tile(o_arr[:, :, np.newaxis], (1, 1, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = [x for x in os.listdir(\"../data/test-s2/\") if x in os.listdir(\"../data/test-s1\")]\n",
    "data = np.load(\"../data/raw/test-raw/{}\".format(test_files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data[0, :, :, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_lookups = { #'x_start, y_start'\n",
    "    'center': [8, 8],\n",
    "    'left': [0, 8],\n",
    "    'right': [16, 8],\n",
    "    'up': [8, 16],\n",
    "    'down': [8, 0],\n",
    "    'ul': [16, 0],\n",
    "    'ur': [16, 16],\n",
    "    'dl': [0, 0],\n",
    "    'dr': [16, 0],\n",
    "}\n",
    "\n",
    "IMSIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tile_lookups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missed_clouds(img):\n",
    "    \"\"\" Removes steps that are likely to be missed cloud or shadows\n",
    "        based on two interquartile ranges for the near infrared band\n",
    "        \n",
    "        Parameters:\n",
    "         img (arr):\n",
    "\n",
    "        Returns:\n",
    "         to_remove (list): \n",
    "    \"\"\"\n",
    "    iqr = np.percentile(img[:, :, :, 3].flatten(), 75) - np.percentile(img[:, :, :, 3].flatten(), 25)\n",
    "    thresh_t = np.percentile(img[:, :, :, 3].flatten(), 75) + iqr*2\n",
    "    thresh_b = np.percentile(img[:, :, :, 3].flatten(), 25) - iqr*2\n",
    "    diffs_fw = np.diff(img, 1, axis = 0)\n",
    "    diffs_fw = np.mean(diffs_fw, axis = (1, 2, 3))\n",
    "    diffs_fw = np.array([0] + list(diffs_fw))\n",
    "    diffs_bw = np.diff(np.flip(img, 0), 1, axis = 0)\n",
    "    diffs_bw = np.flip(np.mean(diffs_bw, axis = (1, 2, 3)))\n",
    "    diffs_bw = np.array(list(diffs_bw) + [0])\n",
    "    diffs = abs(diffs_fw - diffs_bw) * 100 # 3, -3 -> 6, -3, 3 -> 6, -3, -3\n",
    "    #diffs = [int(x) for x in diffs]\n",
    "    outlier_percs = []\n",
    "    for step in range(img.shape[0]):\n",
    "        bottom = len(np.argwhere(img[step, :, :, 3].flatten() > thresh_t))\n",
    "        top = len(np.argwhere(img[step, :, :, 3].flatten() < thresh_b))\n",
    "        p = 100* ((bottom + top) / (48*48))\n",
    "        outlier_percs.append(p)\n",
    "    print(outlier_percs)\n",
    "    to_remove = np.argwhere(np.array(outlier_percs) > 15)\n",
    "    return to_remove\n",
    "\n",
    "def remove_bad_steps(img, probs, shadows, image_dates):\n",
    "    shadow_sums = np.sum(shadows, axis = (1, 2))\n",
    "    shadow_steps = np.argwhere(shadow_sums > (48*48/3))\n",
    "    #probs = np.mean(probs, axis = (1, 2))\n",
    "    args = np.array([len(np.argwhere(probs[x].flatten() > 0.3)) for x in range(probs.shape[0])])\n",
    "    dirty_steps = np.argwhere(args > (48)*(48) / 5)\n",
    "    missing_images = [np.argwhere(img[x, :, : :].flatten() == 0.0) for x in range(img.shape[0])]\n",
    "    missing_images = np.array([len(x) for x in missing_images])\n",
    "    missing_images_p = [np.argwhere(img[x, :, : :].flatten() >= 1) for x in range(img.shape[0])]\n",
    "    missing_images_p = np.array([len(x) for x in missing_images_p])\n",
    "    missing_images += missing_images_p\n",
    "    missing_images = list(np.argwhere(missing_images >= 25))\n",
    "    to_remove = np.unique(np.array(list(dirty_steps) + list(missing_images) + list(shadow_steps)))\n",
    "\n",
    "    # Remove null steps\n",
    "    print(\"There are {}/{} dirty steps: {}\"\n",
    "          \" cloud, {} missing, {} shadow\".format(len(to_remove),\n",
    "                                                 len(img), len(dirty_steps),\n",
    "                                                 len(missing_images),\n",
    "                                                 len(shadow_steps)))\n",
    "\n",
    "    img = np.delete(img, to_remove, 0)\n",
    "    probs = np.delete(probs, to_remove, 0)\n",
    "    image_dates = np.delete(image_dates, to_remove)\n",
    "    shadows = np.delete(shadows, to_remove, 0)\n",
    "\n",
    "    to_remove = remove_missed_clouds(img)\n",
    "    img = np.delete(img, to_remove, 0)\n",
    "    probs = np.delete(probs, to_remove, 0)\n",
    "    image_dates = np.delete(image_dates, to_remove)\n",
    "    shadows = np.delete(shadows, to_remove, 0)\n",
    "    print(\"Removing {} steps based on ratio\".format(len(to_remove)))\n",
    "    return img, probs, image_dates, shadows\n",
    "    \n",
    "def DSen2(d10, d20):\n",
    "    \"\"\"Super resolves 20 meter bans using the DSen2 convolutional\n",
    "       neural network, as specified in Lanaras et al. 2018\n",
    "       https://github.com/lanha/DSen2\n",
    "\n",
    "        Parameters:\n",
    "         d10 (arr): (4, X, Y) shape array with 10 meter resolution\n",
    "         d20 (arr): (6, X, Y) shape array with 20 meter resolution\n",
    "\n",
    "        Returns:\n",
    "         prediction (arr): (6, X, Y) shape array with 10 meter superresolved\n",
    "                          output of DSen2 on d20 array\n",
    "    \"\"\"\n",
    "    test = [d10, d20]\n",
    "    input_shape = ((4, None, None), (6, None, None))\n",
    "    prediction = _predict(test, input_shape, deep=False)\n",
    "    #prediction *= 5\n",
    "    return prediction\n",
    "\n",
    "def _predict(test, input_shape, model = MODEL, deep=False, run_60=False):\n",
    "    \n",
    "    prediction = model.predict(test, verbose=1)\n",
    "    return prediction\n",
    "\n",
    "def calculate_proximal_steps_index(date, satisfactory):\n",
    "    \"\"\"Returns proximal steps that are cloud and shadow free\n",
    "\n",
    "         Parameters:\n",
    "          date (int): current time step\n",
    "          satisfactory (list): time steps with no clouds or shadows\n",
    "\n",
    "         Returns:\n",
    "          arg_before (str): index of the prior clean image\n",
    "          arg_after (int): index of the next clean image\n",
    "    \"\"\"\n",
    "    arg_before, arg_after = None, None\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    return arg_before, arg_after\n",
    "\n",
    "def speyediff(N, d, format = 'csc'):\n",
    "    \"\"\"Calculates the d-th order sparse difference matrix based on \n",
    "       an initial N x N identity matrix\n",
    "\n",
    "         Parameters:\n",
    "          N (int): input length\n",
    "          d (int): smoothing order\n",
    "\n",
    "         Returns:\n",
    "          spmat (arr): sparse difference matrix\n",
    "    \"\"\"\n",
    "    shape = (N-d, N)\n",
    "    diagonals = np.zeros(2*d + 1)\n",
    "    diagonals[d] = 1.\n",
    "    for i in range(d):\n",
    "        diff = diagonals[:-1] - diagonals[1:]\n",
    "        diagonals = diff\n",
    "    offsets = np.arange(d+1)\n",
    "    spmat = sparse.diags(diagonals, offsets, shape, format = format)\n",
    "    return spmat\n",
    "\n",
    "def smooth(y, lmbd, d = 2):\n",
    "    \"\"\"Calculates the whittaker smoother on input array\n",
    "\n",
    "         Parameters:\n",
    "          y (arr): 1-dimensional input array\n",
    "          lmbd (int): degree of smoothing, higher is more\n",
    "\n",
    "         Returns:\n",
    "          z (arr): smoothed version of y\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    E = sparse.eye(m, format = 'csc')\n",
    "    D = speyediff(m, d, format = 'csc')\n",
    "    coefmat = E + lmbd * D.conj().T.dot(D)\n",
    "    z = splu(coefmat).solve(y)\n",
    "    return z\n",
    "\n",
    "\n",
    "def superresolve(arr):\n",
    "    print(\"Shape before super: {}\".format(arr.shape))\n",
    "\n",
    "    d10 = arr[:, :, :, 0:4]\n",
    "    d20 = arr[:, :, :, 4:10]\n",
    "\n",
    "    d10 = np.swapaxes(d10, 1, -1)\n",
    "    d10 = np.swapaxes(d10, 2, 3)\n",
    "    d20 = np.swapaxes(d20, 1, -1)\n",
    "    d20 = np.swapaxes(d20, 2, 3)\n",
    "    superresolved = DSen2(d10, d20)\n",
    "    superresolved = np.swapaxes(superresolved, 1, -1)\n",
    "    superresolved = np.swapaxes(superresolved, 1, 2)\n",
    "    print(superresolved.shape)\n",
    "    print(arr.shape)\n",
    "\n",
    "    # returns band IDXs 3, 4, 5, 7, 8, 9\n",
    "    arr[:, :, :, 4:10] = superresolved\n",
    "    print(\"Shape after super: {}\".format(arr.shape))\n",
    "    return arr\n",
    "\n",
    "def calculate_and_save_best_images(img_bands, image_dates):\n",
    "    \"\"\" Interpolate input data of (Time, X, Y, Band) to a constant\n",
    "        (72, X, Y, Band) shape with one time step every five days\n",
    "        \n",
    "        Parameters:\n",
    "         img_bands (arr):\n",
    "         image_dates (list):\n",
    "         \n",
    "        Returns:\n",
    "         keep_steps (arr):\n",
    "         max_distance (int)\n",
    "    \"\"\"\n",
    "    biweekly_dates = [day for day in range(0, 360, 5)] # ideal imagery dates are every 15 days\n",
    "    \n",
    "    # Clouds have been removed at this step, so all steps are satisfactory\n",
    "    satisfactory_ids = [x for x in range(0, img_bands.shape[0])]\n",
    "    satisfactory_dates = [value for idx, value in enumerate(image_dates) if idx in satisfactory_ids]\n",
    "    \n",
    "    \n",
    "    selected_images = {}\n",
    "    for i in biweekly_dates:\n",
    "        distances = [abs(date - i) for date in satisfactory_dates]\n",
    "        closest = np.min(distances)\n",
    "        closest_id = np.argmin(distances)\n",
    "        # If there is imagery within 5 days, select it\n",
    "        if closest < 8:\n",
    "            date = satisfactory_dates[closest_id]\n",
    "            image_idx = int(np.argwhere(np.array(image_dates) == date)[0])\n",
    "            selected_images[i] = {'image_date': [date], 'image_ratio': [1], 'image_idx': [image_idx]}\n",
    "        # If there is not imagery within 7 days, look for the closest above and below imagery\n",
    "        else:\n",
    "            distances = np.array([(date - i) for date in satisfactory_dates])\n",
    "            # Number of days above and below the selected date of the nearest clean imagery\n",
    "            above = distances[np.where(distances < 0, distances, -np.inf).argmax()]\n",
    "            below = distances[np.where(distances > 0, distances, np.inf).argmin()]\n",
    "            if abs(above) > 240: # If date is the last date, occassionally argmax would set above to - number\n",
    "                above = below\n",
    "            if abs(below) > 240:\n",
    "                below = above\n",
    "            if above != below:\n",
    "                below_ratio = above / (above - below)\n",
    "                above_ratio = 1 - below_ratio\n",
    "            else:\n",
    "                above_ratio = below_ratio = 0.5\n",
    "                \n",
    "            # Extract the image date and imagery index for the above and below values\n",
    "            above_date = i + above\n",
    "            above_image_idx = int(np.argwhere(np.array(image_dates) == above_date)[0])\n",
    "            \n",
    "            below_date = i + below\n",
    "            below_image_idx = int(np.argwhere(np.array(image_dates) == below_date)[0])\n",
    "            \n",
    "            selected_images[i] = {'image_date': [above_date, below_date], 'image_ratio': [above_ratio, below_ratio],\n",
    "                                 'image_idx': [above_image_idx, below_image_idx]}\n",
    "                               \n",
    "    max_distance = 0\n",
    "    \n",
    "    for i in selected_images.keys():\n",
    "        #print(i, selected_images[i])\n",
    "        if len(selected_images[i]['image_date']) == 2:\n",
    "            dist = selected_images[i]['image_date'][1] - selected_images[i]['image_date'][0]\n",
    "            if dist > max_distance:\n",
    "                max_distance = dist\n",
    "    \n",
    "    print(\"Maximum time distance: {}\".format(max_distance))\n",
    "        \n",
    "    # Compute the weighted average of the selected imagery for each time step\n",
    "    keep_steps = []\n",
    "    use_median = False\n",
    "    for i in selected_images.keys():\n",
    "        step1_additional = None\n",
    "        step2_additional = None\n",
    "        info = selected_images[i]\n",
    "        if len(info['image_idx']) == 1:\n",
    "            step = img_bands[info['image_idx'][0]]\n",
    "        if len(info['image_idx']) == 2:\n",
    "            step1 = img_bands[info['image_idx'][0]] # * info['image_ratio'][0]\n",
    "            step2 = img_bands[info['image_idx'][1]]\n",
    "            step = step1 * 0.5 + step2 * 0.5\n",
    "        keep_steps.append(step)\n",
    "        \n",
    "    keep_steps = np.stack(keep_steps)\n",
    "    return keep_steps, max_distance\n",
    "\n",
    "def remove_cloud_and_shadows(tiles, probs, shadows, image_dates, wsize = 5):\n",
    "    \"\"\" Interpolates clouds and shadows for each time step with \n",
    "        linear combination of proximal clean time steps for each\n",
    "        region of specified window size\n",
    "        \n",
    "        Parameters:\n",
    "         tiles (arr):\n",
    "         probs (arr): \n",
    "         shadows (arr):\n",
    "         image_dates (list):\n",
    "         wsize (int): \n",
    "    \n",
    "        Returns:\n",
    "         tiles (arr): \n",
    "    \"\"\"\n",
    "    c_probs = np.copy(probs)\n",
    "    c_probs = c_probs - np.min(c_probs, axis = 0)\n",
    "    c_probs[np.where(c_probs > 0.33)] = 1.\n",
    "    c_probs[np.where(c_probs < 0.33)] = 0.\n",
    "    c_probs = np.reshape(c_probs, [c_probs.shape[0], int(48/8), 8, int(48/8), 8])\n",
    "    c_probs = np.sum(c_probs, (2, 4))\n",
    "    c_probs = resize(c_probs, (c_probs.shape[0], 48, 48), 0)\n",
    "    c_probs[np.where(c_probs < 12)] = 0.\n",
    "    c_probs[np.where(c_probs >= 12)] = 1.\n",
    "    c_probs += shadows\n",
    "    c_probs[np.where(c_probs >= 1.)] = 1.\n",
    "    n_interp = 0\n",
    "    for cval in range(0, 48 - 5, 1):\n",
    "        for rval in range(0, 48 - 5, 1):\n",
    "            subs = c_probs[:, cval:cval + wsize, rval:rval+wsize]\n",
    "            satisfactory = [x for x in range(c_probs.shape[0]) if np.sum(subs[x, :, :]) < 10]\n",
    "            satisfactory = np.array(satisfactory)\n",
    "            for date in range(0, tiles.shape[0]):\n",
    "                if np.sum(subs[date, :, :]) > 10:\n",
    "                    n_interp += 1\n",
    "                    before, after = calculate_proximal_steps_index(date, satisfactory)\n",
    "                    before = date + before\n",
    "                    after = date + after\n",
    "                    if after >= tiles.shape[0]:\n",
    "                        after = before\n",
    "                    if before < 0:\n",
    "                        before = after\n",
    "                    bef = tiles[before, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    aft = tiles[after, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    before = image_dates[before]\n",
    "                    after = image_dates[after]\n",
    "                    before_diff = abs(image_dates[date] - before)\n",
    "                    after_diff = abs(image_dates[date] - after)\n",
    "                    bef_wt = 1 - before_diff / (before_diff + after_diff)\n",
    "                    aft_wt = 1 - bef_wt\n",
    "                    candidate = bef_wt*bef + aft_wt*aft\n",
    "                    candidate = candidate*c_arr + tiles[date, cval:cval+wsize, rval:rval+wsize, : ]*o_arr\n",
    "                    tiles[date, cval:cval+wsize, rval:rval+wsize, : ] = candidate  \n",
    "    print(\"Interpolated {} px\".format(n_interp))\n",
    "    return tiles\n",
    "\n",
    "\n",
    "def process_array(plot_id):\n",
    "    starting_days = np.cumsum([0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30])\n",
    "    YEAR = 2019\n",
    "    s2 = np.load('../data/raw/test-raw/{}.npy'.format(plot_id))\n",
    "    shadows = np.load('../data/raw/test-shadows/{}.npy'.format(plot_id))\n",
    "    clouds = np.load('../data/raw/test-clouds/{}.npy'.format(plot_id))\n",
    "    dates = np.load('../data/raw/test-dates/{}.npy'.format(plot_id), allow_pickle = True)\n",
    "    s1 = np.load(\"../data/raw/test-s1/{}.npy\".format(plot_id))\n",
    "    dem = np.load(\"../data/raw/test-dem/{}.npy\".format(plot_id))\n",
    "    \n",
    "    print(\"BEF\", s2.shape)\n",
    "    print(\"BEF\", clouds.shape)\n",
    "    print(\"BEF\", dates.shape)\n",
    "    image_dates = []\n",
    "    for date in dates:\n",
    "        if date.year == YEAR - 1:\n",
    "            image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == YEAR:\n",
    "            image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == YEAR + 1:\n",
    "            image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    dates = np.array(image_dates)\n",
    "    \n",
    "    print(\"BEF\", s2.shape)\n",
    "    print(\"BEF\", clouds.shape)\n",
    "    # subset time steps\n",
    "    s2, clouds, dates, shadows = remove_bad_steps(s2, clouds, shadows, dates)\n",
    "    print(\"AFT\", s2.shape)\n",
    "    print(\"AFT\", clouds.shape)\n",
    "    \n",
    "    # Concatenate DEM\n",
    "    dem = np.tile(dem.reshape((1, 48, 48, 1)), (s2.shape[0], 1, 1, 1))\n",
    "    s2 = np.concatenate([s2, dem], axis = -1)\n",
    "    s2[:, :, :, -1] /= 90\n",
    "    \n",
    "    # remove clouds and shadows\n",
    "    print(s2.shape)\n",
    "    print(clouds.shape)\n",
    "    print(shadows.shape)\n",
    "    s2 = remove_cloud_and_shadows(s2, clouds, shadows, dates)\n",
    "    \n",
    "    # super resolve\n",
    "    s2 = superresolve(s2)\n",
    "    \n",
    "    \n",
    "    # indices\n",
    "    s2, amin = evi(s2, True)\n",
    "    s2 = bi(s2, True)\n",
    "    s2 = msavi2(s2, True)\n",
    "    s2 = si(s2, True)\n",
    "    \n",
    "    s2 = s2[:, 8:40, 8:40, :]\n",
    "\n",
    "\n",
    "    \n",
    "    # whittaker smooth\n",
    "    # Smooth linear interpolation\n",
    "    for row in range(0, 32):\n",
    "        for column in range(0, 32):\n",
    "            for band in [x for x in range(0, 15) if x != 10]:\n",
    "                sm = smooth(s2[:, row, column, band], 800, d = 2)\n",
    "                s2[:, row, column, band] = sm\n",
    "    \n",
    "    s2, _ = calculate_and_save_best_images(s2, dates)\n",
    "    biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "    to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "    s2 = np.delete(s2, to_remove, 0)\n",
    "    fused = np.concatenate([s1, s2], axis = -1)\n",
    "    # save fused data\n",
    "    np.save(\"../data/raw/test-processed/{}.npy\".format(plot_id), fused)\n",
    "    #print(fused.shape)\n",
    "    #windows = make_5d_array(fused, tile_lookups)\n",
    "    #return windows\n",
    "    # return (24, 32, 32, 16) array\n",
    "\n",
    "def make_5d_array(arr, tile_lookups):\n",
    "    arr_5d = np.empty((9, 24, 16, 16, 17))\n",
    "    print(arr.shape)\n",
    "    for i in range(len(tile_lookups.keys())):\n",
    "        key = list(tile_lookups.keys())[i]\n",
    "        start_x = tile_lookups[key][0]\n",
    "        start_y = tile_lookups[key][1]\n",
    "        arr_5d[i] = arr[:24, start_x:start_x+16, start_y:start_y+16, :]\n",
    "    return arr_5d\n",
    "\n",
    "def reconstruct_array(arr):\n",
    "    \n",
    "    out = np.copy(arr[0])\n",
    "    \n",
    "    center = arr[0]\n",
    "    left = arr[1]\n",
    "    right = arr[2]\n",
    "    up = arr[3]\n",
    "    down = arr[4]\n",
    "    ul = arr[5]\n",
    "    ur = arr[6]\n",
    "    dl = arr[7]\n",
    "    dr = arr[8]\n",
    "    \n",
    "    out[:, :8, :, :] = (center[:, :8, :, :] + left[:, 8:, :, :]) / 2\n",
    "    print(np.sum(out) - np.sum(center))\n",
    "    out[:, 8:, :, :] = (center[:, 8:, :, :] + right[:, :8, :, :]) / 2\n",
    "    print(np.sum(out) - np.sum(center))\n",
    "    out[:, :, 8:, :] = (center[:, :, 8:, :] + up[:, :, :8, :]) / 2\n",
    "    print(np.sum(out) - np.sum(center))\n",
    "    out[:, :, :8, :] = (center[:, :, :8, :] + down[:, :, 8:, :]) / 2\n",
    "    print(np.sum(out) - np.sum(center))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_process = [x for x in os.listdir(\"../data/raw/test-raw/\")]\n",
    "to_process = [x for x in to_process if x  in os.listdir(\"../data/raw/test-s1/\")]\n",
    "to_process = [str(x[:-4]) for x in  to_process if x not in os.listdir(\"../data/raw/test-processed/\")]\n",
    "for i in to_process:\n",
    "    process_array(i)\n",
    "#map(process_array, to_process)\n",
    "#windows = process_array(\"135804022\")    \n",
    "        \n",
    "#data_5d = make_5d_array(data, tile_lookups)\n",
    "#reconstruct_array(data_5d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_images(plot_id):\n",
    "    '''Takes a plot ID and subsets the input pd.DataFrame to that plot ID\n",
    "       returns a (14, 14) array-like list with binary labels\n",
    "       \n",
    "        Parameters:\n",
    "          batch_ids (list):\n",
    "          batch_size (int):\n",
    "          \n",
    "         Returns:\n",
    "          x_batch (arr):\n",
    "          y_batch (arr):\n",
    "    '''\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    rows = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        rows.append(list(subs_lat['TREE']))\n",
    "    return rows\n",
    "\n",
    "source = 'test'\n",
    "sentinel_1 = True\n",
    "s2_path = \"../data/raw/test-processed/\"\n",
    "s1_path = \"../data/{}-s1/\".format(source)\n",
    "csv_path = \"../data/{}-csv/\".format(source)\n",
    "output_path = \"../data/{}-processed/\".format(source)\n",
    "\n",
    "# For either train or test data, loop through each plot and determine whether there is\n",
    "# labelled Y data for it -- returning one dataframe for the entire data set\n",
    "\n",
    "dfs = []\n",
    "for i in os.listdir(csv_path):\n",
    "    if \".csv\" in i:\n",
    "        print(i)\n",
    "        df = pd.read_csv(csv_path + i).drop('IMAGERY_TITLE', axis = 1)\n",
    "        df['country'] = i.split(\".\")[0]\n",
    "        dfs.append(df)\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    if \"PL_PLOTID\" in dfs[i].columns:\n",
    "            dfs[i] = dfs[i].drop(\"PL_PLOTID\", axis = 1)\n",
    "    if 'STACKINGPROFILEDG' in dfs[i].columns:\n",
    "        dfs[i] = dfs[i].drop('STACKINGPROFILEDG', axis = 1)\n",
    "    if 'IMAGERYYEARDG' in dfs[i].columns:\n",
    "        dfs[i] = dfs[i].drop('IMAGERYYEARDG', axis = 1)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index = True)\n",
    "df = df.dropna(axis = 0)\n",
    "\n",
    "existing = [x for x in os.listdir(s2_path) if \".DS\" not in x]\n",
    "existing = [x for x in existing if x in os.listdir(\"../data/test-s1/\") if \".DS\" not in x]\n",
    "existing = [int(x[:-4]) for x in existing if x in os.listdir(\"../data/test-s2/\") if \".DS\" not in x]\n",
    "df = df[df['PLOT_ID'].isin(existing)]\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "print(len(plot_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match these up with the 2-preprocessing-notebook\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "for i in range(len(plot_ids)):\n",
    "    if str(plot_ids[i]) + \".npy\" in os.listdir(s2_path):\n",
    "        if plot_ids[i] != 136077593:\n",
    "            x = np.load(\"../data/raw/test-processed/\" + str(plot_ids[i]) + \".npy\")\n",
    "            s1 = x[:, :, :, :2]\n",
    "            x = x[:, :, :, 2:]\n",
    "            x = np.concatenate([x, s1], axis = -1)\n",
    "            test_x.append(x)\n",
    "            y = reconstruct_images(plot_ids[i])\n",
    "            test_y.append(y)\n",
    "test_x = np.stack(test_x)\n",
    "test_y = np.stack(test_y)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../tile_data/processed/test_x_tile.npy\", test_x)\n",
    "np.save(\"../tile_data/processed/test_y_tile.npy\", test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data[0, \n",
    "                 tile_lookups['center'][0]:tile_lookups['center'][0]+16,\n",
    "                 tile_lookups['center'][1]:tile_lookups['center'][1]+16,\n",
    "                 2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
