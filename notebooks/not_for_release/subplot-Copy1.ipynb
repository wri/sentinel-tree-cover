{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/subplot.csv\")\n",
    "df1 = pd.read_csv(\"../data/subplot2.csv\")\n",
    "\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1)\n",
    "df1 = df1.drop('IMAGERY_TITLE', axis = 1)\n",
    "df = pd.concat([df, df1], ignore_index = True)\n",
    "df = df.dropna(axis = 0)\n",
    "\n",
    "\n",
    "N_SAMPLES = int(df.shape[0]/196)\n",
    "print(N_SAMPLES)\n",
    "\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bbox(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # TOP, LEFT, BOTTOM, RIGHT\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])), (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "locations = [calc_bbox(x) for x in plot_ids]\n",
    "locations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_images(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    rows = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        rows.append(list(subs_lat['TREE']))\n",
    "    return rows\n",
    "\n",
    "data = [reconstruct_images(x) for x in plot_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_blank_steps(array):\n",
    "    to_update = {}\n",
    "    sets = []\n",
    "    for k in range(6):\n",
    "        for i in range(array.shape[0]):\n",
    "            for k in range(array.shape[-1]):\n",
    "                mean = (np.mean(array[i, :, :, k]))\n",
    "                if mean == 0:\n",
    "                    sets.append(i)\n",
    "                    if i < array.shape[0] - 1:\n",
    "                        array[i, :, :, k] = array[i + 1, :, :, k]\n",
    "                    else:\n",
    "                        array[i, :, :, k] = array[i - 1, :, :, k]\n",
    "                if mean == 1:\n",
    "                    sets.append(i)\n",
    "                    if i < array.shape[0] - 1:\n",
    "                        array[i, :, :, k] = array[i + 1, :, :, k]\n",
    "                    else:\n",
    "                        array[i, :, :, k] = array[i - 1, :, :, k]\n",
    "    for i in range(array.shape[0]):\n",
    "        for k in range(array.shape[-1]):\n",
    "            mean = (np.mean(array[i, :, :, k]))\n",
    "            if mean == 0:\n",
    "                if i < array.shape[0] - 2:\n",
    "                    array[i, :, :, k] = array[i + 2, :, :, k]\n",
    "                else:\n",
    "                    array[i, :, :, k] = array[i - 2, :, :, k]\n",
    "            if mean == 1:\n",
    "                if i < array.shape[0] - 2:\n",
    "                    array[i, :, :, k] = array[i + 2, :, :, k]\n",
    "                else:\n",
    "                    array[i, :, :, k] = array[i - 2, :, :, k]\n",
    "    print(set(sets))\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "image_size = 14\n",
    "\n",
    "def ndvi(x):\n",
    "    # (B8 - B4)/(B8 + B4)\n",
    "    ndvis = [(im[:, :, 6] - im[:, :, 2]) / (im[:, :, 6] + im[:, :, 2]) for im in x]\n",
    "    min_ndvi = min([np.mean(x) for x in ndvis])\n",
    "    max_ndvi = max([np.mean(x) for x in ndvis])\n",
    "    ndvis = [(x - min_ndvi / (max_ndvi - min_ndvi)) for x in ndvis]\n",
    "    x_padding = np.zeros((x.shape[0], image_size, image_size, 1))\n",
    "    x = np.concatenate((x, x_padding), axis = 3)\n",
    "    # Iterate over each time step and add NDVI in as the 11th channel\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i, :, :, 10] = ndvis[i]\n",
    "    return x\n",
    "\n",
    "# Initiate empty lists to store the X and Y data in\n",
    "data_x = []\n",
    "data_y = []\n",
    "binary_y = []\n",
    "data_location_x = []\n",
    "data_location_y = []\n",
    "lengths = []\n",
    "\n",
    "# Iterate over each plot\n",
    "pad = True\n",
    "flip = True\n",
    "for i in plot_ids:\n",
    "    # Load the sentinel imagery\n",
    "    x = np.load(\"../data/ids/\" + str(i) + \".npy\")\n",
    "    # Shape check\n",
    "    if x.shape[1] == image_size:\n",
    "        x = ndvi(x)                # calc NDVI\n",
    "        x = remove_blank_steps(x)\n",
    "        y = reconstruct_images(i)\n",
    "        if sum([sum(x) for x in y]) >= 1:\n",
    "            binary_y.append(1)\n",
    "        else:\n",
    "            binary_y.append(0)\n",
    "        lengths.append(x.shape[0])\n",
    "        #x = np.median(x, axis = 0) # and calculate the median over the time steps\n",
    "        if pad:\n",
    "            if x.shape[0] < 24:\n",
    "                print(x.shape[0])\n",
    "                padding = np.zeros((24 - x.shape[0], image_size, image_size, 11))\n",
    "                x = np.concatenate((x, padding), axis = 0)\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "        if flip:\n",
    "                # FLIP HORIZONTAL\n",
    "            x1 = np.flip(x, 1)\n",
    "            data_x.append(x1)\n",
    "            data_y.append(np.flip(y, 0))\n",
    "            lengths.append(x.shape[0])\n",
    "    \n",
    "                # FLIP BOTH\n",
    "            x2 = np.flip(x, 2)\n",
    "            x2 = np.flip(x2, 1)\n",
    "            data_x.append(x2)\n",
    "            data_y.append(np.flip(y, [0, 1]))\n",
    "            lengths.append(x.shape[0])\n",
    "                # FLIP VERTICAL\n",
    "            x3 = np.flip(x, 2)\n",
    "            data_x.append(x3)\n",
    "            data_y.append(np.flip(y, 1))\n",
    "            lengths.append(x.shape[0])\n",
    "\n",
    "data_x = np.stack(data_x)\n",
    "data_y = np.stack(data_y)\n",
    "data_y = np.reshape(data_y, (N_SAMPLES*4, 14, 14, 1))\n",
    "binary_y = np.stack(binary_y)\n",
    "lengths = np.stack(lengths)\n",
    "lengths = np.reshape(lengths, (lengths.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data_x[86, 16, :, :, 10]) # time 1, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data_y[86].reshape(14, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = data_x[:int(len(data_x) * 0.8)]\n",
    "train_y = data_y[:int(len(data_x) * 0.8)]\n",
    "train_lengths = lengths[:int(len(data_x) * 0.8)]\n",
    "train_binary = binary_y[:int(len(data_x) * 0.8)]\n",
    "\n",
    "test_x = data_x[int(len(data_x) * 0.8):]\n",
    "test_y = data_y[int(len(data_x) * 0.8):]\n",
    "test_lengths = lengths[int(len(data_x) * 0.8):]\n",
    "test_binary = binary_y[int(len(data_x) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    gts = tf.reduce_sum(gt_sorted)\n",
    "    intersection = gts - tf.cumsum(gt_sorted)\n",
    "    union = gts + tf.cumsum(1. - gt_sorted)\n",
    "    jaccard = 1. - intersection / union\n",
    "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def lovasz_hinge(logits, labels):\n",
    "        \"\"\"\n",
    "        Binary Lovasz hinge loss\n",
    "          logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "          labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "          per_image: compute the loss per image instead of per batch\n",
    "          ignore: void class id\n",
    "        \"\"\"\n",
    "        def treat_image(log_lab):\n",
    "            log, lab = log_lab\n",
    "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
    "            log = tf.reshape(log, (-1,))\n",
    "            lab = tf.reshape(lab, (-1,))\n",
    "            return lovasz_hinge_flat(log, lab)\n",
    "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
    "        losses.set_shape((None,))\n",
    "        loss = tf.reduce_mean(losses)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss():\n",
    "        labelsf = tf.cast(labels, logits.dtype)\n",
    "        signs = 2. * labelsf - 1.\n",
    "        errors = 1. - logits * tf.stop_gradient(signs)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
    "        gt_sorted = tf.gather(labelsf, perm)\n",
    "        grad = lovasz_grad(gt_sorted)\n",
    "        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
    "        return loss\n",
    "\n",
    "    # deal with the void prediction case (only void pixels)\n",
    "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
    "                   lambda: tf.reduce_sum(logits) * 0.,\n",
    "                   compute_loss,\n",
    "                   strict=True,\n",
    "                   name=\"loss\"\n",
    "                   )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(y_true, y_pred, smooth=100):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    jac_loss = (1 - jac) * smooth\n",
    "    return jac_loss\n",
    "\n",
    "def bin_foc(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(0.5 * K.pow(1. - pt_1, 2) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - 0.5) * K.pow(pt_0, 2) * K.log(1. - pt_0))\n",
    "\n",
    "\n",
    "def foc_jaccard(y_true, y_pred):\n",
    "    jaccard_loss = jaccard_distance(y_true,y_pred)\n",
    "    focal_loss = bin_foc(y_true, y_pred)\n",
    "    summed = jaccard_loss + focal_loss\n",
    "    return summed\n",
    "\n",
    "def foc_lovasz(y_true, y_pred):\n",
    "    #jaccard_loss = jaccard_distance(y_true, y_pred)\n",
    "    lovasz = lovasz_hinge(y_pred, y_true)\n",
    "    #pred_reshape = tf.reshape(y_pred, (-1, 14, 14))\n",
    "    #true_reshape = tf.reshape(y_true, (-1, 14, 14))\n",
    "    focal_loss = bin_foc(y_true, y_pred)\n",
    "    summed = lovasz + focal_loss\n",
    "    return summed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGRUCell(tf.nn.rnn_cell.RNNCell):\n",
    "  \"\"\"A GRU cell with convolutions instead of multiplications.\"\"\"\n",
    "\n",
    "  def __init__(self, shape, filters, kernel, activation=tf.tanh, normalize=False, data_format='channels_last', reuse=None):\n",
    "    super(ConvGRUCell, self).__init__(_reuse=reuse)\n",
    "    self._filters = filters\n",
    "    self._kernel = kernel\n",
    "    self._activation = activation\n",
    "    self._normalize = normalize\n",
    "    if data_format == 'channels_last':\n",
    "        self._size = tf.TensorShape(shape + [self._filters])\n",
    "        self._feature_axis = self._size.ndims\n",
    "        self._data_format = None\n",
    "    elif data_format == 'channels_first':\n",
    "        self._size = tf.TensorShape([self._filters] + shape)\n",
    "        self._feature_axis = 0\n",
    "        self._data_format = 'NC'\n",
    "    else:\n",
    "        raise ValueError('Unknown data_format')\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._size\n",
    "\n",
    "  def call(self, x, h):\n",
    "    channels = x.shape[self._feature_axis].value\n",
    "\n",
    "    with tf.variable_scope('gates'):\n",
    "      inputs = tf.concat([x, h], axis=self._feature_axis)\n",
    "      n = channels + self._filters\n",
    "      m = 2 * self._filters if self._filters > 1 else 2\n",
    "      W = tf.get_variable('kernel', self._kernel + [n, m])\n",
    "      y = tf.nn.convolution(inputs, W, 'SAME', data_format=self._data_format)\n",
    "      if self._normalize:\n",
    "        r, u = tf.split(y, 2, axis=self._feature_axis)\n",
    "        r = tf.contrib.layers.layer_norm(r)\n",
    "        u = tf.contrib.layers.layer_norm(u)\n",
    "      else:\n",
    "        y += tf.get_variable('bias', [m], initializer=tf.ones_initializer())\n",
    "        r, u = tf.split(y, 2, axis=self._feature_axis)\n",
    "      r, u = tf.sigmoid(r), tf.sigmoid(u)\n",
    "\n",
    "    with tf.variable_scope('candidate'):\n",
    "      inputs = tf.concat([x, r * h], axis=self._feature_axis)\n",
    "      n = channels + self._filters\n",
    "      m = self._filters\n",
    "      W = tf.get_variable('kernel', self._kernel + [n, m])\n",
    "      y = tf.nn.convolution(inputs, W, 'SAME', data_format=self._data_format)\n",
    "      if self._normalize:\n",
    "        y = tf.contrib.layers.layer_norm(y)\n",
    "      else:\n",
    "        y += tf.get_variable('bias', [m], initializer=tf.zeros_initializer())\n",
    "      h = u * h + (1 - u) * self._activation(y)\n",
    "\n",
    "    return h, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def castF(x):\n",
    "    return K.cast(x, K.floatx())\n",
    "\n",
    "def castB(x):\n",
    "    return K.cast(x, bool)\n",
    "\n",
    "def iou_loss_core(true,pred):  #this can be used as a loss if you make it negative\n",
    "    intersection = true * pred\n",
    "    notTrue = 1 - true\n",
    "    union = true + (notTrue * pred)\n",
    "\n",
    "    return (K.sum(intersection, axis=-1) + K.epsilon()) / (K.sum(union, axis=-1) + K.epsilon())\n",
    "\n",
    "def competitionMetric2(true, pred): #any shape can go - can't be a loss function\n",
    "\n",
    "    tresholds = [0.5 + (i*.05)  for i in range(10)]\n",
    "\n",
    "    #flattened images (batch, pixels)\n",
    "    true = K.batch_flatten(true)\n",
    "    pred = K.batch_flatten(pred)\n",
    "    pred = castF(K.greater(pred, 0.5))\n",
    "\n",
    "    #total white pixels - (batch,)\n",
    "    trueSum = K.sum(true, axis=-1)\n",
    "    predSum = K.sum(pred, axis=-1)\n",
    "\n",
    "    #has mask or not per image - (batch,)\n",
    "    true1 = castF(K.greater(trueSum, 1))    \n",
    "    pred1 = castF(K.greater(predSum, 1))\n",
    "\n",
    "    #to get images that have mask in both true and pred\n",
    "    truePositiveMask = castB(true1 * pred1)\n",
    "\n",
    "    #separating only the possible true positives to check iou\n",
    "    testTrue = tf.boolean_mask(true, truePositiveMask)\n",
    "    testPred = tf.boolean_mask(pred, truePositiveMask)\n",
    "\n",
    "    #getting iou and threshold comparisons\n",
    "    iou = iou_loss_core(testTrue,testPred) \n",
    "    truePositives = [castF(K.greater(iou, tres)) for tres in tresholds]\n",
    "\n",
    "    #mean of thressholds for true positives and total sum\n",
    "    truePositives = K.mean(K.stack(truePositives, axis=-1), axis=-1)\n",
    "    truePositives = K.sum(truePositives)\n",
    "\n",
    "    #to get images that don't have mask in both true and pred\n",
    "    trueNegatives = (1-true1) * (1 - pred1) # = 1 -true1 - pred1 + true1*pred1\n",
    "    trueNegatives = K.sum(trueNegatives) \n",
    "\n",
    "    return (truePositives + trueNegatives) / castF(K.shape(true)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers import *\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras.layers import Lambda\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import ELU\n",
    "\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "\n",
    "def conv_lstm_model():\n",
    "    inp = Input(shape = (24, 14, 14, 11))\n",
    "    lengths = Input(shape = (1,), dtype=tf.int32)\n",
    "    length2 = tf.reshape(lengths, (-1,))\n",
    "    \n",
    "        \n",
    "    with tf.variable_scope('medium'):\n",
    "        cell_m = ConvGRUCell(shape = [14, 14],\n",
    "                       filters = 64,\n",
    "                       kernel = [3,3])\n",
    "        state_m = Lambda(\n",
    "        lambda x: tf.concat(\n",
    "            tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_m, cell_m, x, sequence_length = length2, dtype = tf.float32)[1], -1\n",
    "            )\n",
    "        )(inp)\n",
    "\n",
    "    average = Lambda(\n",
    "        lambda x: tf.reduce_mean(x, 1))(inp)\n",
    "    \n",
    "    encoding_out = keras.layers.Concatenate(axis=-1)([state_m, average,])\n",
    "    padded = ReflectionPadding2D((1, 1))(encoding_out)\n",
    "    fm = Conv2D(filters = 64,\n",
    "                kernel_size = (3, 3), \n",
    "                padding = 'valid',\n",
    "                )(padded)\n",
    "    elu = ELU()(fm)\n",
    "    padded = ReflectionPadding2D((1, 1))(elu)\n",
    "    fm = Conv2D(filters = 64,\n",
    "                kernel_size = (3, 3), \n",
    "                padding = 'valid',\n",
    "                )(padded)\n",
    "    elu = ELU()(fm)\n",
    "    #norm = BatchNormalization()(elu)\n",
    "    #padded = ReflectionPadding2D((1, 1))(elu_out)\n",
    "    fm = Conv2D(filters = 1,\n",
    "                kernel_size = (1, 1), \n",
    "                padding = 'valid',\n",
    "                activation = 'sigmoid'\n",
    "                )(elu)\n",
    "    \n",
    "    mod = Model(inputs = [inp, lengths],  outputs = fm)\n",
    "    mod.summary()\n",
    "    mod.compile(#loss=[jaccard_distance],\n",
    "        loss = [foc_lovasz],\n",
    "                optimizer= keras.optimizers.Adam(lr=0.001),  metrics=[competitionMetric2])\n",
    "    return mod\n",
    "\n",
    "m = conv_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit([train_x, train_lengths],\n",
    "      train_y, \n",
    "      validation_data = (([test_x, test_lengths], test_y)),\n",
    "      batch_size=32,\n",
    "      epochs = 500,\n",
    "      #class_weight=[0.1, 0.9],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 looks good, 0, 76, 90, 96, 108, 22, 56, 63, 55, 80\n",
    "\n",
    "## 24, 98, 105, 15, 210, 235\n",
    "\n",
    "plt.figure(figsize=(8.5, 7))\n",
    "sns.heatmap(test_y[24].reshape(14, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = m.predict([test_x[24].reshape(1, 24, 14, 14, 11), test_lengths[0]])\n",
    "pred = pred.reshape(14, 14)\n",
    "#pred[np.where(pred > 0.4)] = 1\n",
    "#pred[np.where(pred < 0.5)] = 0\n",
    "\n",
    "plt.figure(figsize=(8.5, 7))\n",
    "\n",
    "# 111\n",
    "sns.heatmap(pred, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = m.predict([test_x[108].reshape(1, 24, 14, 14, 11), test_lengths[0]])\n",
    "pred = pred.reshape(14, 14)\n",
    "#pred[np.where(pred > 0.3)] = 1\n",
    "#pred[np.where(pred > 0.4)] = 1\n",
    "\n",
    "\n",
    "sns.heatmap(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICT ON TEST WALL-TO-WALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = []\n",
    "pred_len = []\n",
    "pred_files = [\"../data/test/\" + str(x) + \".npy\" for x in range(0, 14)]\n",
    "SHAPE_X = 7\n",
    "SHAPE_Y = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pred_files:\n",
    "    data = np.load(i)\n",
    "    data = ndvi(data)\n",
    "    data = remove_blank_steps(data)\n",
    "    pred_len.append(data.shape[0])\n",
    "    if data.shape[0] < 24:\n",
    "        padding = np.zeros((24 - data.shape[0], 14, 14, 11))\n",
    "        data = np.concatenate((data, padding), axis = 0)\n",
    "    pred_x.append(data)\n",
    "pred_len = np.stack(pred_len).reshape((len(pred_len), 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in range(0, SHAPE_X * SHAPE_Y):\n",
    "    pr = m.predict([pred_x[i].reshape(1, 24, 14, 14, 11), pred_len[i]])\n",
    "    pr = pr.reshape(14, 14)\n",
    "    preds.append(pr)\n",
    "\n",
    "row1 = np.concatenate(preds[:SHAPE_X], axis = 1)\n",
    "row2 = np.concatenate(preds[SHAPE_X:], axis = 1)\n",
    "row2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = np.concatenate((row2, row1), axis = 0)\n",
    "stacked.shape\n",
    "#tacked = stacked.reshape((14, 70))\n",
    "#stacked[np.where(stacked > 0.5)] = 1\n",
    "#stacked[np.where(stacked < 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 7))\n",
    "sns.heatmap(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 7))\n",
    "sns.heatmap(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
