{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data download pipeline\n",
    "\n",
    "Downloads 16x16 training data plots from Sentinel Hub, with the following steps:\n",
    "\n",
    "*  Convert coordinates to UTM, identify bounding boxes of 160 and 180 meter borders\n",
    "*  Download all L1C steps, correct missing bands, and calculate cloud cover\n",
    "*  Select L2A imagery corresponding to the best imagery per 15 days, with missing imagery calculated as the weighted average of the nearest time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants\n",
    "from s2cloudless import S2PixelCloudDetector, CloudMaskRequest\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOCATION = '../data/test.csv'\n",
    "OUTPUT_FOLDER = '../data/test-data-nov-22/'\n",
    "EPSG = CRS.WGS84\n",
    "existing = [int(x[:-4]) for x in os.listdir(OUTPUT_FOLDER) if \".DS\" not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/slope.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup function to reproject coordinates\n",
    "def convertCoords(xy, src='', targ=''):\n",
    "\n",
    "    srcproj = osr.SpatialReference()\n",
    "    srcproj.ImportFromEPSG(src)\n",
    "    targproj = osr.SpatialReference()\n",
    "    if isinstance(targ, str):\n",
    "        targproj.ImportFromProj4(targ)\n",
    "    else:\n",
    "        targproj.ImportFromEPSG(targ)\n",
    "    transform = osr.CoordinateTransformation(srcproj, targproj)\n",
    "\n",
    "    pt = ogr.Geometry(ogr.wkbPoint)\n",
    "    pt.AddPoint(xy[0], xy[1])\n",
    "    pt.Transform(transform)\n",
    "    return([pt.GetX(), pt.GetY()])\n",
    "\n",
    "def bounding_box(points, expansion = 160):\n",
    "    # LONG, LAT FOR SOME REASON\n",
    "    bl = list(points[0])\n",
    "    tr = list(points[1])\n",
    "    \n",
    "    if 48 <= bl[0] <= 54:\n",
    "        epsg = 32639 if bl[1] > 0 else 32739\n",
    "    if 42 <= bl[0] <= 48:\n",
    "        epsg = 32638 if bl[1] > 0 else 32738\n",
    "    if 36 <= bl[0] <= 42:\n",
    "        epsg = 32637 if bl[1] > 0 else 32737\n",
    "    if 30 <= bl[0] <= 36:\n",
    "        epsg = 32636 if bl[1] > 0 else 32736\n",
    "    if 24 <= bl[0] <= 30:\n",
    "        epsg = 32635 if bl[1] > 0 else 32735\n",
    "    if 18 <= bl[0] <= 24:\n",
    "        epsg = 32634 if bl[1] > 0 else 32734\n",
    "\n",
    "    bl = convertCoords(bl, 4326, epsg)\n",
    "    tr = convertCoords(tr, 4326, epsg)\n",
    "    init = [b - a for a,b in zip(bl, tr)]\n",
    "    distance1 = tr[0] - bl[0]\n",
    "    distance2 = tr[1] - bl[1]\n",
    "    EXPANSION = (expansion - np.mean([distance1, distance2]))/2 # should this be 155 or 160?\n",
    "    \n",
    "    bl = [a - EXPANSION for a in bl]\n",
    "    tr = [a + EXPANSION for a in tr]\n",
    "    \n",
    "    after = [b - a for a,b in zip(bl, tr)]    \n",
    "    if max(init) > 130:\n",
    "        print(\"ERROR: Initial field greater than 130m\")\n",
    "    if min(init) < 120:\n",
    "        print(\"ERROR: Initial field less than 130m\")\n",
    "        \n",
    "    if min(after) < (expansion - 4.5):\n",
    "        print(\"ERROR\")\n",
    "    if max(after) > (expansion + 5):\n",
    "        print(\"ERROR\")\n",
    "    diffs = [b - a for b, a in zip(after, init)]\n",
    "\n",
    "    bl = convertCoords(bl, epsg, 4326)\n",
    "    tr = convertCoords(tr, epsg, 4326)\n",
    "    return bl, tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "location = calc_bbox(val)\n",
    "location = bounding_box(location)\n",
    "box = BBox(location, crs = EPSG)\n",
    "\n",
    "image_request = WcsRequest(\n",
    "                layer='ALL_BANDS_NDVI',\n",
    "                bbox=box,\n",
    "                time = ('2018-01-01', '2018-12-31'),\n",
    "                image_format = MimeType.TIFF_d32f,\n",
    "                maxcc=1,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=24),\n",
    "            )\n",
    "img_bands = image_request.get_data()\n",
    "img_bands[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bbox(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # TOP, LEFT, BOTTOM, RIGHT\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])),\n",
    "            (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATA_LOCATION)\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1)\n",
    "df = df.dropna(axis = 0)\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box(calc_bbox(plot_ids[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_detector = S2PixelCloudDetector(threshold=0.4, average_over=4, dilation_size=2)\n",
    "\n",
    "def calculate_proximal_steps(uniques, date, clean_steps):\n",
    "    arg_before = None\n",
    "    arg_after = None\n",
    "    uniques = np.array(uniques)\n",
    "    satisfactory = np.argwhere(uniques > 2)\n",
    "    satisfactory = np.array([x for x in satisfactory if x in clean_steps])\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    print(arg_before, date, arg_after)\n",
    "    return arg_before, arg_after\n",
    "\n",
    "\n",
    "def identify_clouds(bbox, epsg = EPSG, time = ('2018-01-01', '2018-12-31')):\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        cloud_request = WmsRequest(\n",
    "            layer='CLOUD_DETECTION',\n",
    "            bbox=box,\n",
    "            time=time,\n",
    "            width=16,\n",
    "            height=16,\n",
    "            image_format = MimeType.TIFF_d32f,\n",
    "            maxcc=0.33,\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "            time_difference=datetime.timedelta(hours=24),\n",
    "        )\n",
    "        \n",
    "        cloud_img = cloud_request.get_data()\n",
    "        cloud_probs = cloud_detector.get_cloud_probability_maps(np.array(cloud_img))\n",
    "        means = np.mean(cloud_probs, (1, 2))\n",
    "        cloud_steps = [i for i, val in enumerate(means) if val > 0.25]\n",
    "        return cloud_steps, means, cloud_probs\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "    \n",
    "    \n",
    "def download_dem(val, epsg = EPSG):\n",
    "    location = calc_bbox(val)\n",
    "    bbox = bounding_box(location, expansion = 180)\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM',\n",
    "                         bbox=box,\n",
    "                         width=18,\n",
    "                         height=18,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format=MimeType.TIFF_d32f,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    dem_image = calcSlope(dem_image.reshape((1, 18, 18)),\n",
    "                  np.full((18, 18), 10), np.full((18, 18), 10), zScale = 1, minSlope = 0.02)\n",
    "    dem_image = dem_image.reshape((18, 18, 1))\n",
    "    dem_image = dem_image[1:17, 1:17, :]\n",
    "    return dem_image #/ np.max(dem_image)\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "def download_tiles(bbox, clean_steps, epsg = EPSG, time = ('2018-01-01', '2018-12-31')):\n",
    "    try:\n",
    "        clean_steps = np.argwhere(clean_steps <= 0.2)\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WmsRequest(\n",
    "                layer='ALL_BANDS_NDVI',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                width=16,\n",
    "                height=16,\n",
    "                image_format = MimeType.TIFF_d32f,\n",
    "                maxcc=0.33,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=24),\n",
    "            )\n",
    "        img_bands = image_request.get_data()\n",
    "        img_bands = np.array(img_bands)\n",
    "        print(\"There are {}/{} clean steps\".format(len(clean_steps), len(img_bands)))\n",
    "        num_broken_steps = 0\n",
    "        for date in range(img_bands.shape[0]):\n",
    "            if date in clean_steps:\n",
    "                for band in range(10):\n",
    "                    uniques = [len(np.unique(img_bands[i, :, :, band])) for i in range(img_bands.shape[0])]\n",
    "                    uniques = [val for x, val in enumerate(uniques) if x in clean_steps]\n",
    "                    maxs = np.max(img_bands[date, :, :, band])\n",
    "                    mins = np.min(img_bands[date, :, :, band])\n",
    "                    if maxs >= 1.0 or mins <= 0.0:\n",
    "                        #print(\"Found null outlier at {} {}\".format(date, band))\n",
    "                        num_broken_steps += 1\n",
    "                        before, after = calculate_proximal_steps(uniques, date, clean_steps)\n",
    "                        before = img_bands[date + int(before), :, :, band]\n",
    "                        after = img_bands[date + int(after), :, :, band]\n",
    "                        img_bands[date, :, :, band] = (before + after) / 2\n",
    "                    if len(np.unique(img_bands[date, :, :, band])) <= 3:\n",
    "                        num_broken_steps += 1\n",
    "                        before, after = calculate_proximal_steps(uniques, date, clean_steps)\n",
    "                        before = img_bands[date + int(before), :, :, band]\n",
    "                        after = img_bands[date + int(after), :, :, band]\n",
    "                        img_bands[date, :, :, band] = (before + after) / 2\n",
    "        print(\"{} broken normal steps\".format(num_broken_steps))\n",
    "        return img_bands, image_request, len(clean_steps)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "    \n",
    "\n",
    "        \n",
    "def calculate_and_save_best_images(cloud_steps, img_bands, image_request, means):\n",
    "    # Identify the date of the imagery\n",
    "    image_dates = []\n",
    "    for date in image_request.get_dates():\n",
    "        image_dates.append(date.month*30 + date.day)\n",
    "\n",
    "    biweekly_dates = [day for day in range(0, 360, 15)] # ideal imagery dates are every 15 days\n",
    "    \n",
    "    # Identify the dates where there is < 20% cloud cover\n",
    "    satisfactory_ids = list(np.argwhere(np.array(means) < 0.2).reshape(-1, )) \n",
    "    satisfactory_dates = [value for idx, value in enumerate(image_dates) if idx in satisfactory_ids]\n",
    "    \n",
    "    \n",
    "    selected_images = {}\n",
    "    for i in biweekly_dates:\n",
    "        distances = [abs(date - i) for date in satisfactory_dates]\n",
    "        closest = np.min(distances)\n",
    "        closest_id = np.argmin(distances)\n",
    "        # If there is imagery within 8 days, select it\n",
    "        if closest < 8:\n",
    "            date = satisfactory_dates[closest_id]\n",
    "            image_idx = int(np.argwhere(np.array(image_dates) == date)[0])\n",
    "            selected_images[i] = {'image_date': [date], 'image_ratio': [1], 'image_idx': [image_idx]}\n",
    "        # If there is not imagery within 8 days, look for the closest above and below imagery\n",
    "        else:\n",
    "            distances = np.array([(date - i) for date in satisfactory_dates])\n",
    "            # Number of days above and below the selected date of the nearest clean imagery\n",
    "            above = distances[np.where(distances < 0, distances, -np.inf).argmax()]\n",
    "            below = distances[np.where(distances > 0, distances, np.inf).argmin()]\n",
    "            if above != below:\n",
    "                below_ratio = above / (above - below)\n",
    "                above_ratio = 1 - below_ratio\n",
    "            else:\n",
    "                above_ratio = below_ratio = 0.5\n",
    "                \n",
    "            # Extract the image date and imagery index for the above and below values\n",
    "            above_date = i + above\n",
    "            above_image_idx = int(np.argwhere(np.array(image_dates) == above_date)[0])\n",
    "            \n",
    "            below_date = i + below\n",
    "            below_image_idx = int(np.argwhere(np.array(image_dates) == below_date)[0])\n",
    "            \n",
    "            selected_images[i] = {'image_date': [above_date, below_date], 'image_ratio': [above_ratio, below_ratio],\n",
    "                                 'image_idx': [above_image_idx, below_image_idx]}\n",
    "                            \n",
    "    max_distance = 0\n",
    "    \n",
    "    for i in selected_images.keys():\n",
    "        #print(i, selected_images[i])\n",
    "        if len(selected_images[i]['image_date']) == 2:\n",
    "            dist = selected_images[i]['image_date'][1] - selected_images[i]['image_date'][0]\n",
    "            if dist > max_distance:\n",
    "                max_distance = dist\n",
    "    \n",
    "    print(\"Maximum time distance: {}\".format(max_distance))\n",
    "        \n",
    "    # Compute the weighted average of the selected imagery for each time step\n",
    "    keep_steps = []\n",
    "    for i in selected_images.keys():\n",
    "        info = selected_images[i]\n",
    "        if len(info['image_idx']) == 1:\n",
    "            step = img_bands[info['image_idx'][0]]\n",
    "        if len(info['image_idx']) == 2:\n",
    "            step1 = img_bands[info['image_idx'][0]] * info['image_ratio'][0]\n",
    "            step2 = img_bands[info['image_idx'][1]] * info['image_ratio'][1]\n",
    "            step = step1 + step2\n",
    "        keep_steps.append(step)\n",
    "        \n",
    "    keep_steps = np.stack(keep_steps)\n",
    "    return keep_steps, max_distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_best(tiles, cloud_probs, request, offset_x, offset_y):\n",
    "    c_probs = cloud_probs[:, offset_x:offset_x+16, offset_y:offset_y+16]\n",
    "    images = np.stack(tiles)[:, offset_x:offset_x+16, offset_y:offset_y+16]\n",
    "    means = np.mean(c_probs, (1, 2))\n",
    "    cloud_steps = [i for i, val in enumerate(means) if val > 0.20]\n",
    "    best = calculate_and_save_best_images(cloud_steps, images, request, means)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentinelhub import DataSource\n",
    "from sentinelhub import CustomUrlParam\n",
    "\n",
    "to_download = [x for x in plot_ids if x not in existing]\n",
    "#to_download = [plot_ids[0]]\n",
    "errors = []\n",
    "print(\"STARTING DOWNLOAD OF {} plots from {} to {}\".format(len(to_download), DATA_LOCATION, OUTPUT_FOLDER))\n",
    "for i, val in enumerate(plot_ids):\n",
    "    if val in plot_ids:#not in existing:\n",
    "        print(\"Downloading {}, number {}\".format(val, i))\n",
    "        location = calc_bbox(val)\n",
    "        location = bounding_box(location, expansion = 160)\n",
    "        try:\n",
    "            # Initiate hash tables\n",
    "            cloud, means, probs = identify_clouds(location)\n",
    "            dem = download_dem(val)\n",
    "            img, image_request, numb = download_tiles(location, means)\n",
    "            tiles, max_distance = calculate_and_save_best_images(cloud, img, image_request, means) # 22, 16, 16, 10\n",
    "            dem = np.tile(dem.reshape((1, 16, 16, 1)), (tiles.shape[0], 1, 1, 1))\n",
    "            tiles = np.concatenate([tiles, dem], axis = -1)\n",
    "            print(tiles.shape)\n",
    "            if numb >= 24 and max_distance < 90:\n",
    "                np.save(OUTPUT_FOLDER + str(val), tiles)\n",
    "            else:\n",
    "                if os.path.exists(OUTPUT_FOLDER + str(val) + \".npy\"):\n",
    "                    print(\"Deleting {}\".format(OUTPUT_FOLDER + str(val) + \".npy\"))\n",
    "                    os.remove(OUTPUT_FOLDER + str(val) + \".npy\")\n",
    "                print(\"Skipping {} because there are only {} samples, with {} distance\".format(val, numb, max_distance))\n",
    "            print('\\n')\n",
    "        except Exception as e:\n",
    "        #    print(e)\n",
    "            logging.fatal(e, exc_info=True)\n",
    "            errors.append(img)\n",
    "            #continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy-toolkit",
   "language": "python",
   "name": "policy-toolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
