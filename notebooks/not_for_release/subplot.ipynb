{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/subplot.csv\")\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1)\n",
    "df = df.dropna(axis = 0)\n",
    "plot_ids = set(df['PLOT_ID'])\n",
    "#df['TREE'][df['TREE'] == 'No'] = 0\n",
    "#df['TREE'][df['TREE'] == 'Yes'] = 1\n",
    "df.shape[0]/196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bbox(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # TOP, LEFT, BOTTOM, RIGHT\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])), (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "locations = [calc_bbox(x) for x in plot_ids]\n",
    "locations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_images(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    rows = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        rows.append(list(subs_lat['TREE']))\n",
    "    return rows\n",
    "\n",
    "\n",
    "def reconstruct_images_categorical(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    positive = []\n",
    "    negative = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        tree = subs_lat['TREE']\n",
    "        background = [1 - x for x in tree]\n",
    "        positive.append(tree)\n",
    "        negative.append(background)\n",
    "    total = np.stack((positive, negative))\n",
    "    return total\n",
    "\n",
    "data = [reconstruct_images(x) for x in plot_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_blank_steps(array):\n",
    "    to_update = {}\n",
    "    sets = []\n",
    "    for k in range(6):\n",
    "        for i in range(array.shape[0]):\n",
    "            for k in range(array.shape[-1]):\n",
    "                mean = (np.mean(array[i, :, :, k]))\n",
    "                if mean == 0:\n",
    "                    sets.append(i)\n",
    "                    if i < array.shape[0] - 1:\n",
    "                        array[i, :, :, k] = array[i + 1, :, :, k]\n",
    "                    else:\n",
    "                        array[i, :, :, k] = array[i - 1, :, :, k]\n",
    "                if mean == 1:\n",
    "                    sets.append(i)\n",
    "                    if i < array.shape[0] - 1:\n",
    "                        array[i, :, :, k] = array[i + 1, :, :, k]\n",
    "                    else:\n",
    "                        array[i, :, :, k] = array[i - 1, :, :, k]\n",
    "    for i in range(array.shape[0]):\n",
    "        for k in range(array.shape[-1]):\n",
    "            mean = (np.mean(array[i, :, :, k]))\n",
    "            if mean == 0:\n",
    "                if i < array.shape[0] - 2:\n",
    "                    array[i, :, :, k] = array[i + 2, :, :, k]\n",
    "                else:\n",
    "                    array[i, :, :, k] = array[i - 2, :, :, k]\n",
    "            if mean == 1:\n",
    "                if i < array.shape[0] - 2:\n",
    "                    array[i, :, :, k] = array[i + 2, :, :, k]\n",
    "                else:\n",
    "                    array[i, :, :, k] = array[i - 2, :, :, k]\n",
    "    print(set(sets))\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sorted_files = [str(x) + \".npy\" for x in range(0, 112)]\n",
    "image_size = 14\n",
    "\n",
    "def ndvi(x):\n",
    "    # (B8 - B4)/(B8 + B4)\n",
    "    ndvis = [(im[:, :, 6] - im[:, :, 2]) / (im[:, :, 6] + im[:, :, 2]) for im in x]\n",
    "    min_ndvi = min([np.mean(x) for x in ndvis])\n",
    "    max_ndvi = max([np.mean(x) for x in ndvis])\n",
    "    ndvis = [(x - min_ndvi / (max_ndvi - min_ndvi)) for x in ndvis]\n",
    "    x_padding = np.zeros((x.shape[0], image_size, image_size, 1))\n",
    "    x = np.concatenate((x, x_padding), axis = 3)\n",
    "    # Iterate over each time step and add NDVI in as the 11th channel\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i, :, :, 10] = ndvis[i]\n",
    "    return x\n",
    "\n",
    "# Initiate empty lists to store the X and Y data in\n",
    "data_x = []\n",
    "data_y = []\n",
    "data_location_x = []\n",
    "data_location_y = []\n",
    "\n",
    "# Iterate over each plot\n",
    "pad = True\n",
    "flip = True\n",
    "for i in sorted_files:\n",
    "    # Load the sentinel imagery\n",
    "    x = np.load(\"../data/processed/\" + i)\n",
    "    i = int(i[:-4])\n",
    "    # Shape check\n",
    "    if x.shape[1] == image_size:\n",
    "        x = ndvi(x)                # calc NDVI\n",
    "        x = remove_blank_steps(x)\n",
    "        y = data[i]\n",
    "        #x = np.median(x, axis = 0) # and calculate the median over the time steps\n",
    "        if pad:\n",
    "            if x.shape[0] < 24:\n",
    "                print(x.shape[0])\n",
    "                padding = np.zeros((24 - x.shape[0], image_size, image_size, 11))\n",
    "                x = np.concatenate((x, padding), axis = 0)\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "        if flip:\n",
    "                # FLIP HORIZONTAL\n",
    "            x1 = np.flip(x, 1)\n",
    "            data_x.append(x1)\n",
    "            data_y.append(np.flip(y, 1))\n",
    "                # FLIP BOTH\n",
    "            x2 = np.flip(x, 2)\n",
    "            x2 = np.flip(x2, 1)\n",
    "            data_x.append(x2)\n",
    "            data_y.append(np.flip(y, [0, 1]))\n",
    "                # FLIP VERTICAL\n",
    "            x3 = np.flip(x, 2)\n",
    "            data_x.append(x3)\n",
    "            data_y.append(np.flip(y, 1))\n",
    "\n",
    "data_x = np.stack(data_x)\n",
    "data_y = np.stack(data_y)\n",
    "data_y = np.reshape(data_y, (112*4, 14, 14, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sns.heatmap(data_x[20, 4, :, :, 0]) # time 1, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = data_x[:int(len(data_x) * 0.75)]\n",
    "train_y = data_y[:int(len(data_x) * 0.75)]\n",
    "\n",
    "test_x = data_x[int(len(data_x) * 0.75):]\n",
    "test_y = data_y[int(len(data_x) * 0.75):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "def categorical_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Softmax version of focal loss.\n",
    "           m\n",
    "      FL = âˆ‘  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
    "          c=1\n",
    "      where m = number of classes, c = class and o = observation\n",
    "    Parameters:\n",
    "      alpha -- the same as weighing factor in balanced cross entropy\n",
    "      gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "      gamma -- 2.0 as mentioned in the paper\n",
    "      alpha -- 0.25 as mentioned in the paper\n",
    "    References:\n",
    "        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
    "    Usage:\n",
    "     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred: A tensor resulting from a softmax\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # Scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "\n",
    "        # Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Calculate Cross Entropy\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "        # Calculate Focal Loss\n",
    "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "\n",
    "        # Sum the losses in mini_batch\n",
    "        return K.sum(loss, axis=1)\n",
    "\n",
    "    return categorical_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGRUCell(tf.nn.rnn_cell.RNNCell):\n",
    "  \"\"\"A GRU cell with convolutions instead of multiplications.\"\"\"\n",
    "\n",
    "  def __init__(self, shape, filters, kernel, activation=tf.tanh, normalize=False, data_format='channels_last', reuse=None):\n",
    "    super(ConvGRUCell, self).__init__(_reuse=reuse)\n",
    "    self._filters = filters\n",
    "    self._kernel = kernel\n",
    "    self._activation = activation\n",
    "    self._normalize = normalize\n",
    "    if data_format == 'channels_last':\n",
    "        self._size = tf.TensorShape(shape + [self._filters])\n",
    "        self._feature_axis = self._size.ndims\n",
    "        self._data_format = None\n",
    "    elif data_format == 'channels_first':\n",
    "        self._size = tf.TensorShape([self._filters] + shape)\n",
    "        self._feature_axis = 0\n",
    "        self._data_format = 'NC'\n",
    "    else:\n",
    "        raise ValueError('Unknown data_format')\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._size\n",
    "\n",
    "  def call(self, x, h):\n",
    "    channels = x.shape[self._feature_axis].value\n",
    "\n",
    "    with tf.variable_scope('gates'):\n",
    "      inputs = tf.concat([x, h], axis=self._feature_axis)\n",
    "      n = channels + self._filters\n",
    "      m = 2 * self._filters if self._filters > 1 else 2\n",
    "      W = tf.get_variable('kernel', self._kernel + [n, m])\n",
    "      y = tf.nn.convolution(inputs, W, 'SAME', data_format=self._data_format)\n",
    "      if self._normalize:\n",
    "        r, u = tf.split(y, 2, axis=self._feature_axis)\n",
    "        r = tf.contrib.layers.layer_norm(r)\n",
    "        u = tf.contrib.layers.layer_norm(u)\n",
    "      else:\n",
    "        y += tf.get_variable('bias', [m], initializer=tf.ones_initializer())\n",
    "        r, u = tf.split(y, 2, axis=self._feature_axis)\n",
    "      r, u = tf.sigmoid(r), tf.sigmoid(u)\n",
    "\n",
    "    with tf.variable_scope('candidate'):\n",
    "      inputs = tf.concat([x, r * h], axis=self._feature_axis)\n",
    "      n = channels + self._filters\n",
    "      m = self._filters\n",
    "      W = tf.get_variable('kernel', self._kernel + [n, m])\n",
    "      y = tf.nn.convolution(inputs, W, 'SAME', data_format=self._data_format)\n",
    "      if self._normalize:\n",
    "        y = tf.contrib.layers.layer_norm(y)\n",
    "      else:\n",
    "        y += tf.get_variable('bias', [m], initializer=tf.zeros_initializer())\n",
    "      h = u * h + (1 - u) * self._activation(y)\n",
    "\n",
    "    return h, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers import *\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras.layers import Lambda\n",
    "from keras import regularizers\n",
    "\n",
    "tf.reset_default_graph()\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "\n",
    "def conv_lstm_model():\n",
    "    inp = Input(shape = (24, 14, 14, 11))\n",
    "    #state = Bidirectional(ConvLSTM2D(filters=16, kernel_size=(2, 2),\n",
    "    #              padding='same', return_sequences=False, recurrent_dropout = 0.25))(inp)\n",
    "    \n",
    "    cell = ConvGRUCell(shape = [14, 14],\n",
    "                       filters = 128,\n",
    "                       kernel = [3,3])\n",
    "    \n",
    "    #cell = tf.contrib.rnn.DropoutWrapper(\n",
    "    #    cell, \n",
    "    #    input_keep_prob = 0.95,\n",
    "    #)\n",
    "    \n",
    "    # Convolutional GRU Keras layer\n",
    "    state = Lambda(\n",
    "        lambda x: tf.concat(\n",
    "            tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell, cell, x, dtype = tf.float32)[1], -1\n",
    "            )\n",
    "        )(inp)\n",
    "    padded_inputs = ReflectionPadding2D((1, 1))(state)\n",
    "    fm = Conv2D(filters = 1,\n",
    "                kernel_size = (3, 3), \n",
    "                padding = 'valid',\n",
    "                activation = 'sigmoid',\n",
    "                #kernel_regularizer = regularizers.l1(0.001)\n",
    "                )(padded_inputs)\n",
    "    mod = Model(inputs = [inp],  outputs = fm)\n",
    "    mod.summary()\n",
    "    mod.compile(loss=[binary_focal_loss(alpha=0.15, gamma=2)],\n",
    "                optimizer= keras.optimizers.Adam(lr=0.01),  metrics=['accuracy'])\n",
    "    #mod.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return mod\n",
    "\n",
    "m = conv_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(train_x,\n",
    "      train_y, \n",
    "      validation_data = ((test_x, test_y)),\n",
    "      batch_size=10,\n",
    "      epochs = 200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 looks good, 0, 76, 90, 96, 108, 22, 56, 63\n",
    "\n",
    "sns.heatmap(train_y[328].reshape(14, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = m.predict(train_x[328].reshape(1, 24, 14, 14, 11))\n",
    "pred = pred.reshape(14, 14)\n",
    "#pred[np.where(pred > 0.5)] = 1\n",
    "#pred[np.where(pred < 0.5)] = 0\n",
    "\n",
    "\n",
    "sns.heatmap(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.heatmap(train_x[76, 21, :, :, 10])\n",
    "plt.imshow(train_x[328][3][:, :, [3, 2, 1]] + 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#((int(len(data_x) * 0.75) + 108) / 4)\n",
    "330/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PLOT_ID'].unique()[82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
