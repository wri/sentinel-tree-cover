{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master model development\n",
    "\n",
    "## John Brandt\n",
    "\n",
    "### Last updated: November 1 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Package loading\n",
    "*  Hyperparameter definitions\n",
    "*  Additional layer definitions\n",
    "*  Model definition\n",
    "*  Data loading\n",
    "*  Data preprocessing\n",
    "*  K means clustering\n",
    "*  Augment training data\n",
    "*  Loss definition\n",
    "*  Equibatch creation\n",
    "*  Model training\n",
    "*  Model validation and sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "# The model is very sensitive to zoneout prob, do not go above 0.05\n",
    "# AdaBound seems to perform worse, stick to Adam with step down loss\n",
    "# 32 x 24 x 32 model overfits after 100 epochs\n",
    "# Investigate more hypercolumn parametrizations to increase available data at output \n",
    "#    while maintaining low dimensionality of filters\n",
    "# 5e-4 LR worked, investigating 8e-4 for 60% faster training time\n",
    "# ONLY CHANGE ONE THING AT A TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Remove imports that aren't needed to save RAM\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras.layers import Conv2D, Lambda, Dense, Multiply, Add, Bidirectional, ConvLSTM2D\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim import conv2d\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import itertools\n",
    "from tflearn.layers.conv import global_avg_pool\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.layers import batch_normalization\n",
    "from tensorflow.python.util import deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/zoneout.py\n",
    "%run ../src/convgru.py\n",
    "%run ../src/lovasz.py\n",
    "%run ../src/utils.py\n",
    "%run ../src/adabound.py\n",
    "%run ../src/slope.py\n",
    "%run ../src/dropblock.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONE_OUT_PROB = 0.25\n",
    "L2_REG = 0.0\n",
    "INITIAL_LR = 2e-4\n",
    "FINAL_LR = 1e-3\n",
    "BN_MOMENTUM = 0.9\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_RATIO = 0.8\n",
    "TEST_RATIO = 0.2\n",
    "MAX_DROPBLOCK = 0.85\n",
    "INP_FILTERS = 17\n",
    "\n",
    "gru_flt = 12\n",
    "fpa_flt = 16\n",
    "out_conv_flt = 32\n",
    "\n",
    "\n",
    "AUGMENTATION_RATIO = 4\n",
    "IMAGE_SIZE = 16\n",
    "existing = [int(x[:-4]) for x in os.listdir('../data/final/') if \".DS\" not in x]\n",
    "N_SAMPLES = len(existing)\n",
    "\n",
    "LABEL_SIZE = 14\n",
    "\n",
    "    \n",
    "TRAIN_SAMPLES = int((N_SAMPLES * AUGMENTATION_RATIO) * TRAIN_RATIO)\n",
    "TEST_SAMPLES = int((N_SAMPLES * AUGMENTATION_RATIO) - TRAIN_SAMPLES)\n",
    "print(TRAIN_SAMPLES // AUGMENTATION_RATIO, N_SAMPLES - (TRAIN_SAMPLES // AUGMENTATION_RATIO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional layer definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_elu(inp, is_training, kernel_size, scope, filter_count = 16, pad = True, padding = 'valid'):\n",
    "    if kernel_size == 3:\n",
    "        if pad:\n",
    "            padded = ReflectionPadding2D((1, 1,))(inp)\n",
    "        else:\n",
    "            padded = inp\n",
    "        padding = padding\n",
    "    else:\n",
    "        padded = inp\n",
    "        padding = padding\n",
    "    conv = Conv2D(filters = filter_count, kernel_size = (kernel_size, kernel_size),\n",
    "                      padding = padding, kernel_initializer = 'he_normal')(padded)\n",
    "    elu = ELU()(conv)\n",
    "    bn = Batch_Normalization(elu, training=is_training, scope = scope + \"bn\")\n",
    "    return bn\n",
    "\n",
    "def td_conv_bn_elu(inp, is_training, scope, filter_count = 16, pad = True, padding = 'valid'):\n",
    "    #padded = TimeDistributed(ReflectionPadding2D((1, 1,)))(inp)\n",
    "    #padded = tf.pad(inp, [[0,0], [0,0], [1,1], [1,1], [0,0] ], 'REFLECT')\n",
    "    conv = TimeDistributed(Conv2D(filters = filter_count, kernel_size = (1, 1),\n",
    "                      padding = 'SAME', kernel_initializer = 'he_normal'))(inp)\n",
    "    elu = TimeDistributed(ELU())(conv)\n",
    "    #bn = TimeDistributed(Batch_Normalization(elu, training=is_training, scope = scope + \"bn\")\n",
    "    return elu\n",
    "\n",
    "def resnet_block(inp, is_training, scope, flt):\n",
    "    drop1 = DropBlock2D(keep_prob = keep_rate, block_size = 3)\n",
    "    drop2 = DropBlock2D(keep_prob = keep_rate, block_size = 3)\n",
    "    padded = ReflectionPadding2D((1, 1,))(inp)\n",
    "    conv1 = Conv2D(filters = flt, kernel_size = (3, 3),\n",
    "                      padding = 'valid', kernel_initializer = 'he_normal')(padded)\n",
    "    elu1 = ELU()(conv1)\n",
    "    bn1 = Batch_Normalization(elu1, training=is_training, scope = scope + \"bn1\")\n",
    "    d1 = drop1(bn1, is_training)\n",
    "    \n",
    "    pad2 = ReflectionPadding2D((1, 1))(d1)\n",
    "    conv2 = Conv2D(filters = flt, kernel_size = (3, 3),\n",
    "                      padding = 'valid', kernel_initializer = 'he_normal')(pad2)\n",
    "    elu2 = ELU()(conv2)\n",
    "    bn2 = Batch_Normalization(elu2, training=is_training, scope = scope + \"bn2\")\n",
    "    d2 = drop2(bn2, is_training)\n",
    "    skip = tf.add(inp, d2)\n",
    "    reweighted = csse_block(skip, scope + \"csse\")\n",
    "    return reweighted\n",
    "    \n",
    "    \n",
    "def fpa(inp, is_training, filter_count):\n",
    "    one = conv_bn_elu(inp, is_training, 1, 'forward1', filter_count, False, 'valid')\n",
    "    five = conv_bn_elu(inp, is_training, 5, 'down1', filter_count, False, 'valid')\n",
    "    five_f = conv_bn_elu(five, is_training, 5, 'down1_f', filter_count, False, 'valid')\n",
    "    three = conv_bn_elu(five, is_training, 3, 'down2', filter_count, False, 'valid')\n",
    "    three_f = conv_bn_elu(three, is_training, 3, 'down2_f', filter_count, False, 'valid')\n",
    "    \n",
    "    three_up = get_deconv2d(three_f, filter_count, filter_count, \"fpa1\", is_training)\n",
    "    five_up = get_deconv2d(five_f, filter_count, filter_count, \"fpa2\", is_training)\n",
    "    \n",
    "    print(\"One: {}\".format(one.shape))\n",
    "    print(\"Five: {}\".format(five.shape))\n",
    "    print(\"Five_F: {}\".format(five_f.shape))\n",
    "    print(\"Three: {}\".format(three.shape))\n",
    "    print(\"Three_f: {}\".format(three_f.shape))\n",
    "    print(\"Three_up: {}\".format(three_up.shape))\n",
    "    print(\"Five_up: {}\".format(five_up.shape))\n",
    "    \n",
    "    # top block\n",
    "    pooled = tf.keras.layers.GlobalAveragePooling2D()(inp)\n",
    "    one_top = conv_bn_elu(tf.reshape(pooled, (-1, 1, 1, pooled.shape[-1])),\n",
    "                          is_training, 1, 'top1', filter_count)\n",
    "    four_top = tf.keras.layers.UpSampling2D((16, 16))(one_top)\n",
    "    \n",
    "    \n",
    "    concat_1 = tf.multiply(one, tf.add(three_up, five_up))\n",
    "    concat_2 = tf.add(concat_1, four_top)\n",
    "    print(\"Feature pyramid attention shape {}\".format(concat_2.shape))\n",
    "    return concat_2\n",
    "\n",
    "    \n",
    "def create_deconv_init(filter_size, num_channels):\n",
    "    bilinear_kernel = np.zeros([filter_size, filter_size], dtype=np.float32)\n",
    "    scale_factor = (filter_size + 1) // 2\n",
    "    if filter_size % 2 == 1:\n",
    "        center = scale_factor - 1\n",
    "    else:\n",
    "        center = scale_factor - 0.5\n",
    "    for x in range(filter_size):\n",
    "        for y in range(filter_size):\n",
    "            bilinear_kernel[x,y] = (1 - abs(x - center) / scale_factor) * \\\n",
    "                                   (1 - abs(y - center) / scale_factor)\n",
    "    weights = np.zeros((filter_size, filter_size, num_channels, num_channels))\n",
    "    for i in range(num_channels):\n",
    "        weights[:, :, i, i] = bilinear_kernel\n",
    "\n",
    "    #assign numpy array to constant_initalizer and pass to get_variable\n",
    "    bilinear_init = tf.constant_initializer(value=weights, dtype=tf.float32)\n",
    "    return bilinear_init\n",
    "\n",
    "\n",
    "def get_deconv2d(inp, filter_count, num_channels, scope, is_training):\n",
    "    bilinear_init = create_deconv_init(4, filter_count)\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = filter_count, kernel_size = (4, 4),\n",
    "                                        strides=(2, 2), padding='same', \n",
    "                                        kernel_initializer = bilinear_init)(inp)\n",
    "    x = ELU()(x)\n",
    "    x = Batch_Normalization(x, training=is_training, scope = scope + \"bn\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    return batch_normalization(inputs=x, \n",
    "                               momentum = BN_MOMENTUM, \n",
    "                               training=training,\n",
    "                               renorm = True,\n",
    "                               reuse=None,\n",
    "                               name = scope)\n",
    "\n",
    "\n",
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    inputs_orig = inputs\n",
    "    inputs = tf.reduce_mean(inputs, axis = [2, 3])\n",
    "    #print(\"ATT means: {}\".format(inputs.shape))\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
    "    print(\"Alphas: {}\".format(alphas.shape))\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs_orig * tf.reshape(alphas, (-1, 24, 1, 1, 1)), axis = 1)\n",
    "    #output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "    print(output.shape)\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas\n",
    "\n",
    "def cse_block(prevlayer, prefix):\n",
    "    mean = Lambda(lambda xin: K.mean(xin, axis=[1, 2]))(prevlayer)\n",
    "    lin1 = Dense(K.int_shape(prevlayer)[3] // 2, name=prefix + 'cse_lin1', activation='relu')(mean)\n",
    "    lin2 = Dense(K.int_shape(prevlayer)[3], name=prefix + 'cse_lin2', activation='sigmoid')(lin1)\n",
    "    x = Multiply()([prevlayer, lin2])\n",
    "    return x\n",
    "\n",
    "def temporal_attention(inp, scope):\n",
    "    # This rescales each output\n",
    "    # Timesteps that are more important get weighted higher\n",
    "    # Timesteps that are least important get weighted lower --> B, N, H, W, C\n",
    "    conved = TimeDistributed(Conv2D(50, (1, 1), padding = 'same', kernel_initializer = 'he_normal',\n",
    "                            activation = 'tanh', strides = (1, 1)))(inp)\n",
    "    \n",
    "    conved = TimeDistributed(Conv2D(1, (1, 1), padding = 'same', kernel_initializer = 'he_normal',\n",
    "                            activation = 'sigmoid', use_bias = False, strides = (1, 1)))(conved)\n",
    "    \n",
    "    # We need to calculate the total sum for each pixel for each channel, so that we can combine them\n",
    "    conved = conved / tf.reduce_sum(conved, axis = 1, keep_dims=True)\n",
    "    print(\"Attention weight shapes {}\".format(conved.shape))\n",
    "    \n",
    "    # This actually multiplies the Conv by the input\n",
    "    multiplied = tf.reduce_sum(conved * inp, axis = 1)\n",
    "    return multiplied\n",
    "    \n",
    "\n",
    "\n",
    "def sse_block(prevlayer, prefix):\n",
    "    conv = Conv2D(1, (1, 1), padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                  activation='sigmoid', strides=(1, 1),\n",
    "                  name=prefix + \"_conv\")(prevlayer)\n",
    "    conv = Multiply(name=prefix + \"_mul\")([prevlayer, conv])\n",
    "    return conv\n",
    "\n",
    "\n",
    "def csse_block(x, prefix):\n",
    "    '''\n",
    "    Implementation of Concurrent Spatial and Channel ‘Squeeze & Excitation’ in Fully Convolutional Networks\n",
    "    https://arxiv.org/abs/1803.02579\n",
    "    '''\n",
    "    cse = cse_block(x, prefix)\n",
    "    sse = sse_block(x, prefix)\n",
    "    x = Add(name=prefix + \"_csse_mul\")([cse, sse])\n",
    "\n",
    "    return x\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "    \n",
    "    \n",
    "def gru_block(inp, length, size, flt, scope, train, normalize = True):\n",
    "    with tf.variable_scope(scope):\n",
    "        print(\"GRU input shape {}, zoneout: {}\".format(inp.shape, ZONE_OUT_PROB))\n",
    "        cell_fw = ConvLSTMCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], normalize = normalize)\n",
    "        cell_bw = ConvLSTMCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], normalize = normalize)\n",
    "        cell_fw = ZoneoutWrapper(\n",
    "           cell_fw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        cell_bw = ZoneoutWrapper(\n",
    "            cell_bw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        output, final_state = convGRU(inp, cell_fw, cell_bw, length)\n",
    "        final_state = tf.concat(final_state, axis = -1)\n",
    "        output = tf.concat(output, axis = -1)\n",
    "        print(\"Hidden output shape: {}\".format(output))\n",
    "        \n",
    "        attended = temporal_attention(output, \"attention\")\n",
    "        #output = output[:, -1, :, :, :]\n",
    "        #attended = attention(output, 50)\n",
    "        means = tf.reduce_mean(inp, axis = 1)\n",
    "        out = tf.concat([attended, means], axis = -1)\n",
    "        print(\"GRU Output shape: {}\".format(out.shape))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = keras.regularizers.l2(L2_REG)\n",
    "inp = tf.placeholder(tf.float32, shape=(None, 24, IMAGE_SIZE, IMAGE_SIZE, INP_FILTERS))\n",
    "length = tf.placeholder(tf.int32, shape = (None, 1))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 14, 14))#, 1))\n",
    "keep_rate = tf.placeholder_with_default(1.0, ())\n",
    "length2 = tf.reshape(length, (-1,))\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training')\n",
    "    \n",
    "    \n",
    "# GRU -> Conv -> Conv\n",
    "#feature_engineering = td_conv_bn_elu(inp, is_training, 'feats', filter_count = 16, pad = False, padding = 'valid')\n",
    "#feats = tf.concat([inp, feature_engineering], axis = -1)\n",
    "gru_output = gru_block(inp = inp, length = length2, size = [16, 16],\n",
    "                      flt = gru_flt, scope = 'down_16', train = is_training)\n",
    "gru_conv = conv_bn_elu(gru_output, is_training, 3, 'gru_conv', out_conv_flt, pad = True)\n",
    "gru_csse1 = csse_block(gru_conv, \"csse_conv1\")\n",
    "\n",
    "# Skip -> FPA\n",
    "drop2 = DropBlock2D(keep_prob = keep_rate, block_size = 3)\n",
    "drop = drop2(gru_csse1, is_training)\n",
    "\n",
    "#fpa = fpa(drop, is_training, fpa_flt)\n",
    "#fpa_csse = csse_block(fpa, 'csse_fpa')\n",
    "#fpa_skip = tf.concat([fpa_csse, gru_csse1, gru_output], axis = -1)\n",
    "#drop3 = DropBlock2D(keep_prob = keep_rate, block_size = 3)\n",
    "#out_drop1 = drop3(fpa_skip, is_training)\n",
    "\n",
    "# Conv -> Hyperpyramid -> Conv\n",
    "out_skip = tf.concat([gru_csse1, drop], axis = -1)\n",
    "out_conv1 = conv_bn_elu(drop, is_training, 3, \"out_conv1\", out_conv_flt, pad = False)\n",
    "\n",
    "print(\"Initializing last sigmoid bias with -2.94 constant\")\n",
    "init = tf.constant_initializer([-np.log(0.9/0.1)]) # For focal loss\n",
    "fm = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1), \n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid',\n",
    "            bias_initializer = init,\n",
    "           )(out_conv1) # For focal loss\n",
    "print(\"Output shape: {}\".format(fm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(\"This model has {} parameters\".format(total_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/subplot.csv\")\n",
    "df1 = pd.read_csv(\"../data/subplot2.csv\")\n",
    "df2 = pd.read_csv(\"../data/subplot3.csv\")\n",
    "df3 = pd.read_csv(\"../data/subplot4.csv\")\n",
    "\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df1 = df1.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df2 = df2.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df3 = df3.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "\n",
    "lens = [len(x) for x in [df, df1, df2, df3]]\n",
    "\n",
    "df = pd.concat([df, df1, df2, df3], ignore_index = True)\n",
    "df = df.dropna(axis = 0)\n",
    "\n",
    "existing = [int(x[:-4]) for x in os.listdir('../data/correct_dem/') if \".DS\" not in x]\n",
    "N_SAMPLES = len(existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['PLOT_ID'].isin(existing)]\n",
    "N_SAMPLES = int(df.shape[0]/196)\n",
    "N_YEARS = 1\n",
    "\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "\n",
    "\n",
    "def reconstruct_images(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    rows = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        rows.append(list(subs_lat['TREE']))\n",
    "    return rows\n",
    "\n",
    "data = [reconstruct_images(x) for x in plot_ids]\n",
    "\n",
    "\n",
    "# Initiate empty lists to store the X and Y data in\n",
    "data_x, data_y, lengths = [], [], []\n",
    "\n",
    "# Iterate over each plot\n",
    "for i in tnrange(len(plot_ids)):\n",
    "    # Load the sentinel imagery\n",
    "    for year in [\"correct_dem\"]:  \n",
    "        x = np.load(\"../data/\" + year + \"/\" + str(plot_ids[i]) + \".npy\")\n",
    "        x = ndvi(x, image_size = 16)\n",
    "        x = evi(x, image_size = 16)\n",
    "        x = savi(x, image_size = 16)\n",
    "        x = bi(x)\n",
    "        x = msavi2(x)\n",
    "        x = si(x)\n",
    "        #x = ndmi(x)\n",
    "        x = remove_blank_steps(x)\n",
    "        y = reconstruct_images(plot_ids[i])\n",
    "        x[:, :, :, 10] /= 90\n",
    "        lengths.append(x.shape[0])\n",
    "        if x.shape[0] < 24:\n",
    "            padding = np.zeros((24 - x.shape[0], IMAGE_SIZE, IMAGE_SIZE, 14))\n",
    "            x = np.concatenate((x, padding), axis = 0)\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "print(\"Finished data loading\")\n",
    "\n",
    "data_x = np.stack(data_x)\n",
    "data_y = np.stack(data_y)\n",
    "lengths = np.stack(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(data_x[:, :, :, :, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "below_1 = [i for i, val in enumerate(data_x) if np.min(val) < -1.5]\n",
    "above_1 = [i for i, val in enumerate(data_x) if np.max(val) > 1.5]\n",
    "min_vals = [np.min(val) for i, val in enumerate(data_x) if np.min(val) < -1.5]\n",
    "max_vals = [np.max(val) for i, val in enumerate(data_x) if np.max(val) > 1.5]\n",
    "outliers = below_1 + above_1\n",
    "print(\"The outliers are: {}, totalling {}\".format(outliers, len(outliers)))\n",
    "print(\"\\n\")\n",
    "print(min_vals, max_vals)\n",
    "data_x = data_x[[x for x in range(0, len(data_x)) if x not in outliers]]\n",
    "data_y = data_y[[x for x in range(0, len(data_y)) if x not in outliers]]\n",
    "lengths = lengths[[x for x in range(0, len(lengths)) if x not in outliers]]\n",
    "\n",
    "min_all = []\n",
    "max_all = []\n",
    "for x in range(0, data_x.shape[-1]):\n",
    "    mins, maxs = (np.min(data_x[:, :, :, :, x]), np.max(data_x[:, :, :, :, x]))\n",
    "    min_all.append(mins)\n",
    "    max_all.append(maxs)\n",
    "    \n",
    "    data_x[:, :, :, :, x] = (data_x[:, :, :, :, x] - mins) / (maxs - mins)\n",
    "    \n",
    "print(\"The data has been scaled to [{}, {}]\".format(np.min(data_x), np.max(data_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ids2 = [val for x, val in enumerate(plot_ids) if x not in list(set([x for x in outliers]))]\n",
    "N_SAMPLES = len(data_x)\n",
    "\n",
    "region_lengths = []\n",
    "for x in [df1, df2, df3]:\n",
    "    subs = [i for i in set(x['PLOT_ID']) if i in plot_ids2]\n",
    "    region_lengths.append(len(subs))\n",
    "    \n",
    "region_lengths = [N_SAMPLES - sum(region_lengths)] + region_lengths\n",
    "\n",
    "print(\"The region sample distribution is {}\".format(region_lengths))\n",
    "print(sum(region_lengths))\n",
    "train_ordering = []\n",
    "test_ordering = []\n",
    "ordering = []\n",
    "total_samples = 0\n",
    "for r in TRAIN_RATIO, TEST_RATIO:\n",
    "    for i, val in enumerate(region_lengths):\n",
    "        start = int(np.sum(region_lengths[:i]))\n",
    "        end = start + val\n",
    "        if r == 0.8:\n",
    "            start = start\n",
    "            end = end-((end-start)*(1-r))\n",
    "            start = int(start)\n",
    "            end = int(end)\n",
    "            total_samples += (end - start)\n",
    "            train_ordering += [x for x in range(start, end)]\n",
    "        if r == 0.2:\n",
    "            start = start + ((end-start)*(1-r))\n",
    "            end = end\n",
    "            start = int(start)\n",
    "            end = int(end)\n",
    "            total_samples += (end-start)\n",
    "            test_ordering += [x for x in range(start, end)]\n",
    "\n",
    "ordering = train_ordering + test_ordering\n",
    "\n",
    "data_x = data_x[ordering]\n",
    "data_y = data_y[ordering]\n",
    "lengths = lengths[ordering]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "NONZERO_CLUSTERS = 10\n",
    "ZERO_CLUSTERS = 6\n",
    "\n",
    "kmeans = KMeans(n_clusters=NONZERO_CLUSTERS, random_state = 50)\n",
    "kmeans_zero = KMeans(n_clusters = ZERO_CLUSTERS, random_state = 50)\n",
    "unaugmented = [x for x in range(0, len(data_y))]\n",
    "zeros = [x for x in unaugmented if np.sum(data_y[x]) == 0]\n",
    "nonzero = [x for x in unaugmented if x not in zeros]\n",
    "kmeans.fit(data_y[nonzero, :, :].reshape((len(nonzero), 14*14)))\n",
    "kmeans_zero.fit(np.mean(data_x[zeros, :, :], axis = 1).reshape((len(zeros), 16*16*INP_FILTERS)))             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplot(matrices):\n",
    "    '''Plot multiple heatmaps with subplots'''\n",
    "    fig, axs = plt.subplots(ncols=4)\n",
    "    fig.set_size_inches(20, 4)\n",
    "    for i, matrix in enumerate(matrices):\n",
    "        sns.heatmap(data = matrix, ax = axs[i], vmin = 0, vmax = 0.9)\n",
    "        axs[i].set_xlabel(\"\")\n",
    "        axs[i].set_ylabel(\"\")\n",
    "        axs[i].set_yticks([])\n",
    "        axs[i].set_xticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "samples_x = [val for x, val in enumerate(nonzero) if kmeans.labels_[x] == 1]\n",
    "print(samples_x)\n",
    "randoms = random.sample(samples_x, 4)\n",
    "randoms = [data_y[x] for x in randoms]\n",
    "randoms = [x.reshape((14, 14)) for x in randoms]\n",
    "multiplot(randoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percs = np.sum(data_y.reshape((-1, 14*14)), axis = 1)\n",
    "idx = [x for x in range(0, len(data_y))]\n",
    "print([i for x, i in enumerate(sorted(percs)) if x % (len(data_y)//15) == 0 ])\n",
    "\n",
    "ids = {\n",
    "    0: [x for x, z in zip(idx, percs) if 0 < z <= 5],\n",
    "    1: [x for x, z in zip(idx, percs) if 5 < z <= 9],\n",
    "    2: [x for x, z in zip(idx, percs) if 9 < z <= 14],\n",
    "    3: [x for x, z in zip(idx, percs) if 14 < z <= 19],\n",
    "    4: [x for x, z in zip(idx, percs) if 19 < z <= 27],\n",
    "    5: [x for x, z in zip(idx, percs) if 27 < z <= 33],\n",
    "    6: [x for x, z in zip(idx, percs) if 33 < z <= 41],\n",
    "    7: [x for x, z in zip(idx, percs) if 41 < z <= 56],\n",
    "    8: [x for x, z in zip(idx, percs) if 56  < z <= 93],\n",
    "    9: [x for x, z in zip(idx, percs) if 93 < z <= 120],\n",
    "    10: [x for x, z in zip(idx, percs) if 120 < z]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(data_y, ids, labels, labels2, unaugmented = unaugmented):\n",
    "    # Loop over the nonzero clusters\n",
    "    #for i in range(0, NONZERO_CLUSTERS):\n",
    "    #    tmp = [val for x, val in enumerate(nonzero) if labels[x] == i]\n",
    "    #    ids[i] = tmp\n",
    "    # Loop over the zero clusters\n",
    "    for i in range(0, ZERO_CLUSTERS):\n",
    "        tmp = [val for x, val in enumerate(zeros) if labels2[x] == i]\n",
    "        ids[i + 11] = tmp\n",
    "    #ids[10] = zeros\n",
    "    return ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = balance_data(data_y, ids, kmeans.labels_, kmeans_zero.labels_ )\n",
    "items = [v for k, v in ids.items()]\n",
    "items = [item for sublist in items for item in sublist]\n",
    "print(\"The {} samples have been balanced between the sampling sites\".format(len(items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = []\n",
    "test_ids = []\n",
    "for i in ids:\n",
    "    ln = len(ids[i])\n",
    "    train_len = int(np.floor([ln * TRAIN_RATIO]))\n",
    "    test_len = ln - train_len\n",
    "    print(train_len, test_len, ln)\n",
    "    trains = ids[i][:train_len]\n",
    "    tests = ids[i][train_len:]\n",
    "    train_ids += trains\n",
    "    test_ids += tests\n",
    "    \n",
    "train_labels = []\n",
    "for i in train_ids:\n",
    "    train_labels.append([k for k, v in ids.items() if i in v][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train and test characteristics:\")\n",
    "print(\"Train mean Y {}\".format(np.mean([np.sum(x) for x in data_y[train_ids]])))\n",
    "print(\"Test mean Y {}\".format(np.mean([np.sum(x) for x in data_y[test_ids]])))\n",
    "print(\"Train STD Y {}\".format(np.std([np.sum(x) for x in data_y[train_ids]])))\n",
    "print(\"Test STD Y {}\".format(np.std([np.sum(x) for x in data_y[test_ids]])))\n",
    "print(\"Train number with zero trees {}\".format(0.2*len([x for x in data_y[train_ids] if np.sum(x) == 0])))\n",
    "print(\"Test number with zero trees {}\".format(0.8*len([x for x in data_y[test_ids] if np.sum(x) == 0])))\n",
    "print(\"Train mean NDVI\")\n",
    "print(\"Test mean NDVI\")\n",
    "print(\"There are {} train and {} test samples\".format(len(train_ids), len(test_ids)))\n",
    "print(\"There is {} overlap between train and test\".format(len([x for x in train_ids if x in test_ids])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_augmented = []\n",
    "data_y_augmented = []\n",
    "lengths_augmented = []\n",
    "labels_augmented = []\n",
    "for i, val in enumerate(train_ids):\n",
    "    data_x_augmented.append(data_x[val])\n",
    "    data_y_augmented.append(data_y[val])\n",
    "    lengths_augmented.append(data_x[val].shape[0])\n",
    "    labels_augmented.append(train_labels[i])\n",
    "    \n",
    "    x1 = np.flip(data_x[val], 1)\n",
    "    y1 = np.flip(data_y[val], 0)\n",
    "    lengths_augmented.append(x1.shape[0])\n",
    "    labels_augmented.append(train_labels[i])\n",
    "    data_x_augmented.append(x1)\n",
    "    data_y_augmented.append(y1)\n",
    "    \n",
    "    x1 = np.flip(data_x[val], [2, 1])\n",
    "    y1 = np.flip(data_y[val], [1, 0])\n",
    "    lengths_augmented.append(x1.shape[0])\n",
    "    labels_augmented.append(train_labels[i])\n",
    "    data_x_augmented.append(x1)\n",
    "    data_y_augmented.append(y1)\n",
    "    \n",
    "    x1 = np.flip(data_x[val], 2)\n",
    "    y1 = np.flip(data_y[val], 1)\n",
    "    lengths_augmented.append(x1.shape[0])\n",
    "    labels_augmented.append(train_labels[i])\n",
    "    data_x_augmented.append(x1)\n",
    "    data_y_augmented.append(y1)\n",
    "\n",
    "train_x = np.stack(data_x_augmented)\n",
    "train_y = np.stack(data_y_augmented)\n",
    "train_y = np.reshape(train_y, (train_y.shape[0], 14, 14, 1))\n",
    "train_l = np.stack(lengths_augmented)\n",
    "train_l = np.reshape(train_l, (train_y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = data_x[test_ids]\n",
    "test_y = data_y[test_ids]\n",
    "test_lengths = lengths[test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RANDOM TRAIN SAMPLES - SHOULD BE AUGMENTED\")\n",
    "multiplot([x.reshape(14, 14) for x in train_y[25:29]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RANDOM TEST SAMPLES - SHOULD BE NOT AUGMENTED\")\n",
    "multiplot([x.reshape(14, 14) for x in test_y[25:29]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "import math\n",
    "\n",
    "def smooth_jaccard(y_true, y_pred, smooth=1):\n",
    "    y_true = tf.reshape(y_true, (-1, 14*14))\n",
    "    y_pred = tf.reshape(y_pred, (-1, 14*14))\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def focal_loss_fixed(y_true, y_pred, gamma = 2., alpha = 0.25):\n",
    "    y_true = tf.reshape(y_true, (-1, 14*14, 1))\n",
    "    y_pred = tf.reshape(y_pred, (-1, 14*14, 1))\n",
    "    y_pred = K.clip(y_pred, 1e-8, 1-1e-8)\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "    epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "    pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "    pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "    loss = -K.mean(alpha * 1 * K.log(K.epsilon()+pt_1)) - K.mean((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
    "    return 2 * tf.reduce_mean(loss)\n",
    "\n",
    "def bce_jaccard(y_true, y_pred):\n",
    "    jac = smooth_jaccard(y_true, y_pred)\n",
    "    bce = binary_crossentropy(y_true, y_pred)\n",
    "    return bce + 0.15*tf.reshape(jac, (-1, 1, 1, 1))\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight, smooth = 0.01):\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    #y_true = K.clip(y_true, smooth, 1. - smooth)\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "        y_true,\n",
    "        logit_y_pred,\n",
    "        weight,\n",
    "    )\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def lovasz_foc(y_true, y_pred):\n",
    "    foc_losses = []\n",
    "    #bce = weighted_bce_loss(y_true, y_pred, weight = 1.3)\n",
    "    foc = focal_loss_fixed(y_true, y_pred, gamma = 1.3)\n",
    "    lv_image = lovasz_softmax(y_pred, tf.reshape(y_true, (-1, 14, 14)), classes=[1], per_image=True)\n",
    "    return foc + 0.5 * lv_image\n",
    "\n",
    "def lovasz(y_true, y_pred):\n",
    "    lv = lovasz_softmax(y_pred, tf.reshape(y_true, (-1, 14, 14)), classes=[1], per_image=True)\n",
    "    return lv\n",
    "\n",
    "def calculate_metrics():\n",
    "    best_f1 = 0\n",
    "    best_thresh = 0\n",
    "    p = 0\n",
    "    r = 0\n",
    "    error = 0\n",
    "    for j in range(7, 12):\n",
    "        tps = []\n",
    "        fps = []\n",
    "        fns = []\n",
    "        perc_error = []\n",
    "        trues = []\n",
    "        preds = []\n",
    "        val_loss = []\n",
    "        for m in test_ids:\n",
    "            y, vl = sess.run([fm, test_loss], feed_dict={inp: test_x[m].reshape(1, 24, 16, 16, INP_FILTERS),\n",
    "                                      length: test_lengths[m].reshape(1, 1),\n",
    "                                      is_training: False,\n",
    "                                      labels: test_y[m, :, :].reshape(1, 14, 14),\n",
    "                                      })\n",
    "            true = test_y[m].reshape((14, 14))\n",
    "            pred = y.reshape((14, 14))\n",
    "            pred[np.where(pred > j*0.05)] = 1\n",
    "            pred[np.where(pred < j*0.05)] = 0\n",
    "            true_s = np.sum(true)\n",
    "            pred_s = np.sum(pred)\n",
    "            #true_p = true_s - (true_s - pred_s)\n",
    "            perc_error.append(abs(pred_s - true_s) / 196)\n",
    "            tp, fp, fn = thirty_meter(true, pred)\n",
    "            tps.append(tp)\n",
    "            fps.append(fp)\n",
    "            fns.append(fn)\n",
    "            trues.append(true_s)\n",
    "            preds.append(pred_s)\n",
    "            val_loss.append(np.mean(vl))\n",
    "        oa_error = abs(np.sum(preds) - np.sum(trues)) / np.sum(trues)\n",
    "        precision = np.sum(tps) / (np.sum(tps) + np.sum(fps))\n",
    "        recall = np.sum(tps) / (np.sum(tps) + np.sum(fns))\n",
    "        f1 = 2*((precision* recall) / (precision + recall))\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            p = precision\n",
    "            r = recall\n",
    "            error = oa_error\n",
    "            best_thresh = j*0.05\n",
    "    print(\"Val loss: {} Thresh: {} F1: {} Recall: {} Precision: {} Error: {}\".format(np.around(np.mean(val_loss), 3), np.around(best_thresh, 2),\n",
    "                                                                                     np.around(best_f1, 3), np.around(p, 3), np.around(r, 3), \n",
    "                                                                                     np.around(error, 3)))\n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [x for x in range(0, len(train_y))]\n",
    "print(len(train_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equibatch creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def equibatch(train_ids, lovasz = False):\n",
    "    first_len = 5\n",
    "    second_len = 9\n",
    "    third_len = 14\n",
    "    np.random.shuffle(train_ids)\n",
    "    ix = train_ids\n",
    "    percs = [np.sum(x) for x in train_y[ix]]\n",
    "    zero_ids = [x for x, z in zip(ix, percs) if z == 0]\n",
    "    one_ids = [x for x, z in zip(ix, percs) if 0 < z <= first_len]\n",
    "    two_ids = [x for x, z in zip(ix, percs) if first_len < z <= second_len]\n",
    "    three_ids = [x for x, z in zip(ix, percs) if second_len < z <= third_len]\n",
    "    four_ids = [x for x, z in zip(ix, percs) if third_len < z <= 19]\n",
    "    five_ids = [x for x, z in zip(ix, percs) if 19 < z < 27]\n",
    "    six_ids = [x for x, z in zip(ix, percs) if 27 < z <= 33]\n",
    "    seven_ids = [x for x, z in zip(ix, percs) if 33 < z <= 41]\n",
    "    eight_ids = [x for x, z in zip(ix, percs) if 41 < z <= 56]\n",
    "    nine_ids =  [x for x, z in zip(ix, percs) if 56 < z <= 80]\n",
    "    ten_ids =  [x for x, z in zip(ix, percs) if 80 < z <= 120]\n",
    "    eleven_ids = [x for x, z in zip(ix, percs) if 120 < z]\n",
    "    #ten_ids = [x for x, z in zip(ix, percs) if 125 < z]\n",
    "    \n",
    "    \n",
    "\n",
    "    all_ids = [x for x in [zero_ids, one_ids, two_ids, three_ids, four_ids, five_ids, six_ids,\n",
    "              seven_ids, eight_ids, nine_ids, ten_ids, eleven_ids]]\n",
    "    \n",
    "    new_batches = []\n",
    "    maxes = [len(zero_ids), len(one_ids), len(two_ids), len(three_ids), len(four_ids),\n",
    "             len(five_ids), len(six_ids), len(seven_ids), len(eight_ids), len(nine_ids), len(ten_ids), len(eleven_ids)]#, len(ten_ids)]\n",
    "    cur_ids = [0] * 12\n",
    "    iter_len = len(train_ids)// 16\n",
    "    for i in range(0, iter_len):\n",
    "        random_ids = np.random.randint(0, 12, 3)\n",
    "        for i, val in enumerate(cur_ids):\n",
    "            if val > maxes[i] - 5:\n",
    "                cur_ids[i] = 0\n",
    "        if cur_ids[0] >= (maxes[0] - 2):\n",
    "            cur_ids[0] = 0\n",
    "        to_append = [zero_ids[cur_ids[0]], zero_ids[cur_ids[0] + 1], one_ids[cur_ids[1]], two_ids[cur_ids[2]],\n",
    "                    three_ids[cur_ids[3]], four_ids[cur_ids[4]], five_ids[cur_ids[5]],\n",
    "                    six_ids[cur_ids[6]], seven_ids[cur_ids[7]], eight_ids[cur_ids[8]],\n",
    "                               nine_ids[cur_ids[9]], ten_ids[cur_ids[10]], eleven_ids[cur_ids[11]],\n",
    "                    all_ids[random_ids[0]][cur_ids[random_ids[0]]+1],\n",
    "                     all_ids[random_ids[1]][cur_ids[random_ids[1]]+1],\n",
    "                     all_ids[random_ids[2]][cur_ids[random_ids[2]]]+1]\n",
    "        np.random.shuffle(to_append)\n",
    "        new_batches.append(to_append)\n",
    "        cur_ids = [x + 1 for x in cur_ids]\n",
    "        cur_ids[0] += 1\n",
    "        for x in random_ids:\n",
    "            cur_ids[x] += 1\n",
    "        \n",
    "    new_batches = [item for sublist in new_batches for item in sublist]\n",
    "    #overlap = [x for x in new_batches if x in test_ids]\n",
    "    #print(\"There is {} overlap. Error if > 0\".format(len(overlap)))\n",
    "    return new_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = equibatch(train_ids, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[:4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[4:8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[8:12]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([np.sum(x) for x in train_y[batch]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_saver = tf.train.import_meta_graph('../models/f1_auc/model.meta')\n",
    "#new_saver.restore(sess, tf.train.latest_checkpoint('../models/f1_auc/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRESH_START = False\n",
    "FINE_TUNE = False\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "print(\"Starting model with: \\n {} zone out \\n {} l2 \\n {} initial LR \\n {} final LR \\n {} parameters\"\n",
    "     .format(ZONE_OUT_PROB, L2_REG, INITIAL_LR, FINAL_LR, total_parameters))\n",
    "best_val = 0.620\n",
    "if not FRESH_START:\n",
    "    print(\"Resuming training with a best validation score of {}\".format(best_val))\n",
    "if FRESH_START:\n",
    "    print(\"Restarting training from scratch on {} train and {} test samples, total {}\".format(len(train_ids), len(test_ids), N_SAMPLES))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(2e-4, epsilon = 1e-7)\n",
    "    optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, clip_norm=1e-1)\n",
    "\n",
    "    ft_optimizer = tf.train.AdamOptimizer(1e-4, epsilon = 1e-4)\n",
    "    ft_optimizer = tf.contrib.estimator.clip_gradients_by_norm(ft_optimizer, clip_norm=0.1)\n",
    "    \n",
    "    train_loss = lovasz_foc(tf.reshape(labels, (-1, 14, 14, 1)), fm)\n",
    "    #train_loss = focal_loss_fixed(labels, fm, gamma = 1.1, alpha = 0.5)\n",
    "    test_loss = weighted_bce_loss(tf.reshape(labels, (-1, 14, 14, 1)), fm, weight = 1.)\n",
    "    ft_loss = lovasz(tf.reshape(labels, (-1, 14, 14, 1)), fm)\n",
    "    l2_loss = tf.losses.get_regularization_loss()\n",
    "    train_loss += l2_loss\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(train_loss)   \n",
    "        #wu_op = wu_optimizer.minimize(wu_loss)\n",
    "        ft_op = ft_optimizer.minimize(ft_loss)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver(max_to_keep = 2)\n",
    "\n",
    "# Run training 1oop\n",
    "for i in range(1, 200):\n",
    "    randomize = equibatch(train_ids, lovasz = False)\n",
    "    randomize = [np.min([x, len(train_y) - 1]) for x in randomize]\n",
    "        #print(\"Loss: BCE + 0.30 Lovasz\")\n",
    "    op = train_op\n",
    "    loss = train_loss\n",
    "    #op = ft_op\n",
    "    #loss = ft_loss\n",
    "    BATCH_SIZE = 16\n",
    "    test_ids = [x for x in range(0, len(test_x))]\n",
    "    losses = []\n",
    "    bce_losses = []\n",
    "    \n",
    "    for k in tnrange(int(len(train_ids) // BATCH_SIZE)):\n",
    "        batch_ids = randomize[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "        batch_y = train_y[batch_ids, :, :].reshape(len(batch_ids), 14, 14)\n",
    "        opt, tr = sess.run([op, loss],\n",
    "                              feed_dict={inp: train_x[batch_ids, :, :, :],\n",
    "                                         length: train_l[batch_ids].reshape((-1, 1)),\n",
    "                                         labels: batch_y,\n",
    "                                         is_training: True,\n",
    "                                         keep_rate: np.max((1 - (i*0.005), 0.85))\n",
    "                                         })\n",
    "        losses.append(tr)\n",
    "    \n",
    "    print(\"Epoch {}: Loss {}\".format(i, np.around(np.mean(losses[:-1]), 3)))\n",
    "    f1 = calculate_metrics()\n",
    "    if f1 > best_val:\n",
    "        best_val = f1\n",
    "        print(\"Saving model with {}\".format(f1))\n",
    "        save_path = saver.save(sess, \"../models/f1_auc/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation and sanity checks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "test_ids = [x for x in range(0, len(test_x))]\n",
    "\n",
    "def multiplot(matrices, nrows = 2, ncols = 4):\n",
    "    '''Plot multiple heatmaps with subplots'''\n",
    "    fig, axs = plt.subplots(ncols=4, nrows = nrows)\n",
    "    fig.set_size_inches(20, 4*nrows)\n",
    "    to_iter = [[x for x in range(i, i + ncols + 1)] for i in range(0, nrows*ncols, ncols)]\n",
    "    for r in range(1, nrows + 1):\n",
    "        min_i = min(to_iter[r-1])\n",
    "        max_i = max(to_iter[r-1])\n",
    "        for i, matrix in enumerate(matrices[min_i:max_i]):\n",
    "            sns.heatmap(data = matrix, ax = axs[r - 1, i], vmin = 0, vmax = 0.9)\n",
    "            axs[r - 1, i].set_xlabel(\"\")\n",
    "            axs[r - 1, i].set_ylabel(\"\")\n",
    "            axs[r - 1, i].set_yticks([])\n",
    "            axs[r - 1, i].set_xticks([])\n",
    "    plt.show()\n",
    "start = 15\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "    \n",
    "test_losses = []\n",
    "print(start/len(test_ids))\n",
    "test_ids = sorted(test_ids)\n",
    "matrix_ids = [test_ids[start], test_ids[start + 1], test_ids[start + 2], test_ids[start + 3],\n",
    "              test_ids[start + 4], test_ids[start + 5], test_ids[start + 6], test_ids[start + 7]]\n",
    "#matrix_ids = random.sample(test_ids, 4)z\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    print(i)\n",
    "    y = sess.run([fm], feed_dict={inp: test_x[idx].reshape(1, 24, IMAGE_SIZE, IMAGE_SIZE, INP_FILTERS),\n",
    "                                  length: test_lengths[idx].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })\n",
    "    y = np.array(y).reshape(14, 14)\n",
    "    preds.append(y)\n",
    "    true = test_y[idx].reshape(LABEL_SIZE, LABEL_SIZE)\n",
    "    trues.append(true)\n",
    "    \n",
    "to_plot = trues[0:4] + preds[0:4]# + trues[5:] + preds[5:]\n",
    "multiplot(to_plot, nrows = 2, ncols = 4)\n",
    "#plot_ids[ordering[976]//4] \n",
    "\n",
    "start = start+ 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "    \n",
    "test_losses = []\n",
    "print(start/len(test_ids))\n",
    "test_ids = sorted(test_ids)\n",
    "#matrix_ids = [test_ids[start], test_ids[start + 1], test_ids[start + 2], test_ids[start + 3],\n",
    "#              test_ids[start + 4], test_ids[start + 5], test_ids[start + 6], test_ids[start + 7]]\n",
    "matrix_ids = random.sample(train_ids, 4)\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    print(i)\n",
    "    y = sess.run([fm], feed_dict={inp: train_x[idx].reshape(1, 24, IMAGE_SIZE, IMAGE_SIZE, INP_FILTERS),\n",
    "                                  length: train_l[idx].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })\n",
    "    y = np.array(y).reshape(14, 14)\n",
    "    preds.append(y)\n",
    "    true = train_y[idx].reshape(LABEL_SIZE, LABEL_SIZE)\n",
    "    trues.append(true)\n",
    "    \n",
    "\n",
    "to_plot = trues[0:4] + preds[0:4]# + trues[5:] + preds[5:]\n",
    "multiplot(to_plot, nrows = 2, ncols = 4)\n",
    "#plot_ids[ordering[976]//4] \n",
    "start = start + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ids2 = [val for x, val in enumerate(plot_ids) if x not in list(set([x // 4 for x in outliers]))]\n",
    "plot_ids2[ordering[460]//4] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate ROC for best threshold selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO @jombrandt top 10 worst training, test samples by IOU \n",
    "\n",
    "These should be written to a tmp/ .txt file and indexed by validate-data.ipynb to ensure that original classifications were correct, and to identify regions that need more training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy-toolkit",
   "language": "python",
   "name": "policy-toolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
