{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - UNET -- 16x16 none padding\n",
    "\n",
    "## John Brandt\n",
    "\n",
    "### Last updated: Oct 3 2019, 75% precision, 76% recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "import tensorflow as tf\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU, LeakyReLU\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras.layers import Conv2D, Lambda, Dense, Multiply, Add\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import itertools\n",
    "from tflearn.layers.conv import global_avg_pool\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.layers import batch_normalization\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/zoneout.py\n",
    "%run ../src/convgru.py\n",
    "%run ../src/lovasz.py\n",
    "%run ../src/utils.py\n",
    "%run ../src/adabound.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONE_OUT_PROB = 0.3 #(0.05, 0.20, 0.05) --> 4\n",
    "L2_REG = 0.005 #(1-e6, 1-e1, x10) --> 5\n",
    "INITIAL_LR = 2e-6 #(1e-6, 1e-3, x5) --> 10\n",
    "FINAL_LR = 2e-4 # (1e - 5, 1e-2, x5) --> 10\n",
    "LOSS_WEIGHTING = 0.5 #(0.2, 1, 0.2) --> 5\n",
    "SQUEEZE_RATIO = 4 # --> 4, 8, 12, 16 --> 4\n",
    "BN_MOMENTUM = 0.9 # --> 3\n",
    "N_LAYERS = 4 # --> 3\n",
    "REG_TYPE = 'kernel' # kernel # --> 2\n",
    "SQUEEZE = True\n",
    "LAYER_NORM = True \n",
    "BATCH_SIZE = 4 # -->4\n",
    "LOSS_TYPE = 'bce-jaccard' #bce-jaccard, bce-dice, bce-lovasz, focal-jaccard, etc. --> 4\n",
    "N_CONV_PER_LAYER = 1 # --> 2\n",
    "ACTIVATION_TYPE = 'ELU' #RELU, PRELU --> 2\n",
    "MASK_LOSS = False # --> 2\n",
    "PAD_INPUT_TYPE = 'none' # zero, reflect, none # --> 2\n",
    "RENORM_CLIPPING = None # --> 5\n",
    "FRESH_START = False\n",
    "TRAIN_RATIO = 0.8\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "\n",
    "AUGMENTATION_RATIO = 4\n",
    "IMAGE_SIZE = 16\n",
    "existing = [int(x[:-4]) for x in os.listdir('../data/2018/') if \".DS\" not in x]\n",
    "#existing = [x for x in existing1 if x in existing2]\n",
    "N_SAMPLES = len(existing)\n",
    "RESIZE_OUTPUT = False\n",
    "\n",
    "LABEL_SIZE = 14\n",
    "#if LABEL_SIZE == 16 and not RESIZE_OUTPUT:\n",
    "#    LABEL_SIZE = IMAGE_SIZE\n",
    "    \n",
    "TRAIN_SAMPLES = int((N_SAMPLES * AUGMENTATION_RATIO) * TRAIN_RATIO)\n",
    "TEST_SAMPLES = int((N_SAMPLES * AUGMENTATION_RATIO) - TRAIN_SAMPLES)\n",
    "print(TRAIN_SAMPLES // AUGMENTATION_RATIO, N_SAMPLES - (TRAIN_SAMPLES // AUGMENTATION_RATIO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_elu(inp, is_training, kernel_size, scope, filter_count = 16):\n",
    "    if kernel_size == 3:\n",
    "        padded = ReflectionPadding2D((1, 1,))(inp)\n",
    "        padding = 'valid'\n",
    "    else:\n",
    "        padded = inp\n",
    "        padding = 'same'\n",
    "    conv = Conv2D(filters = filter_count, kernel_size = (kernel_size, kernel_size),\n",
    "                      padding = padding, kernel_regularizer=reg)(padded)\n",
    "    elu = ELU()(conv)\n",
    "    bn = Batch_Normalization(elu, training=is_training, scope = scope + \"bn\")\n",
    "    return bn\n",
    "    \n",
    "def fpa(inp, filter_count):\n",
    "    one = conv_bn_elu(inp, is_training, 1, 'forward1', filter_count)\n",
    "    three = conv_bn_elu(inp, is_training, 3, 'down1', filter_count)\n",
    "    three_f = conv_bn_elu(three, is_training, 3, 'down1_f', filter_count)\n",
    "    two = conv_bn_elu(three, is_training, 2, 'down2', filter_count)\n",
    "    two_f = conv_bn_elu(two, is_training, 2, 'down2_f', filter_count)\n",
    "    \n",
    "    # top block\n",
    "    pooled = tf.keras.layers.GlobalAveragePooling2D()(inp)\n",
    "    one_top = conv_bn_elu(tf.reshape(pooled, (-1, 1, 1, pooled.shape[-1])), is_training, 1, 'top1', filter_count)\n",
    "    four_top = tf.keras.layers.UpSampling2D((4, 4))(one_top)\n",
    "    \n",
    "    \n",
    "    concat_1 = tf.multiply(one, tf.add(three_f, two_f))\n",
    "    concat_2 = tf.add(concat_1, four_top)\n",
    "    print(\"Feature pyramid attention shape {}\".format(concat_2.shape))\n",
    "    return concat_2\n",
    "    \n",
    "    \n",
    "\n",
    "def gau(x_low_level, x_high_level, scope, filter_count, size = 4):\n",
    "    \"\"\"\n",
    "    The global attention upsample to replace the up_cat_conv element\n",
    "    \"\"\"\n",
    "    low_feat = conv_bn_elu(x_low_level, is_training, 3, 'gauforward' + scope, filter_count)\n",
    "    high_gap = tf.keras.layers.GlobalAveragePooling2D()(x_high_level)\n",
    "    high_feat = tf.keras.layers.Reshape((1, 1, -1))(high_gap)\n",
    "    high_feat_gate = tf.keras.layers.UpSampling2D((size, size))(high_feat)\n",
    "    gated_low = tf.keras.layers.multiply([low_feat, high_feat_gate])\n",
    "    gated_low = conv_bn_elu(gated_low, is_training, 3, 'gauforward5' + scope, filter_count)\n",
    "    gated_high = tf.keras.layers.Conv2DTranspose(filters = filter_count, kernel_size = (3, 3),\n",
    "                                             strides=(2, 2), padding='same', kernel_regularizer = reg)(gated_low)\n",
    "    high_clamped = conv_bn_elu(x_high_level, is_training, 3, 'gauforward1' + scope, filter_count)\n",
    "    return tf.keras.layers.add([gated_high, high_clamped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.contrib.slim import conv2d\n",
    "from tensorflow.contrib.resampler import resampler\n",
    "\n",
    "def cse_block(prevlayer, prefix):\n",
    "    mean = Lambda(lambda xin: K.mean(xin, axis=[1, 2]))(prevlayer)\n",
    "    lin1 = Dense(K.int_shape(prevlayer)[3] // 2, name=prefix + 'cse_lin1', activation='relu')(mean)\n",
    "    lin2 = Dense(K.int_shape(prevlayer)[3], name=prefix + 'cse_lin2', activation='sigmoid')(lin1)\n",
    "    x = Multiply()([prevlayer, lin2])\n",
    "    return x\n",
    "\n",
    "\n",
    "def sse_block(prevlayer, prefix):\n",
    "    # Bug? Should be 1 here?\n",
    "    conv = Conv2D(K.int_shape(prevlayer)[3], (1, 1), padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                  activation='sigmoid', strides=(1, 1),\n",
    "                  name=prefix + \"_conv\")(prevlayer)\n",
    "    conv = Multiply(name=prefix + \"_mul\")([prevlayer, conv])\n",
    "    return conv\n",
    "\n",
    "\n",
    "def csse_block(x, prefix):\n",
    "    '''\n",
    "    Implementation of Concurrent Spatial and Channel ‘Squeeze & Excitation’ in Fully Convolutional Networks\n",
    "    https://arxiv.org/abs/1803.02579\n",
    "    '''\n",
    "    cse = cse_block(x, prefix)\n",
    "    sse = sse_block(x, prefix)\n",
    "    x = Add(name=prefix + \"_csse_mul\")([cse, sse])\n",
    "\n",
    "    return x\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    return batch_normalization(inputs=x, \n",
    "                               momentum = BN_MOMENTUM, \n",
    "                               training=training,\n",
    "                               renorm = True,\n",
    "                               reuse=None,\n",
    "                               name = scope)\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.ones([14, 14], tf.float32)\n",
    "weights = tf.pad(weights, [[1, 1], [1, 1]], 'constant')\n",
    "weights = tf.reshape(weights, (16*16,)) \n",
    "\n",
    "reg = keras.regularizers.l2(L2_REG)\n",
    "inp = tf.placeholder(tf.float32, shape=(None, 9, 24, IMAGE_SIZE, IMAGE_SIZE, 15))\n",
    "length = tf.placeholder(tf.int32, shape = (None, 1))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 14, 14))#, 1))\n",
    "\n",
    "\n",
    "\n",
    "length2 = tf.reshape(length, (-1,))\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training')\n",
    "power = tf.placeholder_with_default(1.0, (), 'power')\n",
    "\n",
    "if PAD_INPUT_TYPE == 'zero':\n",
    "    inp_pad = tf.pad(inp, [[0, 0], [0, 0], [1, 1], [1, 1], [0, 0]], \"CONSTANT\")\n",
    "\n",
    "if PAD_INPUT_TYPE == 'reflect':\n",
    "    inp_pad = tf.pad(inp, [[0, 0], [0, 0], [1,1], [1,1], [0,0] ], 'REFLECT')\n",
    "    \n",
    "if PAD_INPUT_TYPE == 'none':\n",
    "    inp_pad = inp\n",
    "    \n",
    "FILTER_SIZE = LABEL_SIZE if RESIZE_OUTPUT else IMAGE_SIZE\n",
    "\n",
    "down_16 = 10\n",
    "down_8 = 30\n",
    "down_4f = 45\n",
    "#down_2f = 2\n",
    "#up_4 = 30\n",
    "up_8 = 30\n",
    "up_16 = 20\n",
    "\n",
    "def down_block(inp, length, size, flt, scope, train):\n",
    "    with tf.variable_scope(scope):\n",
    "        cell_fw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'SAME')\n",
    "        cell_bw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'SAME')\n",
    "        cell_fw = ZoneoutWrapper(\n",
    "            cell_fw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        cell_bw = ZoneoutWrapper(\n",
    "            cell_bw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        gru = convGRU(inp, cell_fw, cell_bw, length)\n",
    "        down = TimeDistributed(MaxPool2D(pool_size = (2, 2)))(gru[0])\n",
    "        print(\"Down block shape: {}\".format(gru[1].shape))\n",
    "    return down, gru[1]\n",
    "\n",
    "def down_block_no_gru(inp, flt, scope, train):\n",
    "    with tf.variable_scope(scope):\n",
    "        padded = ReflectionPadding2D((1, 1))(inp)\n",
    "        \n",
    "        # Conv block 1\n",
    "        conv = Conv2D(filters = flt, kernel_size = (3, 3),\n",
    "                      padding = 'valid', kernel_regularizer=reg)(padded)\n",
    "        elu = ELU()(conv)\n",
    "        bn = Batch_Normalization(elu, training=is_training, scope = scope + \"bn\")\n",
    "        x = csse_block(bn, prefix='csse_block_{}'.format(scope))\n",
    "        down = MaxPool2D(pool_size = (2, 2))(x)\n",
    "        print(\"Down block shape: {}\".format(down.shape))\n",
    "    return down\n",
    "\n",
    "\n",
    "def up_block(inp, concat_inp, flt, sq, scope, concat, is_training, padding = True):\n",
    "    with tf.variable_scope(scope):\n",
    "        \n",
    "        gau_layer = gau(inp, concat_inp, scope, flt, inp.shape[-2])\n",
    "        x = csse_block(gau_layer, prefix='csse_block_{}'.format(scope))\n",
    "        print(\"Up block conv 1 shape: {}\".format(x.shape))\n",
    "        return x\n",
    "        \n",
    "        \n",
    "down_1, copy_1 = down_block(inp_pad, length2, [FILTER_SIZE, FILTER_SIZE], down_16, 'down_16', is_training)\n",
    "down_2 = down_block_no_gru(copy_1, down_8, 'down_8', is_training)\n",
    "down_3 = down_block_no_gru(down_2, down_4f, 'down_4', is_training)\n",
    "\n",
    "down_fpa = fpa(down_3, down_4f)\n",
    "up_3 = up_block(down_fpa, down_2, up_8, up_8, 'up_8', True, is_training, padding =  True) # 4 - 8\n",
    "up_2 = up_block(up_3, copy_1, up_16, up_16, 'up_16', True, is_training, padding = True) # 8 - 16\n",
    "up_4_16 = tf.keras.layers.Conv2DTranspose(filters = up_8, kernel_size = (3, 3),\n",
    "                                             strides=(2, 2), padding='same', kernel_regularizer = reg)(down_2)\n",
    "\n",
    "up_8_16 = tf.keras.layers.Conv2DTranspose(filters = up_16, kernel_size = (3, 3),\n",
    "                                             strides=(2, 2), padding='same', kernel_regularizer = reg)(up_3)\n",
    "\n",
    "concat_final = tf.concat([up_2, up_4_16, up_8_16], axis = -1)\n",
    "\n",
    "up_2 = Conv2D(filters = 20, kernel_size = (3, 3), padding = 'valid', kernel_regularizer=reg)(concat_final)\n",
    "elu = ELU()(up_2)\n",
    "bn = Batch_Normalization(elu, training=is_training, scope = \"out1bn\")\n",
    "x = csse_block(bn, prefix='csse_block_{}'.format(\"out1\"))\n",
    "\n",
    "up_2 = Conv2D(filters = 20, kernel_size = (3, 3), padding = 'valid', kernel_regularizer=reg)(x)\n",
    "elu = ELU()(up_2)\n",
    "\n",
    "#B = tf.Variable([-np.log(0.99/0.01)]) \n",
    "init = tf.constant_initializer([-np.log(0.9/0.1)])\n",
    "fm = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1), \n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid',\n",
    "            bias_initializer = init,\n",
    "            )(elu)\n",
    "print(fm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/subplot.csv\")\n",
    "df1 = pd.read_csv(\"../data/subplot2.csv\")\n",
    "df2 = pd.read_csv(\"../data/subplot3.csv\")\n",
    "df3 = pd.read_csv(\"../data/subplot4.csv\")\n",
    "\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df1 = df1.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df2 = df2.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df3 = df3.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "\n",
    "lens = [len(x) for x in [df, df1, df2, df3]]\n",
    "\n",
    "df = pd.concat([df, df1, df2, df3], ignore_index = True)\n",
    "df = df.dropna(axis = 0)\n",
    "\n",
    "#existing1 = [int(x[:-4]) for x in os.listdir('../data/2017_data/') if \".DS\" not in x]\n",
    "existing = [int(x[:-4]) for x in os.listdir('../data/2018/') if \".DS\" not in x]\n",
    "#existing = [x for x in existing1 if x in existing2]\n",
    "N_SAMPLES = len(existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['PLOT_ID'].isin(existing)]\n",
    "N_SAMPLES = int(df.shape[0]/196)\n",
    "N_YEARS = 1\n",
    "\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "\n",
    "locs_ls = []\n",
    "\n",
    "def reconstruct_images(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    rows = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        rows.append(list(subs_lat['TREE']))\n",
    "    return rows\n",
    "\n",
    "data = [reconstruct_images(x) for x in plot_ids]\n",
    "\n",
    "# Initiate empty lists to store the X and Y data in\n",
    "data_x, data_y, lengths = [], [], []\n",
    "\n",
    "# Iterate over each plot\n",
    "pad = True\n",
    "flip = True\n",
    "for i in plot_ids:\n",
    "    # Load the sentinel imagery\n",
    "    for year in [\"2018\"]: #\"2017_data\", \n",
    "        x = np.load(\"../data/\" + year + \"/\" + str(i) + \".npy\")\n",
    "        # Shape check\n",
    "        x = ndvi(x, image_size = 16)\n",
    "        x = evi(x, image_size = 16)\n",
    "        x = savi(x, image_size = 16)\n",
    "        x = remove_blank_steps(x)\n",
    "        x_grad, y_grad = np.gradient(np.reshape(x[0, :, :, 10], (16, 16)))\n",
    "        #x[:, :, :, 10] = (x[:, :, :, 10] - np.min(x[:, :, :, 10]) / np.max(x[:, :, :, 10])\n",
    "        mag = np.stack([np.reshape(np.sqrt(x_grad**2 + y_grad**2)*10, (16, 16, 1))]*x.shape[0])\n",
    "        #if np.max(mag) > 0:\n",
    "        #    mag = (mag - np.min(mag)) / np.max(mag)\n",
    "        x = np.concatenate([x, mag], axis = -1)\n",
    "        print(np.max(x[:, :, :, 14]))\n",
    "        y = reconstruct_images(i)\n",
    "        lengths.append(x.shape[0])\n",
    "        if x.shape[0] < 24:\n",
    "            padding = np.zeros((24 - x.shape[0], IMAGE_SIZE, IMAGE_SIZE, 13))\n",
    "            x = np.concatenate((x, padding), axis = 0)\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "        if flip:\n",
    "                # FLIP HORIZONTAL\n",
    "            x1 = np.flip(x, 1)\n",
    "            data_x.append(x1)\n",
    "            data_y.append(np.flip(y, 0))\n",
    "            lengths.append(x.shape[0])\n",
    "\n",
    "                # FLIP BOTH\n",
    "            x2 = np.flip(x, 2)\n",
    "            x2 = np.flip(x2, 1)\n",
    "            data_x.append(x2)\n",
    "            data_y.append(np.flip(y, [0, 1]))\n",
    "            lengths.append(x.shape[0])\n",
    "                # FLIP VERTICAL\n",
    "            x3 = np.flip(x, 2)\n",
    "            data_x.append(x3)\n",
    "            data_y.append(np.flip(y, 1))\n",
    "            lengths.append(x.shape[0])\n",
    "\n",
    "data_x = np.stack(data_x)\n",
    "data_y = np.stack(data_y)\n",
    "data_y = np.reshape(data_y, (N_SAMPLES*4*N_YEARS, 14, 14, 1))\n",
    "lengths = np.stack(lengths)\n",
    "lengths = np.reshape(lengths, (lengths.shape[0], 1))\n",
    "\n",
    "if PAD_INPUT_TYPE == 'zero' and RESIZE_OUTPUT:\n",
    "    data_y = np.pad(data_y, [[0, 0], [1, 1], [1, 1], [0, 0]], 'constant')\n",
    "    \n",
    "if PAD_INPUT_TYPE == 'reflect' and RESIZE_OUTPUT:\n",
    "    data_y = np.pad(data_y, [[0, 0], [1, 1], [1, 1], [0, 0]], 'reflect')\n",
    "print(\"Finished data loading\")\n",
    "print(data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len2 = [x//196 for x in lens]\n",
    "print(len2)\n",
    "MULT = 4 * N_YEARS\n",
    "\n",
    "ordering = [[x for x in range(0,int(200*TRAIN_RATIO))],\n",
    "            [x for x in range(200,200+(int(136*TRAIN_RATIO)))], \n",
    "            [x for x in range(200+136,200+136+(int(162*TRAIN_RATIO)))],\n",
    "           [x for x in range(200+136+162,200+136+162+(int(len2[3]*TRAIN_RATIO)))]]\n",
    "\n",
    "ordering = [item for sublist in ordering for item in sublist]\n",
    "test_ordering = [x for x in range(0, N_SAMPLES) if x not in ordering]\n",
    "ordering = test_ordering + ordering\n",
    "ordering = [[x*MULT, (x*MULT)+1, (x*MULT)+2, (x*MULT)+3] for x in ordering]\n",
    "ordering = [item for sublist in ordering for item in sublist]\n",
    "#randomized = [[x*4, (x*4)+1, (x*4)+2, (x*4)+3] for x in ordering]\n",
    "##shuffle(randomized)\n",
    "#randomized = [item for sublist in randomized for item in sublist]\n",
    "\n",
    "#randomized = [x for x in range(0, N_SAMPLES)]\n",
    "#shuffle(randomized)\n",
    "#randomized = [[x*4, (x*4)+1, (x*4)+2, (x*4)+3] for x in randomized]\n",
    "#randomized = [item for sublist in randomized for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = data_x[ordering]\n",
    "data_y = data_y[ordering]\n",
    "lengths = lengths[ordering]\n",
    "\n",
    "\n",
    "percs = [sum(sum(val)) for x, val in enumerate(data_y) if x % MULT == 0]\n",
    "percs = np.array(percs).flatten()\n",
    "zero = len([x for x in percs if x == 0])# number with 0\n",
    "one = len([x for x in percs if 0 < x <= 8])\n",
    "two = len([x for x in percs if 8 < x <= 20])\n",
    "three = len([x for x in percs if 20 < x <= 35])\n",
    "four = len([x for x in percs if 35 < x <= 70])\n",
    "five = len([x for x in percs if 70 < x <= 100])\n",
    "six = len([x for x in percs if 100 < x])\n",
    "\n",
    "print(\"{} {} {} {} {} {} {}\".format(zero, one, two, three, four, five, six))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(percs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [x for x in range(0, len(percs))]\n",
    "zero_ids = [x for x, z in zip(idx, percs) if z == 0]\n",
    "one_ids = [x for x, z in zip(idx, percs) if 0 < z <= 8]\n",
    "two_ids = [x for x, z in zip(idx, percs) if 8 < z <= 20]\n",
    "three_ids = [x for x, z in zip(idx, percs) if 20 < z <= 35]\n",
    "four_ids = [x for x, z in zip(idx, percs) if 35 < z <= 70]\n",
    "five_ids = [x for x, z in zip(idx, percs) if 70 < z < 100]\n",
    "six_ids = [x for x, z in zip(idx, percs) if 100 < z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = (zero_ids[(int(len(zero_ids) * TEST_RATIO)):] + \n",
    "             one_ids[(int(len(one_ids) * TEST_RATIO)):] +\n",
    "             two_ids[(int(len(two_ids) * TEST_RATIO)):] + \n",
    "             three_ids[(int(len(three_ids) * TEST_RATIO)):] + \n",
    "             four_ids[(int(len(four_ids) * TEST_RATIO)):] + \n",
    "             five_ids[(int(len(five_ids) * TEST_RATIO)):] + \n",
    "             six_ids[(int(len(six_ids) * TEST_RATIO)):])\n",
    "\n",
    "test_ids = (zero_ids[:(int(len(zero_ids) * TEST_RATIO))] + \n",
    "             one_ids[:(int(len(one_ids) * TEST_RATIO))] +\n",
    "             two_ids[:(int(len(two_ids) * TEST_RATIO))] + \n",
    "             three_ids[:(int(len(three_ids) * TEST_RATIO))] + \n",
    "             four_ids[:(int(len(four_ids) * TEST_RATIO))] + \n",
    "             five_ids[:(int(len(five_ids) * TEST_RATIO))] + \n",
    "             six_ids[:(int(len(six_ids) * TEST_RATIO))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ids = [[(x*8), (x*8)+1, (x*8)+2, (x*8)+3, (x*8)+4, (x*8)+5, (x*8)+6, (x*8)+7] for x in train_ids]\n",
    "train_ids = [[(x*MULT), (x*MULT)+1, (x*MULT)+2, (x*MULT)+3] for x in train_ids]\n",
    "train_ids = [item for sublist in train_ids for item in sublist]\n",
    "\n",
    "test_ids = [x*4 for x in test_ids]\n",
    "#test_ids = [item for sublist in test_ids for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def bin_foc(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        y_pred = tf.reshape(y_pred, (-1, 14,14))\n",
    "        #y_true = tf.reshape(y_true, (-1, 14*14))\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(0.5 * K.pow(1. - pt_1, 2) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - 0.5) * K.pow(pt_0, 2) * K.log(1. - pt_0))  \n",
    "    \n",
    "def focal_loss(target_tensor, prediction_tensor, weights=None, alpha=0.25, gamma=1.5):\n",
    "    sigmoid_p = tf.reshape(prediction_tensor, (-1, 14, 14))\n",
    "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)\n",
    "\n",
    "def foc_lovasz(y_true, y_pred):\n",
    "    #jaccard_loss = jaccard_distance(y_true, y_pred)\n",
    "    lovasz = lovasz_hinge(y_pred, y_true)\n",
    "    #pred_reshape = tf.reshape(y_pred, (-1, 14, 14))\n",
    "    #true_reshape = tf.reshape(y_true, (-1, 14, 14))\n",
    "    focal_loss = bin_foc(y_true, y_pred)\n",
    "    summed = lovasz + np.log(focal_loss)\n",
    "    return summed\n",
    "\n",
    "def weighted_cross_entropy(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "    y_pred = tf.log(y_pred / (1 - y_pred))\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred, targets=y_true, pos_weight=1.5)\n",
    "    return loss\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def smooth_jaccard(y_true, y_pred, smooth=1):\n",
    "    y_true = tf.reshape(y_true, (-1, 12*12))\n",
    "    y_pred = tf.reshape(y_pred, (-1, 12*12))\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def bce_dice(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    return 0.5*binary_crossentropy(y_true, y_pred) + (dice_loss(y_true, y_pred))\n",
    "\n",
    "\n",
    "def bce_lovasz(y_true, y_pred):\n",
    "    #return 0.5*binary_crossentropy(tf.reshape(y_true, (-1, 14, 14, 1)), y_pred) + \n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    return lovasz_softmax(y_pred, y_true, classes=[1], per_image=True)\n",
    "\n",
    "def focal_loss_fixed(y_true, y_pred, gamma = 2., alpha = 0.25):\n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
    "\n",
    "def focal_dice(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    foc = focal_loss_fixed(y_true, y_pred, gamma = 0.5, alpha = 0.25)\n",
    "    foc = foc / 196\n",
    "    dice = dice_loss(y_true, y_pred)\n",
    "    return 0.5*foc + tf.log(dice)\n",
    "\n",
    "def foc_jaccard(y_true, y_pred):\n",
    "    jac = dice_loss(y_true, y_pred)\n",
    "    foc = focal_loss_fixed(y_true, y_pred, gamma = 1.3, alpha = 0.25)\n",
    "    return (foc / 196) + 0.5*jac\n",
    "\n",
    "\n",
    "def soft_dice_loss(y_true, y_pred, epsilon=1e-6): \n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    print(y_true.shape)\n",
    "    print(y_pred.shape)\n",
    "    ''' \n",
    "    Soft dice loss calculation for arbitrary batch size, number of classes, and number of spatial dimensions.\n",
    "    Assumes the `channels_last` format.\n",
    "  \n",
    "    # Arguments\n",
    "        y_true: b x X x Y( x Z...) x c One hot encoding of ground truth\n",
    "        y_pred: b x X x Y( x Z...) x c Network output, must sum to 1 over c channel (such as after softmax) \n",
    "        epsilon: Used for numerical stability to avoid divide by zero errors\n",
    "    \n",
    "    # References\n",
    "        V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation \n",
    "        https://arxiv.org/abs/1606.04797\n",
    "        More details on Dice loss formulation \n",
    "        https://mediatum.ub.tum.de/doc/1395260/1395260.pdf (page 72)\n",
    "        \n",
    "        Adapted from https://github.com/Lasagne/Recipes/issues/99#issuecomment-347775022\n",
    "    '''\n",
    "    # skip the batch and class axis for calculating Dice score\n",
    "    axes = tuple(range(1, len(y_pred.shape)-1)) \n",
    "    numerator = 2. * np.sum(y_pred * y_true, (1, 2))\n",
    "    denominator = np.sum(np.square(y_pred) + np.square(y_true), axes)\n",
    "    \n",
    "    return 1 - np.mean(numerator / (denominator + epsilon)) # average over classes and batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "import keras.backend as K\n",
    "import tensorflow as tf \n",
    "\n",
    "epsilon = 1e-5\n",
    "smooth = 1\n",
    "\n",
    "def dsc(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dsc(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def log_dice(y_true, y_pred):\n",
    "    loss = tf.log(dsc(y_true, y_pred))\n",
    "    return loss\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = 0.5*binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def confusion(y_true, y_pred):\n",
    "    smooth=1\n",
    "    y_pred_pos = K.clip(y_pred, 0, 1)\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "    y_pos = K.clip(y_true, 0, 1)\n",
    "    y_neg = 1 - y_pos\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg) \n",
    "    prec = (tp + smooth)/(tp+fp+smooth)\n",
    "    recall = (tp+smooth)/(tp+fn+smooth)\n",
    "    return prec, recall\n",
    "\n",
    "def tp(y_true, y_pred):\n",
    "    smooth = 1\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    tp = (K.sum(y_pos * y_pred_pos) + smooth)/ (K.sum(y_pos) + smooth) \n",
    "    return tp \n",
    "\n",
    "def tn(y_true, y_pred):\n",
    "    smooth = 1\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos \n",
    "    tn = (K.sum(y_neg * y_pred_neg) + smooth) / (K.sum(y_neg) + smooth )\n",
    "    return tn \n",
    "\n",
    "def tversky(y_true, y_pred):\n",
    "    y_true_pos = K.flatten(y_true)\n",
    "    y_pred_pos = K.flatten(y_pred)\n",
    "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
    "    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n",
    "    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n",
    "    alpha = 0.7\n",
    "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    return 1 - tversky(y_true,y_pred)\n",
    "\n",
    "def focal_tversky(y_true,y_pred):\n",
    "    pt_1 = tversky(y_true, y_pred)\n",
    "    gamma = 0.75\n",
    "    return K.pow((1-pt_1), gamma)\n",
    "\n",
    "def ftl_bce(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    return focal_tversky(y_true, y_pred) + 0.5*binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "\n",
    "def focal_dice(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    #foc = focal_loss_fixed(y_true, y_pred, gamma = 0.5, alpha = 0.25)\n",
    "    #foc = foc / 196\n",
    "    #dice = dice_loss(y_true, y_pred)\n",
    "    return 0.5*binary_crossentropy(y_true, y_pred) - log_dice(y_true, y_pred)\n",
    "\n",
    "def bce_dice_count(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    bce = 0.5*binary_crossentropy(y_true, y_pred)\n",
    "    dce = -log_dice(y_true, y_pred)\n",
    "    count = 0.5*count_loss(y_true, y_pred)\n",
    "    return bce + dce + count\n",
    "\n",
    "def lvz_bce(y_true, y_pred):\n",
    "    y_true_r = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    return lovasz_softmax(tf.reshape(y_pred, (-1, 14, 14)), y_true, classes=[1], per_image=True) + 0.5*binary_crossentropy(y_true_r, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    pt_0 = tf.where(tf.math.greater(y_pred_f, 0.5), y_pred_f, tf.zeros_like(y_pred_f))\n",
    "    true_sum = K.sum(y_true_f) # 5 10 15\n",
    "    pred_sum = K.sum(y_pred_f) # 1 10 25\n",
    "    score = K.abs(pred_sum - true_sum) / 196\n",
    "    return score\n",
    "\n",
    "def dsc_np(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = y_true.flatten().astype(np.float32)\n",
    "    y_pred_f = y_pred.flatten().astype(np.float32)\n",
    "    intersection = sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (sum(y_true_f) + sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def bce_shift(true, pred, power):\n",
    "    losses = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        true_i = tf.reshape(true[i], (1, 14, 14, 1))\n",
    "        pred_i = tf.reshape(pred[i], (1, 12, 12, 1))\n",
    "        true_p = true_i\n",
    "        #loss_o = binary_crossentropy(true_p, pred)\n",
    "        # extract out the candidate shifts\n",
    "        true_m = true_i[:, 1:13, 1:13]\n",
    "        true_l = true_i[:, 0:12, 1:13]\n",
    "        true_r = true_i[:, 2:14, 1:13]\n",
    "        true_u = true_i[:, 1:13, 0:12]\n",
    "        true_d = true_i[:, 1:13, 2:14]\n",
    "        true_dr = true_i[:, 2:14, 0:12]\n",
    "        true_dl = true_i[:, 0:12, 0:12]\n",
    "        true_ur = true_i[:, 2:14, 2:14]\n",
    "        true_ul = true_i[:, 0:12, 2:14]\n",
    "        true_shifts = [true_m, true_l, true_r, true_u, true_d, true_dr, true_dl, true_ur, true_ul]\n",
    "        bce_shifts = tf.stack([binary_crossentropy(x, pred_i) for x in true_shifts])\n",
    "        jac_shifts = tf.stack([smooth_jaccard(x, pred_i) for x in true_shifts])\n",
    "\n",
    "        # Calculate BCE\n",
    "        \n",
    "        \n",
    "        bce_power = tf.math.pow(1/(tf.reduce_mean(bce_shifts, axis = [2,3])), power)\n",
    "        jac_power = tf.math.pow(1/(jac_shifts+0.1), power)\n",
    "        \n",
    "        sums = tf.reduce_sum(bce_power)\n",
    "        sum_jac = tf.reduce_sum(jac_power)\n",
    "        weights = bce_power/sums\n",
    "        weights_jac = jac_power/sum_jac\n",
    "    \n",
    "        weights = (2*weights + weights_jac)/3\n",
    "        loss = tf.reshape(bce_shifts, (1, 9, 12, 12)) * tf.reshape(weights, (1, 9, 1, 1))\n",
    "        loss = tf.reduce_sum(loss, axis = 1)\n",
    "        loss_j = tf.reshape(jac_shifts, (1, 9)) * tf.reshape(weights, (1, 9))\n",
    "        loss_j = tf.reduce_sum(loss_j, axis = 1)\n",
    "        losses.append(loss + 0.5*loss_j)\n",
    "    loss = tf.reshape(tf.stack(losses), (BATCH_SIZE, 12, 12, 1))\n",
    "    return loss\n",
    "\n",
    "def get_shifts_batched(arr):\n",
    "    true_m = arr[:, 1:13, 1:13]\n",
    "    true_l = arr[:, 0:12, 1:13]\n",
    "    true_r = arr[:, 2:14, 1:13]\n",
    "    true_u = arr[:, 1:13, 0:12]\n",
    "    true_d = arr[:, 1:13, 2:14]\n",
    "    true_dr = arr[:, 2:14, 0:12]\n",
    "    true_dl = arr[:, 0:12, 0:12]\n",
    "    true_ur = arr[:, 2:14, 2:14]\n",
    "    true_ul = arr[:, 0:12, 2:14]\n",
    "    true_shifts = [true_m, true_l, true_r, true_u, true_d, true_dr, true_dl, true_ur, true_ul]\n",
    "    return true_shifts\n",
    "\n",
    "def lovasz_shift(true, pred, power):\n",
    "    batch_shifted = get_shifts_batched(tf.reshape(true, (-1, 14, 14, 1)))\n",
    "    shift_weights = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        true_i = tf.reshape(true[i], (1, 14, 14, 1))\n",
    "        pred_i = tf.reshape(pred[i], (1, 12, 12, 1))\n",
    "        true_p = true_i\n",
    "        true_m = true_i[:, 1:13, 1:13]\n",
    "        true_l = true_i[:, 0:12, 1:13]\n",
    "        true_r = true_i[:, 2:14, 1:13]\n",
    "        true_u = true_i[:, 1:13, 0:12]\n",
    "        true_d = true_i[:, 1:13, 2:14]\n",
    "        true_dr = true_i[:, 2:14, 0:12]\n",
    "        true_dl = true_i[:, 0:12, 0:12]\n",
    "        true_ur = true_i[:, 2:14, 2:14]\n",
    "        true_ul = true_i[:, 0:12, 2:14]\n",
    "        true_shifts = [true_m, true_l, true_r, true_u, true_d, true_dr, true_dl, true_ur, true_ul]\n",
    "        bce_shifts = tf.stack([binary_crossentropy(x, pred_i) for x in true_shifts])\n",
    "        bce_power = tf.math.pow(1/(tf.reduce_mean(bce_shifts, axis = [2,3])), power)\n",
    "        sums = tf.reduce_sum(bce_power)\n",
    "        weights = bce_power/sums\n",
    "        weights = tf.reshape(weights, (1, 9))\n",
    "        shift_weights.append(weights)\n",
    "    weights = tf.reshape(tf.stack(shift_weights), (BATCH_SIZE, 9))\n",
    "    print(\"WEIGHT\", weights.shape)\n",
    "    lovasz = tf.stack([lovasz_softmax(tf.reshape(pred, (-1, 12, 12)), x, classes = [1], per_image = True) for x in batch_shifted])\n",
    "    #losses = []\n",
    "    #for i in range(0, 9):\n",
    "    #    lovasz = lovasz_softmax(tf.reshape(pred, (-1, 12, 12)), batch_shifted[:, i, :, :, :], classes=[1], per_image=True)\n",
    "    #    lovasz = lovasz * weights[:, i]\n",
    "    #losses.append(lovasz)\n",
    "    #losses = tf.reshape(tf.stack(losses), (-1,))\n",
    "   \n",
    "    return lovasz\n",
    "\n",
    "def ce(targets, predictions, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions (N, k) ndarray\n",
    "           targets (N, k) ndarray        \n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    targets = targets.reshape(1, 144)\n",
    "    predictions = predictions.reshape(1, 144)\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.mean(targets*np.log(predictions+1e-9))/N\n",
    "    return ce\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "def prec_shift(true, pred):\n",
    "    true_m = true[1:13, 1:13]\n",
    "    true_l = true[0:12, 1:13]\n",
    "    true_r = true[2:14, 1:13]\n",
    "    true_u = true[1:13, 0:12]\n",
    "    true_d = true[1:13, 2:14]\n",
    "    true_dr = true[2:14, 0:12]\n",
    "    true_dl = true[0:12, 0:12]\n",
    "    true_ur = true[2:14, 2:14]\n",
    "    true_ul = true[0:12, 2:14]\n",
    "    #norm = ce(true_m, pred)\n",
    "    '''\n",
    "    l = ce(true_l, pred)\n",
    "    r = ce(true_r, pred)\n",
    "    u = ce(true_u, pred)\n",
    "    d = ce(true_d, pred)\n",
    "    dr = ce(true_dr, pred)\n",
    "    dl = ce(true_dl, pred)\n",
    "    ur = ce(true_ur, pred)\n",
    "    ul = ce(true_ul, pred)\n",
    "    for_weights = [(1/(i+0.1)**3) for i in [norm, l, r, u, d, dr, dl, ur, ul]]\n",
    "    #print([1/i for i in for_weights])\n",
    "    #print([norm, l, r, u ,d])\n",
    "    sum_for_weights = sum(for_weights)\n",
    "    sum_for_weights = max(sum_for_weights, 1)\n",
    "    #sums = sum([1/i for i in [norm, l, r, u, d, dr, dl, ur, ul]])\n",
    "    #sums = max(sums, 1)\n",
    "    weights = [i/sum_for_weights for i in for_weights]\n",
    "    #weights = [i/sum(weights) for i in weights]\n",
    "    '''\n",
    "    match = dsc_np(true_m, pred)\n",
    "    match_l = dsc_np(true_l, pred)\n",
    "    match_r = dsc_np(true_r, pred)\n",
    "    match_u = dsc_np(true_u, pred)\n",
    "    match_d = dsc_np(true_d, pred)\n",
    "    match_dr = dsc_np(true_dr, pred)\n",
    "    match_dl = dsc_np(true_dl, pred)\n",
    "    match_ur = dsc_np(true_ur, pred)\n",
    "    match_ul = dsc_np(true_ul, pred)\n",
    "    return max([match, match_l, match_r, match_u, match_d, match_dr, match_dl, match_ur, match_ul])\n",
    "    #return sum([(i * l) for i, l in zip([match, match_l, match_r, match_u, match_d, match_dr, match_dl, match_ur, match_ul], weights)])\n",
    "    #best_shift = max([match_l, match_r, match_u, match_d, match_dr, match_dl, match_ur, match_ul])\n",
    "    #if match < (best_shift - 0.2):\n",
    "    #    print(\"The shifted data performs better by {}\".format(best_shift - match))\n",
    "    #    return best_shift\n",
    "    #else:\n",
    "    #    return match\n",
    "    \n",
    "def multi_shift(true, pred, power):\n",
    "    return bce_shift(true, pred, power) + lovasz_shift(true, pred, power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = np.array([[0, 0, 0], [1, 1, 1], [0, 0, 0]])\n",
    "img_2 = np.array([[0, 0, 0], [1, 1, 0], [0, 0, 0]])\n",
    "dsc_np(img_1, img_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRESH_START = False\n",
    "FINE_TUNE = False\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "BATCH_SIZE = 24\n",
    "print(\"Starting model with: \\n {} zone out \\n {} l2 \\n {} initial LR \\n {} final LR \\n {} parameters\"\n",
    "     .format(ZONE_OUT_PROB, L2_REG, INITIAL_LR, FINAL_LR, total_parameters))\n",
    "best_val = 0.66\n",
    "if not FRESH_START:\n",
    "    print(\"Resuming training with a best validation score of {}\".format(best_val))\n",
    "if FRESH_START:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-6)\n",
    "    print(\"Restarting training from scratch on {} train and {} test samples, total {}\".format(len(train_ids), len(test_ids), N_SAMPLES))\n",
    "    #optimizer = AdaBoundOptimizer(learning_rate=INITIAL_LR/3,\n",
    "    #                              final_lr=FINAL_LR/6,\n",
    "    #                              beta1=0.9, beta2=0.999, \n",
    "    #                              amsbound=True)\n",
    "    \n",
    "    optimizer2 = AdaBoundOptimizer(learning_rate=INITIAL_LR/5,\n",
    "                                  final_lr=FINAL_LR/5,\n",
    "                                  beta1=0.9, beta2=0.999, \n",
    "                                  amsbound=True)\n",
    "    \n",
    "    loss = bce_shift(labels, fm, power)\n",
    "    #loss = bce_dice_count(labels, fm)\n",
    "    l2_loss = tf.losses.get_regularization_loss()\n",
    "    loss += l2_loss\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(loss)    \n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver(max_to_keep = 2)\n",
    "    \n",
    "if FINE_TUNE:\n",
    "    loss_to_run = loss2\n",
    "    op_to_run = tune_op\n",
    "else:\n",
    "    loss_to_run = loss\n",
    "    op_to_run = train_op\n",
    "\n",
    "# Run training loop\n",
    "for i in range(17, 100):\n",
    "    randomize = train_ids\n",
    "    np.random.shuffle(randomize)\n",
    "    test_randomize = test_ids\n",
    "    np.random.shuffle(test_randomize)\n",
    "\n",
    "    losses = []\n",
    "    val_loss = []\n",
    "    \n",
    "    for k in tnrange(int(len(train_ids) // BATCH_SIZE)):\n",
    "        batch_ids = randomize[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "        batch_y = data_y[batch_ids, :, :].reshape(BATCH_SIZE, 14, 14)\n",
    "        #if sum(sum(sum(batch_y))) > 0:\n",
    "        op, tr = sess.run([op_to_run, loss_to_run],\n",
    "                              feed_dict={inp: data_x[batch_ids, :, :, :],\n",
    "                                         length: lengths[batch_ids],\n",
    "                                         labels: data_y[batch_ids, :, :].reshape(BATCH_SIZE, 14, 14),\n",
    "                                         is_training: True,\n",
    "                                         power: 1 + (i*0.06),\n",
    "                                         })\n",
    " \n",
    "        #else:\n",
    "        #    print(\"Skipping minibatch for equibatch reasons\")\n",
    "        losses.append(tr)\n",
    "    for j in range(len(test_ids) // BATCH_SIZE):\n",
    "        batch_ids = test_randomize[j*BATCH_SIZE:(j+1)*BATCH_SIZE]\n",
    "        vl, y = sess.run([loss, fm], \n",
    "                         feed_dict={inp: data_x[batch_ids, :, :, :],\n",
    "                                    length: lengths[batch_ids],\n",
    "                                    labels: data_y[batch_ids, :, :].reshape(BATCH_SIZE, 14, 14),\n",
    "                                    is_training: False,\n",
    "                                    power: 1 + (i*0.06)\n",
    "                                    })\n",
    "        val_loss.append(vl)\n",
    "        \n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    ious = []\n",
    "    for m in test_ids:\n",
    "        y = sess.run([fm], feed_dict={inp: data_x[m].reshape(1, 24, IMAGE_SIZE, IMAGE_SIZE, 15),\n",
    "                                  length: lengths[m].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })[0]\n",
    "        true = data_y[m].reshape((LABEL_SIZE, LABEL_SIZE))\n",
    "        pred = y.reshape((12, 12))\n",
    "        #TODO @jombrandt figure out difference between this in train and inference time\n",
    "        #TODO @jombrandt convert to ROC-AUC instead of threshold F1\n",
    "        pred[np.where(pred > 0.45)] = 1\n",
    "        pred[np.where(pred < 0.45)] = 0\n",
    "        shifts = get_shifts(true)\n",
    "        f1s = []\n",
    "        precs = []\n",
    "        recs = []\n",
    "        for s in shifts:\n",
    "            rec, prec = thirty_meter(s, pred)\n",
    "            rec = np.mean(rec)\n",
    "            prec = np.mean(prec)\n",
    "            f1_score = 2 * ((prec * rec) / (prec + rec))\n",
    "            f1s.append(f1_score)\n",
    "            precs.append(prec)\n",
    "            recs.append(rec)\n",
    "        rec = recs[np.argmax(f1s)]\n",
    "        prec = precs[np.argmax(f1s)]\n",
    "        recalls.append(rec)\n",
    "        precisions.append(prec)\n",
    "        iou = prec_shift(true, pred)\n",
    "        ious.append(iou)\n",
    "    precision = np.mean([x for x in precisions if not np.isnan(x)])\n",
    "    recall = np.mean([x for x in recalls if not np.isnan(x)])\n",
    "    iou = np.mean(ious)\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "    save_path = saver.save(sess, \"../models/dev/model\")\n",
    "    if np.mean(val_loss) < best_val:\n",
    "        best_val = np.mean(val_loss)\n",
    "        print(\"Saving model with {}\".format(best_val))\n",
    "        save_path = saver.save(sess, \"../models/dev_best/model\")\n",
    "    print(\"Epoch {}: Loss {} Val: {} P {} R {} F1 {} iou {}\".format(i + 1,\n",
    "                                                             np.mean(losses), np.mean(val_loss),\n",
    "                                                             precision, recall, f1_score, iou))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation and sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO @jombrandt \n",
    "#TODO @jombrandt -- remove augmentation of val set\n",
    "import random \n",
    "\n",
    "def multiplot(matrices):\n",
    "    '''Plot multiple heatmaps with subplots'''\n",
    "    fig, axs = plt.subplots(ncols=4)\n",
    "    fig.set_size_inches(20, 4)\n",
    "    for i, matrix in enumerate(matrices):\n",
    "        sns.heatmap(data = matrix, ax = axs[i], vmin = 0, vmax = 0.5)\n",
    "        axs[i].set_xlabel(\"\")\n",
    "        axs[i].set_ylabel(\"\")\n",
    "        axs[i].set_yticks([])\n",
    "        axs[i].set_xticks([])\n",
    "    plt.show()\n",
    "    \n",
    "test_losses = []\n",
    "#start = 28\n",
    "start = start + 4\n",
    "print(start/len(test_ids))\n",
    "#matrix_ids = random.sample(train_ids, 4)\n",
    "test_ids = sorted(test_ids)\n",
    "#matrix_ids = [504, 976]\n",
    "matrix_ids = [test_ids[start], test_ids[start + 1], test_ids[start + 2], test_ids[start + 3],]\n",
    "#matrix_ids = random.sample(train_ids, 4)\n",
    "#matrix = [matrix_ids[0], matrix_ids[0] + 4, matrix_ids[0] + 8, matrix_ids[0] + 12]\n",
    "#matrix_ids = [988, 900, 2055, test]\n",
    "# 63\"\"\n",
    "preds = []\n",
    "trues = []\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    print(i)\n",
    "    y = sess.run([fm], feed_dict={inp: data_x[idx].reshape(1, 24, IMAGE_SIZE, IMAGE_SIZE, 15),\n",
    "                                  length: lengths[idx].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  #labels: data_y[idx].reshape(1, 14, 14)\n",
    "                                  })\n",
    "    #print(idx, np.mean(lr))\n",
    "    y = np.array(y).reshape(12, 12)\n",
    "    y = np.pad(y, [[1, 1], [1, 1]], mode = \"constant\")#, constant_values = min([min(i) for i in y]))\n",
    "    #y[np.where(y < 0.05)] = 0\n",
    "    preds.append(y)\n",
    "    true = data_y[idx].reshape(LABEL_SIZE, LABEL_SIZE)\n",
    "    trues.append(true)\n",
    "\n",
    "multiplot(preds)\n",
    "#plot_ids[ordering[976]//4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot(trues) # 140, 160, 236, 296, 324, 416, 460, 504, 976"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(4, 12):\n",
    "    f1_all = []\n",
    "    for i in test_ids:\n",
    "        y = sess.run([fm], feed_dict={inp: data_x[i].reshape(1, 24, 16, 16, 13),\n",
    "                                  length: lengths[i].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })[0]\n",
    "        true = data_y[i].reshape((14, 14))\n",
    "        pred = y.reshape((12, 12))\n",
    "        #pred = pred[1:15, 1:15]\n",
    "        #true = true[1:15, 1:15]\n",
    "        pred[np.where(pred > j*0.05)] = 1\n",
    "        pred[np.where(pred < j*0.05)] = 0\n",
    "        shifts = get_shifts(true)\n",
    "        f1s = []\n",
    "        precs = []\n",
    "        recs = []\n",
    "        for i in shifts:\n",
    "            rec, prec = thirty_meter(i, pred)\n",
    "            rec = np.mean(rec)\n",
    "            prec = np.mean(prec)\n",
    "            f1_score = 2 * ((prec * rec) / (prec + rec))\n",
    "            f1s.append(f1_score)\n",
    "            precs.append(prec)\n",
    "            recs.append(rec)\n",
    "        rec = recs[np.argmax(f1s)]\n",
    "        prec = precs[np.argmax(f1s)]\n",
    "        f1s = max(f1s)\n",
    "        f1_all.append(f1s)\n",
    "        #recalls.append(rec)\n",
    "        #precisions.append(prec)\n",
    "    #recalls = [item for sublist in recalls for item in sublist]\n",
    "    #precisions = [item for sublist in precisions for item in sublist]\n",
    "    print(np.mean(f1_all))\n",
    "    #print(\"{}: Recall: {}\\t Precision: {}\".format(j*0.05, np.mean(recalls), np.mean(precisions)))\n",
    "    #TEST: 1161, 1076, 1267, 1187, 1197,  1109, 1235 TEAIN: 290, 184, 294, 890, 807\n",
    "# 135224667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO @jombrandt top 10 worst training, test samples by IOU \n",
    "\n",
    "These should be written to a tmp/ .txt file and indexed by validate-data.ipynb to ensure that original classifications were correct, and to identify regions that need more training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
