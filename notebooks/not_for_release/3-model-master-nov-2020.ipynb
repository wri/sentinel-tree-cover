{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree segmentation with multitemporal Sentinel 1/2 imagery\n",
    "\n",
    "## John Brandt\n",
    "## April 02, 2020\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook contains the TensorFlow model training and prediction used to segment trees for [Restoration Mapper](https://restorationmapper.org). The notebook uses tensorflow 1.13.1 and additionally relies on Keras and tflearn. \n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Package loading\n",
    "- Utility scripts\n",
    "- Hyperparameter definitions\n",
    "- Custom tensorflow layer functions\n",
    "- Tensorflow graph creation\n",
    "- Data loading\n",
    "- Data preprocessing\n",
    "- Equibatch creation\n",
    "- Loss definition\n",
    "- Tensorflow graph initialization\n",
    "- Training\n",
    "- Model validation\n",
    "- Sanity Checks\n",
    "\n",
    "## Package Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "import tensorflow as tf\n",
    "#import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras.layers import Conv2D, Lambda, Dense, Multiply, Add\n",
    "from tensorflow.initializers import glorot_normal, lecun_normal\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.layers import batch_normalization\n",
    "from tensorflow.python.util import deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/layers/zoneout.py\n",
    "%run ../src/layers/convgru.py\n",
    "%run ../src/layers/adabound.py\n",
    "%run ../src/layers/dropblock.py\n",
    "%run ../src/layers/extra_layers.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/preprocessing/slope.py\n",
    "%run ../src/utils/metrics.py\n",
    "%run ../src/not_for_release/lovasz.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONE_OUT_PROB = 0.20 #0.2 for master model\n",
    "GRU_FILTERS = 32\n",
    "FPA_FILTERS = 28\n",
    "OUT_FILTERS = 32\n",
    "AVERAGE_GRU_FILTERS = True\n",
    "ACTIVATION_FUNCTION = 'relu'\n",
    "\n",
    "INITIAL_LR = 1e-4\n",
    "FINAL_LR = 2e-2\n",
    "DROPBLOCK_MAXSIZE = 5\n",
    "DECONV = 'bilinear'\n",
    "N_CONV_BLOCKS = 1\n",
    "FINAL_ALPHA = 0.7\n",
    "LABEL_SMOOTHING = 0.10 #0.075 for master\n",
    "BATCH_RENORM = 'norm'\n",
    "\n",
    "L2_REG = 0.0\n",
    "BN_MOMENTUM = 0.9\n",
    "BATCH_SIZE = 16\n",
    "MAX_DROPBLOCK = 0.85\n",
    "\n",
    "gru_flt = 32\n",
    "fpa_flt = 28\n",
    "out_conv_flt = 32\n",
    "\n",
    "IMAGE_SIZE = 16\n",
    "LABEL_SIZE = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter grid search definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 3.8 million combinations of hyperparameters\n",
    "params = {\n",
    "    'zone_out' : [0],#range(0, 0.5, 0.1), #6\n",
    "    'average_gru_filters': [True, False], # 2\n",
    "    'final_lr': [5e-3, 1e-2, 2e-2, 5e-2], # 4\n",
    "    'n_conv_blocks': [3, 2, 1], # 3\n",
    "    'final_alpha': [0.8, 0.75, 0.7, 0.65, 0.6], #5\n",
    "    'label_smoothing': [0., 0.025, 0.05, 0.075, 0.10, 0.125],\n",
    "    'batch_renorm': ['renorm', 'norm']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom layer definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility blocks (Batch norm, cSSE, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Batch_Normalization(x, training, scope, clipping_params):\n",
    "    '''Batch renormalization layer from tensorflow.layers\n",
    "\n",
    "         Parameters:\n",
    "          x (tf.Variable): input layer\n",
    "          training (tf.Variable): flag to differentiate between train/test ops\n",
    "          scope (str): tensorflow scope\n",
    "          clipping_params (dict): specifies clipping of \n",
    "                                  rmax, dmax, rmin for renormalization\n",
    "\n",
    "         Returns:\n",
    "          x (tf.Variable): output of batch renormalization\n",
    "          \n",
    "         References:\n",
    "          https://github.com/tensorflow/docs/blob/r1.13/site/en/\n",
    "          api_docs/python/tf/layers/batch_normalization.md\n",
    "    '''\n",
    "    return batch_normalization(inputs=x, \n",
    "                               momentum = BN_MOMENTUM, \n",
    "                               training=training,\n",
    "                               renorm = True,\n",
    "                               reuse=None,\n",
    "                               renorm_clipping = clipping_params,\n",
    "                               name = scope)\n",
    "\n",
    "\n",
    "def calc_renorm_params(epoch, n_samples, batch_size, k = 0):\n",
    "    '''Calculates the clipping parameters for renormalization\n",
    "       based on the learning schedule outlined in the original paper\n",
    "       where rmax is initialized to 1, dmax to 0, rmin to 0,\n",
    "       (effectively batch normalization), and then rmax and dmax\n",
    "       are gradually reaxed to 3 and 5 over 40k and 25k steps,\n",
    "       respectively. In this case, 40k has been reduced to 30k, and 25k\n",
    "       has been reduced to 20k.\n",
    "\n",
    "         Parameters:\n",
    "          epoch (int): number of current training epoch\n",
    "                       if testing, epoch number of model used\n",
    "          n_samples (int): total number of training samples\n",
    "          batch_size (int): training batch size \n",
    "\n",
    "         Returns:\n",
    "          rmax (float)\n",
    "          dmax (float)\n",
    "          rmin (float)\n",
    "    '''\n",
    "    step = epoch * (n_samples // batch_size)\n",
    "    step += k\n",
    "    if step < 2500:\n",
    "        rmax = 1.\n",
    "        dmax = 0.\n",
    "        rmin = 0.\n",
    "    if step >= 2500:\n",
    "        rmax = np.min([1 + 2*((step-2500)/20000), 3])\n",
    "        dmax = np.min([1 + 5*((step-2500)/15000), 5])\n",
    "        rmin = 0.\n",
    "    return rmax, dmax, rmin\n",
    "\n",
    "def cse_block(prevlayer, prefix):\n",
    "    '''Channel excitation and spatial squeeze layer. \n",
    "       Calculates the mean of the spatial dimensions and then learns\n",
    "       two dense layers, one with relu, and one with sigmoid, to rerank the\n",
    "       input channels\n",
    "       \n",
    "         Parameters:\n",
    "          prevlayer (tf.Variable): input layer\n",
    "          prefix (str): prefix for tensorflow scope\n",
    "\n",
    "         Returns:\n",
    "          x (tf.Variable): output of the cse_block\n",
    "    '''\n",
    "    mean = Lambda(lambda xin: K.mean(xin, axis=[1, 2]))(prevlayer)\n",
    "    lin1 = Dense(K.int_shape(prevlayer)[3] // 2, name=prefix + 'cse_lin1', activation='relu')(mean)\n",
    "    lin2 = Dense(K.int_shape(prevlayer)[3], name=prefix + 'cse_lin2', activation='sigmoid')(lin1)\n",
    "    x = Multiply()([prevlayer, lin2])\n",
    "    return x\n",
    "\n",
    "\n",
    "def sse_block(prevlayer, prefix):\n",
    "    '''Spatial excitation and channel squeeze layer.\n",
    "       Calculates a 1x1 convolution with sigmoid activation to create a \n",
    "       spatial map that is multiplied by the input layer\n",
    "\n",
    "         Parameters:\n",
    "          prevlayer (tf.Variable): input layer\n",
    "          prefix (str): prefix for tensorflow scope\n",
    "\n",
    "         Returns:\n",
    "          x (tf.Variable): output of the sse_block\n",
    "    '''\n",
    "    conv = Conv2D(1, (1, 1), padding=\"same\", kernel_initializer=\"glorot_uniform\",\n",
    "                  activation='sigmoid', strides=(1, 1),\n",
    "                  name=prefix + \"_conv\")(prevlayer)\n",
    "    conv = Multiply(name=prefix + \"_mul\")([prevlayer, conv])\n",
    "    return conv\n",
    "\n",
    "\n",
    "def csse_block(x, prefix):\n",
    "    '''Implementation of Concurrent Spatial and Channel \n",
    "       ‘Squeeze & Excitation’ in Fully Convolutional Networks\n",
    "    \n",
    "        Parameters:\n",
    "          prevlayer (tf.Variable): input layer\n",
    "          prefix (str): prefix for tensorflow scope\n",
    "\n",
    "         Returns:\n",
    "          x (tf.Variable): added output of cse and sse block\n",
    "          \n",
    "         References:\n",
    "          https://arxiv.org/abs/1803.02579\n",
    "    '''\n",
    "    cse = cse_block(x, prefix)\n",
    "    sse = sse_block(x, prefix)\n",
    "    x = Add(name=prefix + \"_csse_mul\")([cse, sse])\n",
    "\n",
    "    return x\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv GRU Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_block(inp, length, size, flt, scope, train, normalize = True):\n",
    "    '''Bidirectional convolutional GRU block with \n",
    "       zoneout and CSSE blocks in each time step\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): (B, T, H, W, C) layer\n",
    "          length (tf.Variable): (B, T) layer denoting number of\n",
    "                                steps per sample\n",
    "          size (int): kernel size of convolution\n",
    "          flt (int): number of convolution filters\n",
    "          scope (str): tensorflow variable scope\n",
    "          train (tf.Bool): flag to differentiate between train/test ops\n",
    "          normalize (bool): whether to compute layer normalization\n",
    "\n",
    "         Returns:\n",
    "          gru (tf.Variable): (B, H, W, flt*2) bi-gru output\n",
    "          steps (tf.Variable): (B, T, H, W, flt*2) output of each step\n",
    "    '''\n",
    "    with tf.variable_scope(scope):\n",
    "        print(f\"GRU input shape {inp.shape}, zoneout: {ZONE_OUT_PROB}\")\n",
    "        cell_fw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID', normalize = normalize, sse = True)\n",
    "        cell_bw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID', normalize = normalize, sse = True)\n",
    "        cell_fw = ZoneoutWrapper(\n",
    "           cell_fw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        cell_bw = ZoneoutWrapper(\n",
    "            cell_bw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        steps, out = convGRU(inp, cell_fw, cell_bw, length)\n",
    "        gru = tf.concat(out, axis = -1)\n",
    "        steps = tf.concat(steps, axis = -1)\n",
    "        print(f\"Down block output shape {gru.shape}\")\n",
    "    return gru, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_selu(inp, is_training, kernel_size, scope,\n",
    "                filter_count = 16, pad = True, padding = 'valid', dilated = False,\n",
    "                activation = True):\n",
    "    '''Convolutional 2D layer with SELU activation and Lecun normal initialization\n",
    "       with no batch norm. Only used if params['activation'] = 'selu'\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): (B, H, W, C) input layer\n",
    "          is_training (str): flag to differentiate between train/test ops\n",
    "          kernel_size (int): kernel size of convolution\n",
    "          scope (str): tensorflow variable scope\n",
    "          filter_count (int): number of convolution filters\n",
    "          pad (bool): whether or not to reflect pad input\n",
    "          padding (str): one of ['valid', 'same']\n",
    "          dilated (bool): whether to perform atruous convolution\n",
    "          activation (bool): whether to activate output\n",
    "\n",
    "         Returns:\n",
    "          conv (tf.Variable): output of Conv2D -> SELU\n",
    "          \n",
    "         References:\n",
    "          https://arxiv.org/abs/1706.02515\n",
    "    '''\n",
    "    if activation:\n",
    "        act = selu\n",
    "    else:\n",
    "        act = None\n",
    "    if not dilated:\n",
    "        padded = ReflectionPadding2D((1, 1,))(inp)\n",
    "        conv = Conv2D(filters = filter_count, kernel_size = (kernel_size, kernel_size), activation = act,\n",
    "                        padding = padding, kernel_initializer = lecun_normal())(padded)\n",
    "    if not dilated and not pad:\n",
    "        conv = Conv2D(filters = filter_count, kernel_size = (kernel_size, kernel_size), activation = act,\n",
    "                        padding = padding, kernel_initializer = lecun_normal())(inp)\n",
    "    if dilated:\n",
    "        padded = ReflectionPadding2D((2, 2,))(inp)\n",
    "        conv = Conv2D(filters = filter_count, kernel_size = (3, 3), activation = act, dilation_rate = (2, 2),\n",
    "                        padding = padding, kernel_initializer = lecun_normal())(padded)\n",
    "    return conv\n",
    "\n",
    "def conv_bn_relu(inp, \n",
    "                 is_training, \n",
    "                 kernel_size,\n",
    "                 scope,\n",
    "                 filters, \n",
    "                 clipping_params,\n",
    "                 keep_rate,\n",
    "                 stride = (1, 1),\n",
    "                 activation = True,\n",
    "                 use_bias = False,\n",
    "                 batch_norm = True,\n",
    "                 dropblock = True,\n",
    "                 csse = True):\n",
    "    '''2D convolution, batch renorm, relu block, 3x3 drop block. \n",
    "       Use_bias must be set to False for batch normalization to work. \n",
    "       He normal initialization is used with batch normalization.\n",
    "       RELU is better applied after the batch norm.\n",
    "       DropBlock performs best when applied last, according to original paper.\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): input layer\n",
    "          is_training (str): flag to differentiate between train/test ops\n",
    "          kernel_size (int): size of convolution\n",
    "          scope (str): tensorflow variable scope\n",
    "          filters (int): number of filters for convolution\n",
    "          clipping_params (dict): specifies clipping of \n",
    "                                  rmax, dmax, rmin for renormalization\n",
    "          activation (bool): whether to apply RELU\n",
    "          use_bias (str): whether to use bias. Should always be false\n",
    "\n",
    "         Returns:\n",
    "          bn (tf.Variable): output of Conv2D -> Batch Norm -> RELU\n",
    "        \n",
    "         References:\n",
    "          http://papers.nips.cc/paper/8271-dropblock-a-regularization-\n",
    "              method-for-convolutional-networks.pdf\n",
    "          https://arxiv.org/abs/1702.03275\n",
    "          \n",
    "    '''\n",
    "    \n",
    "    bn_flag = \"Batch Renorm\" if batch_norm else \"\"\n",
    "    activation_flag = \"RELU\" if activation else \"Linear\"\n",
    "    csse_flag = \"CSSE\" if csse else \"No CSSE\"\n",
    "    bias_flag = \"Bias\" if use_bias else \"NoBias\"\n",
    "    drop_flag = \"DropBlock\" if dropblock else \"NoDrop\"\n",
    "        \n",
    "    \n",
    "    print(\"{} {} Conv 2D {} {} {} {} {}\".format(scope, kernel_size,\n",
    "                                                   bn_flag, activation_flag,\n",
    "                                                   csse_flag, bias_flag, drop_flag))\n",
    "    \n",
    "    with tf.variable_scope(scope + \"_conv\"):\n",
    "        conv = Conv2D(filters = filters, kernel_size = (kernel_size, kernel_size),  strides = stride,\n",
    "                      activation = None, padding = 'valid', use_bias = use_bias,\n",
    "                      kernel_initializer = tf.keras.initializers.he_normal())(inp)\n",
    "    if batch_norm:\n",
    "        conv = Batch_Normalization(conv, is_training, scope, clipping_params)\n",
    "    if activation:\n",
    "        conv = tf.nn.relu(conv)\n",
    "    if csse:\n",
    "        conv = csse_block(conv, 'csse_' + scope)\n",
    "    if dropblock: \n",
    "        with tf.variable_scope(scope + \"_drop\"):\n",
    "            drop_block = DropBlock2D(keep_prob=keep_rate, block_size=4)\n",
    "            conv = drop_block(conv, is_training)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def create_deconv_init(filter_size, num_channels):\n",
    "    '''Initializes a kernel weight matrix with a bilinear deconvolution\n",
    "    \n",
    "         Parameters:\n",
    "          filter_size (int): kernel size of convolution\n",
    "          num_channels (int): number of filters for convolution\n",
    "\n",
    "         Returns:\n",
    "          bilinear_init (tf.Variable): [filter_size, filter_size, num_channels] kernel\n",
    "    '''\n",
    "    bilinear_kernel = np.zeros([filter_size, filter_size], dtype=np.float32)\n",
    "    scale_factor = (filter_size + 1) // 2\n",
    "    if filter_size % 2 == 1:\n",
    "        center = scale_factor - 1\n",
    "    else:\n",
    "        center = scale_factor - 0.5\n",
    "    for x in range(filter_size):\n",
    "        for y in range(filter_size):\n",
    "            bilinear_kernel[x,y] = (1 - abs(x - center) / scale_factor) * \\\n",
    "                                   (1 - abs(y - center) / scale_factor)\n",
    "    weights = np.zeros((filter_size, filter_size, num_channels, num_channels))\n",
    "    for i in range(num_channels):\n",
    "        weights[:, :, i, i] = bilinear_kernel\n",
    "\n",
    "    #assign numpy array to constant_initalizer and pass to get_variable\n",
    "    bilinear_init = tf.constant_initializer(value=weights, dtype=tf.float32)\n",
    "    return bilinear_init\n",
    "\n",
    "def get_deconv2d(inp, filter_count, num_channels, scope, is_training, clipping_params):\n",
    "    '''Creates a deconvolution layer with Conv2DTranspose. Following recent\n",
    "       recommendations to use 4 kernel, 2 stride to avoid artifacts. \n",
    "       Initialize kernel with bilinear upsampling.\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): input tensorflow layer (B, X, Y, C) shape\n",
    "          filter_count (int): number of filters for convolution\n",
    "          num_channels (int): number of output channels\n",
    "          scope (str): tensorflow variable scope\n",
    "          is_training (str): flag to differentiate between train/test ops\n",
    "          clipping_params (dict): specifies clipping of \n",
    "                                  rmax, dmax, rmin for renormalization\n",
    "\n",
    "         Returns:\n",
    "          x (tf.Variable): layer with (B, x * 2, y * 2, C) shape\n",
    "          \n",
    "         References:\n",
    "          https://distill.pub/2016/deconv-checkerboard/\n",
    "    '''\n",
    "    bilinear_init = create_deconv_init(4, filter_count)\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = filter_count, kernel_size = (4, 4),\n",
    "                                        strides=(2, 2), padding='same', \n",
    "                                        use_bias = False,\n",
    "                                        kernel_initializer = bilinear_init)(inp)\n",
    "    #x = ELU()(x)\n",
    "    #x = tf.nn.relu(x)\n",
    "    x = Batch_Normalization(x, training=is_training, scope = scope + \"bn\", clipping_params = clipping_params)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpa(inp, is_training, filter_count, clipping_params, \n",
    "        keep_rate, upsample = \"upconv\"):\n",
    "    '''Feature pyramid attention layer block, that allows for cross-scale combination\n",
    "       of different size features without making blurry feature maps.\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): input tensorflow layer\n",
    "          is_training (str): flag to differentiate between train/test ops\n",
    "          filter_count (int): number of filters for convolution\n",
    "          clipping_params (dict): specifies clipping of \n",
    "                                  rmax, dmax, rmin for renormalization\n",
    "\n",
    "         Returns:\n",
    "          concat_1 (tf.Variable): output of FPA\n",
    "          \n",
    "         References:\n",
    "          https://arxiv.org/abs/1805.10180\n",
    "    '''\n",
    "    one = conv_bn_relu(inp = inp, is_training = is_training, \n",
    "                       kernel_size = 1, scope =  'forward1',\n",
    "                       filters = filter_count, clipping_params = clipping_params,\n",
    "                       keep_rate = keep_rate, activation = False,\n",
    "                       use_bias = False, batch_norm = True,\n",
    "                       dropblock = False,\n",
    "                       csse = False)\n",
    "    inp_pad = ReflectionPadding2D(padding = (2, 2))(inp)\n",
    "    seven = conv_bn_relu(inp = inp_pad, is_training = is_training, \n",
    "                       kernel_size = 5, scope =  'down1', stride = (2, 2),\n",
    "                       filters = filter_count, clipping_params = clipping_params,\n",
    "                       keep_rate = keep_rate, activation = True,\n",
    "                       use_bias = False, batch_norm = True, csse = True, dropblock = False)\n",
    "    seven_pad = ReflectionPadding2D(padding = (2, 2))(seven)\n",
    "    seven_f = conv_bn_relu(inp = seven_pad, is_training = is_training, \n",
    "                       kernel_size = 5, scope =  'down1_f',\n",
    "                       filters = filter_count, clipping_params = clipping_params,\n",
    "                       keep_rate = keep_rate, activation = False,\n",
    "                       use_bias = False, batch_norm = True, csse = False, dropblock = False)\n",
    "    \n",
    "    print(\"Seven: {}\".format(seven.shape))\n",
    "    print(\"Seven f: {}\".format(seven_f.shape))\n",
    "    \n",
    "    five_pad = ReflectionPadding2D(padding = (1, 1))(seven)\n",
    "    five = conv_bn_relu(inp = five_pad, is_training = is_training,  stride = (2, 2),\n",
    "                       kernel_size = 3, scope =  'down2',\n",
    "                       filters = filter_count, clipping_params = clipping_params,\n",
    "                       keep_rate = keep_rate, activation = True,\n",
    "                       use_bias = False, batch_norm = True, csse = True, dropblock = False)\n",
    "    \n",
    "    five_pad2 = ReflectionPadding2D(padding = (1, 1))(five)\n",
    "    five_f = conv_bn_relu(inp = five_pad2, is_training = is_training, \n",
    "                       kernel_size = 3, scope =  'down2_f',\n",
    "                       filters = filter_count, clipping_params = clipping_params,\n",
    "                       keep_rate = keep_rate, activation = False,\n",
    "                       use_bias = False, batch_norm = True, csse = False, dropblock = False)\n",
    "    print(\"Five: {}\".format(five.shape))\n",
    "    print(\"Five_F: {}\".format(five_f.shape))\n",
    "    '''\n",
    "    three_pad = ReflectionPadding2D(padding = (1, 1))(five)\n",
    "    three = conv_bn_relu(inp = three_pad, is_training = is_training,  stride = (2, 2),\n",
    "                       kernel_size = 3, scope =  'down3',\n",
    "                       filters = filter_count, clipping_params = clipping_params,\n",
    "                       keep_rate = keep_rate, activation = True,\n",
    "                       use_bias = False, batch_norm = True, csse = True, dropblock = False)\n",
    "    \n",
    "    three_pad2 = ReflectionPadding2D(padding = (1, 1))(three)\n",
    "    three_f = conv_bn_relu(inp = three_pad2, is_training = is_training, \n",
    "                       kernel_size = 3, scope =  'down3_f',\n",
    "                       filters = filter_count, clipping_params = clipping_params,\n",
    "                       keep_rate = keep_rate, activation = False,\n",
    "                       use_bias = False, batch_norm = True, csse = True, dropblock = False)\n",
    "        \n",
    "    \n",
    "    if upsample == 'upconv' or 'bilinear':\n",
    "        three_up = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'bilinear')(three_f)\n",
    "        if upsample == 'upconv':\n",
    "            three_up = ReflectionPadding2D((1, 1,))(three_up)\n",
    "            three_up = conv_bn_relu(inp = three_up, is_training = is_training, \n",
    "                       kernel_size = 3, scope =  'upconv1',\n",
    "                       filters = filter_count, clipping_params = clipping_params,\n",
    "                       keep_rate = keep_rate, activation = True,\n",
    "                       use_bias = False, batch_norm = True,\n",
    "                       csse = False, dropblock = False)\n",
    "            \n",
    "            # 4x4\n",
    "            three_up = tf.nn.relu(tf.add(three_up, five_f))\n",
    "    '''        \n",
    "    \n",
    "    if upsample == 'upconv' or \"bilinear\":\n",
    "        five_up = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(five)\n",
    "        if upsample == 'upconv':\n",
    "            five_up = ReflectionPadding2D((1, 1,))(five_up)\n",
    "            five_up = conv_bn_relu(inp = five_up, is_training = is_training, \n",
    "                       kernel_size = 3, scope =  'upconv2',\n",
    "                       filters = filter_count, clipping_params = clipping_params,\n",
    "                       keep_rate = keep_rate, activation = True,\n",
    "                       use_bias = False, batch_norm = True, \n",
    "                       csse = False, dropblock = False)\n",
    "            five_up = tf.nn.relu(tf.add(five_up, seven_f))\n",
    "            \n",
    "    if upsample == 'upconv' or \"bilinear\":\n",
    "        seven_up = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(five_up)\n",
    "        if upsample == 'upconv':\n",
    "            seven_up = ReflectionPadding2D((1, 1,))(seven_up)\n",
    "            seven_up = conv_bn_relu(inp = seven_up, is_training = is_training, \n",
    "                       kernel_size = 3, scope =  'upconv3',\n",
    "                       filters = filter_count, clipping_params = clipping_params,\n",
    "                       keep_rate = keep_rate, activation = True,\n",
    "                       use_bias = False, batch_norm = True, \n",
    "                       csse = False, dropblock = False)\n",
    "    \n",
    "    print(\"One: {}\".format(one.shape))\n",
    "    print(\"Five_up: {}\".format(five_up.shape))\n",
    "    print(\"Seven_up: {}\".format(seven_up.shape))\n",
    "    \n",
    "    # top block\n",
    "\n",
    "    #pooled = tf.keras.layers.GlobalAveragePooling2D()(inp)\n",
    "    #one_top = conv_bn_relu(inp = tf.reshape(pooled, (-1, 1, 1, pooled.shape[-1])),\n",
    "    #                       is_training = is_training, \n",
    "    #                   kernel_size = 1, scope =  'topconv',\n",
    "    #                   filters = filter_count, clipping_params = clipping_params,\n",
    "    #                   keep_rate = keep_rate, activation = False,\n",
    "    #                   use_bias = False, batch_norm = True, \n",
    "    #                   csse = False, dropblock = False)\n",
    "    #one_top = conv_bn_relu(tf.reshape(pooled, (-1, 1, 1, pooled.shape[-1])),\n",
    "    ##                      is_training, 1, 'top1', filter_count, pad = False)\n",
    "    #four_top = tf.keras.layers.UpSampling2D((16, 16))(one_top)\n",
    "    \n",
    "    #seven_up = tf.multiply(one, seven_up)\n",
    "    out = tf.nn.relu(tf.multiply(seven_up, one))\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bands = 16 # 16 for master model\n",
    "reg = keras.regularizers.l2(L2_REG) # for GRU\n",
    "inp = tf.placeholder(tf.float32, shape=(None, 12, IMAGE_SIZE, IMAGE_SIZE, n_bands))\n",
    "length = tf.placeholder(tf.int32, shape = (None, 1))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 14, 14))#, 1))\n",
    "keep_rate = tf.placeholder_with_default(1.0, ()) # For DropBlock\n",
    "length2 = tf.reshape(length, (-1,)) # Remove\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training') # For BN, DropBlock\n",
    "alpha = tf.placeholder(tf.float32, shape = ()) # For loss scheduling\n",
    "ft_lr = tf.placeholder_with_default(0.001, shape = ()) # For loss scheduling\n",
    "loss_weight = tf.placeholder_with_default(1.0, shape = ())\n",
    "beta_ = tf.placeholder_with_default(0.0, shape = ())\n",
    "\n",
    "\n",
    "inp_median = tf.placeholder(tf.float32, shape = (None, IMAGE_SIZE, IMAGE_SIZE, n_bands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmax = tf.placeholder(tf.float32, shape = ())\n",
    "rmin = tf.placeholder(tf.float32, shape = ())\n",
    "dmax = tf.placeholder(tf.float32, shape = ())\n",
    "\n",
    "clipping_params = {\n",
    "    'rmax': rmax,\n",
    "    'rmin': rmin,\n",
    "    'dmax': dmax\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Encoder ###########\n",
    "# GRU       24 x 24 x 24 x 2\n",
    "# Conv      24 x 24 x 48\n",
    "\n",
    "# Pool      12 x 12 \n",
    "# Conv      12 x 12 x 64\n",
    "\n",
    "# Pool      6 x 6   \n",
    "# Conv      4 x 4 x 96\n",
    "\n",
    "\n",
    "\n",
    "####### Decoder ###########\n",
    "# Upsample  8 x 8 \n",
    "# Concat    8 x 8 (12 x 12)\n",
    "# Conv      8 x 8\n",
    "\n",
    "# Upsample 16 x 16\n",
    "# Concat   16 x 16 (24 x 24)\n",
    "# Conv     14 x 14\n",
    "\n",
    "gru, _ = gru_block(inp = inp, length = length2,\n",
    "                            size = [24, 24],\n",
    "                            flt = 28,\n",
    "                            scope = 'down_16',\n",
    "                            train = is_training)\n",
    "\n",
    "gru = ReflectionPadding2D((1, 1,))(gru)\n",
    "gru_conv = conv_bn_relu(inp = gru, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'mean', filters = 48, clipping_params = clipping_params,\n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, batch_norm = True,\n",
    "            csse = True, dropblock = True)\n",
    "\n",
    "pool1 = MaxPool2D()(gru_conv)\n",
    "conv1 = conv_bn_relu(inp = pool1, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv1', filters = 64, clipping_params = clipping_params,\n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, batch_norm = True,\n",
    "            csse = True, dropblock = True)\n",
    "\n",
    "pool2 = MaxPool2D()(conv1)\n",
    "conv2 = conv_bn_relu(inp = pool2, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv2', filters = 96, clipping_params = clipping_params,\n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, batch_norm = True,\n",
    "            csse = True, dropblock = True)\n",
    "\n",
    "\n",
    "# Decoder 4 - 8\n",
    "up1 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(conv2)\n",
    "up1 = ReflectionPadding2D((1, 1,))(up1)\n",
    "up1 = conv_bn_relu(inp = up1, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up1', filters = 64, clipping_params = clipping_params,\n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, batch_norm = True,\n",
    "                    csse = True, dropblock = True)\n",
    "conv1crop = Crop2D(2)(conv1)\n",
    "up1 = tf.concat([up1, conv1crop], axis = -1)\n",
    "up1 = ReflectionPadding2D((1, 1,))(up1)\n",
    "up1 = conv_bn_relu(inp = up1, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up1conv', filters = 64, clipping_params = clipping_params,\n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, batch_norm = True,\n",
    "                    csse = True, dropblock = True)\n",
    "\n",
    "# Decoder 8 - 16\n",
    "up2 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(up1)\n",
    "up2 = ReflectionPadding2D((1, 1,))(up2)\n",
    "up2 = conv_bn_relu(inp = up2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up2', filters = 48, clipping_params = clipping_params,\n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, batch_norm = True,\n",
    "                    csse = True, dropblock = True)\n",
    "conv2crop = Crop2D(4)(gru_conv)\n",
    "up2 = tf.concat([up2, conv2crop], axis = -1)\n",
    "up2 = conv_bn_relu(inp = up2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up2conv', filters = 48, clipping_params = clipping_params,\n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, batch_norm = True,\n",
    "                    csse = True, dropblock = True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_out, steps = gru_block(inp = inp, length = length2,\n",
    "                            size = [16, 16],\n",
    "                            flt = gru_flt,\n",
    "                            scope = 'down_16',\n",
    "                            train = is_training)\n",
    "\n",
    "median = ReflectionPadding2D((1, 1,))(inp_median)\n",
    "median = conv_bn_relu(inp = median, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'mean', filters = 32, clipping_params = clipping_params,\n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, batch_norm = True,\n",
    "                    csse = True, dropblock = True)\n",
    "print(\"median1\", median.shape)\n",
    "\n",
    "median = tf.concat([gru_out, median], axis = -1)\n",
    "print(\"median\", median.shape)\n",
    "median = conv_bn_relu(inp = median, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 1, scope = 'mean2', filters = 48, clipping_params = clipping_params,\n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, batch_norm = True,\n",
    "                    csse = False, dropblock = False)\n",
    "\n",
    "print(\"median\", median.shape)\n",
    "\n",
    "#mean_2f = conv_bn_relu(inp = mean_1f, is_training = is_training, stride = (1, 1),\n",
    "#                    kernel_size = 1, scope = 'mean_1f', filters = 64, clipping_params = clipping_params,\n",
    "#                    keep_rate = keep_rate, activation = True, use_bias = False, batch_norm = True,\n",
    "#                    csse = False, dropblock = False)\n",
    "\n",
    "max1 = MaxPool2D()(median)\n",
    "print(\"max1\", max1.shape)\n",
    "\n",
    "\n",
    "# Down block 2 (8 - 4)\n",
    "down2 = conv_bn_relu(inp = max1, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'down2', filters = 64, clipping_params = clipping_params,\n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, batch_norm = True,\n",
    "                    csse = True, dropblock = True)\n",
    "print(\"down2\", down2.shape)\n",
    "down2_f = conv_bn_relu(inp = down2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'down2_f', filters = 64, clipping_params = clipping_params,\n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, batch_norm = True,\n",
    "                    csse = True, dropblock = True)\n",
    "print(\"Down2_f\", down2_f.shape)\n",
    "\n",
    "# Upblock 1 (4 - 8)\n",
    "up1 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(down2_f)\n",
    "up1 = ReflectionPadding2D((1, 1,))(up1)\n",
    "up1 = conv_bn_relu(inp = up1, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up1', filters = 48, clipping_params = clipping_params,\n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, batch_norm = True,\n",
    "                    csse = True, dropblock = True)\n",
    "up1 = tf.concat([up1, max1], axis = -1)\n",
    "print(\"up1\", up1.shape)\n",
    "\n",
    "\n",
    "# Upblock2 (8 - 16)\n",
    "up2 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(up1)\n",
    "up2 = ReflectionPadding2D((1, 1,))(up2)\n",
    "up2 = conv_bn_relu(inp = up2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up2', filters = 48, clipping_params = clipping_params,\n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, batch_norm = True,\n",
    "                    csse = True, dropblock = True)\n",
    "up2 = tf.concat([up2, median], axis = -1)\n",
    "print(\"up2\", up2.shape)\n",
    "\n",
    "\n",
    "x = conv_bn_relu(inp = up2,\n",
    "                 is_training = is_training,\n",
    "                 kernel_size = 3,\n",
    "                 scope = \"outconv2\",\n",
    "                 filters = 48,\n",
    "                 clipping_params = clipping_params,\n",
    "                 activation = True,\n",
    "                 keep_rate = keep_rate,\n",
    "                 use_bias = False, dropblock = False,\n",
    "                 batch_norm = True, csse = False)\n",
    "print(\"Finzl output shape: \", x.shape)\n",
    "\n",
    "print(\"Initializing last sigmoid bias with -2.94 constant\")\n",
    "init = tf.constant_initializer([-np.log(0.7/0.3)]) # For focal loss\n",
    "fm = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1),\n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid',\n",
    "            bias_initializer = init,\n",
    "           )(x) # For focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24, 12, 6, 12, 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(f\"This model has {total_parameters} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "*  Load in CSV data from Collect Earth\n",
    "*  Reconstruct the X, Y grid for the Y data per sample\n",
    "*  Calculate NDVI, EVI, SAVI, BI, MSAVI2, and SI\n",
    "*  Stack X, Y, length data\n",
    "*  Apply median filter to DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "normalize = False\n",
    "train_x = hkl.load(\"../tile_data/processed/train_x.hkl\")\n",
    "train_y = hkl.load(\"../tile_data/processed/train_y.hkl\")\n",
    "train_l = hkl.load(\"../tile_data/processed/train_l.hkl\")\n",
    "\n",
    "train_x = np.delete(train_x, 14, -1) # test not deleting the SI inde\n",
    "\n",
    "data = pd.read_csv(\"train_plot_ids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethiopia_x =  hkl.load(\"../tile_data/processed/ethiopia/train_x.hkl\")\n",
    "ethiopia_y =  hkl.load(\"../tile_data/processed/ethiopia/train_y.hkl\")\n",
    "ethiopia_l =  hkl.load(\"../tile_data/processed/ethiopia/train_l.hkl\")\n",
    "ethiopia_x = np.delete(ethiopia_x, 14, -1)\n",
    "\n",
    "train_x = np.concatenate([train_x, ethiopia_x])\n",
    "train_y = np.concatenate([train_y, ethiopia_y])\n",
    "train_l = np.concatenate([train_l, ethiopia_l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data preprocessing\n",
    "\n",
    "*  Identify and remove samples with time steps / channels that have a 0. or 1. value, which indicates missing data\n",
    "*  Identify and remove samples with time steps / channels with no variation, which indicates missing data\n",
    "*  Identify and remove samples with values above or below the allowable values for the band\n",
    "*  Identify and remove samples with null data, or samples with extreme band 0 data (which squash all the \"clean\" samples)\n",
    "*  Smooth per-pixel temporal data with Whittaker smoother, d = 2, lambda = 0.5 to reduce sample noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "below_1 = [i for i, val in enumerate(train_x[..., :10]) if np.min(val) < -2]\n",
    "above_1 = [i for i, val in enumerate(train_x[..., :10]) if np.max(val) > 2]\n",
    "min_vals = [np.min(val) for i, val in enumerate(train_x[..., :10]) if np.min(val) < -1.5]\n",
    "max_vals = [np.max(val) for i, val in enumerate(train_x[..., :10]) if np.max(val) > 1.5]\n",
    "nans = [i for i, val in enumerate(train_x) if np.sum(np.isnan(val)) > 100]\n",
    "oob_vals = [i for i, val in enumerate(train_x) if np.max(val[..., 0]) > 0.7]\n",
    "\n",
    "outliers = below_1 + above_1 + nans + oob_vals\n",
    "outliers = list(set(outliers))\n",
    "print(\"Removing {} outlying training data points\".format(len(outliers)))\n",
    "print(sorted(outliers))\n",
    "train_x = np.delete(train_x, outliers, 0)\n",
    "train_y = np.delete(train_y, outliers, 0)\n",
    "train_l = np.delete(train_l, outliers)\n",
    "data = data.drop(outliers, 0)\n",
    "data.reset_index(inplace = True, drop = True)\n",
    "\n",
    "outliers = [4767, 4769, 4771, 4772, 4773, 4783, 4788, 4789]\n",
    "print(\"Removing {} outlying training data points\".format(len(outliers)))\n",
    "print(sorted(outliers))\n",
    "train_x = np.delete(train_x, outliers, 0)\n",
    "train_y = np.delete(train_y, outliers, 0)\n",
    "train_l = np.delete(train_l, outliers)\n",
    "\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = [4779, 4780,]\n",
    "print(\"Removing {} outlying training data points\".format(len(outliers)))\n",
    "print(sorted(outliers))\n",
    "train_x = np.delete(train_x, outliers, 0)\n",
    "train_y = np.delete(train_y, outliers, 0)\n",
    "train_l = np.delete(train_l, outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalize:\n",
    "    means = []\n",
    "    stds = []\n",
    "    for band in tnrange(0, train_x.shape[-1]):\n",
    "        mn = np.mean(train_x[..., band])\n",
    "        std = np.std(train_x[..., band])\n",
    "        normed = (train_x[..., band] - mn) / std\n",
    "        normed[np.where(normed > 3)] = 3.\n",
    "        normed[np.where(normed < -3)] = -3.\n",
    "        train_x[..., band] = normed\n",
    "        print(np.mean(train_x[..., band]))\n",
    "        print(np.std(train_x[..., band]))\n",
    "        means.append(mn)\n",
    "        stds.append(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Until thanksgiving\n",
    "min_all = [0.016768555792030362, 0.031859364002449986, 0.018888066882654337, 0.08300588143157157, 0.04900411763090337,\n",
    " 0.07196114637821546, 0.0790915859490633, 0.08027978003529285,\n",
    " 0.05206992316991091, 0.033226898834616644, 0.0,  0.0029407649199765626,\n",
    " -0.026773188909673434, 0.0032562207477167242, 0.006299999542534351, 4.999999873689376e-05]\n",
    "\n",
    "max_all = [0.21292338743805878,\n",
    " 0.29813900702779483, 0.4300807791948318, 0.5124803858995437,\n",
    " 0.4764584118127815, 0.4890428495407102, 0.5156670875445272,\n",
    " 0.5284151314082131,0.6588925588130947, 0.6016234779357905,\n",
    " 0.37872035768296985, 0.7223387521306766, 0.40211242556571936,\n",
    " 0.6531514992624433, 0.7853250503540039, 0.15389999747276306]\n",
    "\n",
    "min_all = [0.016485823132097722, 0.03130804654210806, 0.018387083709239957,\n",
    "           0.08212612159550192, 0.04826597087085247, 0.07179847471415998, \n",
    "           0.07879771515727044, 0.07925472483038905, 0.05206638704985381, \n",
    "           0.033287848345935345, 0.0, 7.580870587844813e-05, -0.027176744770258665,\n",
    "           0.001232451893156402, 0.006309605669230223, 5.340657662600279e-05]\n",
    "           \n",
    "max_all = [0.21402996256947499, 0.2989177152514456, 0.43112194091081546, \n",
    "           0.512028709053993, 0.4776528477668758, 0.4895881682634349, \n",
    "           0.5156701475381851, 0.5278021097183228, 0.6594102591276167,\n",
    "           0.6019508838653564, 0.36693188548088074, 0.7198386043310159, 0.40469192266464216,\n",
    "           0.6503296375274656, 0.7188067436218262, 0.14778362214565277]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not normalize:\n",
    "    #\n",
    "    #min_all = []\n",
    "    #max_all = []\n",
    "    #\n",
    "    for band in tnrange(0, train_x.shape[-1]):\n",
    "        #mins = np.percentile(train_x[:-24, ..., band], 1)\n",
    "        #maxs = np.percentile(train_x[:-24, ..., band], 99)\n",
    "        mins = min_all[band]\n",
    "        maxs = max_all[band]\n",
    "        #print(mins1 - mins, maxs1 - maxs)\n",
    "        train_x[..., band] = np.clip(train_x[..., band], mins, maxs)\n",
    "        midrange = (maxs + mins) / 2\n",
    "        rng = maxs - mins\n",
    "        standardized = (train_x[..., band] - midrange) / (rng / 2)\n",
    "        train_x[..., band] = standardized\n",
    "\n",
    "        #min_all.append(mins)\n",
    "        #max_all.append(maxs)\n",
    "\n",
    "    print(\"The data has been scaled to [{}, {}]\".format(np.min(train_x), np.max(train_x)))\n",
    "    print(min_all, max_all)\n",
    "    np.save(\"min_all.npy\", min_all)\n",
    "    np.save(\"max_all.npy\", max_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment training data\n",
    "\n",
    "Horizontal and vertical flips for 4x augmentation.\n",
    "\n",
    "**To do**\n",
    "*  Random guassian noise\n",
    "*  Brightness, contrast\n",
    "*  Region swaps (randomply position positive samples at different locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = hkl.load(\"../tile_data/processed/test_x.hkl\")\n",
    "test_y = hkl.load(\"../tile_data/processed/test_y.hkl\")\n",
    "test_lengths = hkl.load(\"../tile_data/processed/test_lengths.hkl\")\n",
    "test_data = pd.read_csv(\"test_plot_ids.csv\")\n",
    "print(test_x.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "test_x = np.delete(test_x, 14, -1)\n",
    "\n",
    "below_1 = [i for i, val in enumerate(test_x[..., :-2]) if np.min(val) < -1.66]\n",
    "above_1 = [i for i, val in enumerate(test_x[..., :-2]) if np.max(val) > 1.66]\n",
    "nans = [i for i, val in enumerate(test_x) if np.sum(np.isnan(val)) > 0]\n",
    "outliers = below_1 + above_1 + nans\n",
    "outliers = list(set(outliers))\n",
    "print(\"There are {} outliers: {}\".format(len(outliers), outliers))\n",
    "\n",
    "test_x = np.delete(test_x, outliers, 0)\n",
    "test_y = np.delete(test_y, outliers, 0)\n",
    "test_lengths = np.delete(test_lengths, outliers, 0)\n",
    "print([x for x in test_data['plot_id'].iloc[outliers]])\n",
    "test_data = test_data.drop(outliers, 0)\n",
    "test_data = test_data.reset_index(drop = True)\n",
    "\n",
    "#for sample in tnrange(0, len(test_x)):\n",
    "#    filtered = median_filter(test_x[sample, 0, :, :, 10], size = 5)\n",
    "#    filtered = np.reshape(filtered, (8, 2, 8, 2))\n",
    "#    filtered = np.mean(filtered, axis = (1, 3))\n",
    "#    filtered = resize(filtered, (16, 16), 0)\n",
    "#    test_x[sample, ..., 10] = np.stack([filtered] * 12)\n",
    "    \n",
    "#band_10 = test_x[..., 10]\n",
    "#band_10[np.where(band_10 > 0.25)] = 0.25\n",
    "#test_x[..., 10] = band_10\n",
    "\n",
    "#test_x = np.reshape(test_x, (test_x.shape[0], 12, 2, 16, 16, 16))\n",
    "#test_x = np.mean(test_x, axis = 2)\n",
    "print(test_x.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = hkl.load(\"../tile_data/processed/test_x.hkl\")\n",
    "test_y = hkl.load(\"../tile_data/processed/test_y.hkl\")\n",
    "test_lengths = hkl.load(\"../tile_data/processed/test_lengths.hkl\")\n",
    "test_data = pd.read_csv(\"test_plot_ids.csv\")\n",
    "print(test_x.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "test_x = np.delete(test_x, 14, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test normalization\n",
    "if normalize:\n",
    "    print(\"Normalizing data\")\n",
    "    for band in tnrange(0, test_x.shape[-1]):\n",
    "        print(f\"{band}, mean, {abs(np.mean(test_x[..., band]) - means[band])}\")\n",
    "        print(f\"{band}, std, {abs(np.std(test_x[..., band]) - stds[band])}\")\n",
    "        normed = (test_x[..., band] - means[band]) / stds[band]\n",
    "        normed[np.where(normed > 3)] = 3.\n",
    "        normed[np.where(normed < -3)] = -3.\n",
    "        test_x[..., band] = normed\n",
    "\n",
    "        means.append(mn)\n",
    "        stds.append(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not normalize:\n",
    "    for band in range(0, test_x.shape[-1]):\n",
    "        mins = min_all[band]\n",
    "        maxs = max_all[band]\n",
    "        test_x[..., band] = np.clip(test_x[..., band], mins, maxs)\n",
    "        midrange = (maxs + mins) / 2\n",
    "        rng = maxs - mins\n",
    "        standardized = (test_x[..., band] - midrange) / (rng / 2)\n",
    "        test_x[..., band] = standardized\n",
    "    \n",
    "    \n",
    "    print(\"The data has been scaled to [{}, {}]\".format(np.min(test_x), np.max(test_x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train and test characteristics:\")\n",
    "print(\"Train mean Y {}\".format(np.mean([np.sum(x) for x in test_y])))\n",
    "print(\"Test STD Y {}\".format(np.std([np.sum(x) for x in test_y])))\n",
    "\n",
    "print(f\"There are {len(train_ids)} train and {len(test_ids)} test samples\")\n",
    "print(\"There is {} overlap between train and test\".format(len([x for x in train_ids if x in test_ids])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equibatch creation\n",
    "\n",
    "The modelling approach uses equibatch sampling to ensure that there is a near constant standard deviation of the percent tree cover in the output labels for each batch. This helps ensure that the model performs equally well across gradients of tree cover, by mitigating the random possibility that many batches in a row near the end of sampling may be randomly biased towards a tree cover range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = np.sum(train_y, axis = (1, 2))\n",
    "percents = [np.percentile(sums, x) for x in range(30, 100, 9)]\n",
    "print(percents)\n",
    "print(\"There are {} zeros\".format(len(np.argwhere(sums == 0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [x for x in range(0, len(train_y))]\n",
    "\n",
    "def multiplot(matrices):\n",
    "    '''Plot multiple heatmaps with subplots\n",
    "    \n",
    "         Parameters:\n",
    "          matrices (list of arrays):\n",
    "\n",
    "         Returns:\n",
    "          None\n",
    "    '''\n",
    "    fig, axs = plt.subplots(ncols=4)\n",
    "    fig.set_size_inches(20, 4)\n",
    "    for i, matrix in enumerate(matrices):\n",
    "        sns.heatmap(data = matrix, ax = axs[i], vmin = 0, vmax = 0.9)\n",
    "        axs[i].set_xlabel(\"\")\n",
    "        axs[i].set_ylabel(\"\")\n",
    "        axs[i].set_yticks([])\n",
    "        axs[i].set_xticks([])\n",
    "\n",
    "def equibatch(train_ids, p = percents):\n",
    "    '''Docstring\n",
    "    \n",
    "         Parameters:\n",
    "          train_ids (list):\n",
    "          p (list):\n",
    "\n",
    "         Returns:\n",
    "          equibatches (list):\n",
    "    '''\n",
    "    percents = [9.0, 19.0, 29.0, 40.0, 64.0, 95.0, 145.0] # sept 28\n",
    "    percents = [9.0, 19.0, 28.0, 40.0, 62.0, 100.0, 155.0] # november\n",
    "   # percents =  [7.0, 13.0, 21.0, 33.0, 50.0, 80.0, 130.0] # overall\n",
    "    #percents = [12.0, 24.0, 42.0, 65.0, 90.0, 125., 170.0] # lac\n",
    "    #percents = [9.0, 19.0, 29.0, 40.0, 55.0, 85.0, 150.0] # east africa\n",
    "    np.random.shuffle(train_ids)\n",
    "    ix = train_ids\n",
    "    percs = [np.sum(x) for x in train_y[ix]]\n",
    "    ids0 = [x for x, z in zip(ix, percs) if z <= 2]\n",
    "    ids30 = [x for x, z in zip(ix, percs) if 2 < z <= percents[0]]\n",
    "    ids40 = [x for x, z in zip(ix, percs) if percents[0] < z <= percents[1]]\n",
    "    ids50 = [x for x, z in zip(ix, percs) if percents[1] < z <= percents[2]]\n",
    "    ids60 = [x for x, z in zip(ix, percs) if percents[2] < z <= percents[3]]\n",
    "    ids70 = [x for x, z in zip(ix, percs) if percents[3] < z <= percents[4]]\n",
    "    ids80 = [x for x, z in zip(ix, percs) if percents[4] < z <= percents[5]]\n",
    "    ids90 = [x for x, z in zip(ix, percs) if percents[5] < z <= percents[6]]\n",
    "    ids100 = [x for x, z in zip(ix, percs) if percents[6] < z]\n",
    "    \n",
    "    new_batches = []\n",
    "    maxes = [len(ids0), len(ids30), len(ids40), len(ids50), len(ids60), len(ids70),\n",
    "             len(ids80), len(ids90), len(ids100)]\n",
    "    print(maxes)\n",
    "    print(sum(maxes))\n",
    "    cur_ids = [0] * len(maxes)\n",
    "    iter_len = len(train_ids)//(len(maxes))\n",
    "    for i in range(0, iter_len):\n",
    "        for i, val in enumerate(cur_ids):\n",
    "            if val > maxes[i] - 1:\n",
    "                cur_ids[i] = 0\n",
    "        if cur_ids[0] >= (maxes[0] - 2):\n",
    "            cur_ids[0] = 0\n",
    "        to_append = [ids0[cur_ids[0]],\n",
    "                    ids30[cur_ids[1]], ids40[cur_ids[2]],\n",
    "                    ids50[cur_ids[3]], ids60[cur_ids[4]], \n",
    "                    ids70[cur_ids[5]], ids80[cur_ids[6]],\n",
    "                    ids90[cur_ids[7]], ids100[cur_ids[8]]]\n",
    "        \n",
    "        \n",
    "        np.random.shuffle(to_append)\n",
    "        new_batches.append(to_append)\n",
    "        cur_ids = [x + 1 for x in cur_ids]\n",
    "        \n",
    "    new_batches = [item for sublist in new_batches for item in sublist]\n",
    "    return new_batches\n",
    "\n",
    "batch = equibatch(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "f.set_size_inches(15, 6)\n",
    "sns.distplot(np.sum(train_y, axis = (1, 2)), bins = 50, kde = False, ax = ax1)\n",
    "ax1.set_title('Original distribution')\n",
    "ax2.set_title('Equibatch distribution')\n",
    "sns.distplot(np.sum(train_y[batch], axis = (1, 2)),\n",
    "             bins = 50, kde = False, ax = ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example equibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[4:8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[8:12]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight cross entropy by effective number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_pos = np.sum(train_y)\n",
    "sum_neg = len(train_y) * 196 - sum_pos\n",
    "print(sum_pos, sum_neg)\n",
    "beta = 0.999\n",
    "print(\"Beta: {}\".format(beta))\n",
    "samples_per_cls = np.array([sum_neg, sum_pos]) / 196\n",
    "print(samples_per_cls)\n",
    "effective_num = 1.0 - np.power(beta, samples_per_cls)\n",
    "print(effective_num)\n",
    "weights = (1.0 - beta) / np.array(effective_num)\n",
    "weights = weights / np.sum(weights)\n",
    "print(\"Neg and pos weights: {}\".format(weights))\n",
    "weight = weights[1] / weights[0]\n",
    "print(weight)\n",
    "weight = 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline: The positive is: {}\".format(weights[0]))\n",
    "print(\"Baseline: The negative is: {}\".format(weights[1]))\n",
    "print(\"\\n\")\n",
    "print(\"Balanced: The positive is: {}\".format(weight*weights[0]))\n",
    "print(\"Balanced: The negative is: {}\".format(weights[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loss definition\n",
    "\n",
    "The current best loss is a combination of weighted binary cross entropy and per-image Lovasz-Softmax, with a loss schedule with the latter becoming more important each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "import math\n",
    "from scipy.ndimage import distance_transform_edt as distance\n",
    "\n",
    "def calc_mask(seg):\n",
    "\n",
    "    res = np.zeros_like(seg)\n",
    "    posmask = seg.astype(np.bool)\n",
    "    loss_importance = np.array([x for x in range(0, 197, 1)])\n",
    "    loss_importance = loss_importance / 196\n",
    "    loss_importance = np.expm1(loss_importance)\n",
    "    loss_importance[:30] = 0.\n",
    "\n",
    "    if posmask.any():\n",
    "        negmask = ~posmask\n",
    "        res = distance(negmask) * negmask - (distance(posmask) - 1) * posmask\n",
    "        #res[np.where(res == 0)] = -1 * loss_importance[196 - sums]\n",
    "    if np.sum(seg) == 196:\n",
    "        res = np.ones_like(seg)\n",
    "    if np.sum(seg) == 0:\n",
    "        res = np.ones_like(seg)\n",
    "    res[np.logical_and(res < 2, res > 0)] = 0.5\n",
    "    res[np.logical_or(res >= 2, res <= 0)] = 1.\n",
    "    return res# / max_value\n",
    "\n",
    "def calc_mask_batch(y_true):\n",
    "    '''Applies calc_dist_map to each sample in an input batch\n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          \n",
    "         Returns:\n",
    "          loss (arr):\n",
    "    '''\n",
    "    y_true_numpy = y_true.numpy()\n",
    "    bce_batch = np.array([calc_mask(y)\n",
    "                     for y in y_true_numpy]).astype(np.float32)\n",
    "    return bce_batch\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight, mask = True, smooth = 0.03):\n",
    "    '''Calculates the weighted binary cross entropy loss between y_true and\n",
    "       y_pred with optional masking and smoothing for regularization\n",
    "       \n",
    "       For smoothing, we want to weight false positives as less important than\n",
    "       false negatives, so we smooth false negatives 2x as much. \n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          y_pred (arr):\n",
    "          weight (float):\n",
    "          mask (arr):\n",
    "          smooth (float):\n",
    "\n",
    "         Returns:\n",
    "          loss (float):\n",
    "    '''\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    y_true = K.clip(y_true, smooth, 1. - smooth)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "        y_true,\n",
    "        logit_y_pred,\n",
    "        weight,\n",
    "    )\n",
    "\n",
    "    return loss\n",
    "    #return tf.reduce_mean(loss)\n",
    "'''\n",
    "    \n",
    "def calc_dist_map(seg):\n",
    "    #Utility function for calc_dist_map_batch that calculates the loss\n",
    "     #  importance per pixel based on the surface distance function\n",
    "    \n",
    "    #     Parameters:\n",
    "     #     seg (arr):\n",
    "          \n",
    "     #    Returns:\n",
    "     #     res (arr):\n",
    "    #\n",
    "    res = np.zeros_like(seg)\n",
    "    posmask = seg.astype(np.bool)\n",
    "    loss_importance = np.array([x for x in range(0, 197, 1)])\n",
    "    loss_importance = loss_importance / 196\n",
    "    loss_importance = np.expm1(loss_importance)\n",
    "    loss_importance[:30] = 0.\n",
    "\n",
    "    if posmask.any():\n",
    "        negmask = ~posmask\n",
    "        res = distance(negmask) * negmask - (distance(posmask) - 1) * posmask\n",
    "        sums = int(np.sum(seg))\n",
    "        # When % = 1, 0 -> 1.75\n",
    "        # When % = 100, 0 -> 0\n",
    "        res[np.where(res == 0)] = -1 * loss_importance[196 - sums]\n",
    "        res[np.where(np.isclose(res, -0.4142, rtol = 1e-2))] = -1 * loss_importance[196 - sums]\n",
    "        # When % = 1, 1 -> 0\n",
    "        # When % = 100, 1 -> 1.75\n",
    "        res[np.where(res == 1)] = loss_importance[sums]\n",
    "        res[np.where(np.isclose(res, 1.41421356, rtol = 1e-2))] = loss_importance[sums]\n",
    "        \n",
    "    res[np.where(res < -3)] = -3\n",
    "    res[np.where(res > 3)] = 3\n",
    "    if np.sum(seg) == 196:\n",
    "        res = np.ones_like(seg)\n",
    "        res *= -1\n",
    "    if np.sum(seg) == 0:\n",
    "        res = np.ones_like(seg)\n",
    "    return res# / max_value\n",
    "'''\n",
    "\n",
    "def calc_dist_map(seg):\n",
    "    #Utility function for calc_dist_map_batch that calculates the loss\n",
    "    #   importance per pixel based on the surface distance function\n",
    "    \n",
    "     #    Parameters:\n",
    "    #      seg (arr):\n",
    "     #     \n",
    "    #     Returns:\n",
    "    #      res (arr):\n",
    "    #\n",
    "    res = np.zeros_like(seg)\n",
    "    posmask = seg.astype(np.bool)\n",
    "    loss_importance = np.array([x for x in range(0, 197, 1)])\n",
    "    loss_importance = loss_importance / 196\n",
    "    loss_importance[:20] = 0.\n",
    "    loss_importance[20:60] = np.arange(0, 40, 1) / 40\n",
    "    loss_importance[60:] = 1.\n",
    "\n",
    "    mults = np.ones_like(seg)\n",
    "    ones = np.ones_like(seg)\n",
    "    for x in range(1, res.shape[0] -1 ):\n",
    "        for y in range(1, res.shape[0] - 1):\n",
    "            if seg[x, y] == 1:\n",
    "                l = seg[x - 1, y]\n",
    "                r = seg[x + 1, y]\n",
    "                u = seg[x, y + 1]\n",
    "                d = seg[x, y - 1]\n",
    "                lu = seg[x - 1, y + 1]\n",
    "                ru = seg[x + 1, y + 1]\n",
    "                rd = seg[x + 1, y - 1]\n",
    "                ld = seg[x -1, y - 1]\n",
    "                \n",
    "                sums = (l + r + u + d)\n",
    "                sums2 = (l + r + u + d + lu + ru +rd + ld)\n",
    "                if sums >= 2:\n",
    "                    mults[x, y] = 1.5\n",
    "                if sums2 <= 1:\n",
    "                    ones[x - 1, y] = 0.25\n",
    "                    ones[x + 1, y] = 0.25\n",
    "                    ones[x, y + 1] = 0.25\n",
    "                    ones[x, y - 1] = 0.25\n",
    "                    ones[x - 1, y + 1] = 0.25\n",
    "                    ones[x + 1, y + 1] = 0.25\n",
    "                    ones[x + 1, y - 1] = 0.25\n",
    "                    ones[x -1, y - 1] = 0.25\n",
    "\n",
    "    if posmask.any():\n",
    "        \n",
    "        negmask = ~posmask\n",
    "        res = distance(negmask) * negmask - (distance(posmask) - 1) * posmask\n",
    "        #sums = int(np.sum(seg))\n",
    "        # When % = 1, 0 -> 1.75\n",
    "        # When % = 100, 0 -> 0\n",
    "        res = np.round(res, 0)\n",
    "        res[np.where(np.isclose(res, -.41421356, rtol = 1e-2))] = -1\n",
    "        res[np.where(res == -1)] = -1 * mults[np.where(res == -1)]\n",
    "        res[np.where(res == 0)] = -1  * mults[np.where(res == 0)]# * loss_importance[196 - sums]\n",
    "        # When % = 1, 1 -> 0\n",
    "        # When % = 100, 1 -> 1.75\n",
    "        res[np.where(res == 1)] = 1 * ones[np.where(res == 1)]\n",
    "        res[np.where(res == 1)] *= 0.67\n",
    "        #res[np.where(np.isclose(res, 1.41421356, rtol = 1e-2))] = loss_importance[sums]\n",
    "        \n",
    "    res[np.where(res < -3)] = -3\n",
    "    res[np.where(res > 3)] = 3\n",
    "    if np.sum(seg) == 196:\n",
    "        res = np.ones_like(seg)\n",
    "        res *= -1\n",
    "    if np.sum(seg) == 0:\n",
    "        res = np.ones_like(seg)\n",
    "    return res# / max_value\n",
    "\n",
    "\n",
    "def calc_lovasz_weight(y_true):\n",
    "    if np.sum(y_true) > 12:\n",
    "        return np.array(1., dtype = np.float32)\n",
    "    else:\n",
    "        return np.array(0., dtype = np.float32)\n",
    "    \n",
    "def calc_bce_weight(y_true):\n",
    "    if np.sum(y_true) <= 12:\n",
    "        return np.array(1., dtype = np.float32)\n",
    "    else:\n",
    "        return np.array(0., dtype = np.float32)\n",
    "    \n",
    "def calc_lovasz_weight_batch(y_true):\n",
    "    '''Applies calc_dist_map to each sample in an input batch\n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          \n",
    "         Returns:\n",
    "          loss (arr):\n",
    "    '''\n",
    "    y_true_numpy = y_true.numpy()\n",
    "    lovasz_batch = np.array([calc_lovasz_weight(y)\n",
    "                     for y in y_true_numpy]).astype(np.float32)\n",
    "    return lovasz_batch\n",
    "\n",
    "def calc_bce_weight_batch(y_true):\n",
    "    '''Applies calc_dist_map to each sample in an input batch\n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          \n",
    "         Returns:\n",
    "          loss (arr):\n",
    "    '''\n",
    "    y_true_numpy = y_true.numpy()\n",
    "    bce_batch = np.array([calc_bce_weight(y)\n",
    "                     for y in y_true_numpy]).astype(np.float32)\n",
    "    return bce_batch\n",
    "    \n",
    "    \n",
    "\n",
    "def calc_dist_map_batch(y_true):\n",
    "    '''Applies calc_dist_map to each sample in an input batch\n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          \n",
    "         Returns:\n",
    "          loss (arr):\n",
    "    '''\n",
    "    y_true_numpy = y_true.numpy()\n",
    "    return np.array([calc_dist_map(y)\n",
    "                     for y in y_true_numpy]).astype(np.float32)\n",
    "\n",
    "def surface_loss(y_true, y_pred):\n",
    "    '''Calculates the mean surface loss for the input batch\n",
    "       by multiplying the distance map by y_pred\n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          y_pred (arr):\n",
    "          \n",
    "         Returns:\n",
    "          loss (arr):\n",
    "        \n",
    "         References:\n",
    "          https://arxiv.org/abs/1812.07032\n",
    "    '''\n",
    "    y_true_dist_map = tf.py_function(func=calc_dist_map_batch,\n",
    "                                     inp=[y_true],\n",
    "                                     Tout=tf.float32)\n",
    "    y_true_dist_map = tf.stack(y_true_dist_map, axis = 0)\n",
    "    multipled = y_pred * y_true_dist_map\n",
    "    #loss = tf.reduce_mean(multipled, axis = (1, 2, 3))\n",
    "    loss = K.mean(multipled)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def lovasz_surf(y_true, y_pred, alpha, weight, beta):\n",
    "    \n",
    "    #lv = lovasz_softmax(probas = y_pred,\n",
    "    #                    labels = tf.reshape(y_true, (-1, 14, 14)), \n",
    "    #                    classes=[1],\n",
    "    #                    per_image=True) \n",
    "    #print(\"Lovasz Shape: \", lv.shape)\n",
    "    \n",
    "    bce = weighted_bce_loss(y_true = y_true, \n",
    "                             y_pred = y_pred, \n",
    "                             weight = weight,\n",
    "                             smooth = 0.03)\n",
    "    #mask_wt = tf.py_function(func=calc_mask_batch,\n",
    "    #                                 inp=[y_true],\n",
    "    #                                 Tout=tf.float32)\n",
    "   # bce = bce * mask_wt\n",
    "    #print(\"BCE Shape: \", bce.shape)\n",
    "    #return tf.reduce_mean(bce)\n",
    "\n",
    "    \n",
    "    bce = tf.reduce_mean(bce, axis = (1, 2, 3))\n",
    "    #print(\"BCE Shape: \", bce.shape)\n",
    "    surface = surface_loss(y_true, y_pred)\n",
    "    \n",
    "    #lv_mask = tf.math.reduce_sum(y_true, axis = (1, 2, 3))\n",
    "    #lv_mask = tf.cast(lv_mask, tf.float32)\n",
    "    #lv_mask = tf.math.greater_equal(lv_mask, tf.constant([25.]))\n",
    "    #lv_mask = tf.cast(lv_mask, tf.float32)\n",
    "\n",
    "    \n",
    "    #bce_mask = tf.math.reduce_sum(y_true, axis = (1, 2, 3))\n",
    "    #bce_mask = tf.cast(bce_mask, tf.float32)\n",
    "    #bce_mask = tf.math.less(bce_mask, tf.constant([25.]))\n",
    "    #bce_mask = tf.cast(bce_mask, tf.float32)\n",
    "    \n",
    "    \n",
    "    #lovasz = (lv * lv_mask) + (bce * bce_mask)\n",
    "    #lovasz = tf.reduce_mean(lovasz)\n",
    "    bce = tf.reduce_mean(bce)\n",
    "    \n",
    "\n",
    "    #lovasz_portion = (1 - alpha) * lovasz\n",
    "    #lovasz_portion = lovasz_portion * beta\n",
    "    bce = (1 - alpha) * bce\n",
    "    #bce = bce * (1 - beta)\n",
    "    surface_portion = alpha * surface\n",
    "    \n",
    "\n",
    "    \n",
    "    result = bce + surface_portion\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "f.set_size_inches(14, 5)\n",
    "sns.heatmap(calc_mask(train_y[-4]), ax = ax1)\n",
    "ax2.set_title('Ground truth Y')\n",
    "ax1.set_title('Boundary loss mask')\n",
    "sns.heatmap(train_y[-4], ax = ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = {'all': [0, 1150]}\n",
    "\n",
    "def dice_loss_tolerance(y_true, y_pred):\n",
    "    numerator_data = np.zeros_like(y_true)\n",
    "    for x in range(y_true.shape[0]):\n",
    "        for y in range(y_true.shape[1]):\n",
    "            min_x = np.max([0, x-1])\n",
    "            min_y = np.max([0, y-1])\n",
    "            max_y = np.min([y_true.shape[0], y+2])\n",
    "            max_x = np.min([y_true.shape[0], x+2])\n",
    "            if y_true[x, y] == 1:\n",
    "                numerator_data[x, y] = np.max(y_pred[min_x:max_x, min_y:max_y])\n",
    "                \n",
    "    numerator = 2 * np.sum(y_true * numerator_data, axis=-1)\n",
    "    denominator = np.sum(y_true + y_pred, axis=-1)\n",
    "    return (numerator + 1) / (denominator + 1)\n",
    "                    \n",
    "            \n",
    "        \n",
    "def compute_f1_score_at_tolerance(true, pred, tolerance = 1):\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    tp = np.zeros_like(true)\n",
    "    fp = np.zeros_like(true)\n",
    "    fn = np.zeros_like(true)\n",
    "    \n",
    "    \n",
    "    for x in range(true.shape[0]):\n",
    "        for y in range(true.shape[1]):\n",
    "            min_x = np.max([0, x-1])\n",
    "            min_y = np.max([0, y-1])\n",
    "            max_y = np.min([true.shape[0], y+2])\n",
    "            max_x = np.min([true.shape[0], x+2])\n",
    "            if true[x, y] == 1:\n",
    "                if np.sum(pred[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    tp[x, y] = 1\n",
    "                else:\n",
    "                    fn[x, y] = 1\n",
    "            if pred[x, y] == 1:\n",
    "                if np.sum(true[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    if true[x, y] == 1:\n",
    "                        tp[x, y] = 1\n",
    "                else:\n",
    "                    fp[x, y] = 1                \n",
    "                \n",
    "    return np.sum(tp), np.sum(fp), np.sum(fn)\n",
    "\n",
    "def recover_patches(arr, thresh_p):\n",
    "    sum1 = np.sum(arr)\n",
    "    thresh = thresh_p / 2\n",
    "\n",
    "    for window_x in range(2, arr.shape[0]-2, 1):\n",
    "        for window_y in range(2, arr.shape[1]-2, 1):\n",
    "            l, r, u, d =  False, False, False, False\n",
    "            cur_window = arr[window_x-2:window_x+3, window_y-2:window_y+3]\n",
    "            hor_vert_neighbors = False\n",
    "            if (cur_window[2, 2] > thresh and\n",
    "                cur_window[2, 2] < thresh_p):\n",
    "                if cur_window[3, 2] > thresh * 2 and cur_window[4, 2] < thresh * 2 :\n",
    "                    hor_vert_neighbors = True\n",
    "                    r = True\n",
    "                if cur_window[1, 2] > thresh * 2 and cur_window[2, 0] < thresh * 2:\n",
    "                    hor_vert_neighbors = True\n",
    "                    l = True\n",
    "                if cur_window[2, 3] > thresh * 2 and cur_window[2, 4] < thresh * 2:\n",
    "                    hor_vert_neighbors = True\n",
    "                    d = True\n",
    "                if cur_window[2, 1] > thresh * 2 and cur_window[2, 0] < thresh * 2:\n",
    "                    hor_vert_neighbors = True\n",
    "                    u = True\n",
    "            passes = False\n",
    "            if r and not l:\n",
    "                passes = True\n",
    "            if l and not r:\n",
    "                passes = True\n",
    "            if d and not u:\n",
    "                passes = True\n",
    "            if u and not d:\n",
    "                passes = True\n",
    "\n",
    "            if passes:\n",
    "                if r:\n",
    "                    if cur_window[1, 1] < thresh * 2 and cur_window[1, 3] < thresh * 2:\n",
    "                        arr[window_x, window_y] = (thresh_p + 0.01)\n",
    "\n",
    "                if l:\n",
    "                    if cur_window[3, 1] < thresh * 2 and cur_window[3, 3] < thresh * 2:\n",
    "                        arr[window_x, window_y] = (thresh_p + 0.01)\n",
    "\n",
    "                if u:\n",
    "                    if cur_window[1, 3] < thresh * 2 and cur_window[3, 3] < thresh * 2:\n",
    "                        arr[window_x, window_y] = (thresh_p + 0.01)\n",
    "\n",
    "                if d:\n",
    "                    if cur_window[1, 1] < thresh * 2 and cur_window[3, 1] < thresh * 2:\n",
    "                        arr[window_x, window_y] = (thresh_p + 0.01)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def calculate_metrics(country, al = 0.4, canopy_thresh = 100):\n",
    "    '''Calculates the following metrics for an input country, based on\n",
    "       indexing of the country dictionary:\n",
    "       \n",
    "         - Loss\n",
    "         - F1\n",
    "         - Precision\n",
    "         - Recall\n",
    "         - Dice\n",
    "         - Mean surface distance\n",
    "         - Average error\n",
    "    \n",
    "         Parameters:\n",
    "          country (str):\n",
    "          al (float):\n",
    "          \n",
    "         Returns:\n",
    "          val_loss (float):\n",
    "          best_dice (float):\n",
    "          error (float):\n",
    "    '''\n",
    "    print(canopy_thresh)\n",
    "    start_idx = 0\n",
    "    stop_idx = len(test_x)\n",
    "    best_f1 = 0\n",
    "    best_dice = 0\n",
    "    best_thresh = 0\n",
    "    hausdorff = 0\n",
    "    relaxed_f1 = 0\n",
    "    preds = []\n",
    "    vls = []\n",
    "    trues = []\n",
    "    test_ids = [x for x in range(len(test_x))]\n",
    "    for test_sample in test_ids[start_idx:stop_idx]:\n",
    "        if np.sum(test_y[test_sample]) < ((canopy_thresh/100) * 197):\n",
    "            x_input = test_x[test_sample].reshape(1, 12, 16, 16, n_bands)\n",
    "            x_median_input = calc_median_input(x_input)\n",
    "            y, vl = sess.run([fm, test_loss], feed_dict={inp: x_input,\n",
    "                                                         inp_median: x_median_input,\n",
    "                                                          length: np.full((1, 1), 12),\n",
    "                                                          is_training: False,\n",
    "                                                          clipping_params['rmax']: rmax_epoch,\n",
    "                                                          clipping_params['rmin']: rmin_epoch,\n",
    "                                                          clipping_params['dmax']: dmax_epoch,\n",
    "                                                          labels: test_y[test_sample].reshape(1, 14, 14),\n",
    "                                                          loss_weight: 1.7,\n",
    "                                                          alpha: al,\n",
    "                                                          })\n",
    "            preds.append(y.reshape((14, 14)))\n",
    "            vls.append(vl)\n",
    "            trues.append(test_y[test_sample].reshape((14, 14)))\n",
    "    dice_losses = []\n",
    "    for thresh in range(8, 10):\n",
    "        tps_relaxed = np.empty((len(preds), ))\n",
    "        fps_relaxed = np.empty((len(preds), ))\n",
    "        fns_relaxed = np.empty((len(preds), ))\n",
    "        abs_error = np.empty((len(preds), ))\n",
    "        \n",
    "        for sample in range(len(preds)):\n",
    "            pred = np.copy(preds[sample])\n",
    "            true = trues[sample]\n",
    "            if thresh == 8:\n",
    "                if np.sum(true + pred) > 0:\n",
    "                    dice_losses.append(dice_loss_tolerance(np.array(true), np.array(pred)))\n",
    "                else:\n",
    "                    dice_losses.append(1.)\n",
    "            pred[np.where(pred >= thresh*0.05)] = 1\n",
    "            pred[np.where(pred < thresh*0.05)] = 0\n",
    "            \n",
    "            true_s = np.sum(true[1:-1])\n",
    "            pred_s = np.sum(pred[1:-1])\n",
    "            abs_error[sample] = abs(true_s - pred_s)\n",
    "            tp_relaxed, fp_relaxed, fn_relaxed = compute_f1_score_at_tolerance(true, pred)\n",
    "            tps_relaxed[sample] = tp_relaxed\n",
    "            fps_relaxed[sample] = fp_relaxed\n",
    "            fns_relaxed[sample] = fn_relaxed                   \n",
    "            \n",
    "        oa_error = np.mean(abs_error)\n",
    "        precision_r = np.sum(tps_relaxed) / (np.sum(tps_relaxed) + np.sum(fps_relaxed))\n",
    "        recall_r = np.sum(tps_relaxed) / (np.sum(tps_relaxed) + np.sum(fns_relaxed))\n",
    "        f1_r = 2*((precision_r* recall_r) / (precision_r + recall_r))\n",
    "        \n",
    "        if f1_r > best_f1:\n",
    "            haus = np.zeros((len(preds), ))\n",
    "            for sample in range(len(preds)):\n",
    "                pred = np.copy(preds[sample])\n",
    "                pred[np.where(pred >= thresh*0.05)] = 1\n",
    "                pred[np.where(pred < thresh*0.05)] = 0\n",
    "                true = trues[sample]\n",
    "                dists = compute_surface_distances(np.array(true).reshape(14, 14, 1).astype(int),\n",
    "                                                  np.array(pred).reshape(14, 14, 1).astype(int),\n",
    "                                                  [1, 1, 1])\n",
    "                if np.sum(true + pred) > 0:\n",
    "                    haus_i = compute_robust_hausdorff(dists, 50)\n",
    "                    if not np.isinf(haus_i):\n",
    "                        haus[sample] = haus_i\n",
    "                if np.sum(true + pred) == 0:\n",
    "                    haus[sample] = 0.\n",
    "                    \n",
    "            dices = np.mean(dice_losses)\n",
    "            haus = np.mean(haus)\n",
    "            best_dice = dices\n",
    "            best_f1 = f1_r\n",
    "            p = precision_r\n",
    "            r = recall_r\n",
    "            error = oa_error\n",
    "            best_thresh = thresh*0.05\n",
    "            best_haus = haus\n",
    "    print(\"{}: Val loss: {} Thresh: {} F1: {}\"\n",
    "          \" R: {} P: {} D: {} H: {} Error: {}\".format(country, \n",
    "                                                     np.around(np.mean(vls), 3),\n",
    "                                                     np.around(best_thresh, 2),\n",
    "                                                     np.around(best_f1, 3), np.around(p, 3),\n",
    "                                                     np.around(r, 3), \n",
    "                                                     np.around(np.mean(best_dice), 3),\n",
    "                                                     np.around(best_haus, 3),\n",
    "                                                     np.around(error, 3)))\n",
    "    return np.mean(vls), best_f1, error, best_haus, np.mean(best_dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_batch(batch_ids, batch_size):\n",
    "    '''Performs random flips and rotations of the X and Y\n",
    "       data for a total of 4 x augmentation\n",
    "    \n",
    "         Parameters:\n",
    "          batch_ids (list):\n",
    "          batch_size (int):\n",
    "          \n",
    "         Returns:\n",
    "          x_batch (arr):\n",
    "          y_batch (arr):\n",
    "    '''\n",
    "    x = train_x[batch_ids]\n",
    "    y = train_y[batch_ids]\n",
    "    x_batch = np.zeros_like(x)\n",
    "    y_batch = np.zeros_like(y)\n",
    "    flips = np.random.choice(np.array([0, 1, 2, 3]), batch_size, replace = True)\n",
    "    for i in range(x.shape[0]):\n",
    "        current_flip = flips[i]\n",
    "        if current_flip == 0:\n",
    "            x_batch[i] = x[i]\n",
    "            y_batch[i] = y[i]\n",
    "        if current_flip == 1:\n",
    "            x_batch[i] = np.flip(x[i], 1)\n",
    "            y_batch[i] = np.flip(y[i], 0)\n",
    "        if current_flip == 2:\n",
    "            x_batch[i] = np.flip(x[i], [2, 1])\n",
    "            y_batch[i] = np.flip(y[i], [1, 0])\n",
    "        if current_flip == 3:\n",
    "            x_batch[i] = np.flip(x[i], 2)\n",
    "            y_batch[i] = np.flip(y[i], 1)\n",
    "\n",
    "    y_batch = y_batch.reshape((batch_size, 14, 14))\n",
    "    return x_batch, y_batch\n",
    "\n",
    "x_batch_test, y_batch_test = augment_batch([0,], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_median_input(x_batch):\n",
    "    x_median = np.median(x_batch, axis = (1))\n",
    "    return x_median\n",
    "\n",
    "x_batch_med = calc_median_input(x_batch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRESH_START = True\n",
    "best_val = 0.2\n",
    "\n",
    "START_EPOCH = 1\n",
    "END_EPOCH = 100\n",
    "LEARNING_RATE = 3e-3\n",
    "test_ids = [x for x in range(0, len(test_x))]\n",
    "\n",
    "print(f\"Starting model with: \\n {ZONE_OUT_PROB} zone out \\n {L2_REG} l2 \\n\"\n",
    "      f\"{INITIAL_LR} initial LR \\n {FINAL_LR} final LR \\n {total_parameters} parameters\")        \n",
    "    \n",
    "if FRESH_START:\n",
    "    print(f\"Restarting training from scratch on {len(train_ids)} train and {len(test_ids)} test samples\")\n",
    "    #optimizer = AdaBoundOptimizer(1e-4, 7e-3)\n",
    "    optimizer = tf.train.AdamOptimizer(5e-5)\n",
    "    train_loss = lovasz_surf(tf.reshape(labels, (-1, 14, 14, 1)), \n",
    "                             fm, weight = loss_weight, \n",
    "                             alpha = alpha, beta = beta_)\n",
    "    #l2_loss = tf.losses.get_regularization_l05oss()\n",
    "    #train_loss += l2_loss\n",
    "#\n",
    "    #ft_optimizer = tf.train.GradientDescentOptimizer(ft_lr)\n",
    "    ft_optimizer = tf.train.MomentumOptimizer(ft_lr, momentum = 0.8, use_nesterov = True)\n",
    "    test_loss = lovasz_surf(tf.reshape(labels, (-1, 14, 14, 1)),\n",
    "                            fm, weight = loss_weight, \n",
    "                            alpha = alpha, beta = beta_)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(train_loss)   \n",
    "        ft_op = ft_optimizer.minimize(train_loss)\n",
    "        \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver(max_to_keep = 150)\n",
    "    \n",
    "print(\"The graph has been finalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path  = \"../models/unet-adam-5e-5/\"\n",
    "if os.path.isfile(f\"{model_path}metrics.npy\"):\n",
    "    metrics = np.load(f\"{model_path}metrics.npy\")\n",
    "    print(f\"Loading {model_path}metrics.npy\")\n",
    "else:\n",
    "    metrics = np.zeros((6, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should be run to do fine-tuning, if commented - train from scratch\n",
    "#@path = '../models/tropics/2e-5/ft-bce-surf/322-92-0/'\n",
    "path = \"../models/unet-adam-5e-5/190-90-3/\"\n",
    "#path = \"../models/unet-new-dem/145-88-4/\"\n",
    "#path = \"../models/unet-s1/9-74-4/\"\n",
    "\n",
    "#path = '../models/nov-unet/85-86-6/'\n",
    "#new_saver = tf.train.import_meta_graph(path + 'model.meta')\n",
    "saver.restore(sess, tf.train.latest_checkpoint(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 150\n",
    "end = 170\n",
    "\n",
    "f, ((c1r1, c1r2, c1r3), (c2r1, c2r2, c2r3)) = plt.subplots(2, 3, sharey=False)\n",
    "f.set_size_inches(15, 12)\n",
    "\n",
    "c1r1.set_title(\"Train losses - 0.33 a, 1.7 w, 1e-4, 7e-3 \")\n",
    "sns.scatterplot(y = metrics[0, start:end], x = np.arange(start, end),\n",
    "               ax = c1r1)\n",
    "\n",
    "c1r2.set_title(\"Nov 25 - F1 score\")\n",
    "sns.scatterplot(y = metrics[5, start:end], x = np.arange(start, end),\n",
    "               ax = c1r2)\n",
    "\n",
    "c2r1.set_title(\"Test losses\")\n",
    "sns.scatterplot(y = metrics[1, start:end], x = np.arange(start, end),\n",
    "               ax = c2r1)\n",
    "\n",
    "c2r2.set_title(\"Errors\")\n",
    "sns.scatterplot(y = metrics[2, start:end], x = np.arange(start, end),\n",
    "               ax = c2r2)\n",
    "\n",
    "c2r3.set_title(\"Hausdorff\")\n",
    "sns.scatterplot(y = metrics[3, start:end], x = np.arange(start, end),\n",
    "               ax = c2r3)\n",
    "\n",
    "c1r3.set_title(\"Dice\")\n",
    "sns.scatterplot(y = metrics[4, start:end], x = np.arange(start, end),\n",
    "               ax = c1r3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the equibatch is working with the augmentation\n",
    "randomize = equibatch(train_ids)\n",
    "sum_no_equibatch = []\n",
    "sum_equibatch = []\n",
    "for k in tnrange(int(len(randomize) // 20)):\n",
    "    rand = [x for x in range(len(randomize))]\n",
    "    batch_ids = rand[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "    _, y_batch = augment_batch(batch_ids, BATCH_SIZE)\n",
    "    sum_no_equibatch.append(np.sum(y_batch))\n",
    "    \n",
    "for k in tnrange(int(len(randomize) // 20)):\n",
    "    batch_ids = randomize[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "    _, y_batch = augment_batch(batch_ids, BATCH_SIZE)\n",
    "    sum_equibatch.append(np.sum(y_batch))\n",
    "    \n",
    "print(\"No equibatch SD: {}\".format(np.std(np.array(sum_no_equibatch))))\n",
    "print(\"Equibatch SD: {}\".format(np.std(np.array(sum_equibatch))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# September changes\n",
    "- Implement equibatch\n",
    "- Implement 4x4 FPA, with CSSE in middle blocks\n",
    "- Reduce label smoothing from 0.08 to 0.03\n",
    "- reduce dropblock to 0.95 from 0.8\n",
    "\n",
    "# Things to test\n",
    "- Concatenating mean after GRU\n",
    "- Turning of sse in GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "best_val = 0.72\n",
    "fine_tune = True\n",
    "countries['all'] = [0, len(test_x)]\n",
    "ft_epochs = 0\n",
    "ft_learning_rate = 5e-4\n",
    "\n",
    "\n",
    "\n",
    "ft_e = 0\n",
    "for i in range(189, 250):\n",
    "    ft_e += 1\n",
    "    al = np.min( [0.01 * (i - 1), 0.33] )\n",
    "    be = 0.0\n",
    "    test_al = 0.33 if al < 0.8 else al\n",
    "    print(ft_epochs)\n",
    "    if fine_tune == True:\n",
    "        op = ft_op\n",
    "        if ft_epochs >= 10:\n",
    "            ft_learning_rate /= 2\n",
    "            ft_epochs = 0\n",
    "            print(\"DIVIDING LR\", ft_learning_rate)\n",
    "        #ft_epochs += 1\n",
    "        #if ft_epochs % 20 == 0:\n",
    "        #    ft_learning_rate /= 2\n",
    "        #    print(\"DIVIDING LR\")\n",
    "        print(f\"FINE TUNING WITH {ft_learning_rate} LR\")\n",
    "    else:\n",
    "        op = op = train_op\n",
    "        \n",
    "    train_ids = [x for x in range(len(train_y))]\n",
    "    randomize = equibatch(train_ids)\n",
    "    print(f\"starting epoch {i}, alpha: {al}, beta: {be} drop: {np.max((1. - (i*0.0025), 0.85))}\")\n",
    "    \n",
    "    loss = train_loss\n",
    "    BATCH_SIZE = 20\n",
    "    test_ids = [x for x in range(0, len(test_x))]\n",
    "    losses = []\n",
    "    \n",
    "    for k in tnrange(int(len(randomize) // BATCH_SIZE)):\n",
    "        rmax_epoch, dmax_epoch, rmin_epoch = calc_renorm_params(i, len(train_y), 20, k)\n",
    "        if k % 8 == 0:\n",
    "            sleep(1.9)\n",
    "        batch_ids = randomize[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "        if any([x for x in batch_ids if x in np.arange(len(train_ids) - 16, len(train_ids))]):\n",
    "            print(\"New sample\")\n",
    "        x_batch, y_batch = augment_batch(batch_ids, BATCH_SIZE)\n",
    "        x_median_input = calc_median_input(x_batch)\n",
    "        opt, tr = sess.run([op, loss],\n",
    "                          feed_dict={inp: x_batch,\n",
    "                                     inp_median: x_median_input,\n",
    "                                     length: np.full((BATCH_SIZE, 1), 12),\n",
    "                                     labels: y_batch,\n",
    "                                     is_training: True,\n",
    "                                     clipping_params['rmax']: rmax_epoch,\n",
    "                                     clipping_params['rmin']: rmin_epoch,\n",
    "                                     clipping_params['dmax']: dmax_epoch,\n",
    "                                     loss_weight: 1.7,\n",
    "                                     keep_rate: np.max((1. - (i * 0.0025), 0.875)),\n",
    "                                     alpha: al,\n",
    "                                     beta_: be,\n",
    "                                     ft_lr: ft_learning_rate,\n",
    "                                     })\n",
    "        losses.append(tr)\n",
    "    \n",
    "    print(f\"Epoch {i}: Loss {np.around(np.mean(losses[:-1]), 3)}\")\n",
    "    val_loss, f1, error, haus, dice = calculate_metrics('all', al = test_al, canopy_thresh = 75)\n",
    "    metrics[0, i] = np.mean(losses[:-1])\n",
    "    metrics[1, i] = val_loss\n",
    "    metrics[2, i] = error\n",
    "    metrics[3, i] = haus\n",
    "    metrics[4, i] = dice\n",
    "    metrics[5, i] = f1\n",
    "    if f1 < (best_val - 0.002):\n",
    "        ft_epochs += 1\n",
    "    if f1 > (best_val - 0.02):\n",
    "        print(f\"Saving model with {f1}\")\n",
    "        np.save(f\"{model_path}metrics.npy\", metrics)\n",
    "        os.mkdir(f\"{model_path}{str(i)}-{str(f1*100)[:2]}-{str(f1*100)[3]}/\")\n",
    "        save_path = saver.save(sess, f\"{model_path}{str(i)}-{str(f1*100)[:2]}-{str(f1*100)[3]}/model\")\n",
    "        if f1 > best_val:\n",
    "            best_val = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 188\n",
    "f1 = 0.901\n",
    "os.mkdir(f\"{model_path}{str(i)}-{str(f1*100)[:2]}-{str(f1*100)[3]}/\")\n",
    "save_path = saver.save(sess, f\"{model_path}{str(i)}-{str(f1*100)[:2]}-{str(f1*100)[3]}/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = [x for x in range(len(test_x))]\n",
    "val_loss, f1, error, haus, dice = calculate_metrics('all', al = test_al, canopy_thresh = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = [x for x in range(len(test_x))]\n",
    "val_loss, f1, error, haus, dice = calculate_metrics('all', al = test_al, canopy_thresh = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 time step, 16, 8, 6, 4, 8, 16, 14 UNET \n",
    "#Epoch 100: Loss 0.25600001215934753\n",
    "#all: Val loss: 0.2709 Thresh: 0.5 F1: 0.878 R: 0.9 P: 0.857 D: 0.8044324218159632 H: 0.511 Error: 8.829\n",
    "\n",
    "# 12 time step, ibid, no sse in convGRU\n",
    "#Epoch 67: Loss 0.26600000262260437\n",
    "#all: Val loss: 0.2750 Thresh: 0.4 F1: 0.87 R: 0.882 P: 0.858 D: 0.717465778 H: 0.559 Error: 10.59\n",
    "\n",
    "\n",
    "# Mean concat\n",
    "#Epoch 51: Loss 0.2709999978542328\n",
    "# all: Val loss: 0.272000 Thresh: 0.45 F1: 0.878 R: 0.899 P: 0.857 D: 0.764088 H: 0.584 Error: 9.635\n",
    "\n",
    "# Avbove, 5e-3\n",
    "# Epoch 165: Loss 0.24699999392032623\n",
    "#all: Val loss: 0.2590 Thresh: 0.4 F1: 0.887 R: 0.91 P: 0.866 D: 0.7396746 H: 0.499 Error: 7.854\n",
    "# Updated test data\n",
    "#all: Val loss: 0.2560 Thresh: 0.4 F1: 0.89 R: 0.917 P: 0.866 D: 0.6593579 H: 0.453 Error: 7.809\n",
    "\n",
    "# Fixed sentinel 1 , epoch 129\n",
    "#all: Val loss: 0.257999986410141 Thresh: 0.4 F1: 0.893 R: 0.92 P: 0.869 D: 0.651 H: 0.437 Error: 8.401\n",
    "\n",
    "# ADAM, 0.33 Alpha\n",
    "# all: Val loss: 0.24799999594688416 Thresh: 0.45 F1: 0.904 R: 0.932 P: 0.878 D: 0.672 H: 0.418 Error: 7.191"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation and sanity checks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = np.concatenate(preds).flatten()\n",
    "trues2 = np.concatenate(trues).flatten()\n",
    "tp = preds2 * trues2\n",
    "fn = [1 if x > y else 0 for (x, y) in zip(trues2, preds2)]\n",
    "fp = [1 if y > x else 0 for (x, y) in zip(trues2, preds2)]\n",
    "tn = (len(test_y) * 196) - np.sum(tp) - np.sum(fn) - np.sum(fp)\n",
    "print(\"TP {}, FN {}, FP {}, TN {}\".format(np.sum(tp), sum(fn), np.sum(fp), tn))\n",
    "print(len(preds2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tps = []\n",
    "fns = []\n",
    "fps = []\n",
    "for i in range(0, len(test_y)*196, 196):\n",
    "    tps.append(np.sum(tp[i:i+196]))\n",
    "    fns.append(np.sum(fn[i:i+196]))\n",
    "    fps.append(np.sum(fp[i:i+196]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "true_canopy = []\n",
    "error_canopy = []\n",
    "pred_canopy = []\n",
    "f1_hard = []\n",
    "tp_softs = []\n",
    "fp_softs = []\n",
    "fn_softs = []\n",
    "for i in range(len(trues)):\n",
    "    true_canopy.append(np.sum(trues[i]) / 1.96)\n",
    "    error_canopy.append(abs(np.sum(preds[i]) - np.sum(trues[i])) / 1.96)\n",
    "    pred_canopy.append(np.sum(preds[i]) / 1.96)\n",
    "    f1_hard.append(f1_score(trues[i], preds[i]))\n",
    "    tp_soft, fp_soft, fn_soft = compute_f1_score_at_tolerance(np.array(trues[i].reshape((14, 14))),\n",
    "                                                 np.array(preds[i].reshape((14, 14))))\n",
    "    tp_softs.append(tp_soft)\n",
    "    fp_softs.append(fp_soft)\n",
    "    fn_softs.append(fn_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = np.sum(tp_softs) / (np.sum(tp_softs) + np.sum(fp_softs))\n",
    "recall = np.sum(tp_softs) / (np.sum(tp_softs) + np.sum(fn_softs))\n",
    "print(precision, recall, np.mean(error_canopy), np.mean(true_canopy), np.mean(pred_canopy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ids_loaded = np.load(\"../data/metrics/plotids.npy\")\n",
    "plot_ids_loaded = np.delete(plot_ids_loaded, outliers, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame({'true': true_canopy,\n",
    "                        'pred': pred_canopy,\n",
    "                        'f1_hard': f1_hard,\n",
    "                        'error': error_canopy,\n",
    "                        'tp': tps,\n",
    "                        'fp': fps,\n",
    "                        'fn': fns,\n",
    "                        'tp_soft': tp_softs,\n",
    "                        'fp_soft': fp_softs,\n",
    "                        'fn_soft': fn_softs,\n",
    "                       })\n",
    "\n",
    "res = map(lambda x: int(math.floor(np.min([x, 90]) / 10.0)) * 10, true_canopy)\n",
    "res = [x for x in res]\n",
    "metrics['group'] = res\n",
    "metrics['model'] = 'proposed'\n",
    "#metrics['slope'] = slopes\n",
    "metrics['plot_id'] = plot_ids_loaded\n",
    "metrics.to_csv(\"../data/metrics/proposed-sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(\"../data/metrics/proposed-sample.csv\")\n",
    "continents = pd.read_csv(\"../data/latlongs/test_continents.csv\")\n",
    "continents = continents.join(metrics, how = 'inner')\n",
    "continents.groupby(\"CONTINENT\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_precisions = metrics.groupby('group').apply(lambda x: (np.sum(x.tp) / np.sum(x.tp + x.fp)))\n",
    "hard_recalls = metrics.groupby('group').apply(lambda x: (np.sum(x.tp) / np.sum(x.tp + x.fn)))\n",
    "errors = metrics.groupby('group').apply(lambda x: np.mean(x.error))\n",
    "hard_f1 = 2 *  ((hard_precisions * hard_recalls) / (hard_precisions + hard_recalls))\n",
    "\n",
    "precisions = metrics.groupby('group').apply(lambda x: (np.sum(x.tp_soft) / np.sum(x.tp_soft + x.fp_soft)))\n",
    "recalls = metrics.groupby('group').apply(lambda x: (np.sum(x.tp_soft) / np.sum(x.tp_soft + x.fn_soft)))\n",
    "soft_f1 = 2 *  ((precisions * recalls) / (precisions + recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metrics = pd.DataFrame({'group': [x for x in range(0, 100, 10)],\n",
    "                            'hard_rec': hard_recalls,\n",
    "                            'soft_rec': recalls,\n",
    "                            'hard_prec': hard_precisions,\n",
    "                            'soft_prec': precisions,\n",
    "                            'hard_f1': hard_f1,\n",
    "                            'soft_f1': soft_f1,\n",
    "                            'error': errors,\n",
    "                            'model': 'proposed'\n",
    "                           })\n",
    "\n",
    "#new_metrics.to_csv(\"../data/metrics/proposed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = [x for x in range(len(test_x))]\n",
    "diffs = []\n",
    "for idx in tnrange(len(test_ids)):\n",
    "    x_input = test_x[idx].reshape(1, 12, IMAGE_SIZE, IMAGE_SIZE, n_bands)\n",
    "    median_input = calc_median_input(x_input)\n",
    "    y = sess.run([fm], feed_dict={inp: x_input,\n",
    "                                  inp_median: median_input,\n",
    "                                  length: np.full((1, 1), 12),\n",
    "                                  is_training: False,\n",
    "                                  clipping_params['rmax']: rmax_epoch,\n",
    "                                  clipping_params['rmin']: rmin_epoch,\n",
    "                                  clipping_params['dmax']: dmax_epoch,\n",
    "                                  })\n",
    "    y = np.array(y).reshape(14, 14)\n",
    "    y[np.where(y > 0.4)] = 1.0\n",
    "    y[np.where(y < 0.4)] = 0.\n",
    "    diff = np.sum(y) - np.sum(test_y[idx])\n",
    "    diffs.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "\n",
    "\n",
    "test_ids = [x for x in range(0, len(test_x))]\n",
    "\n",
    "def multiplot(matrices, nrows = 2, ncols = 4):\n",
    "    '''Docstring\n",
    "    \n",
    "         Parameters:\n",
    "          matrices (list):\n",
    "          nrows (int):\n",
    "          \n",
    "         Returns:\n",
    "          None\n",
    "    '''\n",
    "    fig, axs = plt.subplots(ncols=4, nrows = nrows)\n",
    "    fig.set_size_inches(20, 4*nrows)\n",
    "    to_iter = [[x for x in range(i, i + ncols + 1)] for i in range(0, nrows*ncols, ncols)]\n",
    "    for r in range(1, nrows + 1):\n",
    "        min_i = min(to_iter[r-1])\n",
    "        max_i = max(to_iter[r-1])\n",
    "        for i, matrix in enumerate(matrices[min_i:max_i]):\n",
    "            sns.heatmap(data = matrix, ax = axs[r - 1, i], vmin = 0, vmax = 0.9)\n",
    "            axs[r - 1, i].set_xlabel(\"\")\n",
    "            axs[r - 1, i].set_ylabel(\"\")\n",
    "            axs[r - 1, i].set_yticks([])\n",
    "            axs[r - 1, i].set_xticks([])\n",
    "    plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_alignment(true, pred, wsize = 3, difference = 0.2):\n",
    "    '''Docstring\n",
    "    \n",
    "         Parameters:\n",
    "          true (arr):\n",
    "          pred (arr):\n",
    "          wsize (int):\n",
    "          difference (float):\n",
    "          \n",
    "         Returns:\n",
    "          None\n",
    "    '''\n",
    "    n_single_trees = 0\n",
    "    for x in range(1, true.shape[0] - 1, 1):\n",
    "        for y in range(1, true.shape[1] - 1, 1):\n",
    "            wind_true = true[x-1:x+2, y-1:y+2]\n",
    "            wind_pred = pred[x-1:x+2, y-1:y+2]\n",
    "            if wind_true[1, 1] == 1:\n",
    "                if np.sum(wind_true) == 1:\n",
    "                    n_single_trees += 1\n",
    "                    pred_place = np.argmax(wind_pred.flatten())\n",
    "                    diff = wind_pred.flatten()[pred_place] - wind_pred.flatten()[4]\n",
    "                    if pred_place != 4:\n",
    "                        if diff > difference:\n",
    "                            x_lv = pred_place // 3\n",
    "                            y_lv = pred_place % 3\n",
    "                            print(x_lv, y_lv)\n",
    "                            proposed = wind_true[x_lv - 1:x_lv+2, y_lv-1:y_lv+2]\n",
    "                            if np.sum(proposed) == 0:\n",
    "                                print(\"There is a missed position at {} x, {} y: {}\".format(x, y, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_maxes(y_true, y_pred):\n",
    "    y_out = np.copy(y_pred)\n",
    "    change_map = np.zeros((14, 14))\n",
    "    for x in range(y_true.shape[0]):\n",
    "        for y in range(y_true.shape[0]):\n",
    "            min_x = np.max([0, x-1])\n",
    "            min_y = np.max([0, y-1])\n",
    "            max_y = np.min([y_true.shape[1], y+2])\n",
    "            max_x = np.min([y_true.shape[1], x+2])\n",
    "            \n",
    "            y_true_sum = np.sum(y_true[min_x:max_x, min_y:max_y])\n",
    "            y_pred_max = np.max(y_pred[min_x:max_x, min_y:max_y])\n",
    "            y_pred_min = np.min(y_pred[min_x:max_x, min_y:max_y])\n",
    "            \n",
    "    \n",
    "            if y_true[x, y] == 1: # if positive and unsure\n",
    "                if y_true_sum < 9: # if edge and there is a candidate\n",
    "                    diffs = y_pred_max - y_pred[x, y]\n",
    "                    percentage = np.min([1/(1 + np.exp(-(diffs * 10 - 5))), 0.5])\n",
    "                    y_out[x, y] += percentage\n",
    "                    change_map[x, y] = percentage\n",
    "            if y_true[x, y] == 0:# and y_pred[x, y] > 0.75: # if negative and unsure\n",
    "                if y_true_sum > 0: #and y_pred_min < 0.25: # if edge and there is a candidate\n",
    "                    diffs = y_pred[x, y] - y_pred_min\n",
    "                    percentage = np.min([1/(1 + np.exp(-(diffs * 10 - 5))), 0.5])\n",
    "                    y_out[x, y] -= percentage #= np.min(y_pred[min_x:max_x, min_y:max_y])\n",
    "                    change_map[x, y] = -percentage\n",
    "    return y_out, change_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(start/len(test_ids))\n",
    "#test_ids = [x for x in range(test_x.shape[0])]\n",
    "#test_ids = sorted(test_ids)\n",
    "#test_ids = np.argwhere(abs(np.array(diffs)) > 40)\n",
    "matrix_ids = [test_ids[start], test_ids[start + 1], test_ids[start + 2], test_ids[start + 3],\n",
    "              test_ids[start + 4], test_ids[start + 5], test_ids[start + 6], test_ids[start + 7]]\n",
    "\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    #print(i)\n",
    "    x_input = test_x[idx].reshape(1, 12, IMAGE_SIZE, IMAGE_SIZE, n_bands)\n",
    "    median_input = calc_median_input(x_input)\n",
    "    y = sess.run([fm], feed_dict={inp: x_input,\n",
    "                                  inp_median: median_input,\n",
    "                                  length: np.full((1, 1), 12),\n",
    "                                  is_training: False,\n",
    "                                  clipping_params['rmax']: rmax_epoch,\n",
    "                                  clipping_params['rmin']: rmin_epoch,\n",
    "                                  clipping_params['dmax']: dmax_epoch,\n",
    "                                  })\n",
    "    y = np.array(y).reshape(14, 14)\n",
    "    #y, mapshape = aggregate_maxes(test_y[idx], y)\n",
    "    preds.append(y)\n",
    "    y2 = np.copy(y)\n",
    "    #print(idx, list(test_data.iloc[i, 0])[0],\n",
    "    #      list(test_data.iloc[i, 1])[0],\n",
    "    #      list(test_data.iloc[i, 2])[0], diffs_new[i[0]])\n",
    "    print(idx, test_data.iloc[i, 0],\n",
    "          test_data.iloc[i, 1],\n",
    "          test_data.iloc[i, 2])#, diffs[i[0]])\n",
    "    true = test_y[idx].reshape(14, 14)\n",
    "    trues.append(true)\n",
    "\n",
    "\n",
    "to_plot = trues[0:4] + preds[0:4] + trues[4:] + preds[4:]\n",
    "\n",
    "multiplot(to_plot, nrows = 4, ncols = 4)\n",
    "\n",
    "\n",
    "start = start + 8 \n",
    "\n",
    "# 123, 334, 680, 875, 917, 950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [x for x in range(len(train_y))]\n",
    "diffs = []\n",
    "for idx in tnrange(len(train_ids)):\n",
    "    x_input = train_x[idx].reshape(1, 12, IMAGE_SIZE, IMAGE_SIZE, n_bands)\n",
    "    median_input = calc_median_input(x_input)\n",
    "    y = sess.run([fm], feed_dict={inp: x_input,\n",
    "                                  inp_median: median_input,\n",
    "                                  length: train_l[idx].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  clipping_params['rmax']: rmax_epoch,\n",
    "                                  clipping_params['rmin']: rmin_epoch,\n",
    "                                  clipping_params['dmax']: dmax_epoch,\n",
    "                                  })\n",
    "    y = np.array(y).reshape(14, 14)\n",
    "    y[np.where(y > 0.45)] = 1.0\n",
    "    y[np.where(y < 0.45)] = 0.\n",
    "    diff = np.sum(y) - np.sum(train_y[idx])\n",
    "    diffs.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [x for x in range(train_x.shape[0])]\n",
    "train_ids = sorted(train_ids)\n",
    "start = len(train_ids) - 16\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##matrix_ids = [70, 139, 456, 630, 718, 800, 835, 1101]\n",
    "\n",
    "#train_ids = np.argwhere(np.logical_and(abs(np.array(diffs)) >= 50,\n",
    "#                                       abs(np.array(diffs)) < 60))\n",
    "\n",
    "matrix_ids = [train_ids[start], train_ids[start + 1], train_ids[start + 2],\n",
    "             train_ids[start + 3], train_ids[start + 4],\n",
    "             train_ids[start + 5], train_ids[start + 6], train_ids[start + 7]]\n",
    "#matrix_ids = to_update[-8:]\n",
    "preds = []\n",
    "trues = []\n",
    "#print(start//4)\n",
    "print(matrix_ids)\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    x_input = train_x[idx].reshape(1, 12, IMAGE_SIZE, IMAGE_SIZE, n_bands)\n",
    "    median_input = calc_median_input(x_input)\n",
    "    y = sess.run([fm], feed_dict={inp: x_input,\n",
    "                                  inp_median: median_input,\n",
    "                                  length: np.full((1, 1), 12),\n",
    "                                  is_training: False,\n",
    "                                  clipping_params['rmax']: rmax_epoch,\n",
    "                                  clipping_params['rmin']: rmin_epoch,\n",
    "                                  clipping_params['dmax']: dmax_epoch,\n",
    "                                    })\n",
    "    y = np.array(y).reshape(14, 14)\n",
    "    \n",
    "    #y, _ = aggregate_maxes(train_y[idx], y)\n",
    "    \n",
    "    preds.append(y)\n",
    "    true = train_y[idx].reshape(14, 14)\n",
    "    \n",
    "    #print(idx, (list(data.iloc[idx, 1])[0], list(data.iloc[idx, 2])[0]), diffs[i[0]])\n",
    "    #print(idx, test_data.iloc[i, 0], test_data.iloc[i, 1],\n",
    "    #      test_data.iloc[i, 2], diffs[i[0]])\n",
    "    trues.append(true)\n",
    "    \n",
    "start += 8\n",
    "\n",
    "to_plot = trues[0:4] + preds[0:4] + trues[4:] + preds[4:]\n",
    "multiplot(to_plot, nrows = 4, ncols = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy-toolkit",
   "language": "python",
   "name": "policy-toolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
