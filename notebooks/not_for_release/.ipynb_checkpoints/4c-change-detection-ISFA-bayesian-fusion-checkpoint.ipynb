{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hickle as hkl\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from matplotlib import image\n",
    "from scipy.cluster.vq import kmeans as km\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from scipy.misc import imread, imresize, imsave\n",
    "\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%run ../../src/downloading/utils.py\n",
    "%run ../../src/models/utils.py\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landscape = 'brazil-streambank-2'\n",
    "# brazil-planting 3 needs to be reprocesed for 2017, 2018, no\n",
    "# brazil small needs to be reprocessed for 2017, 2018, not for 2019\n",
    "im1 = hkl.load(f\"../../tile_data/{landscape}/2018/interim/0_0.hkl\")\n",
    "print(f\"There are {np.sum(np.isnan(im1))} NA values in 2018\")\n",
    "im1 = im1[..., :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = hkl.load(f\"../../tile_data/{landscape}/2017/interim/0_0.hkl\")\n",
    "print(f\"There are {np.sum(np.isnan(im1))} NA values in 2017\")\n",
    "im1 = im1[..., :8]\n",
    "\n",
    "im1_med = np.median(im1, axis = 0)\n",
    "im1_stdev = np.std(im1, axis = 0)\n",
    "im1 = np.concatenate([im1_med, im1_stdev], axis = -1)\n",
    "        \n",
    "\n",
    "im2 = hkl.load(f\"../../tile_data/{landscape}/2019/interim/0_0.hkl\")\n",
    "print(f\"There are {np.sum(np.isnan(im1))} NA values in 2019\")\n",
    "\n",
    "\n",
    "im2 = im2[..., :8]\n",
    "\n",
    "im2_med = np.median(im2, axis = 0)\n",
    "im2_stdev = np.std(im2, axis = 0)\n",
    "im2 = np.concatenate([im2_med, im2_stdev], axis = -1)\n",
    "net_shape = [128, 128, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_image = np.sum(abs(im1 - im2), axis = -1)\n",
    "plt.figure(figsize=(12, 9))\n",
    "sns.heatmap(diff_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "sns.heatmap(im1[..., 3] - im2[..., 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change vector analysis with canonical correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "x = im1[:, ..., :16].reshape(646*646, 16)\n",
    "y = im2[:, ..., :16].reshape(646*646, 16)\n",
    "cca = CCA(n_components=3)\n",
    "cca.fit(x, y)\n",
    "xs = cca.transform(x)\n",
    "ys = cca.transform(y)\n",
    "#xs = np.sum(xs, axis = 1)\n",
    "#ys = np.sum(ys, axis = 1)\n",
    "diffs = abs(xs - ys)\n",
    "diffs = np.sum(diffs, axis = 1)\n",
    "#diffs = (diffs - np.mean(diffs)) / np.std(diffs)\n",
    "\n",
    "diffs = diffs.reshape(646, 646)\n",
    "cutoff = np.percentile(diffs.flatten(), 1)\n",
    "print(len(diffs.flatten())/ 200)\n",
    "new = np.ones_like(diffs)\n",
    "new[np.where(diffs < cutoff)] = 0.\n",
    "#ys = ys.reshape(142, 142)\n",
    "#diffs = abs(xs - ys)\n",
    "plt.figure(figsize=(12, 9))\n",
    "sns.heatmap(new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep slow feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(img=None, chg_ref=None):\n",
    "\n",
    "    chg_ref = np.array(chg_ref, dtype=np.float32)\n",
    "    chg_ref = chg_ref / np.max(chg_ref)\n",
    "\n",
    "    img = np.reshape(img, [-1])\n",
    "    chg_ref = np.reshape(chg_ref, [-1])\n",
    "\n",
    "    loc1 = np.where(chg_ref == 1)[0]\n",
    "    num1 = np.sum(img[loc1] == 1)\n",
    "    acc_chg = np.divide(float(num1), float(np.shape(loc1)[0]))\n",
    "\n",
    "    loc2 = np.where(chg_ref == 0)[0]\n",
    "    num2 = np.sum(img[loc2] == 0)\n",
    "    acc_un = np.divide(float(num2), float(np.shape(loc2)[0]))\n",
    "\n",
    "    acc_all = np.divide(float(num1 + num2), float(np.shape(loc1)[0] + np.shape(loc2)[0]))\n",
    "\n",
    "    loc3 = np.where(img == 1)[0]\n",
    "    num3 = np.sum(chg_ref[loc3] == 1)\n",
    "    acc_tp = np.divide(float(num3), float(np.shape(loc3)[0]))\n",
    "\n",
    "    print('')\n",
    "    print('Accuracy of Unchanged Regions is: %.4f' % (acc_un))\n",
    "    print('Accuracy of Changed Regions is:   %.4f' % (acc_chg))\n",
    "    print('The True Positive ratio is:       %.4f' % (acc_tp))\n",
    "    print('Accuracy of all testing sets is : %.4f' % (acc_all))\n",
    "\n",
    "    return acc_un, acc_chg, acc_all, acc_tp\n",
    "\n",
    "\n",
    "def getTrainSamples(index, im1, im2, number=4000):\n",
    "\n",
    "    loc = np.where(index != 1)[0]\n",
    "    perm = np.random.permutation(np.shape(loc)[0])\n",
    "\n",
    "    ind = loc[perm[0:number]]\n",
    "\n",
    "    return im1[ind, :], im2[ind, :]\n",
    "\n",
    "\n",
    "def normlize(data):\n",
    "    meanv = np.mean(data, axis=0)\n",
    "    stdv = np.std(data, axis=0)\n",
    "\n",
    "    delta = data - meanv\n",
    "    data = delta / stdv\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def linear_sfa(fcx, fcy, vp, shape):\n",
    "\n",
    "    delta = np.matmul(fcx, vp) - np.matmul(fcy, vp)\n",
    "\n",
    "    #delta = delta / np.std(delta, axis=0)\n",
    "\n",
    "    #delta = delta**2\n",
    "\n",
    "    differ_map = delta#normlize(delta)\n",
    "    # / np.var(delta, axis = 0)\n",
    "\n",
    "    magnitude = np.sum(delta**2 / np.var(delta, axis = 0), axis=1)\n",
    "\n",
    "    vv = magnitude / np.max(magnitude)\n",
    "\n",
    "    im = np.reshape(kmeans(vv), shape[0:-1])\n",
    "\n",
    "    return im, magnitude, differ_map\n",
    "\n",
    "\n",
    "def kmeans(data):\n",
    "    shape = np.shape(data)\n",
    "    # print((data))\n",
    "    ctr, _ = km(data, 2)\n",
    "\n",
    "    for k1 in range(shape[0]):\n",
    "        if abs(ctr[0] - data[k1]) >= abs(ctr[1] - data[k1]):\n",
    "            data[k1] = 0\n",
    "        else:\n",
    "            data[k1] = 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)-15s %(levelname)s: %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n",
    "\n",
    "def dsfa(xtrain, ytrain, xtest, ytest, net_shape=None):\n",
    "\n",
    "    train_num = np.shape(xtrain)[0]\n",
    "    bands = np.shape(xtrain)[-1]\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    activation = tf.nn.softsign\n",
    "\n",
    "    xd = tf.placeholder(dtype=tf.float32, shape=[None, bands])\n",
    "    yd = tf.placeholder(dtype=tf.float32, shape=[None, bands])\n",
    "\n",
    "    # fc1\n",
    "    fc1w1 = tf.Variable(tf.truncated_normal(shape=[bands, net_shape[0]], dtype=tf.float32, stddev=1e-1))\n",
    "    fc1w2 = tf.Variable(tf.truncated_normal(shape=[bands, net_shape[0]], dtype=tf.float32, stddev=1e-1))\n",
    "    fc1b1 = tf.Variable(tf.constant(1e-1, shape=[net_shape[0]], dtype=tf.float32))\n",
    "    fc1b2 = tf.Variable(tf.constant(1e-1, shape=[net_shape[0]], dtype=tf.float32))\n",
    "\n",
    "    fc1x = tf.nn.bias_add(tf.matmul(xd, fc1w1), fc1b1)\n",
    "    fc1y = tf.nn.bias_add(tf.matmul(yd, fc1w2), fc1b2)\n",
    "\n",
    "    fc11 = activation(fc1x)\n",
    "    fc12 = activation(fc1y)\n",
    "\n",
    "    # fc2\n",
    "    fc2w1 = tf.Variable(tf.truncated_normal(shape=[net_shape[0], net_shape[1]], dtype=tf.float32, stddev=1e-1))\n",
    "    fc2w2 = tf.Variable(tf.truncated_normal(shape=[net_shape[0], net_shape[1]], dtype=tf.float32, stddev=1e-1))\n",
    "    fc2b1 = tf.Variable(tf.constant(1e-1, shape=[net_shape[1]], dtype=tf.float32))\n",
    "    fc2b2 = tf.Variable(tf.constant(1e-1, shape=[net_shape[1]], dtype=tf.float32))\n",
    "\n",
    "    fc2x = tf.nn.bias_add(tf.matmul(fc11, fc2w1), fc2b1)\n",
    "    fc2y = tf.nn.bias_add(tf.matmul(fc12, fc2w2), fc2b2)\n",
    "\n",
    "    fc21 = activation(fc2x)\n",
    "    fc22 = activation(fc2y)\n",
    "\n",
    "    # fc3\n",
    "    fc3w1 = tf.Variable(tf.truncated_normal(shape=[net_shape[1], net_shape[2]], dtype=tf.float32, stddev=1e-1))\n",
    "    fc3w2 = tf.Variable(tf.truncated_normal(shape=[net_shape[1], net_shape[2]], dtype=tf.float32, stddev=1e-1))\n",
    "    fc3b1 = tf.Variable(tf.constant(1e-1, shape=[net_shape[2]], dtype=tf.float32))\n",
    "    fc3b2 = tf.Variable(tf.constant(1e-1, shape=[net_shape[2]], dtype=tf.float32))\n",
    "\n",
    "    fc3x = tf.nn.bias_add(tf.matmul(fc21, fc3w1), fc3b1)\n",
    "    fc3y = tf.nn.bias_add(tf.matmul(fc22, fc3w2), fc3b2)\n",
    "\n",
    "    fc3x = activation(fc3x)\n",
    "    fc3y = activation(fc3y)\n",
    "\n",
    "    #fc3x - tf.cast(tf.divide(1, bands), tf.float32) * tf.matmul(fc3x, tf.ones([bands, bands]))\n",
    "    m = tf.shape(fc3x)[1]\n",
    "    fc_x = fc3x - tf.cast(tf.divide(1, m), tf.float32) * tf.matmul(fc3x, tf.ones([m, m]))\n",
    "    fc_y = fc3y - tf.cast(tf.divide(1, m), tf.float32) * tf.matmul(fc3y, tf.ones([m, m]))\n",
    "\n",
    "    Differ = fc_x - fc_y\n",
    "\n",
    "    A = tf.matmul(Differ, Differ, transpose_a=True)\n",
    "    A = A / train_num\n",
    "\n",
    "    sigmaX = tf.matmul(fc_x, fc_x, transpose_a=True)\n",
    "    sigmaY = tf.matmul(fc_y, fc_y, transpose_a=True)\n",
    "    sigmaX = sigmaX / train_num + 1e-4  * tf.eye(net_shape[-1])\n",
    "    sigmaY = sigmaY / train_num + 1e-4  * tf.eye(net_shape[-1])\n",
    "\n",
    "    B = (sigmaX + sigmaY) / 2# + args.reg * tf.eye(net_shape[-1])\n",
    "\n",
    "    # B_inv, For numerical stability.\n",
    "    D_B, V_B = tf.self_adjoint_eig(B)\n",
    "    idx = tf.where(D_B > 1e-12)[:, 0]\n",
    "    D_B = tf.gather(D_B, idx)\n",
    "    V_B = tf.gather(V_B, idx, axis=1)\n",
    "    B_inv = tf.matmul(tf.matmul(V_B, tf.diag(tf.reciprocal(D_B))), tf.transpose(V_B))\n",
    "\n",
    "    sigma = tf.matmul(B_inv, A)#+ args.reg * tf.eye(net_shape[-1])\n",
    "\n",
    "    D, V = tf.self_adjoint_eig(sigma)\n",
    "    \n",
    "    #loss = tf.sqrt(tf.trace(tf.matmul(sigma,sigma)))\n",
    "    loss = tf.trace(tf.matmul(sigma,sigma))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    loss_log = []\n",
    "\n",
    "    #gpu_options = tf.GPUOptions(allow_growth = True)\n",
    "    #conf        = tf.ConfigProto(gpu_options=gpu_options)\n",
    "    sess = tf.Session()\n",
    "\n",
    "    sess.run(init)\n",
    "    #writer = tf.summary.FileWriter('graph')\n",
    "    #writer.add_graph(sess.graph)\n",
    "\n",
    "    for k in range(4000):\n",
    "        sess.run(optimizer, feed_dict={xd: xtrain, yd: ytrain})\n",
    "\n",
    "        if k % 100 == 0:\n",
    "            ll = sess.run(loss, feed_dict={xd: xtrain, yd: ytrain})\n",
    "            ll = ll / net_shape[-1]\n",
    "            logging.info('The %4d-th epochs, loss is %4.4f ' % (k, ll))\n",
    "            loss_log.append(ll)\n",
    "\n",
    "    matV = sess.run(V, feed_dict={xd: xtest, yd: ytest})\n",
    "    bVal = sess.run(B, feed_dict={xd: xtest, yd: ytest})\n",
    "\n",
    "    fcx = sess.run(fc_x, feed_dict={xd: xtest, yd: ytest})\n",
    "    fcy = sess.run(fc_y, feed_dict={xd: xtest, yd: ytest})\n",
    "\n",
    "    sess.close()\n",
    "    print('')\n",
    "\n",
    "    return loss_log, matV, fcx, fcy, bVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = np.shape(im1)\n",
    "\n",
    "im1 = np.reshape(im1, newshape=[-1,img_shape[-1]])\n",
    "im2 = np.reshape(im2, newshape=[-1,img_shape[-1]])\n",
    "\n",
    "im1 = normlize(im1)\n",
    "im2 = normlize(im2)\n",
    "\n",
    "imm = None\n",
    "all_magnitude = None\n",
    "cva_new = new\n",
    "#cva_new = np.zeros((646, 646))\n",
    "#cva_new[np.where(diff_image < np.percentile(diff_image, 2))] = 1.\n",
    "cva_ind = np.reshape(cva_new, newshape=[-1])\n",
    "\n",
    "i1, i2 = getTrainSamples(cva_ind, im1, im2, 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log, vpro, fcx, fcy, bval = dsfa(xtrain=i1, ytrain=i2, xtest=im1, ytest=im2, net_shape=net_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imm, magnitude, differ_map = linear_sfa(fcx, fcy, vpro, shape=img_shape)\n",
    "\n",
    "magnitude = np.reshape(magnitude, img_shape[0:-1])\n",
    "magnitude_float = np.copy(magnitude)\n",
    "differ = differ_map\n",
    "\n",
    "change_map = np.reshape(kmeans(np.reshape(magnitude, [-1])), img_shape[0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.stats import chi2\n",
    "#plt.figure(figsize=(13, 10))\n",
    "#probability_map = chi2.cdf(magnitude_float, 16+15).reshape(646, 646)\n",
    "#sns.heatmap(probability_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#magnitude_float2 = magnitude_float / np.percentile(magnitude_float, 97.5)\n",
    "magnitude_float2 = np.copy(magnitude_float)\n",
    "magnitude_float2[np.where(magnitude_float2 > np.percentile(magnitude_float2, 90))] = np.percentile(magnitude_float2, 90)\n",
    "magnitude_float2 = magnitude_float2 / np.percentile(magnitude_float2, 100)\n",
    "plt.figure(figsize=(15, 17))\n",
    "sns.heatmap(magnitude_float2.reshape((646, 646)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = (-21.205183, -51.588244)\n",
    "coords = (coords[1], coords[0])\n",
    "point = bounding_box(coords, (5*1260)-0, ((5)*1260)-0, expansion = 0)\n",
    "west = point[1][0]\n",
    "east = point[0][0]\n",
    "north = point[0][1]\n",
    "south = point[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian soft fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "start_probs = np.array(Image.open(f'../../tile_data/{landscape}/2017.tif'))\n",
    "middle_probs = np.array(Image.open(f'../../tile_data/{landscape}/2018.tif'))\n",
    "end_probs = np.array(Image.open(f'../../tile_data/{landscape}/2019.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = magnitude_float2.reshape((646, 646))[8:-8, 8:-8]\n",
    "cv[np.where(cv > 1)] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 10))\n",
    "sns.heatmap(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = rasterio.transform.from_bounds(west = west, south = south,\n",
    "                                           east = east, north = north,\n",
    "                                           width = cv.shape[1], height = cv.shape[1])\n",
    "\n",
    "print(\"Writing\" + '../../tile_data/{}/{}.tif'.format(landscape, \"gain-cva\"))\n",
    "new_dataset = rasterio.open('../../tile_data/{}/{}.tif'.format(landscape, \"cva-file\"), 'w', driver = 'GTiff',\n",
    "                           height = cv.shape[1], width = cv.shape[1], count = 1,\n",
    "                           dtype = 'float32',#str(stacked.dtype),\n",
    "                           crs = '+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs',\n",
    "                           transform=transform)\n",
    "new_dataset.write(cv.astype(np.float32), 1)\n",
    "new_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_possibilities = [[0, 1, 1],\n",
    "                      [0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "possibilities = [list(x) for x in product([0, 1], [0, 1], [0, 1])]\n",
    "print(possibilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_map = np.zeros((630, 630, 8))\n",
    "for i, val in enumerate(possibilities):\n",
    "    start = val[0]\n",
    "    middle = val[1]\n",
    "    end = val[2]\n",
    "    if start == 1:\n",
    "        pwi_x = start_probs\n",
    "    else:\n",
    "        pwi_x = 1 - start_probs\n",
    "        \n",
    "    if middle == 1:\n",
    "        pxk_z = middle_probs\n",
    "    else:\n",
    "        pxk_z = 1 - middle_probs\n",
    "\n",
    "    if end == 1:\n",
    "        pvj_y = end_probs\n",
    "    else:\n",
    "        pvj_y = 1 - end_probs\n",
    "\n",
    "    if start == end:\n",
    "        pwi_vj = 1 - (cv)\n",
    "    else:\n",
    "        pwi_vj = (cv)\n",
    "\n",
    "    prior = np.logical_and(pwi_x > 0.95, pvj_y > 0.95)\n",
    "    prior = np.logical_and(prior, pxk_z > 0.95)\n",
    "\n",
    "    #change_map[..., i] = pwi_x * pvj_y * pxk_z * pwi_vj \n",
    "    change_map[prior, i] = pwi_x[prior] * pvj_y[prior] * pxk_z[prior]# * pwi_vj[prior] \n",
    "    change_map[~prior, i] = pwi_x[~prior] * pvj_y[~prior] * pxk_z[~prior] * pwi_vj[~prior] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_map = np.argmax(change_map, axis = -1)\n",
    "change_new = np.zeros_like(change_map)\n",
    "gain_areas = np.logical_or(change_map == 1, change_map == 3)\n",
    "change_new[gain_areas] = 1.\n",
    "#change_map[np.where(change_map != 3)] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = np.copy(change_new)\n",
    "for x_window in range(1, change_new.shape[0] - 2, 2):\n",
    "    for y_window in range(1, change_new.shape[1] - 2, 2):\n",
    "        if np.sum(change_new[x_window - 1:x_window + 1, y_window - 1:y_window + 1]) < 2:\n",
    "            gain[x_window - 1:x_window + 1, y_window - 1:y_window + 1] *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 11))\n",
    "sns.heatmap(gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = rasterio.transform.from_bounds(west = west, south = south,\n",
    "                                           east = east, north = north,\n",
    "                                           width = change_map.shape[1], height = change_map.shape[1])\n",
    "\n",
    "print(\"Writing\" + '../../tile_data/{}/{}.tif'.format(landscape, \"gain-cva\"))\n",
    "new_dataset = rasterio.open('../../tile_data/{}/{}.tif'.format(landscape, \"gain-cva-11\"), 'w', driver = 'GTiff',\n",
    "                           height = change_map.shape[1], width = change_map.shape[1], count = 1,\n",
    "                           dtype = 'float32',#str(stacked.dtype),\n",
    "                           crs = '+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs',\n",
    "                           transform=transform)\n",
    "new_dataset.write(gain.astype(np.float32), 1)\n",
    "new_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to figure out -> what is the right chi square distance degrees of freedom?\n",
    "# Things to figure out -> that's about it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
