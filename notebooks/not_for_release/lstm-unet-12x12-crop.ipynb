{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import itertools\n",
    "from tflearn.layers.conv import global_avg_pool\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.python.keras.layers import BatchNormalization\n",
    "from tensorflow.layers import batch_normalization\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGRUCell(tf.nn.rnn_cell.RNNCell):\n",
    "  \"\"\"A GRU cell with convolutions instead of multiplications.\"\"\"\n",
    "\n",
    "  def __init__(self, shape, filters, kernel, padding = 'SAME', activation=tf.tanh, normalize=False, data_format='channels_last', reuse=None):\n",
    "    super(ConvGRUCell, self).__init__(_reuse=reuse)\n",
    "    self._filters = filters\n",
    "    self._kernel = kernel\n",
    "    self._activation = activation\n",
    "    self._normalize = normalize\n",
    "    self._padding = padding\n",
    "    if data_format == 'channels_last':\n",
    "        self._size = tf.TensorShape(shape + [self._filters])\n",
    "        self._feature_axis = self._size.ndims\n",
    "        self._data_format = None\n",
    "    elif data_format == 'channels_first':\n",
    "        self._size = tf.TensorShape([self._filters] + shape)\n",
    "        self._feature_axis = 0\n",
    "        self._data_format = 'NC'\n",
    "    else:\n",
    "        raise ValueError('Unknown data_format')\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._size\n",
    "\n",
    "  def call(self, x, h):\n",
    "    channels = x.shape[self._feature_axis].value\n",
    "\n",
    "    with tf.variable_scope('gates'):\n",
    "      inputs = tf.concat([x, h], axis=self._feature_axis)\n",
    "      n = channels + self._filters\n",
    "      m = 2 * self._filters if self._filters > 1 else 2\n",
    "      W = tf.get_variable('kernel', self._kernel + [n, m])\n",
    "      y = tf.nn.convolution(inputs, W, self._padding, data_format=self._data_format)\n",
    "      if self._normalize:\n",
    "        r, u = tf.split(y, 2, axis=self._feature_axis)\n",
    "        r = tf.contrib.layers.layer_norm(r)\n",
    "        u = tf.contrib.layers.layer_norm(u)\n",
    "      else:\n",
    "        y += tf.get_variable('bias', [m], initializer=tf.ones_initializer())\n",
    "        r, u = tf.split(y, 2, axis=self._feature_axis)\n",
    "      r, u = tf.sigmoid(r), tf.sigmoid(u)\n",
    "\n",
    "    with tf.variable_scope('candidate'):\n",
    "      inputs = tf.concat([x, r * h], axis=self._feature_axis)\n",
    "      n = channels + self._filters\n",
    "      m = self._filters\n",
    "      W = tf.get_variable('kernel', self._kernel + [n, m])\n",
    "      y = tf.nn.convolution(inputs, W, self._padding, data_format=self._data_format)\n",
    "      if self._normalize:\n",
    "        y = tf.contrib.layers.layer_norm(y)\n",
    "      else:\n",
    "        y += tf.get_variable('bias', [m], initializer=tf.zeros_initializer())\n",
    "      h = u * h + (1 - u) * self._activation(y)\n",
    "\n",
    "    return h, h\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "def norm(x, norm_type, is_train, G=32, esp=1e-5, scope = 'gn'):\n",
    "    with tf.variable_scope('{}_norm{}'.format(norm_type, scope)):\n",
    "        if norm_type == 'none':\n",
    "            output = x\n",
    "        elif norm_type == 'batch':\n",
    "            output = tf.contrib.layers.batch_norm(\n",
    "                x, center=True, scale=True, decay=0.999,\n",
    "                is_training=is_train, updates_collections=None\n",
    "            )\n",
    "        elif norm_type == 'group':\n",
    "            # normalize\n",
    "            # tranpose: [bs, h, w, c] to [bs, c, h, w] following the paper\n",
    "            x = tf.transpose(x, [0, 3, 1, 2])\n",
    "            N, C, H, W = x.get_shape().as_list()\n",
    "            G = min(G, C)\n",
    "            x = tf.reshape(x, [-1, G, C // G, H, W])\n",
    "            mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)\n",
    "            x = (x - mean) / tf.sqrt(var + esp)\n",
    "            # per channel gamma and beta\n",
    "            gamma = tf.Variable(tf.constant(1.0, shape=[C]), dtype=tf.float32, name='gamma')\n",
    "            beta = tf.Variable(tf.constant(0.0, shape=[C]), dtype=tf.float32, name='beta')\n",
    "            gamma = tf.reshape(gamma, [1, C, 1, 1])\n",
    "            beta = tf.reshape(beta, [1, C, 1, 1])\n",
    "\n",
    "            output = tf.reshape(x, [-1, C, H, W]) * gamma + beta\n",
    "            # tranpose: [bs, c, h, w, c] to [bs, h, w, c] following the paper\n",
    "            output = tf.transpose(output, [0, 2, 3, 1])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    return output\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    return batch_normalization(inputs=x, momentum = 0.9, training=training, renorm = True, reuse=None, name = scope)\n",
    "        #return tf.cond(training,\n",
    "         #              lambda : batch_normalization(inputs=x, momentum = 0.9, training=True, reuse=None, name = scope),\n",
    "        #               lambda : batch_normalization(inputs=x, momentum=0.9, training=False, reuse=True, name = scope))\n",
    "\n",
    "        \n",
    "#def Batch_Normalization(x, training, groups, scope):\n",
    "#    print(x)\n",
    "#    return norm(x = x, norm_type = 'group', is_train = training, G = groups, scope = scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 14\n",
    "shift_size = 9\n",
    "\n",
    "down_16 = 8\n",
    "down_8 = 16\n",
    "down_4 = 32\n",
    "up_8 = 24\n",
    "up_16 = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = keras.regularizers.l2(0.01)\n",
    "inp = tf.placeholder(tf.float32, shape=(None, 24, image_size, image_size, 13))\n",
    "length = tf.placeholder(tf.int32, shape = (None, 1))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, image_size, image_size))#, 1))\n",
    "\n",
    "length2 = tf.reshape(length, (-1,))\n",
    "is_training = is_training = tf.placeholder_with_default(False, (), 'is_training')\n",
    "\n",
    "def Fully_connected(x, units, layer_name='fully_connected') :\n",
    "    with tf.name_scope(layer_name) :\n",
    "        return tf.layers.dense(inputs=x, use_bias=True, units=units)\n",
    "\n",
    "def Relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def Sigmoid(x):\n",
    "    return tf.nn.sigmoid(x)\n",
    "\n",
    "def Global_Average_Pooling(x):\n",
    "    return global_avg_pool(x, name='Global_avg_pooling')\n",
    "\n",
    "def Squeeze_excitation_layer(input_x, out_dim, ratio, layer_name):\n",
    "    with tf.name_scope(layer_name) :\n",
    "        squeeze = global_avg_pool(input_x)\n",
    "\n",
    "        excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+'_fully_connected1')\n",
    "        excitation = Relu(excitation)\n",
    "        excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+'_fully_connected2')\n",
    "        excitation = Sigmoid(excitation)\n",
    "\n",
    "        excitation = tf.reshape(excitation, [-1,1,1,out_dim])\n",
    "\n",
    "        scale = input_x * excitation\n",
    "\n",
    "        return scale\n",
    "\n",
    "with tf.variable_scope('10'):\n",
    "    # Downsampling Block 1 (14 x 14)\n",
    "    cell_10 = ConvGRUCell(shape = [image_size, image_size],\n",
    "                   filters = down_16,\n",
    "                   kernel = [3,3],\n",
    "                   padding = 'SAME')\n",
    "\n",
    "    def convGRU(x, cell, ln):\n",
    "        output, final = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell, cell, x, ln, dtype=tf.float32)\n",
    "        output = tf.concat(output, -1)\n",
    "        final = tf.concat(final, -1)\n",
    "        return [output, final]\n",
    "\n",
    "# Return the final state and the output states\n",
    "first_conv = convGRU(inp, cell_10, length2)\n",
    "print(\"FIRST GRU {}\".format(first_conv[0].shape))\n",
    "\n",
    "downsampled = TimeDistributed(MaxPool2D(pool_size = (2, 2)))(first_conv[0])\n",
    "print(\"DOWNSAMPLE {}\".format(downsampled.shape))\n",
    "\n",
    "# Downsampling block 2 (7 x 7)\n",
    "with tf.variable_scope('8'):\n",
    "    cell_7 = ConvGRUCell(shape = [image_size / 2, image_size / 2],\n",
    "                   filters = down_8,\n",
    "                   kernel = [3,3],\n",
    "                   padding = 'SAME')\n",
    "    state_7 = convGRU(downsampled, cell_7, length2)\n",
    "downsampled_4 = TimeDistributed(MaxPool2D(pool_size = (2, 2)))(state_7[0])\n",
    "print(\"SECOND GRU {}\".format(state_7[1].shape))\n",
    "\n",
    "with tf.variable_scope('4'):\n",
    "    cell_4 = ConvGRUCell(shape = [image_size / 4, image_size / 4],\n",
    "                   filters = down_4,\n",
    "                   kernel = [3,3],\n",
    "                   padding = 'SAME')\n",
    "    state_4 = convGRU(downsampled_4, cell_4, length2)\n",
    "print(\"THIRD GRU {}\".format(state_4[1].shape))\n",
    "\n",
    "# 4x4 - 4x4\n",
    "conv_block_7_u = Conv2D(filters = 32, kernel_size = (3, 3), padding = 'same', activity_regularizer=reg)(state_4[1])\n",
    "elu_7_u = ELU()(conv_block_7_u)\n",
    "x = Batch_Normalization(elu_7_u, training=is_training, scope = 'bn1')\n",
    "#x = batchnorm(elu_7_u, is_training, 'bn1')\n",
    "squeezed = Squeeze_excitation_layer(input_x = x, out_dim = 32, ratio = 4, layer_name = \"squeezed\")\n",
    "print(\"Down block conv {}\".format(elu_7_u.shape))\n",
    "\n",
    "# 4x4 - 8x8\n",
    "upsampling_8 = tf.keras.layers.Conv2DTranspose(filters = 24, kernel_size = (3, 3), strides=(2, 2), padding='same')(squeezed)\n",
    "concat_8 = Concatenate(axis = -1)([upsampling_8, state_7[1]])\n",
    "padded_8 = ReflectionPadding2D((1, 1))(concat_8)\n",
    "conv_8 = Conv2D(filters = 24,\n",
    "            kernel_size = (3, 3), \n",
    "            padding = 'valid',\n",
    "            activity_regularizer=reg,\n",
    "            )(padded_8)\n",
    "elu_8 = ELU()(conv_8)\n",
    "#bn_8 = batchnorm(elu_8, is_training, 'bn2')\n",
    "bn_8 = Batch_Normalization(elu_8, training=is_training, scope = 'bn8')\n",
    "squeeze_8 = Squeeze_excitation_layer(input_x = bn_8, out_dim = 24, ratio = 4, layer_name = \"squeezed_8\")\n",
    "print(\"Upblock 8 {}\".format(squeeze_8.shape))\n",
    "\n",
    "# 8x8 - 16 x 16\n",
    "upsampling_16 = tf.keras.layers.Conv2DTranspose(filters = 16, kernel_size = (3, 3), strides=(2, 2), padding='same')(squeeze_8)\n",
    "concat_16 = Concatenate(axis = -1)([upsampling_16, first_conv[1]])\n",
    "padded_16 = ReflectionPadding2D((1, 1))(concat_16)\n",
    "conv_16 = Conv2D(filters = 16,\n",
    "            kernel_size = (3, 3), \n",
    "            padding = 'valid',\n",
    "            activity_regularizer=reg,\n",
    "            )(padded_16)\n",
    "elu_16 = ELU()(conv_16)\n",
    "#bn_16 = batchnorm(elu_16, is_training, 'bn3')\n",
    "bn_16 = Batch_Normalization(elu_16, training=is_training, scope = 'bn16')\n",
    "squeezed_16 = Squeeze_excitation_layer(input_x = bn_16, out_dim = 16, ratio = 4, layer_name = \"squeezed_16\")\n",
    "print(\"Up block 16 {}\".format(squeezed_16.shape))\n",
    "\n",
    "#padded = ReflectionPadding2D((1, 1))(squeezed_16)\n",
    "#fm = Conv2D(filters = 12,\n",
    "#            kernel_size = (3, 3), \n",
    "#            padding = 'valid',\n",
    "#            activity_regularizer=reg,\n",
    "#            )(padded)\n",
    "#elu = ELU()(fm)\n",
    "#bn_final = Batch_Normalization(elu, training=is_training, scope = 'bnfinal')\n",
    "#squeezed_16 = Squeeze_excitation_layer(input_x = bn_final, out_dim = 12, ratio = 4, layer_name = \"squeezed_final\")\n",
    "#print(\"Up block conv 3 {}\".format(squeezed_16.shape))\n",
    "# Output layer\n",
    "fm = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1), \n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid'\n",
    "            )(squeezed_16)\n",
    "print(fm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"../data/subplot.csv\")\n",
    "df1 = pd.read_csv(\"../data/subplot2.csv\")\n",
    "\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1)\n",
    "df1 = df1.drop('IMAGERY_TITLE', axis = 1)\n",
    "df = pd.concat([df, df1], ignore_index = True)\n",
    "df = df.dropna(axis = 0)\n",
    "\n",
    "\n",
    "N_SAMPLES = int(df.shape[0]/196)\n",
    "print(N_SAMPLES)\n",
    "\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "\n",
    "def reconstruct_images(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    rows = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        rows.append(list(subs_lat['TREE']))\n",
    "    return rows\n",
    "\n",
    "data = [reconstruct_images(x) for x in plot_ids]\n",
    "\n",
    "def remove_blank_steps(array):\n",
    "    to_update = {}\n",
    "    sets = []\n",
    "    for k in range(6):\n",
    "        for i in range(array.shape[0]):\n",
    "            for k in range(array.shape[-1]):\n",
    "                mean = (np.mean(array[i, :, :, k]))\n",
    "                if mean == 0:\n",
    "                    sets.append(i)\n",
    "                    if i < array.shape[0] - 1:\n",
    "                        array[i, :, :, k] = array[i + 1, :, :, k]\n",
    "                    else:\n",
    "                        array[i, :, :, k] = array[i - 1, :, :, k]\n",
    "                if mean == 1:\n",
    "                    sets.append(i)\n",
    "                    if i < array.shape[0] - 1:\n",
    "                        array[i, :, :, k] = array[i + 1, :, :, k]\n",
    "                    else:\n",
    "                        array[i, :, :, k] = array[i - 1, :, :, k]\n",
    "    for i in range(array.shape[0]):\n",
    "        for k in range(array.shape[-1]):\n",
    "            mean = (np.mean(array[i, :, :, k]))\n",
    "            if mean == 0:\n",
    "                if i < array.shape[0] - 2:\n",
    "                    array[i, :, :, k] = array[i + 2, :, :, k]\n",
    "                else:\n",
    "                    array[i, :, :, k] = array[i - 2, :, :, k]\n",
    "            if mean == 1:\n",
    "                if i < array.shape[0] - 2:\n",
    "                    array[i, :, :, k] = array[i + 2, :, :, k]\n",
    "                else:\n",
    "                    array[i, :, :, k] = array[i - 2, :, :, k]\n",
    "    return array\n",
    "\n",
    "import os\n",
    "\n",
    "def ndvi(x):\n",
    "    # (B8 - B4)/(B8 + B4)\n",
    "    ndvis = [(im[:, :, 6] - im[:, :, 2]) / (im[:, :, 6] + im[:, :, 2]) for im in x]\n",
    "    min_ndvi = min([np.min(x) for x in ndvis])\n",
    "    max_ndvi = max([np.max(x) for x in ndvis])\n",
    "    if min_ndvi < -1 or max_ndvi > 1:\n",
    "        print(\"ERROR\")\n",
    "    ndvis = [((x + 1) / 2) for x in ndvis]\n",
    "    min_ndvi = min([np.min(x) for x in ndvis])\n",
    "    max_ndvi = max([np.max(x) for x in ndvis])\n",
    "    x_padding = np.zeros((x.shape[0], image_size, image_size, 1))\n",
    "    x = np.concatenate((x, x_padding), axis = 3)\n",
    "    # Iterate over each time step and add NDVI in as the 11th channel\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i, :, :, 10] = ndvis[i]\n",
    "    return x\n",
    "\n",
    "def evi(x):\n",
    "    # 2.5 x (08 - 04) / (08 + 6 * 04 - 7.5 * 02 + 1)\n",
    "    evis = [2.5 * ((im[:, :, 6] - im[:, :, 2]) / (im[:, :, 6] + 6 * im[:,:, 2] - 7.5 * im[:, :, 0] + 1)) for im in x]\n",
    "    x_padding = np.zeros((x.shape[0], image_size, image_size, 1))\n",
    "    x = np.concatenate((x, x_padding), axis = 3)\n",
    "    # Iterate over each time step and add NDVI in as the 11th channel\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i, :, :, 11] = evis[i]\n",
    "    return x\n",
    "    \n",
    "def savi(x):\n",
    "    # (1.5)(08 - 04)/ (08 + 04 + 0.5)\n",
    "    savis = [(1.5 * im[:, :, 6] - im[:, :, 2]) / (im[:, :, 6] + im[:, :, 2] + 0.5) for im in x]\n",
    "    x_padding = np.zeros((x.shape[0], image_size, image_size, 1))\n",
    "    x = np.concatenate((x, x_padding), axis = 3)\n",
    "    # Iterate over each time step and add NDVI in as the 11th channel\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i, :, :, 12] = savis[i]\n",
    "    return x\n",
    "\n",
    "data_x = []\n",
    "data_y = []\n",
    "binary_y = []\n",
    "data_location_x = []\n",
    "data_location_y = []\n",
    "lengths = []\n",
    "\n",
    "def append_shifts(x, y):\n",
    "    data_x.append(x[:, 0:12, 0:12, :])\n",
    "    data_x.append(x[:, 0:12, 1:13, :])\n",
    "    data_x.append(x[:, 0:12, 2:14, :])\n",
    "    data_x.append(x[:, 1:13, 0:12, :])\n",
    "    data_x.append(x[:, 1:13, 1:13, :])\n",
    "    data_x.append(x[:, 1:13, 2:14, :])\n",
    "    data_x.append(x[:, 2:14, 0:12, :])\n",
    "    data_x.append(x[:, 2:14, 1:13, :])\n",
    "    data_x.append(x[:, 2:14, 2:14, :])\n",
    "    data_y.append(y[0:12, 0:12])\n",
    "    data_y.append(y[0:12, 1:13])\n",
    "    data_y.append(y[0:12, 2:14])\n",
    "    data_y.append(y[1:13, 0:12])\n",
    "    data_y.append(y[1:13, 1:13])\n",
    "    data_y.append(y[1:13, 2:14])\n",
    "    data_y.append(y[2:14, 0:12])\n",
    "    data_y.append(y[2:14, 1:13])\n",
    "    data_y.append(y[2:14, 2:14])\n",
    "    for i in range(9):\n",
    "        lengths.append(x.shape[0])\n",
    "\n",
    "# Initiate empty lists to store the X and Y data in\n",
    "\n",
    "\n",
    "# Iterate over each plot\n",
    "pad = True\n",
    "flip = True\n",
    "for i in plot_ids:\n",
    "    # Load the sentinel imagery\n",
    "    x = np.load(\"../data/ids/\" + str(i) + \".npy\")\n",
    "    # Shape check\n",
    "    if x.shape[1] == 14:\n",
    "        x = ndvi(x)                # calc NDVI\n",
    "        x = evi(x)\n",
    "        x = savi(x)\n",
    "        x = remove_blank_steps(x)\n",
    "        y = reconstruct_images(i)\n",
    "        #x = x[:, 1:13, 1:13, :]\n",
    "        y = np.array(y)\n",
    "        #y = y[1:13, 1:13]\n",
    "        if sum([sum(x) for x in y]) >= 1:\n",
    "            binary_y.append(1)\n",
    "        else:\n",
    "            binary_y.append(0)\n",
    "        #x = np.median(x, axis = 0) # and calculate the median over the time steps\n",
    "        if pad:\n",
    "            if x.shape[0] < 24:\n",
    "                print(x.shape[0])\n",
    "                padding = np.zeros((24 - x.shape[0], 14, 14, 13))\n",
    "                x = np.concatenate((x, padding), axis = 0)\n",
    "        append_shifts(x, y)\n",
    "        print(len(data_x))\n",
    "        if flip:\n",
    "                # FLIP HORIZONTAL\n",
    "            x1 = np.flip(x, 1)\n",
    "            append_shifts(x1, np.flip(y, 0))\n",
    "    \n",
    "                # FLIP BOTH\n",
    "            x2 = np.flip(x, 2)\n",
    "            x2 = np.flip(x2, 1)\n",
    "            #data_x.append(x2)\n",
    "            #data_y.append(np.flip(y, [0, 1]))\n",
    "            append_shifts(x2, np.flip(y, [0, 1]))\n",
    "            #lengths.append(x.shape[0])\n",
    "                # FLIP VERTICAL\n",
    "            x3 = np.flip(x, 2)\n",
    "            append_shifts(x3, np.flip(y, 1))\n",
    "            #data_x.append(x3)\n",
    "            #data_y.append(np.flip(y, 1))\n",
    "            #lengths.append(x.shape[0])\n",
    "\n",
    "data_x = np.stack(data_x)\n",
    "data_y = np.stack(data_y)\n",
    "data_y = np.reshape(data_y, (N_SAMPLES*4*9, 12, 12, 1))\n",
    "binary_y = np.stack(binary_y)\n",
    "lengths = np.stack(lengths)\n",
    "lengths = np.reshape(lengths, (lengths.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_foc(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(0.5 * K.pow(1. - pt_1, 2) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - 0.5) * K.pow(pt_0, 2) * K.log(1. - pt_0))  \n",
    "    \n",
    "def focal_loss(prediction_tensor, target_tensor, weights=None, alpha=0.25, gamma=2):\n",
    "    r\"\"\"Compute focal loss for predictions.\n",
    "        Multi-labels Focal loss formula:\n",
    "            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "    Args:\n",
    "     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "     target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "     weights: A float tensor of shape [batch_size, num_anchors]\n",
    "     alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "     gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "    Returns:\n",
    "        loss: A (scalar) tensor representing the value of the loss function\n",
    "    \"\"\"\n",
    "    sigmoid_p = prediction_tensor\n",
    "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)\n",
    "\n",
    "def foc_lovasz(y_true, y_pred):\n",
    "    #jaccard_loss = jaccard_distance(y_true, y_pred)\n",
    "    lovasz = lovasz_hinge(y_pred, y_true)\n",
    "    #pred_reshape = tf.reshape(y_pred, (-1, 14, 14))\n",
    "    #true_reshape = tf.reshape(y_true, (-1, 14, 14))\n",
    "    focal_loss = bin_foc(y_true, y_pred)\n",
    "    summed = lovasz + np.log(focal_loss)\n",
    "    return summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thirty_meter(true, pred):\n",
    "    indices_x = [x for x in range(0, 11, 1)]\n",
    "    indices_y = [x for x in range(0, 11, 1)]\n",
    "    perms = [(y, x) for x, y in itertools.product(indices_y, indices_x)]\n",
    "    \n",
    "    #perms = ([list(zip(indices_x, p)) for p in itertools.permutations(indices_y)])\n",
    "    #perms = [item for sublist in perms for item in sublist]\n",
    "    #perms = list(set(perms))\n",
    "    indexes = [([a, a + 1], [b, b + 1]) for a,b in perms]\n",
    "    subs_true = []\n",
    "    subs_pred = []\n",
    "    for i in indexes:\n",
    "        true_i = true[i[0][0]:i[0][1], i[1][0]:i[1][1]]\n",
    "        pred_i = pred[i[0][0]:i[0][1], i[1][0]:i[1][1]]\n",
    "        subs_true.append(true_i)\n",
    "        subs_pred.append(pred_i)\n",
    "    pred = [np.sum(x) for x in subs_pred]\n",
    "    true = [np.sum(x) for x in subs_true]\n",
    "    true_positives = []\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "    for p, t in zip(pred, true):\n",
    "        if p > t:\n",
    "            tp = p - (p - t)\n",
    "            fp = p - tp\n",
    "            fn = 0\n",
    "        if t >= p:\n",
    "            tp = t\n",
    "            fp = 0\n",
    "            fn = t - p\n",
    "        true_positives.append(tp)\n",
    "        false_positives.append(fp)\n",
    "        false_negatives.append(fn)\n",
    "    prec = [x / (x + y) for x,y in zip(true_positives, false_positives) if (x+y) > 0]\n",
    "    prec = [x for x in prec if not np.isnan(x)]\n",
    "    rec = [x / (x + y) for x,y in zip(true_positives, false_negatives) if (x+y) > 0]\n",
    "    rec = [x for x in rec if not np.isnan(x)]\n",
    "    \n",
    "    #recall = [min(x / y, 1) for x, y in zip(pred, true) if y > 0]\n",
    "    #precision = [(y - x) / x for x, y in zip(pred, true)]\n",
    "    #print(precision)\n",
    "    return rec, prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    gts = tf.reduce_sum(gt_sorted)\n",
    "    intersection = gts - tf.cumsum(gt_sorted)\n",
    "    union = gts + tf.cumsum(1. - gt_sorted)\n",
    "    jaccard = 1. - intersection / union\n",
    "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "# --------------------------- BINARY LOSSES ---------------------------\n",
    "\n",
    "\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        def treat_image(log_lab):\n",
    "            log, lab = log_lab\n",
    "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
    "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
    "            return lovasz_hinge_flat(log, lab)\n",
    "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
    "        loss = tf.reduce_mean(losses)\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss():\n",
    "        labelsf = tf.cast(labels, logits.dtype)\n",
    "        signs = 2. * labelsf - 1.\n",
    "        errors = 1. - logits * tf.stop_gradient(signs)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
    "        gt_sorted = tf.gather(labelsf, perm)\n",
    "        grad = lovasz_grad(gt_sorted)\n",
    "        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
    "        return loss\n",
    "\n",
    "    # deal with the void prediction case (only void pixels)\n",
    "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
    "                   lambda: tf.reduce_sum(logits) * 0.,\n",
    "                   compute_loss,\n",
    "                   strict=True,\n",
    "                   name=\"loss\"\n",
    "                   )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = tf.reshape(scores, (-1,))\n",
    "    labels = tf.reshape(labels, (-1,))\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = tf.not_equal(labels, ignore)\n",
    "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
    "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
    "    return vscores, vlabels\n",
    "\n",
    "\n",
    "# --------------------------- MULTICLASS LOSSES ---------------------------\n",
    "\n",
    "\n",
    "def lovasz_softmax(probas, labels, classes='present', per_image=False, ignore=None, order='BHWC'):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [B, H, W, C] or [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n",
    "      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class labels\n",
    "      order: use BHWC or BCHW\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        def treat_image(prob_lab):\n",
    "            prob, lab = prob_lab\n",
    "            prob, lab = tf.expand_dims(prob, 0), tf.expand_dims(lab, 0)\n",
    "            prob, lab = flatten_probas(prob, lab, ignore, order)\n",
    "            return lovasz_softmax_flat(prob, lab, classes=classes)\n",
    "        losses = tf.map_fn(treat_image, (probas, labels), dtype=tf.float32)\n",
    "        loss = tf.reduce_mean(losses)\n",
    "    else:\n",
    "        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore, order), classes=classes)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_softmax_flat(probas, labels, classes='present'):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
    "    \"\"\"\n",
    "    C = probas.shape[1]\n",
    "    losses = []\n",
    "    present = []\n",
    "    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes\n",
    "    for c in class_to_sum:\n",
    "        fg = tf.cast(tf.equal(labels, c), probas.dtype)  # foreground for class c\n",
    "        if classes == 'present':\n",
    "            present.append(tf.reduce_sum(fg) > 0)\n",
    "        if C == 1:\n",
    "            if len(classes) > 1:\n",
    "                raise ValueError('Sigmoid output possible only with 1 class')\n",
    "            class_pred = probas[:, 0]\n",
    "        else:\n",
    "            class_pred = probas[:, c]\n",
    "        errors = tf.abs(fg - class_pred)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort_{}\".format(c))\n",
    "        fg_sorted = tf.gather(fg, perm)\n",
    "        grad = lovasz_grad(fg_sorted)\n",
    "        losses.append(\n",
    "            tf.tensordot(errors_sorted, tf.stop_gradient(grad), 1, name=\"loss_class_{}\".format(c))\n",
    "                      )\n",
    "    if len(class_to_sum) == 1:  # short-circuit mean when only one class\n",
    "        return losses[0]\n",
    "    losses_tensor = tf.stack(losses)\n",
    "    if classes == 'present':\n",
    "        present = tf.stack(present)\n",
    "        losses_tensor = tf.boolean_mask(losses_tensor, present)\n",
    "    loss = tf.reduce_mean(losses_tensor)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_probas(probas, labels, ignore=None, order='BHWC'):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch\n",
    "    \"\"\"\n",
    "    if len(probas.shape) == 3:\n",
    "        probas, order = tf.expand_dims(probas, 3), 'BHWC'\n",
    "    if order == 'BCHW':\n",
    "        probas = tf.transpose(probas, (0, 2, 3, 1), name=\"BCHW_to_BHWC\")\n",
    "        order = 'BHWC'\n",
    "    if order != 'BHWC':\n",
    "        raise NotImplementedError('Order {} unknown'.format(order))\n",
    "    C = probas.shape[3]\n",
    "    probas = tf.reshape(probas, (-1, C))\n",
    "    labels = tf.reshape(labels, (-1,))\n",
    "    if ignore is None:\n",
    "        return probas, labels\n",
    "    valid = tf.not_equal(labels, ignore)\n",
    "    vprobas = tf.boolean_mask(probas, valid, name='valid_probas')\n",
    "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
    "    return vprobas, vlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "def weighted_cross_entropy(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "    y_pred = tf.log(y_pred / (1 - y_pred))\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred, targets=y_true, pos_weight=2.)\n",
    "\n",
    "    # or reduce_sum and/or axis=-1\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def smooth_jaccard(y_true, y_pred, smooth=100):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def bce_jaccard(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, (-1, 12, 12, 1))\n",
    "    #y_true = tf.slice(y_true, [0, 1, 1, 0], [-1, 14, 14, -1])\n",
    "    #y_pred = tf.slice(y_pred, [0, 1, 1, 0], [-1, 14, 14, -1])\n",
    "    print(y_true.shape)\n",
    "    return binary_crossentropy(y_true, y_pred) + (0.5 * smooth_jaccard(y_true, y_pred))\n",
    "\n",
    "def bce_lovasz(y_true, y_pred):\n",
    "    return binary_crossentropy(tf.reshape(y_true, (-1, 14, 14, 1)), y_pred) + lovasz_softmax(y_pred, y_true, classes=[1], per_image=False)\n",
    "\n",
    "def foc_jaccard(y_true, y_pred):\n",
    "    y_true_r = tf.reshape(y_true, (-1, 16, 16, 1))\n",
    "    y_pred_r = tf.reshape(y_pred, (-1, 16, 16))\n",
    "    y_true_f = tf.reshape(y_true, (-1, 16*16, 1))\n",
    "    y_pred_f = tf.reshape(y_pred, (-1, 16*16, 1))\n",
    "    jac = smooth_jaccard(y_true_r, y_pred)\n",
    "    foc = focal_loss(y_pred_f, y_true_f, alpha=0.5, gamma=1, weights = weights)\n",
    "    return foc\n",
    "\n",
    "#optimizer = AdaBoundOptimizer(learning_rate=2e-6, final_lr=1e-4, beta1=0.9, beta2=0.999, amsbound=True)\n",
    "#loss2 = bce_jaccard(labels, fm)\n",
    "#\n",
    "#update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "#with tf.control_dependencies(update_ops):\n",
    "#    train_op = optimizer.minimize(loss2)\n",
    "\n",
    "#init_op = tf.global_variables_initializer()\n",
    "#sess.run(init_op)\n",
    "#saver = tf.train.Saver(max_to_keep = 2)\n",
    "\n",
    "# dev --> 225\n",
    "\n",
    "# Run training loop\n",
    "for i in range(100):\n",
    "    randomize = np.arange(1088*9)\n",
    "    np.random.shuffle(randomize)\n",
    "    test_randomize = np.arange(1088*9, 1344*9)\n",
    "    np.random.shuffle(test_randomize)\n",
    "\n",
    "    losses = []\n",
    "    val_loss = []\n",
    "\n",
    "    for k in range(68*4*9):\n",
    "        batch_ids = randomize[k*4:(k+1)*4]\n",
    "        op, tr = sess.run([train_op, loss2], feed_dict={inp: data_x[batch_ids, :, :, :],\n",
    "                                                        length: lengths[batch_ids],\n",
    "                                                        labels: data_y[batch_ids, :, :].reshape(4, 12, 12),\n",
    "                                                        is_training: True\n",
    "                                                        })\n",
    "        losses.append(tr)\n",
    "    for j in range(32*9):\n",
    "        batch_ids = test_randomize[j*8:(j+1)*8]\n",
    "        vl, y = sess.run([loss2, fm], feed_dict={inp: data_x[batch_ids, :, :, :],\n",
    "                                                 length: lengths[batch_ids],\n",
    "                                                 labels: data_y[batch_ids][:, :].reshape(8, 12, 12),\n",
    "                                                 is_training: False,\n",
    "                                                })\n",
    "        val_loss.append(vl)\n",
    "        \n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    for m in range(1088*9, 1344*9):\n",
    "        y = sess.run([fm], feed_dict={inp: data_x[m].reshape(1, 24, 12, 12, 13),\n",
    "                                  length: lengths[m].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })[0]\n",
    "        true = data_y[m].reshape((12, 12))\n",
    "        pred = y.reshape((12, 12))\n",
    "        #pred = np.sigmoid(pred)\n",
    "        pred[np.where(pred > 0.45)] = 1\n",
    "        pred[np.where(pred < 0.45)] = 0\n",
    "        rec, prec = thirty_meter(true, pred)\n",
    "        recalls.append(rec)\n",
    "        precisions.append(prec)\n",
    "    precisions = [item for sublist in precisions for item in sublist]\n",
    "    recalls = [item for sublist in recalls for item in sublist]\n",
    "    precision = np.mean(precisions)\n",
    "    recall = np.mean(recalls)\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "    save_path = saver.save(sess, \"../models/dev/model\")\n",
    "    if np.mean(val_loss) < 0.27:\n",
    "        save_path = saver.save(sess, \"../models/dev_best/model\")\n",
    "    print(\"Epoch {}: Loss {} Val: {} P {} R {} F1 {}\".format(i + 1,\n",
    "                                                             np.mean(losses), np.mean(val_loss),\n",
    "                                                             precision, recall, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int((len(data_x))*0.8) + (21*9)+6# 105, 25, 215, 90, 140\n",
    "y = sess.run([fm], feed_dict={inp: data_x[idx].reshape(1, 24, 12, 12, 13),\n",
    "                              length: lengths[idx].reshape(1, 1),\n",
    "                              is_training: False,\n",
    "                              })\n",
    "\n",
    "sns.heatmap(data_y[idx][:, :, :].reshape(12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = y[0][:, :, :].reshape(12, 12)\n",
    "#pred[np.where(pred > 0.3)] = 1\n",
    "sns.heatmap(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = []\n",
    "precisions = []\n",
    "for i in range(1088*9, 1344*9):\n",
    "    y = sess.run([fm], feed_dict={inp: data_x[i].reshape(1, 24, 12, 12, 13),\n",
    "                              length: lengths[i].reshape(1, 1),\n",
    "                              is_training: False,\n",
    "                              })[0]\n",
    "    true = data_y[i].reshape((12, 12))\n",
    "    pred = y.reshape((12, 12))\n",
    "    pred[np.where(pred > 0.4)] = 1\n",
    "    pred[np.where(pred < 0.4)] = 0\n",
    "    rec, prec = thirty_meter(true, pred)\n",
    "    #rec, prec = half_hectare_accuracy(true, pred)\n",
    "    recalls.append(rec)\n",
    "    precisions.append(prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = [item for sublist in recalls for item in sublist]\n",
    "np.mean(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = [item for sublist in precisions for item in sublist]\n",
    "np.mean(precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"AdaBound for TensorFlow.\"\"\"\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import resource_variable_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.training import optimizer\n",
    "from tensorflow.python.ops.clip_ops import clip_by_value\n",
    "\n",
    "\"\"\"Implements AdaBound algorithm.\n",
    "    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound of Learning Rate`_.\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): Adam learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\n",
    "        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsbound (boolean, optional): whether to use the AMSBound variant of this algorithm\n",
    "    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\n",
    "        https://openreview.net/forum?id=Bkg3g2R9FX\n",
    "    \"\"\"\n",
    "\n",
    "class AdaBoundOptimizer(optimizer.Optimizer):\n",
    "    def __init__(self, learning_rate=0.001, final_lr=0.1, beta1=0.9, beta2=0.999,\n",
    "                 gamma=1e-3, epsilon=1e-8, amsbound=False,\n",
    "                 use_locking=False, name=\"AdaBound\"):\n",
    "        super(AdaBoundOptimizer, self).__init__(use_locking, name)\n",
    "        self._lr = learning_rate\n",
    "        self._final_lr = final_lr\n",
    "        self._beta1 = beta1\n",
    "        self._beta2 = beta2\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "        self._gamma = gamma\n",
    "        self._amsbound = amsbound\n",
    "\n",
    "        self._lr_t = None\n",
    "        self._beta1_t = None\n",
    "        self._beta2_t = None\n",
    "        self._epsilon_t = None\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        first_var = min(var_list, key=lambda x: x.name)\n",
    "\n",
    "        graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "        create_new = self._get_non_slot_variable(\"beta1_power\", graph) is None\n",
    "        if not create_new and context.in_graph_mode():\n",
    "            create_new = (self._get_non_slot_variable(\"beta1_power\", graph).graph is not first_var.graph)\n",
    "\n",
    "        if create_new:\n",
    "            self._create_non_slot_variable(initial_value=self._beta1,\n",
    "                                           name=\"beta1_power\",\n",
    "                                           colocate_with=first_var)\n",
    "            self._create_non_slot_variable(initial_value=self._beta2,\n",
    "                                           name=\"beta2_power\",\n",
    "                                           colocate_with=first_var)\n",
    "            self._create_non_slot_variable(initial_value=self._gamma,\n",
    "                                           name=\"gamma_multi\",\n",
    "                                           colocate_with=first_var)\n",
    "        # Create slots for the first and second moments.\n",
    "        for v in var_list :\n",
    "            self._zeros_slot(v, \"m\", self._name)\n",
    "            self._zeros_slot(v, \"v\", self._name)\n",
    "            self._zeros_slot(v, \"vhat\", self._name)\n",
    "\n",
    "\n",
    "    def _prepare(self):\n",
    "        self._lr_t = ops.convert_to_tensor(self._lr)\n",
    "        self._base_lr_t = ops.convert_to_tensor(self._lr)\n",
    "        self._beta1_t = ops.convert_to_tensor(self._beta1)\n",
    "        self._beta2_t = ops.convert_to_tensor(self._beta2)\n",
    "        self._epsilon_t = ops.convert_to_tensor(self._epsilon)\n",
    "        self._gamma_t = ops.convert_to_tensor(self._gamma)\n",
    "\n",
    "    def _apply_dense(self, grad, var):\n",
    "        graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "        beta1_power = math_ops.cast(self._get_non_slot_variable(\"beta1_power\", graph=graph), var.dtype.base_dtype)\n",
    "        beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), var.dtype.base_dtype)\n",
    "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n",
    "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
    "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
    "        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
    "        gamma_multi = math_ops.cast(self._get_non_slot_variable(\"gamma_multi\", graph=graph), var.dtype.base_dtype)\n",
    "\n",
    "        step_size = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n",
    "        final_lr = self._final_lr * lr_t / base_lr_t\n",
    "        lower_bound = final_lr * (1. - 1. / (gamma_multi + 1.))\n",
    "        upper_bound = final_lr * (1. + 1. / (gamma_multi))\n",
    "\n",
    "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_scaled_g_values = grad * (1 - beta1_t)\n",
    "        m_t = state_ops.assign(m, beta1_t * m + m_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
    "        v_t = state_ops.assign(v, beta2_t * v + v_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # amsgrad\n",
    "        vhat = self.get_slot(var, \"vhat\")\n",
    "        if self._amsbound :\n",
    "            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
    "            v_sqrt = math_ops.sqrt(vhat_t)\n",
    "        else :\n",
    "            vhat_t = state_ops.assign(vhat, vhat)\n",
    "            v_sqrt = math_ops.sqrt(v_t)\n",
    "\n",
    "\n",
    "        # Compute the bounds\n",
    "        step_size_bound = step_size / (v_sqrt + epsilon_t)\n",
    "        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n",
    "\n",
    "        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
    "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "        beta1_power = math_ops.cast(self._get_non_slot_variable(\"beta1_power\", graph=graph), grad.dtype.base_dtype)\n",
    "        beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), grad.dtype.base_dtype)\n",
    "        lr_t = math_ops.cast(self._lr_t, grad.dtype.base_dtype)\n",
    "        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n",
    "        beta1_t = math_ops.cast(self._beta1_t, grad.dtype.base_dtype)\n",
    "        beta2_t = math_ops.cast(self._beta2_t, grad.dtype.base_dtype)\n",
    "        epsilon_t = math_ops.cast(self._epsilon_t, grad.dtype.base_dtype)\n",
    "        gamma_multi = math_ops.cast(self._get_non_slot_variable(\"gamma_multi\", graph=graph), var.dtype.base_dtype)\n",
    "\n",
    "        step_size = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n",
    "        final_lr = self._final_lr * lr_t / base_lr_t\n",
    "        lower_bound = final_lr * (1. - 1. / (gamma_multi + 1.))\n",
    "        upper_bound = final_lr * (1. + 1. / (gamma_multi))\n",
    "\n",
    "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_scaled_g_values = grad * (1 - beta1_t)\n",
    "        m_t = state_ops.assign(m, beta1_t * m + m_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
    "        v_t = state_ops.assign(v, beta2_t * v + v_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # amsgrad\n",
    "        vhat = self.get_slot(var, \"vhat\")\n",
    "        if self._amsbound:\n",
    "            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
    "            v_sqrt = math_ops.sqrt(vhat_t)\n",
    "        else:\n",
    "            vhat_t = state_ops.assign(vhat, vhat)\n",
    "            v_sqrt = math_ops.sqrt(v_t)\n",
    "\n",
    "        # Compute the bounds\n",
    "        step_size_bound = step_size / (v_sqrt + epsilon_t)\n",
    "        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n",
    "\n",
    "        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
    "\n",
    "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "\n",
    "    def _apply_sparse_shared(self, grad, var, indices, scatter_add):\n",
    "        graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "        beta1_power = math_ops.cast(self._get_non_slot_variable(\"beta1_power\", graph=graph), var.dtype.base_dtype)\n",
    "        beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), var.dtype.base_dtype)\n",
    "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n",
    "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
    "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
    "        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
    "        gamma_t = math_ops.cast(self._gamma_t, var.dtype.base_dtype)\n",
    "\n",
    "        step_size = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n",
    "        final_lr = self._final_lr * lr_t / base_lr_t\n",
    "        lower_bound = final_lr * (1. - 1. / (gamma_t + 1.))\n",
    "        upper_bound = final_lr * (1. + 1. / (gamma_t))\n",
    "\n",
    "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_scaled_g_values = grad * (1 - beta1_t)\n",
    "        m_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([m_t]):\n",
    "            m_t = scatter_add(m, indices, m_scaled_g_values)\n",
    "\n",
    "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
    "        v_t = state_ops.assign(v, v * beta2_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([v_t]):\n",
    "            v_t = scatter_add(v, indices, v_scaled_g_values)\n",
    "\n",
    "        # amsgrad\n",
    "        vhat = self.get_slot(var, \"vhat\")\n",
    "        if self._amsbound:\n",
    "            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
    "            v_sqrt = math_ops.sqrt(vhat_t)\n",
    "        else:\n",
    "            vhat_t = state_ops.assign(vhat, vhat)\n",
    "            v_sqrt = math_ops.sqrt(v_t)\n",
    "\n",
    "        # Compute the bounds\n",
    "        step_size_bound = step_size / (v_sqrt + epsilon_t)\n",
    "        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n",
    "\n",
    "        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
    "\n",
    "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "\n",
    "    def _apply_sparse(self, grad, var):\n",
    "        return self._apply_sparse_shared(\n",
    "            grad.values, var, grad.indices,\n",
    "            lambda x, i, v: state_ops.scatter_add(  # pylint: disable=g-long-lambda\n",
    "                x, i, v, use_locking=self._use_locking))\n",
    "\n",
    "    def _resource_scatter_add(self, x, i, v):\n",
    "        with ops.control_dependencies(\n",
    "                [resource_variable_ops.resource_scatter_add(x, i, v)]):\n",
    "            return x.value()\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var, indices):\n",
    "        return self._apply_sparse_shared(\n",
    "            grad, var, indices, self._resource_scatter_add)\n",
    "\n",
    "    def _finish(self, update_ops, name_scope):\n",
    "        # Update the power accumulators.\n",
    "        with ops.control_dependencies(update_ops):\n",
    "            graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "            beta1_power = self._get_non_slot_variable(\"beta1_power\", graph=graph)\n",
    "            beta2_power = self._get_non_slot_variable(\"beta2_power\", graph=graph)\n",
    "            gamma_multi = self._get_non_slot_variable(\"gamma_multi\", graph=graph)\n",
    "            with ops.colocate_with(beta1_power):\n",
    "                update_beta1 = beta1_power.assign(\n",
    "                    beta1_power * self._beta1_t,\n",
    "                    use_locking=self._use_locking)\n",
    "                update_beta2 = beta2_power.assign(\n",
    "                    beta2_power * self._beta2_t,\n",
    "                    use_locking=self._use_locking)\n",
    "                update_gamma = gamma_multi.assign(\n",
    "                    gamma_multi + self._gamma_t,\n",
    "                    use_locking=self._use_locking)\n",
    "        return control_flow_ops.group(*update_ops + [update_beta1, update_beta2, update_gamma],\n",
    "                                      name=name_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
