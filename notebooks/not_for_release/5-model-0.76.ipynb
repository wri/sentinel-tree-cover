{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master model development\n",
    "\n",
    "## John Brandt\n",
    "\n",
    "### Last updated: November 16 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Package loading\n",
    "*  Hyperparameter definitions\n",
    "*  Additional layer definitions\n",
    "*  Model definition\n",
    "*  Data loading\n",
    "*  Data preprocessing\n",
    "*  K means clustering\n",
    "*  Augment training data\n",
    "*  Loss definition\n",
    "*  Equibatch creation\n",
    "*  Model training\n",
    "*  Model validation and sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment log\n",
    "*  Master version - 0.620 F1 after 70 epochs\n",
    "    * Stacked 1x1 intro conv + conv - selu, no BN - 0.616 after 20 epochs\n",
    "    * Remove 2 Conv Selus, GRU - FPA - Conv - Sigmoid, 60k params, 0.6\n",
    "    * New indices are calculated correctly\n",
    "    * Simpler FPA is more efficient with more stable learning\n",
    "    * Lovasz works best per-image (0.61 vs. 0.56)\n",
    "    * Boundary loss overfits with non-smoothed data\n",
    "    * 14x14 input is a bad idea, because the border pixel makes sense for the final 3x3 conv\n",
    "    * Temporal attention and CSSE work together\n",
    "    * Min max scaling does not work\n",
    "    * [-1, 1] scaling is better than [0, 1] scaling\n",
    "    * DEM messes everything up because of banding every 10-15 meters, removing the slope improves test accuracy, but median filter with size of 5 may fix this issue\n",
    "    * Smoothing the time dimension seems to help\n",
    "    \n",
    "* Reduced parameter size to 10, 24, 24 -- 0.61\n",
    "\n",
    "* Smooth time dimension for all variable bands --\n",
    "\n",
    "* Stack FPA module rather than FPA - Conv SELU\n",
    "\n",
    "* FPA GRU - 1x1 GRU, 3x3 GRU, 5x5 Dilated GRU, combine hidden states within GRU\n",
    "\n",
    "* Test adding versus multiplying for the concatenation of scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Remove imports that aren't needed to save RAM\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras.layers import Conv2D, Lambda, Dense, Multiply, Add, Bidirectional, ConvLSTM2D\n",
    "from tensorflow.python.keras.activations import selu\n",
    "from tensorflow.initializers import glorot_normal, lecun_normal\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim import conv2d\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from tflearn.layers.conv import global_avg_pool\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.layers import batch_normalization\n",
    "from tensorflow.python.util import deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/zoneout.py\n",
    "%run ../src/convgru.py\n",
    "%run ../src/lovasz.py\n",
    "%run ../src/utils.py\n",
    "%run ../src/adabound.py\n",
    "%run ../src/slope.py\n",
    "%run ../src/dropblock.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONE_OUT_PROB = 0.30\n",
    "L2_REG = 0.0\n",
    "INITIAL_LR = 2e-4\n",
    "FINAL_LR = 1e-3\n",
    "BN_MOMENTUM = 0.9\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_RATIO = 0.8\n",
    "TEST_RATIO = 0.2\n",
    "MAX_DROPBLOCK = 0.85\n",
    "\n",
    "gru_flt = 22\n",
    "fpa_flt = 24\n",
    "out_conv_flt = 20\n",
    "\n",
    "\n",
    "AUGMENTATION_RATIO = 4\n",
    "IMAGE_SIZE = 16\n",
    "existing = [int(x[:-4]) for x in os.listdir('../data/training-data-nov-22/') if \".DS\" not in x]\n",
    "N_SAMPLES = len(existing)\n",
    "\n",
    "LABEL_SIZE = 14\n",
    "\n",
    "    \n",
    "TRAIN_SAMPLES = int((N_SAMPLES * AUGMENTATION_RATIO) * TRAIN_RATIO)\n",
    "TEST_SAMPLES = int((N_SAMPLES * AUGMENTATION_RATIO) - TRAIN_SAMPLES)\n",
    "print(TRAIN_SAMPLES // AUGMENTATION_RATIO, N_SAMPLES - (TRAIN_SAMPLES // AUGMENTATION_RATIO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional layer definitions\n",
    "\n",
    "Some of these may be able to be removed, or moved to a src/*.py\n",
    "\n",
    "*  Conv SELU\n",
    "*  Conv BN ELU\n",
    "*  Feature pyramid attention (with downsample / upsample)\n",
    "*  Feature pyramid attention (w/o downsample / upsample)\n",
    "*  Temporal attention\n",
    "*  CSE, SSE, cCSE\n",
    "*  Reflection padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_elu(inp, is_training, kernel_size, scope, filter_count = 16, pad = True, padding = 'valid', dilated = False):\n",
    "    if not dilated:\n",
    "        padded = ReflectionPadding2D((1, 1,))(inp)\n",
    "        conv = Conv2D(filters = filter_count, kernel_size = (kernel_size, kernel_size), activation = selu,\n",
    "                        padding = padding, kernel_initializer = lecun_normal())(padded)\n",
    "    if not dilated and not pad:\n",
    "        conv = Conv2D(filters = filter_count, kernel_size = (kernel_size, kernel_size), activation = selu,\n",
    "                        padding = padding, kernel_initializer = lecun_normal())(inp)\n",
    "    if dilated:\n",
    "        padded = ReflectionPadding2D((2, 2,))(inp)\n",
    "        conv = Conv2D(filters = filter_count, kernel_size = (3, 3), activation = selu, dilation_rate = (2, 2),\n",
    "                        padding = padding, kernel_initializer = lecun_normal())(padded)\n",
    "    return conv\n",
    "    \n",
    "def fpa(inp, is_training, filter_count):\n",
    "    one = conv_bn_elu(inp, is_training, 1, 'forward1', filter_count, False, 'valid')\n",
    "    five = conv_bn_elu(inp, is_training, 5, 'down1', filter_count, False, 'valid')\n",
    "    five_f = conv_bn_elu(five, is_training, 5, 'down1_f', filter_count, False, 'valid')\n",
    "    three = conv_bn_elu(five, is_training, 3, 'down2', filter_count, False, 'valid')\n",
    "    three_f = conv_bn_elu(three, is_training, 3, 'down2_f', filter_count, False, 'valid')\n",
    "    \n",
    "    three_up = get_deconv2d(three_f, filter_count, filter_count, \"fpa1\", is_training)\n",
    "    five_up = get_deconv2d(five_f, filter_count, filter_count, \"fpa2\", is_training)\n",
    "    \n",
    "    print(\"One: {}\".format(one.shape))\n",
    "    print(\"Five: {}\".format(five.shape))\n",
    "    print(\"Five_F: {}\".format(five_f.shape))\n",
    "    print(\"Three: {}\".format(three.shape))\n",
    "    print(\"Three_f: {}\".format(three_f.shape))\n",
    "    print(\"Three_up: {}\".format(three_up.shape))\n",
    "    print(\"Five_up: {}\".format(five_up.shape))\n",
    "    \n",
    "    # top block\n",
    "    pooled = tf.keras.layers.GlobalAveragePooling2D()(inp)\n",
    "    one_top = conv_bn_elu(tf.reshape(pooled, (-1, 1, 1, pooled.shape[-1])),\n",
    "                          is_training, 1, 'top1', filter_count, pad = False)\n",
    "    four_top = tf.keras.layers.UpSampling2D((16, 16))(one_top)\n",
    "    print(\"Sixteen top: {}\".format(four_top.shape))\n",
    "    \n",
    "    \n",
    "    concat_1 = tf.multiply(one, tf.add(three_up, five_up))\n",
    "    concat_2 = tf.add(concat_1, four_top)\n",
    "    print(\"Feature pyramid attention shape {}\".format(concat_2.shape))\n",
    "    return concat_2\n",
    "\n",
    "def fpa(inp, is_training, filter_count, scope = 'fpa', topblock = False):\n",
    "    one = conv_bn_elu(inp, is_training, 1, scope + 'forward1', filter_count, False, 'valid')\n",
    "    three = conv_bn_elu(inp, is_training, 3, scope + 'down2', filter_count, True, 'valid') \n",
    "    five = conv_bn_elu(inp, is_training, 3, scope + 'down2', filter_count, True, 'valid', dilated = True) \n",
    "    concat_1 = tf.multiply(one, tf.add(three, five))\n",
    "    \n",
    "    if topblock:\n",
    "        pooled = tf.keras.layers.GlobalAveragePooling2D()(inp)\n",
    "        one_top = conv_bn_elu(tf.reshape(pooled, (-1, 1, 1, pooled.shape[-1])),\n",
    "                              is_training, 1, 'top1', filter_count, pad = False)\n",
    "        four_top = tf.keras.layers.UpSampling2D((16, 16))(one_top)\n",
    "        print(\"Sixteen top: {}\".format(four_top.shape))\n",
    "\n",
    "        concat_1 = tf.add(concat_1, four_top)\n",
    "    \n",
    "    return concat_1\n",
    "\n",
    "    \n",
    "def create_deconv_init(filter_size, num_channels):\n",
    "    bilinear_kernel = np.zeros([filter_size, filter_size], dtype=np.float32)\n",
    "    scale_factor = (filter_size + 1) // 2\n",
    "    if filter_size % 2 == 1:\n",
    "        center = scale_factor - 1\n",
    "    else:\n",
    "        center = scale_factor - 0.5\n",
    "    for x in range(filter_size):\n",
    "        for y in range(filter_size):\n",
    "            bilinear_kernel[x,y] = (1 - abs(x - center) / scale_factor) * \\\n",
    "                                   (1 - abs(y - center) / scale_factor)\n",
    "    weights = np.zeros((filter_size, filter_size, num_channels, num_channels))\n",
    "    for i in range(num_channels):\n",
    "        weights[:, :, i, i] = bilinear_kernel\n",
    "\n",
    "    #assign numpy array to constant_initalizer and pass to get_variable\n",
    "    bilinear_init = tf.constant_initializer(value=weights, dtype=tf.float32)\n",
    "    return bilinear_init\n",
    "\n",
    "\n",
    "def get_deconv2d(inp, filter_count, num_channels, scope, is_training):\n",
    "    bilinear_init = create_deconv_init(4, filter_count)\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = filter_count, kernel_size = (4, 4),\n",
    "                                        strides=(2, 2), padding='same', \n",
    "                                        kernel_initializer = bilinear_init)(inp)\n",
    "    x = ELU()(x)\n",
    "    x = Batch_Normalization(x, training=is_training, scope = scope + \"bn\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    return batch_normalization(inputs=x, \n",
    "                               momentum = BN_MOMENTUM, \n",
    "                               training=training,\n",
    "                               renorm = True,\n",
    "                               reuse=None,\n",
    "                               name = scope)\n",
    "\n",
    "def temporal_attention(inp, units):\n",
    "    # This rescales each output\n",
    "    # Timesteps that are more important get weighted higher\n",
    "    # Timesteps that are least important get weighted lower --> B, N, H, W, C\n",
    "    conved = TimeDistributed(Conv2D(units, (1, 1), padding = 'same', kernel_initializer = 'glorot_uniform',\n",
    "                            activation = 'tanh', strides = (1, 1)))(inp)\n",
    "    \n",
    "    \n",
    "    #conved = tf.reshape(conved, (-1, units, 16, 16, STEPS))\n",
    "    print(\"Attention weight shape: {}\".format(conved.shape))\n",
    "    conved = TimeDistributed(Conv2D(1, (1, 1), padding = 'same', kernel_initializer = 'glorot_uniform',\n",
    "                            activation = 'sigmoid', use_bias = False, strides = (1, 1)))(conved)\n",
    "    print(\"Conved sigmoid shape: {}\".format(conved.shape))\n",
    "    #conved = tf.reshape(conved, (-1, 24, 1, 1, 1))\n",
    "    \n",
    "    alphas = tf.reduce_sum(conved, axis = 1, keep_dims = True)\n",
    "    print(\"Attention alphas: {}\".format(alphas.shape))\n",
    "    # We need to calculate the total sum for each pixel for each channel, so that we can combine them\n",
    "    alphas = conved / alphas\n",
    "    print(\"Attention weight shapes {}\".format(alphas.shape))\n",
    "    \n",
    "    # This actually multiplies the Conv by the input\n",
    "    multiplied = tf.reduce_sum(alphas * inp, axis = 1)\n",
    "    return multiplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cse_block(prevlayer, prefix):\n",
    "    mean = Lambda(lambda xin: K.mean(xin, axis=[1, 2]))(prevlayer)\n",
    "    lin1 = Dense(K.int_shape(prevlayer)[3] // 2, name=prefix + 'cse_lin1', activation='relu')(mean)\n",
    "    lin2 = Dense(K.int_shape(prevlayer)[3], name=prefix + 'cse_lin2', activation='sigmoid')(lin1)\n",
    "    x = Multiply()([prevlayer, lin2])\n",
    "    return x\n",
    "\n",
    "\n",
    "def sse_block(prevlayer, prefix):\n",
    "    conv = Conv2D(1, (1, 1), padding=\"same\", kernel_initializer=\"glorot_uniform\",\n",
    "                  activation='sigmoid', strides=(1, 1),\n",
    "                  name=prefix + \"_conv\")(prevlayer)\n",
    "    conv = Multiply(name=prefix + \"_mul\")([prevlayer, conv])\n",
    "    return conv\n",
    "\n",
    "\n",
    "def csse_block(x, prefix):\n",
    "    '''\n",
    "    Implementation of Concurrent Spatial and Channel ‘Squeeze & Excitation’ in Fully Convolutional Networks\n",
    "    https://arxiv.org/abs/1803.02579\n",
    "    '''\n",
    "    cse = cse_block(x, prefix)\n",
    "    sse = sse_block(x, prefix)\n",
    "    x = Add(name=prefix + \"_csse_mul\")([cse, sse])\n",
    "\n",
    "    return x\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "    \n",
    "def gru_block(inp, length, size, flt, scope, train, normalize = True):\n",
    "    with tf.variable_scope(scope):\n",
    "        print(\"GRU input shape {}, zoneout: {}\".format(inp.shape, ZONE_OUT_PROB))\n",
    "        cell_fw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID', normalize = normalize, fpa = True)\n",
    "        cell_bw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID', normalize = normalize, fpa = True)\n",
    "        cell_fw = ZoneoutWrapper(\n",
    "           cell_fw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        cell_bw = ZoneoutWrapper(\n",
    "            cell_bw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        steps, out = convGRU(inp, cell_fw, cell_bw, length)\n",
    "        gru = tf.concat(out, axis = -1)\n",
    "        steps = tf.concat(steps, axis = -1)\n",
    "        print(\"Down block output shape {}\".format(gru.shape))\n",
    "    return gru, steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bands = 17\n",
    "reg = keras.regularizers.l2(L2_REG) # for GRU\n",
    "inp = tf.placeholder(tf.float32, shape=(None, 24, IMAGE_SIZE, IMAGE_SIZE, n_bands))\n",
    "length = tf.placeholder(tf.int32, shape = (None, 1))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 14, 14))#, 1))\n",
    "keep_rate = tf.placeholder_with_default(1.0, ()) # For DropBlock\n",
    "length2 = tf.reshape(length, (-1,)) # Remove\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training') # For BN, DropBlock\n",
    "alpha = tf.placeholder(tf.float32, shape = ()) # For loss scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsure if this block is better replaced by 1x1 ConvGRU output\n",
    "#stacked = tf.reshape(inp, (-1, 16, 16, n_bands*24))\n",
    "#pixel_information = Conv2D(filters = gru_flt, kernel_size = (1, 1),\n",
    "#            padding = 'valid', activation = selu, kernel_initializer = lecun_normal())(stacked)\n",
    "    \n",
    "# 3 x 3 GRU\n",
    "\n",
    "inp_first_half = inp[:, :, :, :, :10]\n",
    "inp_second_half = inp[:, :, :, :, 11:]\n",
    "no_dem = tf.concat([inp_first_half, inp_second_half], axis = -1)\n",
    "dem = tf.reshape(tf.reduce_mean(inp[:, :, :, :, 10], axis = 1), (-1, 16, 16, 1))\n",
    "gru_out, steps = gru_block(inp = no_dem, length = length2, \n",
    "                            size = [16, 16], \n",
    "                            flt = gru_flt, \n",
    "                            scope = 'down_16', \n",
    "                            train = is_training)\n",
    "\n",
    "# Skip connect, CSSE, 5x5 drop block\n",
    "#steps = temporal_attention(steps, gru_flt*2)# - unsure if more useful after smoothing\n",
    "#gru_out = gru_out + steps # identity concatenation for a skip connect\n",
    "gru_out = tf.concat([gru_out, dem], axis = -1)\n",
    "#gru_out = tf.concat([gru_out, pixel_information], axis = -1)\n",
    "csse1 = csse_block(gru_out, 'csse1')\n",
    "drop_block1 = DropBlock2D(keep_prob=keep_rate, block_size=5)\n",
    "csse1 = drop_block1(csse1, is_training)\n",
    "\n",
    "# Light FPA, CSSE, 4x4 Drop block\n",
    "fpa1 = fpa(csse1, is_training, fpa_flt, topblock = True)\n",
    "csse2 = csse_block(fpa1, 'csse2')\n",
    "drop_block2 = DropBlock2D(keep_prob=keep_rate, block_size=4)\n",
    "csse2 = drop_block2(csse2, is_training)\n",
    "\n",
    "\n",
    "# Skip connect\n",
    "x = tf.concat([csse2, csse1], axis = -1)\n",
    "\n",
    "# Conv SELU, 3x3 drop\n",
    "x = fpa(x, is_training, fpa_flt)\n",
    "x = csse_block(x, 'csse3')\n",
    "#x = conv_bn_elu(x, is_training, 3, \"out_1\", out_conv_flt, True, 'valid')\n",
    "drop_block4 = DropBlock2D(keep_prob=keep_rate, block_size=3)\n",
    "x = drop_block4(x, is_training)\n",
    "\n",
    "hypercolumn = tf.concat([csse1, csse2, x], axis = -1)\n",
    "\n",
    "# Conv SELU, 16x16 - 14x14\n",
    "x = conv_bn_elu(hypercolumn, is_training, 3, \"out_2\", out_conv_flt, False, 'valid')\n",
    "\n",
    "print(\"Initializing last sigmoid bias with -2.94 constant\")\n",
    "init = tf.constant_initializer([-np.log(0.7/0.3)]) # For focal loss\n",
    "fm = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1), \n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid',\n",
    "            bias_initializer = init,\n",
    "           )(x) # For focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(\"This model has {} parameters\".format(total_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "*  Load in CSV data from Collect Earth\n",
    "*  Reconstruct the X, Y grid for the Y data per sample\n",
    "*  Calculate NDVI, EVI, SAVI, BI, MSAVI2, and SI\n",
    "*  Stack X, Y, length data\n",
    "*  Apply median filter to DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.load(\"../data/processed/data_x_processed.npy\")\n",
    "data_y = np.load(\"../data/processed/data_y_processed.npy\")\n",
    "lengths = np.load(\"../data/processed/length_processed.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import median_filter\n",
    "for sample in tnrange(0, len(data_x)):\n",
    "    filtered = median_filter(data_x[sample, 0, :, :, 10], size = 5)\n",
    "    data_x[sample, :, :, :, 10] = np.stack([filtered] * 24)\n",
    "    \n",
    "#data_x = np.delete(data_x, 10, -1)\n",
    "print(data_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "*  Identify and remove samples with time steps / channels that have a 0. or 1. value, which indicates missing data\n",
    "*  Identify and remove samples with time steps / channels with no variation, which indicates missing data\n",
    "*  Identify and remove samples with values above or below the allowable values for the band\n",
    "*  Smooth per-pixel temporal data with Whittaker smoother, d = 2, lambda = 0.5 to reduce sample noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "below_1 = [i for i, val in enumerate(data_x) if np.min(val) < -1.10]\n",
    "above_1 = [i for i, val in enumerate(data_x) if np.max(val) > 1.10]\n",
    "min_vals = [np.min(val) for i, val in enumerate(data_x) if np.min(val) < -1.10]\n",
    "max_vals = [np.max(val) for i, val in enumerate(data_x) if np.max(val) > 1.10]\n",
    "outliers = below_1 + above_1\n",
    "outliers = list(set(outliers))\n",
    "print(\"The outliers are: {}, totalling {}\".format(outliers, len(outliers)))\n",
    "print(\"\\n\")\n",
    "print(min_vals, max_vals)\n",
    "data_x = data_x[[x for x in range(0, len(data_x)) if x not in outliers]]\n",
    "data_y = data_y[[x for x in range(0, len(data_y)) if x not in outliers]]\n",
    "lengths = lengths[[x for x in range(0, len(lengths)) if x not in outliers]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(y = data_x[255, :, 10, 14, 5], x = [x for x in range(24)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_all = []\n",
    "max_all = []\n",
    "for band in range(0, data_x.shape[-1]):\n",
    "\n",
    "    mins, maxs = (np.min(data_x[:, :, :, :, band]), np.max(data_x[:, :, :, :, band]))\n",
    "    midrange = (maxs + mins) / 2\n",
    "    rng = maxs - mins\n",
    "    standardized = (data_x[:, :, :, :, band] - midrange) / (rng / 2)\n",
    "    data_x[:, :, :, :, band] = standardized\n",
    "    \n",
    "    min_all.append(mins)\n",
    "    max_all.append(maxs)\n",
    "    \n",
    "print(\"The data has been scaled to [{}, {}]\".format(np.min(data_x), np.max(data_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(y = data_x[255, :, 10, 14, 5], x = [x for x in range(24)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/subplot.csv\")\n",
    "df1 = pd.read_csv(\"../data/subplot2.csv\")\n",
    "df2 = pd.read_csv(\"../data/subplot3.csv\")\n",
    "df3 = pd.read_csv(\"../data/subplot4.csv\")\n",
    "df4 = pd.read_csv(\"../data/kenya-train.csv\")\n",
    "\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df1 = df1.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df2 = df2.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df3 = df3.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df4 = df4.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "\n",
    "\n",
    "lens = [len(x) for x in [df, df1, df2, df3, df4]]\n",
    "\n",
    "df = pd.concat([df, df1, df2, df3, df4], ignore_index = True)\n",
    "df = df.dropna(axis = 0)\n",
    "\n",
    "existing = [int(x[:-4]) for x in os.listdir('../data/training-data-nov-22/') if \".DS\" not in x]\n",
    "N_SAMPLES = len(existing)\n",
    "df = df[df['PLOT_ID'].isin(existing)]\n",
    "N_SAMPLES = int(df.shape[0]/196)\n",
    "\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "plot_ids2 = [val for x, val in enumerate(plot_ids) if x not in list(set([x for x in outliers]))]\n",
    "N_SAMPLES = len(data_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive (tree) and negative (background) pixel metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! In development\n",
    "\n",
    "positive_x = []\n",
    "negative_x = []\n",
    "\n",
    "#for sample in data_y:\n",
    "#    for row in range(0,14):\n",
    "#        for column in range(0,14):\n",
    "#            if data_y[sample, row, column] == 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment training data\n",
    "\n",
    "Horizontal and vertical flips for 4x augmentation.\n",
    "\n",
    "**To do**\n",
    "*  Random guassian noise\n",
    "*  Brightness, contrast\n",
    "*  Region swaps (randomply position positive samples at different locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_augmented = []\n",
    "data_y_augmented = []\n",
    "lengths_augmented = []\n",
    "\n",
    "for i, val in enumerate([x for x in range(len(data_x))]):\n",
    "    data_x_augmented.append(data_x[val])\n",
    "    data_y_augmented.append(data_y[val])\n",
    "    lengths_augmented.append(data_x[val].shape[0])\n",
    "    \n",
    "    x1 = np.flip(data_x[val], 1)\n",
    "    y1 = np.flip(data_y[val], 0)\n",
    "    lengths_augmented.append(x1.shape[0])\n",
    "    data_x_augmented.append(x1)\n",
    "    data_y_augmented.append(y1)\n",
    "    \n",
    "    x1 = np.flip(data_x[val], [2, 1])\n",
    "    y1 = np.flip(data_y[val], [1, 0])\n",
    "    lengths_augmented.append(x1.shape[0])\n",
    "    data_x_augmented.append(x1)\n",
    "    data_y_augmented.append(y1)\n",
    "    \n",
    "    x1 = np.flip(data_x[val], 2)\n",
    "    y1 = np.flip(data_y[val], 1)\n",
    "    lengths_augmented.append(x1.shape[0])\n",
    "    data_x_augmented.append(x1)\n",
    "    data_y_augmented.append(y1)\n",
    "\n",
    "train_x = np.stack(data_x_augmented)\n",
    "train_y = np.stack(data_y_augmented)\n",
    "train_y = np.reshape(train_y, (train_y.shape[0], 14, 14, 1))\n",
    "train_l = np.stack(lengths_augmented)\n",
    "train_l = np.reshape(train_l, (train_y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.load(\"../data/processed/test_x_processed.npy\")\n",
    "test_y = np.load(\"../data/processed/test_y_processed.npy\")\n",
    "test_lengths = np.load(\"../data/processed/test_length_processed.npy\")\n",
    "\n",
    "\n",
    "below_1 = [i for i, val in enumerate(test_x) if np.min(val) < -1.05]\n",
    "above_1 = [i for i, val in enumerate(test_x) if np.max(val) > 1.05]\n",
    "min_vals = [np.min(val) for i, val in enumerate(test_x) if np.min(val) < -1.05]\n",
    "max_vals = [np.max(val) for i, val in enumerate(test_x) if np.max(val) > 1.05]\n",
    "outliers = below_1 + above_1\n",
    "outliers = list(set(outliers))\n",
    "print(\"The outliers are: {}, totalling {}\".format(outliers, len(outliers)))\n",
    "print(\"\\n\")\n",
    "print(min_vals, max_vals)\n",
    "test_x = test_x[[x for x in range(0, len(test_x)) if x not in outliers]]\n",
    "test_y = test_y[[x for x in range(0, len(test_y)) if x not in outliers]]\n",
    "test_lengths = test_lengths[[x for x in range(0, len(test_lengths)) if x not in outliers]]\n",
    "print(np.min(test_x[19]))\n",
    "\n",
    "for sample in tnrange(0, len(test_x)):\n",
    "    filtered = median_filter(test_x[sample, 0, :, :, 10], size = 5)\n",
    "    test_x[sample, :, :, :, 10] = np.stack([filtered] * 24)\n",
    "\n",
    "#test_x = np.delete(test_x, 10, -1)\n",
    "    \n",
    "for band in range(0, test_x.shape[-1]):\n",
    "    mins = min_all[band]\n",
    "    maxs = max_all[band]\n",
    "    test_x[:, :, :, :, band] = np.clip(test_x[:, :, :, :, band], mins, maxs)\n",
    "    midrange = (maxs + mins) / 2\n",
    "    rng = maxs - mins\n",
    "    standardized = (test_x[:, :, :, :, band] - midrange) / (rng / 2)\n",
    "    test_x[:, :, :, :, band] = standardized\n",
    "    \n",
    "print(\"The data has been scaled to [{}, {}]\".format(np.min(test_x), np.max(test_x)))\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train and test characteristics:\")\n",
    "print(\"Train mean Y {}\".format(np.mean([np.sum(x) for x in test_y])))\n",
    "#print(\"Test mean Y {}\".format(np.mean([np.sum(x) for x in data_y[test_ids]])))\n",
    "#print(\"Train STD Y {}\".format(np.std([np.sum(x) for x in data_y[train_ids]])))\n",
    "print(\"Test STD Y {}\".format(np.std([np.sum(x) for x in test_y])))\n",
    "#print(\"Train number with zero trees {}\".format(0.2*len([x for x in data_y[train_ids] if np.sum(x) == 0])))\n",
    "#print(\"Test number with zero trees {}\".format(0.8*len([x for x in data_y[test_ids] if np.sum(x) == 0])))\n",
    "print(\"Train mean NDVI\")\n",
    "print(\"Test mean NDVI\")\n",
    "#print(\"There are {} train and {} test samples\".format(len(train_ids), len(test_ids)))\n",
    "#print(\"There is {} overlap between train and test\".format(len([x for x in train_ids if x in test_ids])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(test_x[19, :, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss definition\n",
    "\n",
    "The current best loss is a combination of weighted binary cross entropy and per-image Lovasz-Softmax, with a loss schedule with the latter becoming more important each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "import math\n",
    "from scipy.ndimage import distance_transform_edt as distance\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight, smooth = 0.025):\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    y_true = K.clip(y_true, smooth, 1. - smooth)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "        y_true,\n",
    "        logit_y_pred,\n",
    "        weight,\n",
    "    )\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def calc_dist_map(seg):\n",
    "    res = np.zeros_like(seg)\n",
    "    posmask = seg.astype(np.bool)\n",
    "\n",
    "    if posmask.any():\n",
    "        negmask = ~posmask\n",
    "        res = distance(negmask) * negmask - (distance(posmask) - 1) * posmask\n",
    "\n",
    "    return res\n",
    "\n",
    "def calc_dist_map_batch(y_true):\n",
    "    y_true_numpy = y_true.numpy()\n",
    "    return np.array([calc_dist_map(y)\n",
    "                     for y in y_true_numpy]).astype(np.float32)\n",
    "\n",
    "def surface_loss(y_true, y_pred):\n",
    "    y_true_dist_map = tf.py_function(func=calc_dist_map_batch,\n",
    "                                     inp=[y_true],\n",
    "                                     Tout=tf.float32)\n",
    "    multipled = y_pred * y_true_dist_map\n",
    "    return K.mean(multipled)\n",
    "\n",
    "\n",
    "def bce_lv(y_true, y_pred, alpha, smooth = 0.075, mask = None, ):\n",
    "    bce = (0.95 - alpha) * weighted_bce_loss(y_true, y_pred, 1.5, smooth = smooth)\n",
    "    #surf = surface_loss(y_true, y_pred)\n",
    "    lv = (0.05 + alpha) * lovasz_softmax(y_pred, tf.reshape(y_true, (-1, 14, 14)), classes=[1], per_image=True)\n",
    "    #global_loss = (1 - alpha) * (bce + lv)\n",
    "    #regional_loss = alpha * surf\n",
    "    return bce+lv\n",
    "\n",
    "\n",
    "def lovasz(y_true, y_pred):\n",
    "    lv = lovasz_softmax(y_pred, tf.reshape(y_true, (-1, 14, 14)), classes=[1], per_image=True)\n",
    "    return lv\n",
    "\n",
    "#! Move to a src.py\n",
    "def calculate_metrics():\n",
    "    best_f1 = 0\n",
    "    best_thresh = 0\n",
    "    p = 0\n",
    "    r = 0\n",
    "    error = 0\n",
    "    ys = []\n",
    "    vls = []\n",
    "    t_alls = []\n",
    "    test_ids = [x for x in range(len(test_x)) if x not in [11, 110, 3, 100, 47]]\n",
    "    for test_sample in test_ids:\n",
    "        y, vl = sess.run([fm, test_loss], feed_dict={inp: test_x[test_sample].reshape(1, 24, 16, 16, n_bands),\n",
    "                                          length: test_lengths[test_sample].reshape(1, 1),\n",
    "                                          is_training: False,\n",
    "                                          labels: test_y[test_sample, :, :].reshape(1, 14, 14),\n",
    "                                          })\n",
    "        ys.append(y.reshape((14, 14)))\n",
    "        vls.append(vl)\n",
    "        t = test_y[test_sample].reshape((14, 14))\n",
    "        t_alls.append(t)\n",
    "    for thresh in range(8, 13):\n",
    "        tps = []\n",
    "        fps = []\n",
    "        fns = []\n",
    "        perc_error = []\n",
    "        trues = []\n",
    "        preds = []\n",
    "        val_loss = []\n",
    "        for sample in range(len(ys)):\n",
    "            pred = np.copy(ys[sample])\n",
    "            true = t_alls[sample]\n",
    "            vl = vls[sample]\n",
    "            pred[np.where(pred > thresh*0.05)] = 1\n",
    "            pred[np.where(pred < thresh*0.05)] = 0\n",
    "            true_s = np.sum(true)\n",
    "            pred_s = np.sum(pred)\n",
    "\n",
    "            perc_error.append(abs(pred_s - true_s) / 196)\n",
    "            tp, fp, fn = thirty_meter(true, pred)\n",
    "            tps.append(tp)\n",
    "            fps.append(fp)\n",
    "            fns.append(fn)\n",
    "            trues.append(true_s)\n",
    "            preds.append(pred_s)\n",
    "            val_loss.append(np.mean(vl))\n",
    "        oa_error = abs(np.sum(preds) - np.sum(trues)) / np.sum(trues)\n",
    "        precision = np.sum(tps) / (np.sum(tps) + np.sum(fps))\n",
    "        recall = np.sum(tps) / (np.sum(tps) + np.sum(fns))\n",
    "        f1 = 2*((precision* recall) / (precision + recall))\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            p = precision\n",
    "            r = recall\n",
    "            error = oa_error\n",
    "            best_thresh = thresh*0.05\n",
    "    print(\"Val loss: {} Thresh: {} F1: {} Recall: {} Precision: {} Error: {}\".format(np.around(np.mean(val_loss), 3), np.around(best_thresh, 2),\n",
    "                                                                                     np.around(best_f1, 3), np.around(p, 3), np.around(r, 3), \n",
    "                                                                                     np.around(error, 3)))\n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.percentile(np.sum(data_y, axis = (1, 2)), x) for x in range(0, 100, 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equibatch creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [x for x in range(0, len(train_y))]\n",
    "\n",
    "def multiplot(matrices):\n",
    "    '''Plot multiple heatmaps with subplots'''\n",
    "    fig, axs = plt.subplots(ncols=4)\n",
    "    fig.set_size_inches(20, 4)\n",
    "    for i, matrix in enumerate(matrices):\n",
    "        sns.heatmap(data = matrix, ax = axs[i], vmin = 0, vmax = 0.9)\n",
    "        axs[i].set_xlabel(\"\")\n",
    "        axs[i].set_ylabel(\"\")\n",
    "        axs[i].set_yticks([])\n",
    "        axs[i].set_xticks([])\n",
    "    plt.show()\n",
    "\n",
    "def equibatch(train_ids, lovasz = False):\n",
    "    first_len = 5\n",
    "    second_len = 9\n",
    "    third_len = 14\n",
    "    np.random.shuffle(train_ids)\n",
    "    ix = train_ids\n",
    "    percs = [np.sum(x) for x in train_y[ix]]\n",
    "    zero_ids = [x for x, z in zip(ix, percs) if z == 0]\n",
    "    one_ids = [x for x, z in zip(ix, percs) if 0 < z <= first_len]\n",
    "    two_ids = [x for x, z in zip(ix, percs) if first_len < z <= second_len]\n",
    "    three_ids = [x for x, z in zip(ix, percs) if second_len < z <= third_len]\n",
    "    four_ids = [x for x, z in zip(ix, percs) if third_len < z <= 19]\n",
    "    five_ids = [x for x, z in zip(ix, percs) if 19 < z < 27]\n",
    "    six_ids = [x for x, z in zip(ix, percs) if 27 < z <= 33]\n",
    "    seven_ids = [x for x, z in zip(ix, percs) if 33 < z <= 41]\n",
    "    eight_ids = [x for x, z in zip(ix, percs) if 41 < z <= 56]\n",
    "    nine_ids =  [x for x, z in zip(ix, percs) if 56 < z <= 80]\n",
    "    ten_ids =  [x for x, z in zip(ix, percs) if 80 < z <= 120]\n",
    "    eleven_ids = [x for x, z in zip(ix, percs) if 120 < z]\n",
    "    #ten_ids = [x for x, z in zip(ix, percs) if 125 < z]\n",
    "\n",
    "    new_batches = []\n",
    "    maxes = [len(zero_ids), len(one_ids), len(two_ids), len(three_ids), len(four_ids),\n",
    "             len(five_ids), len(six_ids), len(seven_ids), len(eight_ids), len(nine_ids), len(ten_ids), len(eleven_ids)]#, len(ten_ids)]\n",
    "    cur_ids = [0] * 12\n",
    "    iter_len = len(train_ids)//(len(maxes)+3) if not lovasz else len(train_ids)//8\n",
    "    for i in range(0, iter_len):\n",
    "        for i, val in enumerate(cur_ids):\n",
    "            if val > maxes[i] - 1:\n",
    "                cur_ids[i] = 0\n",
    "        if cur_ids[0] >= (maxes[0] - 2):\n",
    "            cur_ids[0] = 0\n",
    "        to_append = [zero_ids[cur_ids[0]], zero_ids[cur_ids[0] + 1], one_ids[cur_ids[1]], two_ids[cur_ids[2]],\n",
    "                    three_ids[cur_ids[3]], four_ids[cur_ids[4]], five_ids[cur_ids[5]],\n",
    "                    six_ids[cur_ids[6]], seven_ids[cur_ids[7]], eight_ids[cur_ids[8]],\n",
    "                               nine_ids[cur_ids[9]], ten_ids[cur_ids[10]], eleven_ids[cur_ids[11]]]\n",
    "        \n",
    "        np.random.shuffle(to_append)\n",
    "        new_batches.append(to_append)\n",
    "        cur_ids = [x + 1 for x in cur_ids]\n",
    "        cur_ids[0] += 1\n",
    "        \n",
    "    new_batches = [item for sublist in new_batches for item in sublist]\n",
    "    #overlap = [x for x in new_batches if x in test_ids]\n",
    "    #print(\"There is {} overlap. Error if > 0\".format(len(overlap)))\n",
    "    return new_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Todo: Move to one cell, rather than 3\n",
    "batch = equibatch(train_ids, True)\n",
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[:4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[4:8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot([x.reshape((14, 14)) for x in train_y[batch[8:12]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should be run to do fine-tuning, if commented - train from scratch\n",
    "\n",
    "new_saver = tf.train.import_meta_graph('../models/nov-21/model.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint('../models/nov-21/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRESH_START = True\n",
    "best_val = 0.73\n",
    "\n",
    "START_EPOCH = 1\n",
    "END_EPOCH = 150\n",
    "LEARNING_RATE = 4e-3\n",
    "test_ids = [x for x in range(0, len(test_x))]\n",
    "\n",
    "\n",
    "print(\"Starting model with: \\n {} zone out \\n {} l2 \\n {} initial LR \\n {} final LR \\n {} parameters\"\n",
    "     .format(ZONE_OUT_PROB, L2_REG, INITIAL_LR, FINAL_LR, total_parameters))\n",
    "\n",
    "if not FRESH_START:\n",
    "    print(\"Resuming training with a best validation score of {}\".format(best_val))\n",
    "    \n",
    "if FRESH_START:\n",
    "    print(\"Restarting training from scratch on {} \"\n",
    "          \"train and {} test samples, total {}\".format(len(train_ids), len(test_ids), N_SAMPLES))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE, epsilon = 1e-8)\n",
    "    train_loss = bce_lv(tf.reshape(labels, (-1, 14, 14, 1)), fm, alpha)\n",
    "    l2_loss = tf.losses.get_regularization_loss()\n",
    "    train_loss += l2_loss\n",
    "\n",
    "    ft_optimizer = tf.train.GradientDescentOptimizer(0.008)\n",
    "    ft_loss = bce_lv(tf.reshape(labels, (-1, 14, 14, 1)), fm, alpha)\n",
    "    #wu_loss = weighted_bce_loss(tf.reshape(labels, (-1, 14, 14, 1)), fm, weight = 1.5)\n",
    "    \n",
    "    test_loss = weighted_bce_loss(tf.reshape(labels, (-1, 14, 14, 1)), fm, weight = 1.)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(train_loss)   \n",
    "        ft_op = ft_optimizer.minimize(ft_loss)\n",
    "        \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver(max_to_keep = 2)\n",
    "    \n",
    "print(\"The graph has been finalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val = 0.73\n",
    "\n",
    "train_ids = [x for x in range(len(train_y))]\n",
    "\n",
    "for i in range(1, 200):\n",
    "    al = np.min([i*0.005, 0.075])\n",
    "    #randomize = train_ids\n",
    "    #np.random.shuffle(randomize)\n",
    "    randomize = equibatch(train_ids, lovasz = False)\n",
    "    if i > 125:\n",
    "        print(\"Switching to SGD\")\n",
    "        op = ft_op\n",
    "        loss = ft_loss\n",
    "    else:\n",
    "        print(\"Loss: {} 1.5 weighted BCE + {} Image Lovasz\".format( (0.95-al), (0.05 + al)))\n",
    "        op = train_op\n",
    "        loss = train_loss\n",
    "        #op = ft_op\n",
    "        #loss = ft_loss\n",
    "    BATCH_SIZE = 16\n",
    "    test_ids = [x for x in range(0, len(test_x))]\n",
    "    losses = []\n",
    "    \n",
    "    for k in tnrange(int(len(randomize) // BATCH_SIZE)):\n",
    "        batch_ids = randomize[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "        batch_y = train_y[batch_ids, :, :].reshape(len(batch_ids), 14, 14)\n",
    "        opt, tr = sess.run([op, loss],\n",
    "                              feed_dict={inp: train_x[batch_ids, :, :, :],\n",
    "                                         length: train_l[batch_ids].reshape((-1, 1)),\n",
    "                                         labels: batch_y,\n",
    "                                         is_training: True,\n",
    "                                         keep_rate: np.max((1 - (i*0.002), 0.85)),\n",
    "                                         alpha: al\n",
    "                                         })\n",
    "        losses.append(tr)\n",
    "    \n",
    "    print(\"Epoch {}: Loss {}\".format(i, np.around(np.mean(losses[:-1]), 3)))\n",
    "    f1 = calculate_metrics()\n",
    "    if f1 > best_val:\n",
    "        best_val = f1\n",
    "        print(\"Saving model with {}\".format(f1))\n",
    "        save_path = saver.save(sess, \"../models/nov-21/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential problem test IDS: 135505813, 135505835, \n",
    "save_path = saver.save(sess, \"../models/nov-21/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation and sanity checks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "test_ids = [x for x in range(0, len(test_x))]\n",
    "\n",
    "def multiplot(matrices, nrows = 2, ncols = 4):\n",
    "    '''Plot multiple heatmaps with subplots'''\n",
    "    fig, axs = plt.subplots(ncols=4, nrows = nrows)\n",
    "    fig.set_size_inches(20, 4*nrows)\n",
    "    to_iter = [[x for x in range(i, i + ncols + 1)] for i in range(0, nrows*ncols, ncols)]\n",
    "    for r in range(1, nrows + 1):\n",
    "        min_i = min(to_iter[r-1])\n",
    "        max_i = max(to_iter[r-1])\n",
    "        for i, matrix in enumerate(matrices[min_i:max_i]):\n",
    "            sns.heatmap(data = matrix, ax = axs[r - 1, i], vmin = 0, vmax = 0.9)\n",
    "            axs[r - 1, i].set_xlabel(\"\")\n",
    "            axs[r - 1, i].set_ylabel(\"\")\n",
    "            axs[r - 1, i].set_yticks([])\n",
    "            axs[r - 1, i].set_xticks([])\n",
    "    plt.show()\n",
    "start = 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "print(start/len(test_ids))\n",
    "test_ids = sorted(test_ids)\n",
    "matrix_ids = [test_ids[start], test_ids[start + 1], test_ids[start + 2], test_ids[start + 3],\n",
    "              test_ids[start + 4], test_ids[start + 5], test_ids[start + 6], test_ids[start + 7]]\n",
    "#matrix_ids = random.sample(test_ids, 4)z\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    print(i)\n",
    "    y = sess.run([fm], feed_dict={inp: test_x[idx].reshape(1, 24, IMAGE_SIZE, IMAGE_SIZE, n_bands),\n",
    "                                  length: test_lengths[idx].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })\n",
    "    y = np.array(y).reshape(14, 14)\n",
    "    preds.append(y)\n",
    "    true = test_y[idx].reshape(14, 14)\n",
    "    trues.append(true)\n",
    "    \n",
    "\"\"\n",
    "to_plot = trues[0:4] + preds[0:4]# + trues[5:] + preds[5:]\n",
    "multiplot(to_plot, nrows = 2, ncols = 4)\n",
    "#plot_ids[/ordering[976]//4] \n",
    "start = start + 4 # 50, 87, 107 is a concern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "test_ids = sorted(test_ids)\n",
    "matrix_ids = random.sample(train_ids, 4)\n",
    "#matrix_ids = [len(train_x)-28]\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    print(i)\n",
    "    y = sess.run([fm], feed_dict={inp: train_x[idx].reshape(1, 24, IMAGE_SIZE, IMAGE_SIZE, n_bands),\n",
    "                                  length: train_l[idx].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })\n",
    "    y = np.array(y).reshape(14, 14)\n",
    "    preds.append(y)\n",
    "    true = train_y[idx].reshape(14, 14)\n",
    "    trues.append(true)\n",
    "    \n",
    "\n",
    "to_plot = trues[0:4] + preds[0:4]# + trues[5:] + preds[5:]\n",
    "multiplot(to_plot, nrows = 2, ncols = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ids2 = [val for x, val in enumerate(plot_ids) if x not in list(set([x // 4 for x in outliers]))]\n",
    "#plot_ids2[ordering[460]//4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO @jombrandt top 10 worst training, test samples by IOU \n",
    "\n",
    "These should be written to a tmp/ .txt file and indexed by validate-data.ipynb to ensure that original classifications were correct, and to identify regions that need more training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
