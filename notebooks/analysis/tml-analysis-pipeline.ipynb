{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rs\n",
    "from rasterio.mask import mask\n",
    "from rasterio.merge import merge\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio import Affine, MemoryFile\n",
    "\n",
    "import numpy as np \n",
    "import numpy.ma as ma \n",
    "import pyproj\n",
    "import geopandas as gpd \n",
    "import shapely\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "import pandas as pd\n",
    "import fiona\n",
    "from contextlib import contextmanager  \n",
    "from skimage.transform import resize\n",
    "import math\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "import osgeo\n",
    "from osgeo import gdal\n",
    "from osgeo import gdalconst\n",
    "import glob\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapefile to Geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_gjson(country):\n",
    "    '''\n",
    "    Imports a country shapefile, translates and saves it as \n",
    "    a geojson, confirming the correct CRS and absence of \n",
    "    duplicates. Prints the number of admin 1 districts.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    \n",
    "    '''\n",
    "    if country == 'Costa Rica':\n",
    "        return 'Using existing geojson file for Costa Rica.'\n",
    "    else: \n",
    "        shapefile = glob.glob(f'{country}/shapefile/*.shp')\n",
    "        new_shp = gpd.read_file(shapefile[0])\n",
    "        new_shp.to_file(f'{country}/{country}_adminboundaries.geojson', driver='GeoJSON')\n",
    "        print(f'There are {len(new_shp)} admins in {country}.')\n",
    "        assert new_shp.crs == 'epsg:4326'\n",
    "        assert new_shp.NAME_1.duplicated().sum() == 0\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hansen Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hansen_tif(country):\n",
    "    '''\n",
    "    Identifies the latitude and longitude coordinates for a country \n",
    "    to download Hansen 2010 tree cover and 2020 tree cover loss tif files. \n",
    "    Returns combined tifs as one file in the country's folder.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    \n",
    "    '''\n",
    "    gdal.UseExceptions()\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries.geojson')\n",
    "\n",
    "    if not os.path.exists(f'hansen_treecover2010'):\n",
    "        os.makedirs(f'hansen_treecover2010')\n",
    "    \n",
    "    if not os.path.exists(f'hansen_lossyear2020'):\n",
    "        os.makedirs(f'hansen_lossyear2020')\n",
    "    \n",
    "    # identify min/max bounds for the country\n",
    "    bounds = shapefile.geometry.bounds\n",
    "    min_x = bounds.minx.min() \n",
    "    min_y = bounds.miny.min()\n",
    "    max_x = bounds.maxx.max()\n",
    "    max_y = bounds.maxy.max()\n",
    "\n",
    "    # identify the lowest and highest 10 lat/lon increments for the country\n",
    "    lower_x = math.floor(min_x / 10) * 10 \n",
    "    lower_y = math.ceil(min_y / 10) * 10 \n",
    "    upper_x = math.ceil(max_x / 10) * 10 \n",
    "    upper_y = math.ceil(max_y / 10) * 10\n",
    "\n",
    "    # create a list of tif file names for the country\n",
    "    tree_cover_files = []\n",
    "    loss_files = []\n",
    "    \n",
    "    print('Downloading files from GLAD...')\n",
    "    \n",
    "    for x_grid in range(lower_x, upper_x, 10):\n",
    "        for y_grid in range(lower_y, upper_y + 10, 10):\n",
    "            \n",
    "            lon = 'N' if y_grid >= 0 else 'S'\n",
    "            lat = 'E' if x_grid >= 0 else 'W'\n",
    "\n",
    "            # download tree cover and loss files from UMD\n",
    "            cover_url =  f'https://glad.umd.edu/Potapov/TCC_2010/treecover2010_' \\\n",
    "                         f'{str(y_grid).zfill(2)}{lon}_{str(np.absolute(x_grid)).zfill(3)}{lat}.tif'\n",
    "            cover_dest = f'hansen_treecover2010/{str(y_grid).zfill(2)}{lon}_{str(np.absolute(x_grid)).zfill(3)}{lat}.tif'\n",
    "\n",
    "            try:\n",
    "                urllib.request.urlretrieve(cover_url, cover_dest)\n",
    "            except urllib.error.HTTPError as err:\n",
    "                if err.code == 404:\n",
    "                    print(f'HTTP Error 404: {cover_url}')\n",
    "                    \n",
    "            loss_url =  f'https://storage.googleapis.com/earthenginepartners-hansen/GFC-2020-v1.8/Hansen_GFC-2020-v1.8_lossyear_' \\\n",
    "                         f'{str(y_grid).zfill(2)}{lon}_{str(np.absolute(x_grid)).zfill(3)}{lat}.tif'\n",
    "            loss_dest = f'hansen_lossyear2020/{str(y_grid).zfill(2)}{lon}_{str(np.absolute(x_grid)).zfill(3)}{lat}.tif'\n",
    "\n",
    "            try:\n",
    "                urllib.request.urlretrieve(loss_url, loss_dest)\n",
    "            except urllib.error.HTTPError as err:\n",
    "                if err.code == 404:\n",
    "                    print(f'HTTP Error 404: {loss_url}')\n",
    "            \n",
    "            if not os.path.exists(cover_dest) or not os.path.exists(loss_dest):\n",
    "                print(f'Files did not download.')\n",
    "                \n",
    "            tree_cover_files.append(cover_dest)\n",
    "            loss_files.append(loss_dest)\n",
    "    \n",
    "    # remove duplicate file names\n",
    "    tree_tifs = [x for x in tree_cover_files if os.path.exists(x)] \n",
    "    loss_tifs = [x for x in loss_files if os.path.exists(x)]\n",
    "    \n",
    "    # convert tree cover and loss tifs into a virtual raster tile  \n",
    "    gdal.BuildVRT(f'{country}/{country}_hansen_treecover2010.vrt', tree_tifs)\n",
    "    gdal.BuildVRT(f'{country}/{country}_hansen_loss2020.vrt', loss_tifs)\n",
    "\n",
    "    # open vrts and convert to a single .tif -- adding tfw=yes increases file size significantly\n",
    "    translateoptions = gdal.TranslateOptions(format='Gtiff', \n",
    "                                              outputSRS='EPSG:4326',\n",
    "                                              outputType=gdal.GDT_Byte,\n",
    "                                              noData=255,\n",
    "                                              creationOptions=['COMPRESS=LZW'],\n",
    "                                              resampleAlg='nearest')\n",
    " \n",
    "    source = gdal.Open(f'{country}/{country}_hansen_treecover2010.vrt', )\n",
    "    ds = gdal.Translate(f'{country}/{country}_hansen_treecover2010.tif', source, options=translateoptions)\n",
    "    os.remove(f'{country}/{country}_hansen_treecover2010.vrt')\n",
    "    source = None\n",
    "    ds = None\n",
    "                      \n",
    "    source = gdal.Open(f'{country}/{country}_hansen_loss2020.vrt')\n",
    "    ds = gdal.Translate(f'{country}/{country}_hansen_loss2020.tif', source, options=translateoptions)\n",
    "    os.remove(f'{country}/{country}_hansen_loss2020.vrt')\n",
    "    source = None\n",
    "    ds = None\n",
    "    \n",
    "    assert os.path.exists(f'{country}/{country}_hansen_treecover2010.tif')\n",
    "    assert os.path.exists(f'{country}/{country}_hansen_loss2020.tif')\n",
    "\n",
    "    # if new files are properly create, delete what is not needed\n",
    "    for file in tree_cover_files:\n",
    "        os.remove(file)\n",
    "    \n",
    "    for file in loss_files:\n",
    "        os.remove(file)\n",
    "    \n",
    "    print('Hansen raster built.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Hansen tree cover loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_loss(country):\n",
    "    '''\n",
    "    Takes in a country name to import hansen tree cover loss tifs. Updates tree cover \n",
    "    to 0 if loss was detected between 2011-2020. Returns updated tif in the country's \n",
    "    folder.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "    gdal.UseExceptions()\n",
    "    hansen_cover = rs.open(f'{country}/{country}_hansen_treecover2010.tif').read(1) \n",
    "    print(f\"The hansen cover data is {hansen_cover.nbytes / 1e6} megabytes\")\n",
    "    hansen_loss = rs.open(f'{country}/{country}_hansen_loss2020.tif').read(1)\n",
    "    print(f\"The hansen loss data is {hansen_loss.nbytes / 1e6} megabytes\")\n",
    "    \n",
    "     # assert raster shape, datatype and max/min values\n",
    "    assert hansen_cover.dtype == 'uint8'\n",
    "    assert hansen_cover.shape != (0, ) and len(hansen_cover.shape) <= 2\n",
    "    assert hansen_cover.max() <= 100 and hansen_cover.min() >= 0\n",
    "    assert hansen_loss.dtype == 'uint8'\n",
    "    assert hansen_loss.shape != (0, ) and len(hansen_loss.shape) <= 2\n",
    "    assert hansen_loss.max() <= 20 and hansen_cover.min() >= 0\n",
    "    \n",
    "    # If there was loss between 2011-2020, make then 0 in tree cover\n",
    "    sum_before_loss = np.sum(hansen_cover > 0) \n",
    "    hansen_cover[(hansen_loss >= 11)] = 0.\n",
    "    #hansen_cover_new = np.where((hansen_loss >= 11) & (hansen_loss <= 20), 0, hansen_cover)\n",
    "    \n",
    "    # check bin counts after loss removed\n",
    "    print(f'{sum_before_loss - (np.sum(hansen_cover > 0))} pixels converted to loss.')\n",
    "    \n",
    "    # write as a new file\n",
    "    out_meta = rs.open(f'{country}/{country}_hansen_treecover2010.tif').meta\n",
    "    out_meta.update({'driver': 'GTiff',    \n",
    "                     'dtype': 'uint8',\n",
    "                     'height': hansen_cover.shape[0],\n",
    "                     'width': hansen_cover.shape[1],\n",
    "                     'count': 1,\n",
    "                     'compress':'lzw'})\n",
    "    outpath = f'{country}/{country}_hansen_treecover2010_wloss.tif'\n",
    "    with rs.open(outpath, 'w', **out_meta) as dest:\n",
    "            dest.write(hansen_cover, 1) \n",
    "    \n",
    "    # remove original hansen tree cover and loss files\n",
    "    os.remove(f'{country}/{country}_hansen_treecover2010.tif')\n",
    "    os.remove(f'{country}/{country}_hansen_loss2020.tif')\n",
    "    hansen_cover = None\n",
    "    hansen_loss = None \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad TCL Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tcl_raster(country):\n",
    "    \n",
    "    '''\n",
    "    Increase the raster extent to match the bounds of a country's shapefile\n",
    "    and fill with no data value.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "    \n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries.geojson')\n",
    "\n",
    "    # identify min/max bounds for the country\n",
    "    bounds = shapefile.geometry.bounds\n",
    "    min_x = bounds.minx.min() \n",
    "    min_y = bounds.miny.min()\n",
    "    max_x = bounds.maxx.max()\n",
    "    max_y = bounds.maxy.max()\n",
    "    \n",
    "    # create new bounds by rounding to the nearest .1 lat/lon \n",
    "    lower_x = math.floor(min_x * 10) / 10 \n",
    "    lower_y = math.floor(min_y * 10) / 10 \n",
    "    upper_x = math.ceil(max_x * 10) / 10\n",
    "    upper_y = math.ceil(max_y * 10) / 10\n",
    "          \n",
    "    # create tif with new bounds\n",
    "    warp_options = gdal.WarpOptions(format='GTiff', \n",
    "                                    dstSRS='EPSG:4326',\n",
    "                                    dstNodata=255,\n",
    "                                    outputBounds=[lower_x, lower_y, upper_x, upper_y],\n",
    "                                    resampleAlg='near',\n",
    "                                    outputType=osgeo.gdalconst.GDT_Byte,\n",
    "                                    creationOptions=['TFW=YES', 'COMPRESS=LZW', 'BIGTIFF=YES'])   \n",
    "        \n",
    "    ds = gdal.Warp(f'{country}/{country}_tof_padded.tif', \n",
    "                   f'{country}/{country}.tif',      \n",
    "                   options=warp_options)\n",
    "    \n",
    "    ds = None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip Rasters by Admin Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clippings(country):\n",
    "    '''\n",
    "    Takes in a country name to import tof/hansen rasters and masks out administrative \n",
    "    boundaries based on the shapefile. Saves exploded shapefile as a geojson with polygons \n",
    "    split/numbered for each admin boundary. Returns clipped rasters as individual \n",
    "    files in the country's \"clipped_rasters\" folder. Deletes the original Hansen file. \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import        \n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(f'{country}/clipped_rasters/hansen'):\n",
    "        os.makedirs(f'{country}/clipped_rasters/hansen')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/clipped_rasters/tof'):\n",
    "        os.makedirs(f'{country}/clipped_rasters/tof')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/clipped_rasters/esa'):\n",
    "        os.makedirs(f'{country}/clipped_rasters/esa')\n",
    "    \n",
    "    orig_shapefile = gpd.read_file(f'{country}/{country}_adminboundaries.geojson')\n",
    "    \n",
    "    # preprocess shapefile from multipolygon to single\n",
    "    counter = 0\n",
    "    for idx, row in orig_shapefile.iterrows():\n",
    "        counter += 1 if type(row.geometry) == MultiPolygon else 0\n",
    "\n",
    "    if counter > 0:\n",
    "        shapefile = orig_shapefile.explode()\n",
    "        \n",
    "        # add integer to admin name if multi polys\n",
    "        shapefile.NAME_1 = np.where(shapefile.NAME_1.duplicated(keep=False), \n",
    "                                     shapefile.NAME_1 + shapefile.groupby('NAME_1').cumcount().add(1).astype(str),\n",
    "                                     shapefile.NAME_1)\n",
    "\n",
    "        shapefile = shapefile.reset_index()\n",
    "        shapefile.drop(columns=['level_0', 'level_1'], inplace=True)\n",
    "    \n",
    "    # if no multi polys save original shapefile under new name\n",
    "    else:\n",
    "        shapefile = orig_shapefile\n",
    "    \n",
    "    shapefile.to_file(f'{country}/{country}_adminboundaries_exp.geojson', driver='GeoJSON')\n",
    "    \n",
    "    def mask_raster(polygon, admin, raster, folder):\n",
    "        out_img, out_transform = mask(dataset=raster, shapes=[polygon], crop=True, nodata=255, filled = False)\n",
    "        out_meta = raster.meta\n",
    "        out_meta.update({'driver': 'GTiff',    \n",
    "                         'dtype': 'uint8',\n",
    "                         'height': out_img.shape[1],\n",
    "                         'width': out_img.shape[2],\n",
    "                         'transform': out_transform})\n",
    "        outpath = f'{country}/clipped_rasters/{folder}/{admin}.tif'\n",
    "        with rs.open(outpath, 'w', **out_meta) as dest:\n",
    "            dest.write(out_img)\n",
    "        out_img = None\n",
    "        out_transform = None\n",
    "        return None\n",
    "    \n",
    "    tof_raster_path = f'{country}/{country}_tof_padded.tif'\n",
    "    hansen_raster_path = f'{country}/{country}_hansen_treecover2010_wloss.tif'\n",
    "    esa_raster_path = 'ESACCI-LC-L4-LCCS-Map-300m-P1Y-2015-v2.0.7.tif'\n",
    "    \n",
    "    files_to_process = [tof_raster_path, hansen_raster_path, esa_raster_path]\n",
    "    types_to_process = ['tof', 'hansen', 'esa']\n",
    "    \n",
    "    for file, file_type in zip(files_to_process, types_to_process):\n",
    "        with rs.open(file) as raster:\n",
    "            for polygon, admin in zip(shapefile.geometry, shapefile.NAME_1):\n",
    "                #print(f\"Clipping {admin}: {file_type}\")\n",
    "                mask_raster(polygon, admin, raster, file_type)\n",
    "\n",
    "    \n",
    "    # delete Tof and Hansen files once clippings created\n",
    "    os.remove(f'{country}/{country}_hansen_treecover2010_wloss.tif')\n",
    "    os.remove(f'{country}/{country}_tof_padded.tif')\n",
    "    os.remove(f'{country}/{country}_tof_padded.tfw')\n",
    "    \n",
    "    print(f\"{country}'s rasters clipped and saved.\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample to Match Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_extent_and_res(source, reference, out_filename, tof=False, esa=False):\n",
    "\n",
    "    '''\n",
    "    GDALâ€™s nearest neighbor interpolation is used match the \n",
    "    projection, bounding box and dimensions of the source dataset \n",
    "    to the reference dataset. \n",
    "    '''\n",
    "    \n",
    "    # set up the source file \n",
    "    src = gdal.Open(source, gdalconst.GA_ReadOnly)\n",
    "    src_proj = src.GetProjection()\n",
    "    src_geotrans = src.GetGeoTransform()\n",
    "\n",
    "    # set up the reference file (esa)\n",
    "    ref_ds = gdal.Open(reference, gdalconst.GA_ReadOnly)\n",
    "    ref_proj = ref_ds.GetProjection()\n",
    "    ref_geotrans = ref_ds.GetGeoTransform()\n",
    "    \n",
    "    # create height/width for the interpolation (ref dataset except for tof)\n",
    "    width = ref_ds.RasterXSize if not tof else src.RasterXSize\n",
    "    height = ref_ds.RasterYSize if not tof else src.RasterYSize\n",
    "\n",
    "    out = gdal.GetDriverByName('GTiff').Create(out_filename, width, height, 1, gdalconst.GDT_Byte, options=['COMPRESS=LZW'])\n",
    "    rb = out.GetRasterBand(1)\n",
    "    rb.SetNoDataValue(255)\n",
    "    \n",
    "    # do not adjust the bounds for esa, use source (esa)\n",
    "    if esa:\n",
    "        ref_proj = src_proj\n",
    "    \n",
    "    # set geotrans, proj and no data val for the out file\n",
    "    out.SetGeoTransform(ref_geotrans)\n",
    "    out.SetProjection(ref_proj)\n",
    "    \n",
    "    interpolation = gdalconst.GRA_NearestNeighbour\n",
    "    gdal.ReprojectImage(src, out, src_proj, ref_proj, interpolation)\n",
    "    \n",
    "    src = None\n",
    "    ref_ds = None\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_extent_res(country):\n",
    "    \n",
    "    '''\n",
    "    Applies match_raster_extent_and_res() to all admin files\n",
    "    for a country. The ESA and Hansen data are upsampled to match \n",
    "    TOF at 10m resolution. TOF and Hansen et al. data are resized to \n",
    "    match the dimensions and bounding box of the ESA data.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(f'{country}/resampled_rasters/hansen'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/hansen')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/resampled_rasters/tof'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/tof')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/resampled_rasters/esa'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/esa')\n",
    "        \n",
    "    \n",
    "    # import new shapefile containing only polygons\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries_exp.geojson')\n",
    "    admin_boundaries = list(shapefile.NAME_1)\n",
    "    \n",
    "    for admin in admin_boundaries:\n",
    "        \n",
    "        # apply to esa\n",
    "        match_extent_and_res(f'{country}/clipped_rasters/esa/{admin}.tif', # source\n",
    "                             f'{country}/clipped_rasters/tof/{admin}.tif', # reference\n",
    "                             f'{country}/resampled_rasters/esa/{admin}.tif', # outpath\n",
    "                             tof = False, \n",
    "                             esa = True) \n",
    "        \n",
    "        # apply to tof\n",
    "        match_extent_and_res(f'{country}/clipped_rasters/tof/{admin}.tif', \n",
    "                             f'{country}/resampled_rasters/esa/{admin}.tif', \n",
    "                             f'{country}/resampled_rasters/tof/{admin}.tif', \n",
    "                             tof = True, \n",
    "                             esa = False) \n",
    "        \n",
    "        # apply to hansen\n",
    "        match_extent_and_res(f'{country}/clipped_rasters/hansen/{admin}.tif', \n",
    "                             f'{country}/resampled_rasters/esa/{admin}.tif', \n",
    "                             f'{country}/resampled_rasters/hansen/{admin}.tif', \n",
    "                             tof = False, \n",
    "                             esa = False) \n",
    "        \n",
    "        # assert no data value added correctly in tof rasters\n",
    "        tof = rs.open(f'{country}/resampled_rasters/tof/{admin}.tif').read(1)\n",
    "        assert tof.max() <= 255\n",
    "        tof.close()\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Admin Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_polygons(country):\n",
    "    '''\n",
    "    Takes in a country's resampled rasters and identifies\n",
    "    which admin boundaries are composed of multipolygons. Combines individual files\n",
    "    into one for the admin district, then deletes the individual files.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries_exp.geojson')\n",
    "    admin_boundaries_all = list(shapefile.NAME_1)\n",
    "    \n",
    "    # creates a list of admins that need to be merged (digits in filename)\n",
    "    no_ints = []\n",
    "    for admin in admin_boundaries_all:\n",
    "        \n",
    "        # if any characters are digits, remove them and ad admin to list\n",
    "        if any(char.isdigit() for char in admin):\n",
    "            clean_admin = ''.join([char for char in admin if not char.isdigit()])\n",
    "            no_ints.append(clean_admin)\n",
    "\n",
    "    no_ints = list(set(no_ints))\n",
    "    print(f'{len(no_ints)} admins will be merged: {no_ints}')\n",
    "\n",
    "    datasets = ['tof', 'hansen', 'esa']\n",
    "    \n",
    "    for data in datasets:\n",
    "        for admin_2 in no_ints:\n",
    "\n",
    "            # gather list of files for that admin (ex: Puntarenas1.tif, Puntarenas2.tif, Puntarenas3.tif)\n",
    "            files_to_merge = [] # items need to be in dataset reader mode\n",
    "            files_to_delete = [] # items are just string of the file name\n",
    "\n",
    "            for path in glob.glob(f'{country}/resampled_rasters/{data}/{admin_2}?.tif'):\n",
    "                filename = os.path.basename(path) \n",
    "                files_to_delete.append(filename)\n",
    "                src = rs.open(f'{country}/resampled_rasters/{data}/{filename}')\n",
    "                files_to_merge.append(src)\n",
    "\n",
    "            # capture double digits\n",
    "            for path in glob.glob(f'{country}/resampled_rasters/{data}/{admin_2}??.tif'):\n",
    "                filename = os.path.basename(path) \n",
    "                files_to_delete.append(filename)\n",
    "                src = rs.open(f'{country}/resampled_rasters/{data}/{filename}')\n",
    "                files_to_merge.append(src)\n",
    "\n",
    "            # capture triple digits\n",
    "            for path in glob.glob(f'{country}/resampled_rasters/{data}/{admin_2}???.tif'):\n",
    "                filename = os.path.basename(path) \n",
    "                files_to_delete.append(filename)\n",
    "                src = rs.open(f'{country}/resampled_rasters/{data}/{filename}')\n",
    "                files_to_merge.append(src)\n",
    "\n",
    "            if len(files_to_merge) < 1:\n",
    "                print(f'No files to merge in {data}.')\n",
    "\n",
    "            mosaic, out_transform = merge(files_to_merge)\n",
    "\n",
    "            outpath = f'{country}/resampled_rasters/{data}/{admin_2}.tif'\n",
    "            out_meta = src.meta.copy()\n",
    "            out_meta.update({'driver': \"GTiff\",\n",
    "                             'dtype': 'uint8',\n",
    "                             'height': mosaic.shape[1],\n",
    "                             'width': mosaic.shape[2],\n",
    "                             'transform': out_transform})\n",
    "\n",
    "            with rs.open(outpath, \"w\", **out_meta) as dest:\n",
    "                dest.write(mosaic)\n",
    "\n",
    "            # delete the old separated tifs\n",
    "            for file in files_to_delete:\n",
    "                os.remove(f'{country}/resampled_rasters/{data}/{file}')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_4d(raster):\n",
    "    \n",
    "    '''\n",
    "    Takes in a GTiff, identifies the dimensions and them down to the nearest 10th.\n",
    "    Returns a reshaped 10x10 grid array. \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    raster : str\n",
    "        GTiff that will be reshaped\n",
    "    '''\n",
    "    \n",
    "    def round_down(num, divisor):\n",
    "         return num - (num%divisor)\n",
    "   \n",
    "    # round down rows and cols to nearest 10th\n",
    "    rows, cols = round_down(raster.shape[0], 10), round_down(raster.shape[1], 10)\n",
    "    \n",
    "    # clip according to rounded numbers and reshape\n",
    "    rounded = raster[:rows, :cols]\n",
    "    reshaped = np.reshape(rounded, (rounded.shape[0] // 10, 10, rounded.shape[1] // 10, 10))\n",
    "        \n",
    "    return reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(country):\n",
    "    \n",
    "    '''\n",
    "    Takes in a country to import appropriate tof/hansen/esa rasters. Returns a csv \n",
    "    with statistics per administrative district, per land cover class and per tree cover\n",
    "    threshold.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(f'{country}/stats'):\n",
    "        os.makedirs(f'{country}/stats')\n",
    "        \n",
    "    # set up the dataframe\n",
    "    df = pd.DataFrame(columns=['country','admin','esa_id','esa_class',\n",
    "                               'esa_sampled_ha','esa_total_ha','tree_cover_class',\n",
    "                               'tof_ha','hans_ha', 'tof_mean', 'hans_mean']) \n",
    "    counter = 0\n",
    "    \n",
    "    folder_contents = [f for f in os.listdir(f'{country}/resampled_rasters/tof') if f != '.ipynb_checkpoints']\n",
    "    \n",
    "    # iterate through the admins\n",
    "    for file in folder_contents:\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        tof = rs.open(f'{country}/resampled_rasters/tof/{file}').read(1).astype(np.float32)\n",
    "        hans = rs.open(f'{country}/resampled_rasters/hansen/{file}').read(1).astype(np.float32)\n",
    "        esa = rs.open(f'{country}/resampled_rasters/esa/{file}').read(1).astype(np.float32)\n",
    "        \n",
    "        lower_rng = [x for x in range(0, 100, 10)]\n",
    "        upper_rng = [x for x in range(10, 110, 10)]\n",
    "\n",
    "        # convert values to their median for binning\n",
    "        for lower, upper in zip(lower_rng, upper_rng):\n",
    "            \n",
    "            tof[(tof >= lower) & (tof < upper)] = lower + 4.5\n",
    "            hans[(hans >= lower) & (hans < upper)] = lower + 4.5\n",
    "    \n",
    "        # iterate through the land cover classes\n",
    "        esa_classes = np.unique(esa)\n",
    "        \n",
    "        if 0 and 255 in esa_classes:\n",
    "            print('ESA contains lc labels 0 and 255.')\n",
    "        \n",
    "        for cover in esa_classes:\n",
    "            \n",
    "            # change all values that are not equal to the lcc to NaN including no data vals\n",
    "            tof_class = tof.copy()\n",
    "            tof_class[esa != cover] = np.nan \n",
    "            tof_class[tof_class == 255] = np.nan\n",
    "\n",
    "            # reshape and calculate stats\n",
    "            # if the entire array in NaNs then tof mean = 0\n",
    "            tof_reshaped = reshape_to_4d(tof_class) \n",
    "            tof_class_mean = np.nanmean(tof_reshaped)\n",
    "            tof_class_mean_per_ha = np.nanmean(tof_reshaped, axis=(1,3))\n",
    "\n",
    "            # same for Hansen\n",
    "            hans_class = hans.copy()\n",
    "            hans_class[esa != cover] = np.nan\n",
    "            hans_class[hans_class == 255] = np.nan\n",
    "\n",
    "            hans_reshaped = reshape_to_4d(hans_class)\n",
    "            hans_class_mean = np.nanmean(hans_reshaped)\n",
    "            hans_class_mean_per_ha = np.nanmean(hans_reshaped, axis=(1,3)) \n",
    "\n",
    "            # iterate through the thresholds (0-10, 10-20, 20-30)\n",
    "            for lower, upper in zip(lower_rng, upper_rng):\n",
    "\n",
    "                # calculate total ha for that threshold \n",
    "                tof_bin = np.sum((tof_class_mean_per_ha >= lower) & (tof_class_mean_per_ha < upper))\n",
    "                hans_bin = np.sum((hans_class_mean_per_ha >= lower) & (hans_class_mean_per_ha < upper))\n",
    "                bin_name = (f'{str(lower)}-{str(upper - 1)}')\n",
    "    \n",
    "                # area of lc sampled (tof is NOT null) and total area (esa raster equals cover)\n",
    "                # /100 converts 10m data to hectares\n",
    "                lc_sampled = np.sum(~np.isnan(tof_class)) / 100   \n",
    "                \n",
    "                # need to ensure this counts the no data class correctly (no data label is 0.0)\n",
    "                lc_total = np.count_nonzero(esa == cover)/100 if cover == 0.0 else np.sum(esa == cover)/100\n",
    "                \n",
    "                # check for erroneous calculations\n",
    "                if lc_sampled > lc_total:\n",
    "                    raise ValueError(f'Sampled area is greater than total area for land cover {cover} in {file}.')\n",
    "                    \n",
    "                df = df.append({'country': country, \n",
    "                               'admin': file[:-4],\n",
    "                               'esa_id': cover,\n",
    "                               'esa_sampled_ha': lc_sampled,\n",
    "                               'esa_total_ha': lc_total,\n",
    "                               'tree_cover_class': bin_name,\n",
    "                               'tof_ha': tof_bin,\n",
    "                               'hans_ha': hans_bin,\n",
    "                               'tof_mean': tof_class_mean, \n",
    "                               'hans_mean': hans_class_mean},\n",
    "                                ignore_index=True)\n",
    "        \n",
    "        # map ESA id numbers to lcc labels\n",
    "        esa_legend = {0: 'ESA No Data',\n",
    "                10: 'Cropland, rainfed',\n",
    "                11: 'Cropland, rainfed',\n",
    "                12: 'Cropland, rainfed',\n",
    "                20: 'Cropland, irrigated or post-flooding',\n",
    "                30: 'Mosaic cropland / natural vegetation',\n",
    "                40: 'Mosaic natural vegetation / cropland',\n",
    "                50: 'Tree cover, broadleaved, evergreen',\n",
    "                60: 'Tree cover, broadleaved, deciduous',\n",
    "                70: 'Tree cover, needleleaved, evergreen',\n",
    "                80: 'Tree cover, needleleaved, deciduous',\n",
    "                90: 'Tree cover, mixed leaf type',\n",
    "                100: 'Mosaic tree and shrub / herbaceous cover',\n",
    "                110: 'Mosaic herbaceous cover / tree and shrub',\n",
    "                120: 'Shrubland',\n",
    "                130: 'Grassland',\n",
    "                140: 'Lichens and mosses',\n",
    "                150: 'Sparse vegetation',\n",
    "                160: 'Tree cover, flooded, fresh or brakish water',\n",
    "                170: 'Tree cover, flooded, saline water',\n",
    "                180: 'Shrub or herbaceous cover, flooded, fresh/saline/brakish water',\n",
    "                190: 'Urban areas',\n",
    "                200: 'Bare areas',\n",
    "                210: 'Water bodies',\n",
    "                220: 'Permanent snow and ice',\n",
    "                255: 'No Data (flag)'}\n",
    "     \n",
    "        df['esa_class'] = df['esa_id'].map(esa_legend)\n",
    "        \n",
    "        # close datasets to save mem\n",
    "        tof.close()\n",
    "        hans.close()\n",
    "        esa.close()\n",
    "        \n",
    "        if counter % 3 == 0:\n",
    "            print(f'{counter}/{len(folder_contents)} admins processed...')\n",
    "    \n",
    "    df.to_csv(f'{country}/stats/{country}_statistics.csv', index=False)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_pipe(country):\n",
    "    print('Converting shapefile to geojson...')\n",
    "    shape_to_gjson(country)\n",
    "    print('Building Hansen tree cover raster...')\n",
    "    create_hansen_tif(country)\n",
    "    print('Removing tree cover loss...')\n",
    "    remove_loss(country)\n",
    "    print('Padding tof raster...')\n",
    "    pad_tcl_raster(country)\n",
    "    print('Clipping rasters by admin boundary...')\n",
    "    create_clippings(country)\n",
    "    print('Resampling to match raster extents and resolutions...')\n",
    "    apply_extent_res(country)\n",
    "    print('Merging admins containing multiple polygons...')\n",
    "    merge_polygons(country)\n",
    "    print('Data preparation complete.')\n",
    "    print('Calculating statistics...')\n",
    "    calculate_stats(country)\n",
    "    print('Analysis complete.')\n",
    "    return None\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "174.006px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
