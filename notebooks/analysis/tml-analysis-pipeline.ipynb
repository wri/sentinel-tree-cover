{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from time import time, strftime\n",
    "import os\n",
    "import os.path\n",
    "import boto3\n",
    "import confuse\n",
    "import rasterio as rs\n",
    "from rasterio.mask import mask\n",
    "from rasterio.merge import merge\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "import numpy as np \n",
    "import numpy.ma as ma \n",
    "import geopandas as gpd \n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "import pandas as pd\n",
    "import pandas.api.types as ptypes\n",
    "import fiona\n",
    "from contextlib import contextmanager  \n",
    "from skimage.transform import resize\n",
    "import math\n",
    "import requests\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "import osgeo\n",
    "from osgeo import gdal\n",
    "from osgeo import gdalconst\n",
    "import glob\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "import scipy\n",
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geojson_admin2(country, gadm_filepath):\n",
    "    \n",
    "    filepath = f'admin_boundaries/{gadm_filepath}.json'\n",
    "    shapefile = gpd.read_file(filepath)\n",
    "    \n",
    "    # if there are duplicate admin 2 names\n",
    "    if shapefile.NAME_2.duplicated().sum() > 0:\n",
    "        \n",
    "        # create a df of the duplicates\n",
    "        dups = shapefile[shapefile.NAME_2.duplicated()]\n",
    "        \n",
    "        # iterate by index and update the name to combine admin 1 and 2 names\n",
    "        for row, column in dups.iterrows():\n",
    "            shapefile.loc[row,['NAME_2']] = shapefile.loc[row,['NAME_1']][0] + '_' + shapefile.loc[row,['NAME_2']][0]\n",
    "    \n",
    "    # run assertions\n",
    "    assert shapefile.NAME_2.duplicated().sum() == 0\n",
    "    assert shapefile.crs == 'epsg:4326'\n",
    "    \n",
    "    # save file\n",
    "    shapefile.to_file(f'admin_boundaries/{country}_adminboundaries2.geojson', driver='GeoJSON')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    '''\n",
    "    Prints the runtime of the decorated function.\n",
    "    '''\n",
    "    \n",
    "    @functools.wraps(func)\n",
    "    def wrapper_timer(*args, **kwargs):\n",
    "        start = datetime.now() \n",
    "        value = func(*args, **kwargs)\n",
    "        end = datetime.now() \n",
    "        run_time = end - start\n",
    "        print(f'Completed {func.__name__!r} in {run_time}.')\n",
    "        return value\n",
    "    return wrapper_timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_inputs(country):\n",
    "    \n",
    "    if not os.path.exists(f'{country}/'):\n",
    "        os.makedirs(f'{country}/')\n",
    "    \n",
    "    config = confuse.Configuration('sentinel-tree-cover')\n",
    "    config.set_file('/Users/jessica.ertel/sentinel-tree-cover/jessica-config.yaml')\n",
    "    aws_access_key = config['aws']['aws_access_key_id']\n",
    "    aws_secret_key = config['aws']['aws_secret_access_key']\n",
    "    s3 = boto3.client('s3', aws_access_key_id=aws_access_key.as_str(), aws_secret_access_key=aws_secret_key.as_str())\n",
    "    \n",
    "    # download 10m res country tif (removing white spaces)\n",
    "    s3.download_file('tof-output', \n",
    "                     f'2020/mosaics/{country.replace(\" \", \"\")}.tif', \n",
    "                     f'{country}/{country}.tif')\n",
    "    \n",
    "    # download admin 1 boundaries\n",
    "    s3.download_file('tof-output', \n",
    "                     f'2020/analysis/2020-full/admin_boundaries/{country}_adminboundaries2.geojson', \n",
    "                     f'{country}/{country}_adminboundaries2.geojson')\n",
    "    print(f'{country} files downloaded.')\n",
    "   \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hansen Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hansen_tif(country):\n",
    "    '''\n",
    "    Identifies the lat/lon coordinates for a single country \n",
    "    to download Hansen 2010 tree cover and 2020 tree cover loss tif files. \n",
    "    Returns combined tifs as one file in the country's folder.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    \n",
    "    '''\n",
    "    gdal.UseExceptions()\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries.geojson')\n",
    "\n",
    "    if not os.path.exists(f'hansen_treecover2010'):\n",
    "        os.makedirs(f'hansen_treecover2010')\n",
    "    \n",
    "    if not os.path.exists(f'hansen_lossyear2020'):\n",
    "        os.makedirs(f'hansen_lossyear2020')\n",
    "    \n",
    "    # identify min/max bounds for the country\n",
    "    bounds = shapefile.geometry.bounds\n",
    "    min_x = bounds.minx.min() \n",
    "    min_y = bounds.miny.min()\n",
    "    max_x = bounds.maxx.max()\n",
    "    max_y = bounds.maxy.max()\n",
    "\n",
    "    # identify the lowest and highest 10 lat/lon increments for the country\n",
    "    lower_x = math.floor(min_x / 10) * 10 \n",
    "    lower_y = math.ceil(min_y / 10) * 10 \n",
    "    upper_x = math.ceil(max_x / 10) * 10 \n",
    "    upper_y = math.ceil(max_y / 10) * 10\n",
    "    \n",
    "    print('Downloading files from GLAD...')\n",
    "    \n",
    "    for x_grid in range(lower_x, upper_x, 10):\n",
    "        for y_grid in range(lower_y, upper_y + 10, 10):\n",
    "            \n",
    "            lon = 'N' if y_grid >= 0 else 'S'\n",
    "            lat = 'E' if x_grid >= 0 else 'W'\n",
    "            \n",
    "            # establish urls\n",
    "            lon_lat = f'{str(np.absolute(y_grid)).zfill(2)}{lon}_{str(np.absolute(x_grid)).zfill(3)}{lat}.tif'\n",
    "            cover_url = f'https://storage.googleapis.com/earthenginepartners-hansen/GFC2015/Hansen_GFC2015_treecover2000_{lon_lat}'\n",
    "            cover_dest = f'hansen_treecover2010/{lon_lat}'\n",
    "            loss_url = f'https://storage.googleapis.com/earthenginepartners-hansen/GFC-2020-v1.8/Hansen_GFC-2020-v1.8_lossyear_{lon_lat}'\n",
    "            loss_dest = f'hansen_lossyear2020/{lon_lat}'\n",
    "\n",
    "            # download tree cover and loss files from UMD website\n",
    "            try:\n",
    "                urllib.request.urlretrieve(cover_url, cover_dest)\n",
    "            except urllib.error.HTTPError as err:\n",
    "                if err.code == 404:\n",
    "                    print(f'HTTP Error 404 for tree cover data: {cover_url}')\n",
    "                    pass\n",
    "            \n",
    "            try:\n",
    "                urllib.request.urlretrieve(loss_url, loss_dest)\n",
    "            except urllib.error.HTTPError as err:\n",
    "                if err.code == 404:\n",
    "                    print(f'HTTP Error 404 for tree cover loss data: {loss_url}')\n",
    "                    pass\n",
    "    \n",
    "    # if the tree cover file doesn't exist, remove loss file\n",
    "    for tif in os.listdir('hansen_lossyear2020/'):\n",
    "        if tif not in os.listdir('hansen_treecover2010/'):\n",
    "            os.remove(f'hansen_lossyear2020/{tif}')\n",
    "\n",
    "    # create list of tifs and ensure no duplicates\n",
    "    tree_tifs = glob.glob('hansen_treecover2010/*.tif')\n",
    "    loss_tifs = glob.glob('hansen_lossyear2020/*.tif')\n",
    "    \n",
    "    # convert tree cover and loss tifs into a virtual raster tile  \n",
    "    gdal.BuildVRT(f'{country}/{country}_hansen_treecover2010.vrt', tree_tifs)\n",
    "    gdal.BuildVRT(f'{country}/{country}_hansen_loss2020.vrt', loss_tifs)\n",
    "\n",
    "    # open vrts and convert to a single .tif -- adding tfw=yes increases file size significantly\n",
    "    translateoptions = gdal.TranslateOptions(format='Gtiff', \n",
    "                                              outputSRS='EPSG:4326',\n",
    "                                              outputType=gdal.GDT_Byte,\n",
    "                                              noData=255,\n",
    "                                              creationOptions=['COMPRESS=LZW'], #BIGTIFF = YES\n",
    "                                              resampleAlg='nearest')\n",
    " \n",
    "    source = gdal.Open(f'{country}/{country}_hansen_treecover2010.vrt', )\n",
    "    ds = gdal.Translate(f'{country}/{country}_hansen_treecover2010.tif', source, options=translateoptions)\n",
    "    os.remove(f'{country}/{country}_hansen_treecover2010.vrt')\n",
    "    source = None\n",
    "    ds = None\n",
    "                      \n",
    "    source = gdal.Open(f'{country}/{country}_hansen_loss2020.vrt')\n",
    "    ds = gdal.Translate(f'{country}/{country}_hansen_loss2020.tif', source, options=translateoptions)\n",
    "    os.remove(f'{country}/{country}_hansen_loss2020.vrt')\n",
    "    source = None\n",
    "    ds = None\n",
    "    \n",
    "    assert os.path.exists(f'{country}/{country}_hansen_treecover2010.tif')\n",
    "    assert os.path.exists(f'{country}/{country}_hansen_loss2020.tif')\n",
    "\n",
    "    # if new files are properly create, delete what is not needed\n",
    "    for file in tree_tifs:\n",
    "        os.remove(file)\n",
    "    \n",
    "    for file in loss_tifs:\n",
    "        os.remove(file)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Hansen tree cover loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_loss(country):\n",
    "    '''\n",
    "    Imports hansen tree cover loss tifs for a single country. Updates tree cover \n",
    "    to 0 if loss was detected between 2011-2020. Returns updated tif in the country's \n",
    "    folder.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "    gdal.UseExceptions()\n",
    "    hansen_cover = rs.open(f'{country}/{country}_hansen_treecover2010.tif').read(1) \n",
    "    hansen_loss = rs.open(f'{country}/{country}_hansen_loss2020.tif').read(1)\n",
    "    \n",
    "     # assert raster shape, datatype and max/min values\n",
    "    assert hansen_cover.dtype == 'uint8'\n",
    "    assert hansen_cover.shape != (0, ) and len(hansen_cover.shape) <= 2\n",
    "    assert hansen_cover.max() <= 100 and hansen_cover.min() >= 0\n",
    "    assert hansen_loss.dtype == 'uint8'\n",
    "    assert hansen_loss.shape != (0, ) and len(hansen_loss.shape) <= 2\n",
    "    assert hansen_loss.max() <= 20 and hansen_cover.min() >= 0\n",
    "    \n",
    "    # If there was loss between 2011-2020, make then 0 in tree cover\n",
    "    sum_before_loss = np.sum(hansen_cover > 0) \n",
    "    hansen_cover[(hansen_loss >= 11)] = 0.\n",
    "    \n",
    "    # check bin counts after loss removed\n",
    "    print(f'{sum_before_loss - (np.sum(hansen_cover > 0))} tree cover pixels converted to loss.')\n",
    "    \n",
    "    # write as a new file\n",
    "    out_meta = rs.open(f'{country}/{country}_hansen_treecover2010.tif').meta\n",
    "    out_meta.update({'driver': 'GTiff',    \n",
    "                     'dtype': 'uint8',\n",
    "                     'height': hansen_cover.shape[0],\n",
    "                     'width': hansen_cover.shape[1],\n",
    "                     'count': 1,\n",
    "                     'compress':'lzw'})\n",
    "    outpath = f'{country}/{country}_hansen_treecover2010_wloss.tif'\n",
    "    with rs.open(outpath, 'w', **out_meta) as dest:\n",
    "            dest.write(hansen_cover, 1) \n",
    "    \n",
    "    # remove original hansen tree cover and loss files\n",
    "    os.remove(f'{country}/{country}_hansen_treecover2010.tif')\n",
    "    os.remove(f'{country}/{country}_hansen_loss2020.tif')\n",
    "    hansen_cover = None\n",
    "    hansen_loss = None \n",
    "    \n",
    "    print('Hansen raster built.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad TML Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tml_raster(country):\n",
    "    \n",
    "    '''\n",
    "    Increase the TML raster extent to match the bounds of a country's shapefile\n",
    "    and fill with no data value to facilitate clipping.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "    \n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries2.geojson')\n",
    "\n",
    "    # identify min/max bounds for the country\n",
    "    bounds = shapefile.geometry.bounds\n",
    "    min_x = bounds.minx.min() \n",
    "    min_y = bounds.miny.min()\n",
    "    max_x = bounds.maxx.max()\n",
    "    max_y = bounds.maxy.max()\n",
    "    \n",
    "    # create new bounds by rounding to the nearest .1 lat/lon \n",
    "    lower_x = math.floor(min_x * 10) / 10 \n",
    "    lower_y = math.floor(min_y * 10) / 10 \n",
    "    upper_x = math.ceil(max_x * 10) / 10\n",
    "    upper_y = math.ceil(max_y * 10) / 10\n",
    "          \n",
    "    # create tif with new bounds\n",
    "    warp_options = gdal.WarpOptions(format='GTiff', \n",
    "                                    dstSRS='EPSG:4326',\n",
    "                                    dstNodata=255,\n",
    "                                    outputBounds=[lower_x, lower_y, upper_x, upper_y],\n",
    "                                    resampleAlg='near',\n",
    "                                    outputType=osgeo.gdalconst.GDT_Byte,\n",
    "                                    creationOptions=['TFW=YES', 'COMPRESS=LZW', 'BIGTIFF=YES'])   \n",
    "        \n",
    "    ds = gdal.Warp(f'{country}/{country}_tof_padded.tif', \n",
    "                   f'{country}/{country}.tif',      \n",
    "                   options=warp_options)\n",
    "    \n",
    "    ds = None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Clip Rasters by Admin Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def mask_raster(polygon, admin, raster, folder):\n",
    "        \n",
    "    out_img, out_transform = mask(dataset=raster, shapes=[polygon], crop=True, nodata=255, filled=True)\n",
    "    out_meta = raster.meta\n",
    "    out_meta.update({'driver': 'GTiff',    \n",
    "                     'dtype': 'uint8',\n",
    "                     'height': out_img.shape[1],\n",
    "                     'width': out_img.shape[2],\n",
    "                     'transform': out_transform,\n",
    "                     'compress':'lzw'})\n",
    "    outpath = f'{country}/clipped_rasters/{folder}/{admin}.tif'\n",
    "    with rs.open(outpath, 'w', **out_meta) as dest:\n",
    "        dest.write(out_img)\n",
    "    out_img = None\n",
    "    out_transform = None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clippings(country, multi_analysis):\n",
    "    '''\n",
    "    Takes in a country name to import tof/hansen rasters and masks out administrative \n",
    "    boundaries based on the shapefile. Saves exploded shapefile as a geojson with polygons \n",
    "    split/numbered for each admin boundary. Returns clipped rasters as individual \n",
    "    files in the country's \"clipped_rasters\" folder. Deletes the original Hansen file. \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import        \n",
    "    '''\n",
    "    \n",
    "    if multi_analysis:\n",
    "        if not os.path.exists(f'{country}/clipped_rasters/hansen'):\n",
    "            os.makedirs(f'{country}/clipped_rasters/hansen')\n",
    "\n",
    "    if not os.path.exists(f'{country}/clipped_rasters/tof'):\n",
    "        os.makedirs(f'{country}/clipped_rasters/tof')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/clipped_rasters/esa'):\n",
    "        os.makedirs(f'{country}/clipped_rasters/esa')\n",
    "    \n",
    "    orig_shapefile = gpd.read_file(f'{country}/{country}_adminboundaries2.geojson')\n",
    "    \n",
    "    # preprocess shapefile from multipolygon to single\n",
    "    counter = 0\n",
    "    for idx, row in orig_shapefile.iterrows():\n",
    "        counter += 1 if type(row.geometry) == MultiPolygon else 0\n",
    "\n",
    "    if counter > 0:\n",
    "        shapefile = orig_shapefile.explode(index_parts=True)\n",
    "        \n",
    "        # add integer to admin name if multi polys\n",
    "        shapefile.NAME_2 = np.where(shapefile.NAME_2.duplicated(keep=False), \n",
    "                                     shapefile.NAME_2 + shapefile.groupby('NAME_2').cumcount().add(1).astype(str),\n",
    "                                     shapefile.NAME_2)\n",
    "\n",
    "        shapefile = shapefile.reset_index()\n",
    "        shapefile.drop(columns=['level_0', 'level_1'], inplace=True)\n",
    "    \n",
    "    # if no multi polys save original shapefile under new name\n",
    "    else:\n",
    "        shapefile = orig_shapefile\n",
    "    \n",
    "    shapefile.to_file(f'{country}/{country}_adminboundaries_exp.geojson', driver='GeoJSON')\n",
    "    \n",
    "    def mask_raster(polygon, admin, raster, folder):\n",
    "        \n",
    "        out_img, out_transform = mask(dataset=raster, shapes=[polygon], crop=True, nodata=255, filled=True)\n",
    "        out_meta = raster.meta\n",
    "        out_meta.update({'driver': 'GTiff',    \n",
    "                         'dtype': 'uint8',\n",
    "                         'height': out_img.shape[1],\n",
    "                         'width': out_img.shape[2],\n",
    "                         'transform': out_transform,\n",
    "                         'compress':'lzw'})\n",
    "        outpath = f'{country}/clipped_rasters/{folder}/{admin}.tif'\n",
    "        with rs.open(outpath, 'w', **out_meta) as dest:\n",
    "            dest.write(out_img)\n",
    "        out_img = None\n",
    "        out_transform = None\n",
    "        return None\n",
    "    \n",
    "    tof_raster_path = f'{country}/{country}_tof_padded.tif'\n",
    "    esa_raster_path = 'ESACCI-LC-L4-LCCS-Map-300m-P1Y-2015-v2.0.7.tif'\n",
    "    \n",
    "    files_to_process = [tof_raster_path, esa_raster_path] \n",
    "    types_to_process = ['tof', 'esa'] \n",
    "    \n",
    "    if multi_analysis:\n",
    "        files_to_process.append(f'{country}/{country}_hansen_treecover2010_wloss.tif')\n",
    "        types_to_process.append('hansen')\n",
    "    \n",
    "    for file, file_type in zip(files_to_process, types_to_process):\n",
    "        with rs.open(file) as raster:\n",
    "            for polygon, admin in zip(shapefile.geometry, shapefile.NAME_2):\n",
    "                mask_raster(polygon, admin, raster, file_type)\n",
    "    \n",
    "    # delete Tof and Hansen files once clippings created \n",
    "    os.remove(f'{country}/{country}_tof_padded.tif')\n",
    "    os.remove(f'{country}/{country}_tof_padded.tfw')\n",
    "    if multi_analysis:\n",
    "        os.remove(f'{country}/{country}_hansen_treecover2010_wloss.tif')\n",
    "        \n",
    "    print(f\"{country}'s rasters clipped and saved.\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample to Match Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_extent_and_res(source, reference, out_filename, tof=False, esa=False):\n",
    "\n",
    "    '''\n",
    "    GDAL’s nearest neighbor interpolation is used match the \n",
    "    projection, bounding box and dimensions of the source dataset \n",
    "    to the reference dataset. \n",
    "    '''\n",
    "    \n",
    "    # set up the source file \n",
    "    src = gdal.Open(source, gdalconst.GA_ReadOnly)\n",
    "    src_proj = src.GetProjection()\n",
    "    src_geotrans = src.GetGeoTransform()\n",
    "\n",
    "    # set up the reference file (esa)\n",
    "    ref_ds = gdal.Open(reference, gdalconst.GA_ReadOnly)\n",
    "    ref_proj = ref_ds.GetProjection()\n",
    "    ref_geotrans = ref_ds.GetGeoTransform()\n",
    "    \n",
    "    # create height/width for the interpolation (ref dataset except for tof)\n",
    "    width = ref_ds.RasterXSize if not tof else src.RasterXSize\n",
    "    height = ref_ds.RasterYSize if not tof else src.RasterYSize\n",
    "\n",
    "    out = gdal.GetDriverByName('GTiff').Create(out_filename, width, height, 1, gdalconst.GDT_Byte, options=['COMPRESS=LZW'])\n",
    "    rb = out.GetRasterBand(1)\n",
    "    rb.SetNoDataValue(255)\n",
    "    \n",
    "    # do not adjust the bounds for esa, use source (esa)\n",
    "    if esa:\n",
    "        ref_proj = src_proj\n",
    "    \n",
    "    # set geotrans, proj and no data val for the out file\n",
    "    out.SetGeoTransform(ref_geotrans)\n",
    "    out.SetProjection(ref_proj)\n",
    "    \n",
    "    interpolation = gdalconst.GRA_NearestNeighbour\n",
    "    gdal.ReprojectImage(src, out, src_proj, ref_proj, interpolation)\n",
    "    \n",
    "    ref_ds = None\n",
    "    src = None\n",
    "#     out = None \n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def apply_extent_res(country, multi_analysis):\n",
    "    \n",
    "    '''\n",
    "    Applies match_raster_extent_and_res() to all admin files\n",
    "    for a country. The ESA and Hansen data are upsampled to match \n",
    "    TOF at 10m resolution. TOF and Hansen et al. data are resized to \n",
    "    match the dimensions and bounding box of the ESA data.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "    \n",
    "    if multi_analysis:\n",
    "        if not os.path.exists(f'{country}/resampled_rasters/hansen'):\n",
    "            os.makedirs(f'{country}/resampled_rasters/hansen')\n",
    "\n",
    "    if not os.path.exists(f'{country}/resampled_rasters/tof'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/tof')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/resampled_rasters/esa'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/esa')\n",
    "        \n",
    "    \n",
    "    # import new shapefile containing only polygons\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries_exp.geojson')\n",
    "    admin_boundaries = list(shapefile.NAME_2)\n",
    "    \n",
    "    for admin in admin_boundaries:\n",
    "        \n",
    "        # apply to esa\n",
    "        match_extent_and_res(f'{country}/clipped_rasters/esa/{admin}.tif', # source\n",
    "                             f'{country}/clipped_rasters/tof/{admin}.tif', # reference\n",
    "                             f'{country}/resampled_rasters/esa/{admin}.tif', # outpath\n",
    "                             tof = False, \n",
    "                             esa = True) \n",
    "        \n",
    "        # apply to tof - doesn't resample, just moves tof tiffs to new folder\n",
    "        match_extent_and_res(f'{country}/clipped_rasters/tof/{admin}.tif', \n",
    "                             f'{country}/resampled_rasters/esa/{admin}.tif', \n",
    "                             f'{country}/resampled_rasters/tof/{admin}.tif', \n",
    "                             tof = True, \n",
    "                             esa = False) \n",
    "        \n",
    "        # apply to hansen\n",
    "        if multi_analysis:\n",
    "            match_extent_and_res(f'{country}/clipped_rasters/hansen/{admin}.tif', \n",
    "                                 f'{country}/resampled_rasters/esa/{admin}.tif', \n",
    "                                 f'{country}/resampled_rasters/hansen/{admin}.tif', \n",
    "                                 tof = False, \n",
    "                                 esa = False) \n",
    "        \n",
    "        # assert no data value added correctly in tof rasters\n",
    "        tof = rs.open(f'{country}/resampled_rasters/tof/{admin}.tif').read(1)\n",
    "        assert tof.max() <= 255\n",
    "        tof = None\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Admin Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_polygons(country, multi_analysis):\n",
    "    '''\n",
    "    Takes in a country's resampled rasters and identifies\n",
    "    which admin boundaries are composed of multipolygons. Combines individual files\n",
    "    into one for the admin district, then deletes the individual files.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries_exp.geojson')\n",
    "    admin_boundaries_all = list(shapefile.NAME_2)\n",
    "    \n",
    "    # creates a list of admins that need to be merged (digits in filename)\n",
    "    no_ints = []\n",
    "    for admin in admin_boundaries_all:\n",
    "        \n",
    "        # if any characters are digits, remove them and add admin to list\n",
    "        if any(char.isdigit() for char in admin):\n",
    "            clean_admin = ''.join([char for char in admin if not char.isdigit()])\n",
    "            no_ints.append(clean_admin)\n",
    "\n",
    "    no_ints = list(set(no_ints))\n",
    "    print(f'{len(no_ints)} admins will be merged: {no_ints}')\n",
    "\n",
    "    datasets = ['tof', 'esa']\n",
    "    if multi_analysis:\n",
    "        datasets.append('hansen')\n",
    "    \n",
    "    for data in datasets:\n",
    "        for admin_2 in no_ints:\n",
    "\n",
    "            # gather list of files for that admin (ex: Puntarenas1.tif, Puntarenas2.tif, Puntarenas3.tif)\n",
    "            files_to_merge = [] # items need to be in dataset reader mode\n",
    "            files_to_delete = [] # items are just filename string\n",
    "\n",
    "            for path in glob.glob(f'{country}/resampled_rasters/{data}/{admin_2}?.tif'):\n",
    "                filename = os.path.basename(path) \n",
    "                files_to_delete.append(filename)\n",
    "                src = rs.open(f'{country}/resampled_rasters/{data}/{filename}')\n",
    "                files_to_merge.append(src)\n",
    "\n",
    "            # capture double digits\n",
    "            for path in glob.glob(f'{country}/resampled_rasters/{data}/{admin_2}??.tif'):\n",
    "                filename = os.path.basename(path) \n",
    "                files_to_delete.append(filename)\n",
    "                src = rs.open(f'{country}/resampled_rasters/{data}/{filename}')\n",
    "                files_to_merge.append(src)\n",
    "\n",
    "            # capture triple digits\n",
    "            for path in glob.glob(f'{country}/resampled_rasters/{data}/{admin_2}???.tif'):\n",
    "                filename = os.path.basename(path) \n",
    "                files_to_delete.append(filename)\n",
    "                src = rs.open(f'{country}/resampled_rasters/{data}/{filename}')\n",
    "                files_to_merge.append(src)\n",
    "            \n",
    "            # capture quadruple digits\n",
    "            for path in glob.glob(f'{country}/resampled_rasters/{data}/{admin_2}????.tif'):\n",
    "                filename = os.path.basename(path) \n",
    "                files_to_delete.append(filename)\n",
    "                src = rs.open(f'{country}/resampled_rasters/{data}/{filename}')\n",
    "                files_to_merge.append(src)\n",
    "\n",
    "            if len(files_to_merge) < 1:\n",
    "                print(f'No files to merge in {data}.')\n",
    "            \n",
    "            mosaic, out_transform = merge(files_to_merge)\n",
    "\n",
    "            outpath = f'{country}/resampled_rasters/{data}/{admin_2}.tif'\n",
    "            out_meta = src.meta.copy()\n",
    "            out_meta.update({'driver': \"GTiff\",\n",
    "                             'dtype': 'uint8',\n",
    "                             'height': mosaic.shape[1],\n",
    "                             'width': mosaic.shape[2],\n",
    "                             'transform': out_transform,\n",
    "                             'compress':'lzw'})\n",
    "\n",
    "            with rs.open(outpath, \"w\", **out_meta) as dest:\n",
    "                dest.write(mosaic)\n",
    "            \n",
    "    # OPTION 2 - AUS\n",
    "#             # where rasterio.merge results in memory error, option to use gdal merge + VRT\n",
    "#             # need to use -ps for resolution?\n",
    "#             outpath = f'{country}/resampled_rasters/{data}/{admin_2}.tif'\n",
    "#             cmd = f'gdal_merge.py -o {outpath}'\n",
    "#             subprocess.call(cmd.split() + files_to_merge)\n",
    "            \n",
    "#             # now build VRT\n",
    "#             gdal.BuildVRT(f'{country}/resampled_rasters/{data}/{admin_2}_VRT.vrt', files_to_merge)\n",
    "#             translateoptions = gdal.TranslateOptions(format='Gtiff', \n",
    "#                                                       outputSRS='EPSG:4326',\n",
    "#                                                       outputType=gdal.GDT_Byte,\n",
    "#                                                       noData=255,\n",
    "#                                                       creationOptions=['COMPRESS=LZW'], \n",
    "#                                                       resampleAlg='nearest')\n",
    "\n",
    "#             source = gdal.Open(f'{country}/resampled_rasters/{data}/{admin_2}_VRT.vrt')\n",
    "#             ds = gdal.Translate(outpath, source, options=translateoptions)\n",
    "#             os.remove(f'{country}/resampled_rasters/{data}/{admin_2}.vrt')\n",
    "#             source = None\n",
    "#             ds = None\n",
    "\n",
    "            # delete the old separated tifs\n",
    "            for file in files_to_delete:\n",
    "                os.remove(f'{country}/resampled_rasters/{data}/{file}')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_check(country):\n",
    "    '''\n",
    "    Calculate the area of an admin district in hectares. Convert hectares to bytes to determine\n",
    "    if the admin can be processed on r5a.2xlarge instance. If it exceeds the processing threshold\n",
    "    flag the country and save to a csv file.\n",
    "    '''    \n",
    "    # import and create a copy\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries2.geojson')\n",
    "    shapefile = shapefile.copy()\n",
    "    \n",
    "    # convert the crs to an equal-area projection to get polygon area in m2\n",
    "    # then convert to hectares (divide the area value by 10000)\n",
    "    shapefile['area'] = shapefile['geometry'].to_crs({'init': 'epsg:3395'}).map(lambda x: x.area / 10**4)\n",
    "    \n",
    "    # calculate the size of the largest area, ha --> bytes\n",
    "    max_area = shapefile['area'].max()\n",
    "    max_bytes = max_area * 3200 \n",
    "    admin = shapefile.loc[shapefile['area'] == max_area]['NAME_2'].item()\n",
    "    \n",
    "    # create a dataframe to store details\n",
    "    too_large = pd.DataFrame(columns=['country','admin','file_size','date'], dtype=object)\n",
    "    \n",
    "    # check if it can fit into RAM, otherwise save to csv\n",
    "    # should be checking the max area in ha?\n",
    "    if max_bytes >= 6.4e10:\n",
    "        print(f'The largest admin in {country} is {admin}. Area: {round(max_area, 2)} ha')\n",
    "        print(f'Warning: That largest admin {admin} is too large to process. As np.float32, array is ({round(max_bytes/10e9, 2)} GB)')\n",
    "        too_large.append({'country': country,\n",
    "                         'largest_admin': admin,\n",
    "                         'file_size': max_bytes,\n",
    "                         'area': round(max_area, 2),\n",
    "                         'date': datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")}, ignore_index=True)\n",
    "    \n",
    "        too_large.to_csv('bigtiff_full_2020.csv', header=False)\n",
    "    else:\n",
    "        print('Passed processing check.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_4d(raster):\n",
    "    \n",
    "    '''\n",
    "    Takes in a GTiff, identifies the dimensions and them down to the nearest 10th.\n",
    "    Then uses those dimensions and reshapes to a 4 dimensional, 10x10 grid.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    raster : str\n",
    "        GTiff that will be reshaped\n",
    "    '''\n",
    "    \n",
    "    def round_down(num, divisor):\n",
    "         return num - (num%divisor)\n",
    "   \n",
    "    # round down rows and cols to nearest 10th\n",
    "    rows, cols = round_down(raster.shape[0], 10), round_down(raster.shape[1], 10)\n",
    "    \n",
    "    # clip according to rounded numbers and reshape\n",
    "    # it's possible this could be inefficient\n",
    "    rounded = raster[:rows, :cols]\n",
    "    reshaped = np.reshape(rounded, (rounded.shape[0] // 10, 10, rounded.shape[1] // 10, 10))\n",
    "        \n",
    "    return reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW\n",
    "@timer\n",
    "def calculate_stats_tml(country, extent):\n",
    "    \n",
    "    '''\n",
    "    Takes in a country and extent (full or partial) and produces zonal stats on tree cover. \n",
    "    Returns a csv with statistics per administrative district, per land cover class and \n",
    "    per tree cover threshold. Only produces statistics for TML data.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    extent : str\n",
    "        a string indicating the processing extent of the geotiff\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(f'{country}/stats'):\n",
    "        os.makedirs(f'{country}/stats')\n",
    "        \n",
    "    df = pd.DataFrame({'country': pd.Series(dtype='str'),\n",
    "                       'admin': pd.Series(dtype='str'),\n",
    "                       'esa_id': pd.Series(dtype='str'),\n",
    "                       'esa_class': pd.Series(dtype='str'),\n",
    "                       'esa_sampled_ha': pd.Series(dtype='float64'),\n",
    "                       'esa_total_ha': pd.Series(dtype='float64'),\n",
    "                       'tree_cover_class': pd.Series(dtype='str'),\n",
    "                       'tof_ha': pd.Series(dtype='int64'),\n",
    "                       'tof_mean': pd.Series(dtype='float64')})\n",
    "    counter = 0\n",
    "    \n",
    "    folder_contents = [f for f in os.listdir(f'{country}/resampled_rasters/tof') if f != '.ipynb_checkpoints' and f != '.DS_Store']\n",
    "        \n",
    "    # iterate through the admins \n",
    "    for file in folder_contents:\n",
    "        print(file)\n",
    "        counter += 1\n",
    "        \n",
    "        tof = rs.open(f'{country}/resampled_rasters/tof/{file}').read(1)\n",
    "        tof_vals = np.unique(tof)\n",
    "        \n",
    "        # if tof raster is only no data values do not process it\n",
    "        if len(tof_vals) == 1:\n",
    "            print(f'{file} only contains value {tof_vals}, skipping processing...')\n",
    "            continue\n",
    "        \n",
    "        # reshape TML admin tif to 4d array and mask where equal to 255\n",
    "        tof = reshape_to_4d(tof)\n",
    "        try:\n",
    "            tof = np.ma.masked_equal(tof, 255, copy=False) \n",
    "        except MemoryError as e:\n",
    "            print(f'Memory error for {file} with size {sys.getsizeof(tof)}')\n",
    "            return e\n",
    "        \n",
    "        # manually calculate the mean per hectare for memory purposes\n",
    "        tof_count_per_ha = np.sum(~tof.mask, axis = (1, 3), dtype=np.uint8) \n",
    "        tof_sum_per_ha = np.sum(tof, axis = (1, 3), dtype=np.uint16)\n",
    "        tof_mean_per_ha = np.divide(tof_sum_per_ha, tof_count_per_ha, dtype=np.float32)\n",
    "        \n",
    "        # OPTION 1:\n",
    "        # open esa admin tif and reshape to 4d array\n",
    "        esa = rs.open(f'{country}/resampled_rasters/esa/{file}').read(1)\n",
    "        esa_vals = np.unique(esa)\n",
    "        \n",
    "        # if esa raster is only 2 values do not process it\n",
    "#         if len(esa_vals) <= 2:\n",
    "#             print(f'{file} only contains value {esa_vals}, skipping processing...')\n",
    "#             continue\n",
    "\n",
    "        esa = reshape_to_4d(esa)\n",
    "        \n",
    "        # Set each hectare to the mode (lcc that appears most often) to prevent doouble counting \n",
    "        # The fastest way is a zipped for loop \n",
    "        for i, l in zip(range(esa.shape[0]), range(esa.shape[2])):\n",
    "            \n",
    "            # if there is > 1 unique value in a hectare of the esa tif (5-10% of cases)\n",
    "            # calculate the mode, otherwise skip this step\n",
    "            if len(np.unique(esa[i, :, l, :])) > 1:\n",
    "                esa[i, :, l, :] = scipy.stats.mode(esa[i, :, l, :].flatten())[0]\n",
    "\n",
    "        # Now that the esa array is set to the mode per hectare,\n",
    "        # We need to make it a 2D array. np.max is a safe way to reshape quickly\n",
    "        esa = np.max(esa, axis = (1, 3))\n",
    "        \n",
    "        lower_rng = [x for x in range(0, 100, 10)]\n",
    "        upper_rng = [x for x in range(10, 110, 10)]\n",
    "        \n",
    "        # Set upper to 101, otherwise it isn't inclusive of 100% hectares.\n",
    "        upper_rng[-1] = 101\n",
    "        \n",
    "        esa_classes = np.unique(esa)\n",
    "        \n",
    "        # OPTION 2: tof = None or del tof here\n",
    "        \n",
    "        for cover in esa_classes:\n",
    "            tof_class_mean_per_ha = tof_mean_per_ha.copy()\n",
    "\n",
    "            # Expand the existing no-data mask so that we calculate mean per lcc\n",
    "            # tof class mean per ha is the mean TML\n",
    "            tof_class_mean_per_ha.mask[esa != cover] = 1\n",
    "            tof_class_mean = np.round(np.mean(tof_class_mean_per_ha), 2)\n",
    "\n",
    "            # calculate the total land cover \n",
    "            lc_total = np.sum(esa == cover)\n",
    "            \n",
    "            # calculate land cover sampled - the sum of values that have not been masked out by 1\n",
    "            lc_sampled = np.sum(~tof_class_mean_per_ha.mask)\n",
    "\n",
    "            # iterate through the thresholds (0-10, 10-20, 20-30)\n",
    "            for lower, upper in zip(lower_rng, upper_rng):\n",
    "\n",
    "                # calculate total ha for that threshold \n",
    "                # if the lc sampled is a mask, then 0 area has been sampled \n",
    "                # which means tof_bin is 0 and tof mean should be NaN for that row\n",
    "                if lc_sampled == 0:\n",
    "                    tof_bin = 0\n",
    "                    tof_class_mean = np.nan\n",
    "                else:\n",
    "                    tof_bin = np.sum((tof_class_mean_per_ha >= lower) & (tof_class_mean_per_ha < upper))\n",
    "                \n",
    "                bin_name = (f'{str(lower)}-{str(upper - 1)}')\n",
    "\n",
    "                # confirm masked array doesn't propogate\n",
    "                vars_to_check = [lc_sampled, lc_total, tof_bin, tof_class_mean]\n",
    "                \n",
    "#                 for index, var in enumerate(vars_to_check):\n",
    "#                     if var == '--':\n",
    "#                         var = 0\n",
    "                    \n",
    "                    # instead set the mask to explicitly equal 0\n",
    "#                     if np.ma.isMaskedArray(var):\n",
    "#                         var = 0\n",
    "                \n",
    "                for index, var in enumerate(vars_to_check):\n",
    "                    if np.ma.isMaskedArray(var):\n",
    "                        print(f'Masked array at index {index} for {var}.')\n",
    "                \n",
    "                # check for erroneous values\n",
    "                assert lc_sampled <= lc_total, f'Sampled area is greater than total area for land cover {cover} in {file}.'\n",
    "\n",
    "                df = df.append({'country': country, \n",
    "                               'admin': file[:-4],\n",
    "                               'esa_id': cover,\n",
    "                               'esa_sampled_ha': lc_sampled,\n",
    "                               'esa_total_ha': lc_total,\n",
    "                               'tree_cover_class': bin_name,\n",
    "                               'tof_ha': tof_bin,\n",
    "                               'tof_mean': tof_class_mean},\n",
    "                                ignore_index=True)\n",
    "\n",
    "                # reinforce datatypes\n",
    "                convert_dict = {'esa_sampled_ha':'float64',\n",
    "                                'esa_total_ha':'float64',\n",
    "                                'tof_ha':'int64',\n",
    "                                'tof_mean': 'float64'}\n",
    "                df = df.astype(convert_dict)\n",
    "                \n",
    "                #assert df.esa_sampled_ha.any() <= df.esa_total_ha.any(), f'Sampled area is greater than total area for land cover {cover} in {file}.'\n",
    "\n",
    "        # map ESA id numbers to lcc labels\n",
    "        esa_legend = {0: 'ESA No Data',\n",
    "                10: 'Cropland, rainfed',\n",
    "                11: 'Cropland, rainfed',\n",
    "                12: 'Cropland, rainfed',\n",
    "                20: 'Cropland, irrigated or post-flooding',\n",
    "                30: 'Mosaic cropland / natural vegetation',\n",
    "                40: 'Mosaic natural vegetation / cropland',\n",
    "                50: 'Tree cover, broadleaved, evergreen',\n",
    "                60: 'Tree cover, broadleaved, deciduous',\n",
    "                61: 'Tree cover, broadleaved, deciduous',\n",
    "                62: 'Tree cover, broadleaved, deciduous',\n",
    "                70: 'Tree cover, needleleaved, evergreen',\n",
    "                71: 'Tree cover, needleleaved, evergreen',\n",
    "                72: 'Tree cover, needleleaved, evergreen',\n",
    "                80: 'Tree cover, needleleaved, deciduous',\n",
    "                81: 'Tree cover, needleleaved, deciduous',\n",
    "                82: 'Tree cover, needleleaved, deciduous',\n",
    "                90: 'Tree cover, mixed leaf type',\n",
    "                100: 'Mosaic tree and shrub / herbaceous cover',\n",
    "                110: 'Mosaic herbaceous cover / tree and shrub',\n",
    "                120: 'Shrubland',\n",
    "                121: 'Shrubland',\n",
    "                122: 'Shrubland',\n",
    "                130: 'Grassland',\n",
    "                140: 'Lichens and mosses',\n",
    "                150: 'Sparse vegetation',\n",
    "                151: 'Sparse vegetation',\n",
    "                152: 'Sparse vegetation',\n",
    "                153: 'Sparse vegetation',\n",
    "                160: 'Tree cover, flooded, fresh or brakish water',\n",
    "                170: 'Tree cover, flooded, saline water',\n",
    "                180: 'Shrub or herbaceous cover, flooded, fresh/saline/brakish water',\n",
    "                190: 'Urban areas',\n",
    "                200: 'Bare areas',\n",
    "                201: 'Bare areas',\n",
    "                202: 'Bare areas',\n",
    "                210: 'Water bodies',\n",
    "                220: 'Permanent snow and ice',\n",
    "                255: 'No Data (flag)'}\n",
    "        df['esa_class'] = df['esa_id'].map(esa_legend)\n",
    "        \n",
    "        tof = None\n",
    "        esa = None\n",
    "        \n",
    "        if counter % 3 == 0:\n",
    "            print(f'{counter}/{len(folder_contents)} admins processed...')\n",
    "    \n",
    "    cols_to_check = ['esa_sampled_ha', 'esa_total_ha', 'tof_ha', 'tof_mean']\n",
    "    assert all(ptypes.is_numeric_dtype(df[col]) for col in cols_to_check)\n",
    "    \n",
    "    df.to_csv(f'{country}/stats/{country}_statistics_{extent}_tmlonly.csv', index=False)\n",
    "    print('Analysis complete.')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW (Hansen)\n",
    "\n",
    "@timer\n",
    "def calculate_stats(country, extent):\n",
    "    \n",
    "    '''\n",
    "    Takes in a country and extent (full or partial) and produces zonal stats on tree cover. \n",
    "    Returns a csv with statistics per administrative district, per land cover class and \n",
    "    per tree cover threshold. Produces stats for TML and Hansen et al (2013) data.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    extent : str\n",
    "        a string indicating the processing extent of the geotiff\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(f'{country}/stats'):\n",
    "        os.makedirs(f'{country}/stats')\n",
    "        \n",
    "    df = pd.DataFrame({'country': pd.Series(dtype='str'),\n",
    "                       'admin': pd.Series(dtype='str'),\n",
    "                       'esa_id': pd.Series(dtype='str'),\n",
    "                       'esa_class': pd.Series(dtype='str'),\n",
    "                       'esa_sampled_ha': pd.Series(dtype='float64'),\n",
    "                       'esa_total_ha': pd.Series(dtype='float64'),\n",
    "                       'tree_cover_class': pd.Series(dtype='str'),\n",
    "                       'tof_ha': pd.Series(dtype='int64'),\n",
    "                       'hans_ha': pd.Series(dtype='int64'),\n",
    "                       'tof_mean': pd.Series(dtype='float64'),\n",
    "                       'hans_mean': pd.Series(dtype='float64')})\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    folder_contents = [f for f in os.listdir(f'{country}/resampled_rasters/tof') if f != '.ipynb_checkpoints']\n",
    "    \n",
    "    # iterate through the admins \n",
    "    for file in folder_contents:\n",
    "        counter += 1\n",
    "        tof = rs.open(f'{country}/resampled_rasters/tof/{file}').read(1)\n",
    "        hans = rs.open(f'{country}/resampled_rasters/hansen/{file}').read(1)\n",
    "        esa = rs.open(f'{country}/resampled_rasters/esa/{file}').read(1)\n",
    "        \n",
    "        # reshape to 4d array and mask where equal to 255\n",
    "        tof = reshape_to_4d(tof)\n",
    "        tof = np.ma.masked_equal(tof, 255, copy=False)\n",
    "        \n",
    "        # manually calculate mean per hectare\n",
    "        tof_count_per_ha = np.sum(~tof.mask, axis = (1,3), dtype=np.uint8) \n",
    "        tof_sum_per_ha = np.sum(tof, axis = (1,3), dtype=np.uint16)\n",
    "        tof_mean_per_ha = np.divide(tof_sum_per_ha, tof_count_per_ha, dtype=np.float32)\n",
    "        \n",
    "        # same for Hansen\n",
    "        hans = reshape_to_4d(hans) \n",
    "        hans = np.ma.masked_equal(hans, 255, copy=False)\n",
    "        hans_count_per_ha = np.sum(~hans.mask, axis=(1,3), dtype=np.uint8) \n",
    "        hans_sum_per_ha = np.sum(hans, axis=(1,3), dtype=np.uint16)\n",
    "        hans_mean_per_ha = np.divide(hans_sum_per_ha, hans_count_per_ha, dtype=np.float32)\n",
    "\n",
    "        # reshape esa and set each hectare to the mode (lcc that appears most often)\n",
    "        # to prevent double counting \n",
    "        esa = reshape_to_4d(esa)\n",
    "\n",
    "        # Use a zipped array to determine if calculating the mode is necessary (only 5-10% of cases)\n",
    "        for i, l in zip(range(esa.shape[0]), range(esa.shape[2])):\n",
    "            if len(np.unique(esa[i, :, l, :])) > 1:\n",
    "                esa[i, :, l, :] = scipy.stats.mode(esa[i, :, l, :].flatten())[0]\n",
    "        \n",
    "        # now reshape esa to a 2d array\n",
    "        esa = np.max(esa, axis = (1,3))\n",
    "        \n",
    "        lower_rng = [x for x in range(0, 100, 10)]\n",
    "        upper_rng = [x for x in range(10, 110, 10)]\n",
    "        \n",
    "        # Set upper to 101, otherwise it isn't inclusive of 100% hectares.\n",
    "        upper_rng[-1] = 101\n",
    "        \n",
    "        esa_classes = np.unique(esa)\n",
    "         \n",
    "        for cover in esa_classes:\n",
    "            \n",
    "            tof_class_mean_per_ha = tof_mean_per_ha.copy()\n",
    "            tof_class_mean_per_ha.mask[esa != cover] = 1\n",
    "            tof_class_mean = np.round(np.mean(tof_class_mean_per_ha), 2)\n",
    "            \n",
    "            hans_class_mean_per_ha = hans_mean_per_ha.copy()\n",
    "            hans_class_mean_per_ha.mask[esa != cover] = 1\n",
    "            hans_class_mean = np.round(np.mean(hans_class_mean_per_ha), 2)\n",
    "\n",
    "            # calculate the area sampled \n",
    "            lc_total = np.sum(esa == cover)/100\n",
    "            lc_sampled = np.sum(~tof_class_mean_per_ha.mask)\n",
    "\n",
    "            # iterate through the thresholds (0-10, 10-20, 20-30)\n",
    "            for lower, upper in zip(lower_rng, upper_rng):\n",
    "\n",
    "                # calculate total ha for that threshold \n",
    "                tof_bin = np.sum((tof_class_mean_per_ha >= lower) & (tof_class_mean_per_ha < upper))\n",
    "                bin_name = (f'{str(lower)}-{str(upper - 1)}')\n",
    "                hans_bin = np.sum((hans_class_mean_per_ha >= lower) & (hans_class_mean_per_ha < upper))\n",
    "                                \n",
    "                # confirm masked array doesn't propogate\n",
    "                vars_to_check = [lc_sampled, lc_total, tof_bin, hans_bin, tof_class_mean, hans_class_mean]\n",
    "                \n",
    "                for index, var in enumerate(vars_to_check):\n",
    "                    if var == '--':\n",
    "                        var = 0\n",
    "                                \n",
    "                # check for erroneous values\n",
    "                assert lc_sampled <= lc_total, f'Sampled area is greater than total area for land cover {cover} in {file}.'\n",
    "                    \n",
    "                df = df.append({'country': country, \n",
    "                               'admin': file[:-4],\n",
    "                               'esa_id': cover,\n",
    "                               'esa_sampled_ha': lc_sampled,\n",
    "                               'esa_total_ha': lc_total,\n",
    "                               'tree_cover_class': bin_name,\n",
    "                               'tof_ha': tof_bin,\n",
    "                               'hans_ha': hans_bin,\n",
    "                               'tof_mean': tof_class_mean,\n",
    "                               'hans_mean': hans_class_mean},\n",
    "                                ignore_index=True)\n",
    "                \n",
    "                # reinforce datatypes\n",
    "                convert_dict = {'esa_sampled_ha':'float64',\n",
    "                                'esa_total_ha':'float64',\n",
    "                                'tof_ha':'int64',\n",
    "                                'hans_ha':'int64',\n",
    "                                'tof_mean': 'float64',\n",
    "                                'hans_mean': 'float64'}\n",
    "                df = df.astype(convert_dict)\n",
    "            \n",
    "        # map ESA id numbers to lcc labels\n",
    "        esa_legend = {0: 'ESA No Data',\n",
    "                10: 'Cropland, rainfed',\n",
    "                11: 'Cropland, rainfed',\n",
    "                12: 'Cropland, rainfed',\n",
    "                20: 'Cropland, irrigated or post-flooding',\n",
    "                30: 'Mosaic cropland / natural vegetation',\n",
    "                40: 'Mosaic natural vegetation / cropland',\n",
    "                50: 'Tree cover, broadleaved, evergreen',\n",
    "                60: 'Tree cover, broadleaved, deciduous',\n",
    "                61: 'Tree cover, broadleaved, deciduous',\n",
    "                62: 'Tree cover, broadleaved, deciduous',\n",
    "                70: 'Tree cover, needleleaved, evergreen',\n",
    "                71: 'Tree cover, needleleaved, evergreen',\n",
    "                72: 'Tree cover, needleleaved, evergreen',\n",
    "                80: 'Tree cover, needleleaved, deciduous',\n",
    "                81: 'Tree cover, needleleaved, deciduous',\n",
    "                82: 'Tree cover, needleleaved, deciduous',\n",
    "                90: 'Tree cover, mixed leaf type',\n",
    "                100: 'Mosaic tree and shrub / herbaceous cover',\n",
    "                110: 'Mosaic herbaceous cover / tree and shrub',\n",
    "                120: 'Shrubland',\n",
    "                121: 'Shrubland',\n",
    "                122: 'Shrubland',\n",
    "                130: 'Grassland',\n",
    "                140: 'Lichens and mosses',\n",
    "                150: 'Sparse vegetation',\n",
    "                151: 'Sparse vegetation',\n",
    "                152: 'Sparse vegetation',\n",
    "                153: 'Sparse vegetation',\n",
    "                160: 'Tree cover, flooded, fresh or brakish water',\n",
    "                170: 'Tree cover, flooded, saline water',\n",
    "                180: 'Shrub or herbaceous cover, flooded, fresh/saline/brakish water',\n",
    "                190: 'Urban areas',\n",
    "                200: 'Bare areas',\n",
    "                201: 'Bare areas',\n",
    "                202: 'Bare areas',\n",
    "                210: 'Water bodies',\n",
    "                220: 'Permanent snow and ice',\n",
    "                255: 'No Data (flag)'}\n",
    "     \n",
    "        df['esa_class'] = df['esa_id'].map(esa_legend)\n",
    "        \n",
    "        tof = None\n",
    "        esa = None\n",
    "        hans = None\n",
    "        \n",
    "        if counter % 3 == 0:\n",
    "            print(f'{counter}/{len(folder_contents)} admins processed...')\n",
    "    \n",
    "    cols_to_check = ['esa_sampled_ha', 'esa_total_ha', 'tof_ha', 'hans_ha', 'tof_mean', 'hans_mean']\n",
    "    assert all(ptypes.is_numeric_dtype(df[col]) for col in cols_to_check)\n",
    "    \n",
    "    df.to_csv(f'{country}/stats/{country}_statistics_{extent}.csv', index=False)\n",
    "    print('Analysis complete.')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def upload_dir(source_dir, bucket, object_name):\n",
    "    \"\"\"\n",
    "    Upload a directory to an S3 bucket. \n",
    "\n",
    "    file_name: File to upload\n",
    "    bucket: Bucket to upload to\n",
    "    object_name: S3 object name. If not specified then file_name is used\n",
    "\n",
    "    \"\"\"\n",
    "    config = confuse.Configuration('sentinel-tree-cover')\n",
    "    config.set_file('/Users/jessica.ertel/sentinel-tree-cover/jessica-config.yaml')\n",
    "    aws_access_key = config['aws']['aws_access_key_id']\n",
    "    aws_secret_key = config['aws']['aws_secret_access_key']\n",
    "    session = boto3.Session(aws_access_key_id=aws_access_key.as_str(), aws_secret_access_key=aws_secret_key.as_str())    \n",
    "    s3 = session.resource('s3') \n",
    "    bucket = s3.Bucket(bucket)\n",
    "    \n",
    "    # use directory tree generator to get list of file paths and upload each\n",
    "    for subdir, dirs, files in os.walk(source_dir):\n",
    "        \n",
    "        for file in files:\n",
    "            dest_path = os.path.join(subdir, file)\n",
    "            \n",
    "            with open(dest_path, 'rb') as data:\n",
    "                bucket.put_object(Key=object_name+dest_path, Body=data)\n",
    "\n",
    "    print('Upload complete.')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def execute_pipe(country, extent, incl_hansen=True):\n",
    "    print(f'Started at: {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "    print('Downloading input data...')\n",
    "    download_inputs(country)\n",
    "    if incl_hansen:\n",
    "        print('Building Hansen tree cover raster...')\n",
    "        create_hansen_tif(country)\n",
    "        print('Removing tree cover loss...')\n",
    "        remove_loss(country)\n",
    "    print('Padding tml raster...')\n",
    "    pad_tml_raster(country)\n",
    "    print('Clipping rasters by admin boundary...')\n",
    "    create_clippings(country, multi_analysis=incl_hansen)\n",
    "    print('Resampling to match raster extents and resolutions...')\n",
    "    apply_extent_res(country, multi_analysis=incl_hansen)\n",
    "    print('Merging admins containing multiple polygons...')\n",
    "    merge_polygons(country, multi_analysis=incl_hansen)\n",
    "    print('Checking size...')\n",
    "    processing_check(country)\n",
    "    print('Calculating statistics...')\n",
    "    if incl_hansen:\n",
    "        calculate_stats(country, extent)\n",
    "    else:\n",
    "        calculate_stats_tml(country, extent)\n",
    "    print('Uploading files to s3...')\n",
    "    upload_dir(country, 'tof-output', '2020/analysis/2020-full/admin2/')\n",
    "    print(f'Finished {extent} processing at: {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tml-analysis",
   "language": "python",
   "name": "tml-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "246px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
