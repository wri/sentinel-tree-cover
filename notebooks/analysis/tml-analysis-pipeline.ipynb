{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from time import time, strftime\n",
    "import os\n",
    "import os.path\n",
    "import rasterio as rs\n",
    "from rasterio.mask import mask\n",
    "from rasterio.merge import merge\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio import Affine, MemoryFile\n",
    "\n",
    "import numpy as np \n",
    "import numpy.ma as ma \n",
    "import pyproj\n",
    "import geopandas as gpd \n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "import pandas as pd\n",
    "import pandas.api.types as ptypes\n",
    "import fiona\n",
    "from contextlib import contextmanager  \n",
    "from skimage.transform import resize\n",
    "import math\n",
    "import requests\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "import osgeo\n",
    "from osgeo import gdal\n",
    "from osgeo import gdalconst\n",
    "import glob\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import confuse\n",
    "\n",
    "config = confuse.Configuration('sentinel-tree-cover')\n",
    "config.set_file('/Users/jessica.ertel/sentinel-tree-cover/jessica-config.yaml')\n",
    "aws_access_key = config['aws']['aws_access_key_id']\n",
    "aws_secret_key = config['aws']['aws_secret_access_key']\n",
    "\n",
    "s3 = boto3.client('s3', aws_access_key_id=aws_access_key.as_str(), aws_secret_access_key=aws_secret_key.as_str())\n",
    "s3.download_file('tof-output', \n",
    "                 '2020/country/Uganda.tif', \n",
    "                 '/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Uganda.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    '''\n",
    "    Prints the runtime of the decorated function.\n",
    "    '''\n",
    "    \n",
    "    @functools.wraps(func)\n",
    "    def wrapper_timer(*args, **kwargs):\n",
    "        start = datetime.now() \n",
    "        value = func(*args, **kwargs)\n",
    "        end = datetime.now() \n",
    "        run_time = end - start\n",
    "        print(f'Completed {func.__name__!r} in {run_time}.')\n",
    "        return value\n",
    "    return wrapper_timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapefile to Geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_gjson(country):\n",
    "    '''\n",
    "    Imports a country shapefile, translates and saves it as \n",
    "    a geojson, confirming the correct CRS and absence of \n",
    "    duplicates. Prints the number of admin 1 districts.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    \n",
    "    '''\n",
    "    if country == 'Costa Rica':\n",
    "        return 'Using existing geojson file for Costa Rica.'\n",
    "    else: \n",
    "        # removed index as glob.glob not ordered\n",
    "        shapefile = glob.glob(f'{country}/shapefile/*.shp')\n",
    "        new_shp = gpd.read_file(shapefile[0])\n",
    "        new_shp.to_file(f'{country}/{country}_adminboundaries.geojson', driver='GeoJSON')\n",
    "        print(f'There are {len(new_shp)} admins in {country}.')\n",
    "        assert new_shp.crs == 'epsg:4326'\n",
    "        assert new_shp.NAME_1.duplicated().sum() == 0\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hansen Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hansen_tif(country):\n",
    "    '''\n",
    "    Identifies the lat/lon coordinates for a single country \n",
    "    to download Hansen 2010 tree cover and 2020 tree cover loss tif files. \n",
    "    Returns combined tifs as one file in the country's folder.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    \n",
    "    '''\n",
    "    gdal.UseExceptions()\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries.geojson')\n",
    "\n",
    "    if not os.path.exists(f'hansen_treecover2010'):\n",
    "        os.makedirs(f'hansen_treecover2010')\n",
    "    \n",
    "    if not os.path.exists(f'hansen_lossyear2020'):\n",
    "        os.makedirs(f'hansen_lossyear2020')\n",
    "    \n",
    "    # identify min/max bounds for the country\n",
    "    bounds = shapefile.geometry.bounds\n",
    "    min_x = bounds.minx.min() \n",
    "    min_y = bounds.miny.min()\n",
    "    max_x = bounds.maxx.max()\n",
    "    max_y = bounds.maxy.max()\n",
    "\n",
    "    # identify the lowest and highest 10 lat/lon increments for the country\n",
    "    lower_x = math.floor(min_x / 10) * 10 \n",
    "    lower_y = math.ceil(min_y / 10) * 10 \n",
    "    upper_x = math.ceil(max_x / 10) * 10 \n",
    "    upper_y = math.ceil(max_y / 10) * 10\n",
    "    \n",
    "    print('Downloading files from GLAD...')\n",
    "    \n",
    "    for x_grid in range(lower_x, upper_x, 10):\n",
    "        for y_grid in range(lower_y, upper_y + 10, 10):\n",
    "            \n",
    "            lon = 'N' if y_grid >= 0 else 'S'\n",
    "            lat = 'E' if x_grid >= 0 else 'W'\n",
    "            \n",
    "            # establish urls\n",
    "            lon_lat = f'{str(np.absolute(y_grid)).zfill(2)}{lon}_{str(np.absolute(x_grid)).zfill(3)}{lat}.tif'\n",
    "            cover_url = f'https://glad.umd.edu/Potapov/TCC_2010/treecover2010_{lon_lat}'\n",
    "            cover_dest = f'hansen_treecover2010/{lon_lat}'\n",
    "            loss_url = f'https://storage.googleapis.com/earthenginepartners-hansen/GFC-2020-v1.8/Hansen_GFC-2020-v1.8_lossyear_{lon_lat}'\n",
    "            loss_dest = f'hansen_lossyear2020/{lon_lat}'\n",
    "\n",
    "            # download tree cover and loss files from UMD website\n",
    "            try:\n",
    "                urllib.request.urlretrieve(cover_url, cover_dest)\n",
    "            except urllib.error.HTTPError as err:\n",
    "                if err.code == 404:\n",
    "                    print(f'HTTP Error 404 for tree cover data: {cover_url}')\n",
    "                    pass\n",
    "            \n",
    "            try:\n",
    "                urllib.request.urlretrieve(loss_url, loss_dest)\n",
    "            except urllib.error.HTTPError as err:\n",
    "                if err.code == 404:\n",
    "                    print(f'HTTP Error 404 for tree cover loss data: {loss_url}')\n",
    "                    pass\n",
    "    \n",
    "    # if the tree cover file doesn't exist, remove loss file\n",
    "    for tif in os.listdir('hansen_lossyear2020/'):\n",
    "        if tif not in os.listdir('hansen_treecover2010/'):\n",
    "            os.remove(f'hansen_lossyear2020/{tif}')\n",
    "\n",
    "    # create list of tifs and ensure no duplicates\n",
    "    tree_tifs = glob.glob('hansen_treecover2010/*.tif')\n",
    "    loss_tifs = glob.glob('hansen_lossyear2020/*.tif')\n",
    "    \n",
    "    # convert tree cover and loss tifs into a virtual raster tile  \n",
    "    gdal.BuildVRT(f'{country}/{country}_hansen_treecover2010.vrt', tree_tifs)\n",
    "    gdal.BuildVRT(f'{country}/{country}_hansen_loss2020.vrt', loss_tifs)\n",
    "\n",
    "    # open vrts and convert to a single .tif -- adding tfw=yes increases file size significantly\n",
    "    translateoptions = gdal.TranslateOptions(format='Gtiff', \n",
    "                                              outputSRS='EPSG:4326',\n",
    "                                              outputType=gdal.GDT_Byte,\n",
    "                                              noData=255,\n",
    "                                              creationOptions=['COMPRESS=LZW'],\n",
    "                                              resampleAlg='nearest')\n",
    " \n",
    "    source = gdal.Open(f'{country}/{country}_hansen_treecover2010.vrt', )\n",
    "    ds = gdal.Translate(f'{country}/{country}_hansen_treecover2010.tif', source, options=translateoptions)\n",
    "    os.remove(f'{country}/{country}_hansen_treecover2010.vrt')\n",
    "    source = None\n",
    "    ds = None\n",
    "                      \n",
    "    source = gdal.Open(f'{country}/{country}_hansen_loss2020.vrt')\n",
    "    ds = gdal.Translate(f'{country}/{country}_hansen_loss2020.tif', source, options=translateoptions)\n",
    "    os.remove(f'{country}/{country}_hansen_loss2020.vrt')\n",
    "    source = None\n",
    "    ds = None\n",
    "    \n",
    "    assert os.path.exists(f'{country}/{country}_hansen_treecover2010.tif')\n",
    "    assert os.path.exists(f'{country}/{country}_hansen_loss2020.tif')\n",
    "\n",
    "    # if new files are properly create, delete what is not needed\n",
    "    for file in tree_tifs:\n",
    "        os.remove(file)\n",
    "    \n",
    "    for file in loss_tifs:\n",
    "        os.remove(file)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Hansen tree cover loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_loss(country):\n",
    "    '''\n",
    "    Imports hansen tree cover loss tifs for a single country. Updates tree cover \n",
    "    to 0 if loss was detected between 2011-2020. Returns updated tif in the country's \n",
    "    folder.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "    gdal.UseExceptions()\n",
    "    hansen_cover = rs.open(f'{country}/{country}_hansen_treecover2010.tif').read(1) \n",
    "    #print(f\"The hansen cover data is {hansen_cover.nbytes / 1e6} megabytes\")\n",
    "    hansen_loss = rs.open(f'{country}/{country}_hansen_loss2020.tif').read(1)\n",
    "    #print(f\"The hansen loss data is {hansen_loss.nbytes / 1e6} megabytes\")\n",
    "    \n",
    "     # assert raster shape, datatype and max/min values\n",
    "    assert hansen_cover.dtype == 'uint8'\n",
    "    assert hansen_cover.shape != (0, ) and len(hansen_cover.shape) <= 2\n",
    "    assert hansen_cover.max() <= 100 and hansen_cover.min() >= 0\n",
    "    assert hansen_loss.dtype == 'uint8'\n",
    "    assert hansen_loss.shape != (0, ) and len(hansen_loss.shape) <= 2\n",
    "    assert hansen_loss.max() <= 20 and hansen_cover.min() >= 0\n",
    "    \n",
    "    # If there was loss between 2011-2020, make then 0 in tree cover\n",
    "    sum_before_loss = np.sum(hansen_cover > 0) \n",
    "    hansen_cover[(hansen_loss >= 11)] = 0.\n",
    "    \n",
    "    # check bin counts after loss removed\n",
    "    print(f'{sum_before_loss - (np.sum(hansen_cover > 0))} tree cover pixels converted to loss.')\n",
    "    \n",
    "    # write as a new file\n",
    "    out_meta = rs.open(f'{country}/{country}_hansen_treecover2010.tif').meta\n",
    "    out_meta.update({'driver': 'GTiff',    \n",
    "                     'dtype': 'uint8',\n",
    "                     'height': hansen_cover.shape[0],\n",
    "                     'width': hansen_cover.shape[1],\n",
    "                     'count': 1,\n",
    "                     'compress':'lzw'})\n",
    "    outpath = f'{country}/{country}_hansen_treecover2010_wloss.tif'\n",
    "    with rs.open(outpath, 'w', **out_meta) as dest:\n",
    "            dest.write(hansen_cover, 1) \n",
    "    \n",
    "    # remove original hansen tree cover and loss files\n",
    "    os.remove(f'{country}/{country}_hansen_treecover2010.tif')\n",
    "    os.remove(f'{country}/{country}_hansen_loss2020.tif')\n",
    "    hansen_cover = None\n",
    "    hansen_loss = None \n",
    "    \n",
    "    print('Hansen raster built.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad TML Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tml_raster(country):\n",
    "    \n",
    "    '''\n",
    "    Increase the TML raster extent to match the bounds of a country's shapefile\n",
    "    and fill with no data value to facilitate clipping.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "    \n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries.geojson')\n",
    "\n",
    "    # identify min/max bounds for the country\n",
    "    bounds = shapefile.geometry.bounds\n",
    "    min_x = bounds.minx.min() \n",
    "    min_y = bounds.miny.min()\n",
    "    max_x = bounds.maxx.max()\n",
    "    max_y = bounds.maxy.max()\n",
    "    \n",
    "    # create new bounds by rounding to the nearest .1 lat/lon \n",
    "    lower_x = math.floor(min_x * 10) / 10 \n",
    "    lower_y = math.floor(min_y * 10) / 10 \n",
    "    upper_x = math.ceil(max_x * 10) / 10\n",
    "    upper_y = math.ceil(max_y * 10) / 10\n",
    "          \n",
    "    # create tif with new bounds\n",
    "    warp_options = gdal.WarpOptions(format='GTiff', \n",
    "                                    dstSRS='EPSG:4326',\n",
    "                                    dstNodata=255,\n",
    "                                    outputBounds=[lower_x, lower_y, upper_x, upper_y],\n",
    "                                    resampleAlg='near',\n",
    "                                    outputType=osgeo.gdalconst.GDT_Byte,\n",
    "                                    creationOptions=['TFW=YES', 'COMPRESS=LZW', 'BIGTIFF=YES'])   \n",
    "        \n",
    "    ds = gdal.Warp(f'{country}/{country}_tof_padded.tif', \n",
    "                   f'{country}/{country}.tif',      \n",
    "                   options=warp_options)\n",
    "    \n",
    "    ds = None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Clip Rasters by Admin Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clippings(country, multi_analysis):\n",
    "    '''\n",
    "    Takes in a country name to import tof/hansen rasters and masks out administrative \n",
    "    boundaries based on the shapefile. Saves exploded shapefile as a geojson with polygons \n",
    "    split/numbered for each admin boundary. Returns clipped rasters as individual \n",
    "    files in the country's \"clipped_rasters\" folder. Deletes the original Hansen file. \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import        \n",
    "    '''\n",
    "    \n",
    "    if multi_analysis:\n",
    "        if not os.path.exists(f'{country}/clipped_rasters/hansen'):\n",
    "            os.makedirs(f'{country}/clipped_rasters/hansen')\n",
    "\n",
    "    if not os.path.exists(f'{country}/clipped_rasters/tof'):\n",
    "        os.makedirs(f'{country}/clipped_rasters/tof')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/clipped_rasters/esa'):\n",
    "        os.makedirs(f'{country}/clipped_rasters/esa')\n",
    "    \n",
    "    orig_shapefile = gpd.read_file(f'{country}/{country}_adminboundaries.geojson')\n",
    "    \n",
    "    # preprocess shapefile from multipolygon to single\n",
    "    counter = 0\n",
    "    for idx, row in orig_shapefile.iterrows():\n",
    "        counter += 1 if type(row.geometry) == MultiPolygon else 0\n",
    "\n",
    "    if counter > 0:\n",
    "        shapefile = orig_shapefile.explode()\n",
    "        \n",
    "        # add integer to admin name if multi polys\n",
    "        shapefile.NAME_1 = np.where(shapefile.NAME_1.duplicated(keep=False), \n",
    "                                     shapefile.NAME_1 + shapefile.groupby('NAME_1').cumcount().add(1).astype(str),\n",
    "                                     shapefile.NAME_1)\n",
    "\n",
    "        shapefile = shapefile.reset_index()\n",
    "        shapefile.drop(columns=['level_0', 'level_1'], inplace=True)\n",
    "    \n",
    "    # if no multi polys save original shapefile under new name\n",
    "    else:\n",
    "        shapefile = orig_shapefile\n",
    "    \n",
    "    shapefile.to_file(f'{country}/{country}_adminboundaries_exp.geojson', driver='GeoJSON')\n",
    "    \n",
    "    def mask_raster(polygon, admin, raster, folder):\n",
    "        out_img, out_transform = mask(dataset=raster, shapes=[polygon], crop=True, nodata=255, filled=True)\n",
    "        out_meta = raster.meta\n",
    "        out_meta.update({'driver': 'GTiff',    \n",
    "                         'dtype': 'uint8',\n",
    "                         'height': out_img.shape[1],\n",
    "                         'width': out_img.shape[2],\n",
    "                         'transform': out_transform,\n",
    "                         'compress':'lzw'})\n",
    "        outpath = f'{country}/clipped_rasters/{folder}/{admin}.tif'\n",
    "        with rs.open(outpath, 'w', **out_meta) as dest:\n",
    "            dest.write(out_img)\n",
    "        out_img = None\n",
    "        out_transform = None\n",
    "        return None\n",
    "    \n",
    "    tof_raster_path = f'{country}/{country}_tof_padded.tif'\n",
    "    esa_raster_path = 'ESACCI-LC-L4-LCCS-Map-300m-P1Y-2015-v2.0.7.tif'\n",
    "    \n",
    "    files_to_process = [tof_raster_path, esa_raster_path] \n",
    "    types_to_process = ['tof', 'esa'] \n",
    "    \n",
    "    if multi_analysis:\n",
    "        files_to_process.append(f'{country}/{country}_hansen_treecover2010_wloss.tif')\n",
    "        types_to_process.append('hansen')\n",
    "    \n",
    "    for file, file_type in zip(files_to_process, types_to_process):\n",
    "        with rs.open(file) as raster:\n",
    "            for polygon, admin in zip(shapefile.geometry, shapefile.NAME_1):\n",
    "                mask_raster(polygon, admin, raster, file_type)\n",
    "    \n",
    "    # delete Tof and Hansen files once clippings created \n",
    "    os.remove(f'{country}/{country}_tof_padded.tif')\n",
    "    os.remove(f'{country}/{country}_tof_padded.tfw')\n",
    "    if multi_analysis:\n",
    "        os.remove(f'{country}/{country}_hansen_treecover2010_wloss.tif')\n",
    "        \n",
    "    print(f\"{country}'s rasters clipped and saved.\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample to Match Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_extent_and_res(source, reference, out_filename, tof=False, esa=False):\n",
    "\n",
    "    '''\n",
    "    GDAL’s nearest neighbor interpolation is used match the \n",
    "    projection, bounding box and dimensions of the source dataset \n",
    "    to the reference dataset. \n",
    "    '''\n",
    "    \n",
    "    # set up the source file \n",
    "    src = gdal.Open(source, gdalconst.GA_ReadOnly)\n",
    "    src_proj = src.GetProjection()\n",
    "    src_geotrans = src.GetGeoTransform()\n",
    "\n",
    "    # set up the reference file (esa)\n",
    "    ref_ds = gdal.Open(reference, gdalconst.GA_ReadOnly)\n",
    "    ref_proj = ref_ds.GetProjection()\n",
    "    ref_geotrans = ref_ds.GetGeoTransform()\n",
    "    \n",
    "    # create height/width for the interpolation (ref dataset except for tof)\n",
    "    width = ref_ds.RasterXSize if not tof else src.RasterXSize\n",
    "    height = ref_ds.RasterYSize if not tof else src.RasterYSize\n",
    "\n",
    "    out = gdal.GetDriverByName('GTiff').Create(out_filename, width, height, 1, gdalconst.GDT_Byte, options=['COMPRESS=LZW'])\n",
    "    rb = out.GetRasterBand(1)\n",
    "    rb.SetNoDataValue(255)\n",
    "    \n",
    "    # do not adjust the bounds for esa, use source (esa)\n",
    "    if esa:\n",
    "        ref_proj = src_proj\n",
    "    \n",
    "    # set geotrans, proj and no data val for the out file\n",
    "    out.SetGeoTransform(ref_geotrans)\n",
    "    out.SetProjection(ref_proj)\n",
    "    \n",
    "    interpolation = gdalconst.GRA_NearestNeighbour\n",
    "    gdal.ReprojectImage(src, out, src_proj, ref_proj, interpolation)\n",
    "    \n",
    "    ref_ds = None\n",
    "    src = None\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def apply_extent_res(country, multi_analysis):\n",
    "    \n",
    "    '''\n",
    "    Applies match_raster_extent_and_res() to all admin files\n",
    "    for a country. The ESA and Hansen data are upsampled to match \n",
    "    TOF at 10m resolution. TOF and Hansen et al. data are resized to \n",
    "    match the dimensions and bounding box of the ESA data.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "    \n",
    "    if multi_analysis:\n",
    "        if not os.path.exists(f'{country}/resampled_rasters/hansen'):\n",
    "            os.makedirs(f'{country}/resampled_rasters/hansen')\n",
    "\n",
    "    if not os.path.exists(f'{country}/resampled_rasters/tof'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/tof')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/resampled_rasters/esa'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/esa')\n",
    "        \n",
    "    \n",
    "    # import new shapefile containing only polygons\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries_exp.geojson')\n",
    "    admin_boundaries = list(shapefile.NAME_1)\n",
    "    \n",
    "    for admin in admin_boundaries:\n",
    "        \n",
    "        # apply to esa\n",
    "        match_extent_and_res(f'{country}/clipped_rasters/esa/{admin}.tif', # source\n",
    "                             f'{country}/clipped_rasters/tof/{admin}.tif', # reference\n",
    "                             f'{country}/resampled_rasters/esa/{admin}.tif', # outpath\n",
    "                             tof = False, \n",
    "                             esa = True) \n",
    "        \n",
    "        # apply to tof\n",
    "        match_extent_and_res(f'{country}/clipped_rasters/tof/{admin}.tif', \n",
    "                             f'{country}/resampled_rasters/esa/{admin}.tif', \n",
    "                             f'{country}/resampled_rasters/tof/{admin}.tif', \n",
    "                             tof = True, \n",
    "                             esa = False) \n",
    "        \n",
    "        # apply to hansen\n",
    "        if multi_analysis:\n",
    "            match_extent_and_res(f'{country}/clipped_rasters/hansen/{admin}.tif', \n",
    "                                 f'{country}/resampled_rasters/esa/{admin}.tif', \n",
    "                                 f'{country}/resampled_rasters/hansen/{admin}.tif', \n",
    "                                 tof = False, \n",
    "                                 esa = False) \n",
    "        \n",
    "        # assert no data value added correctly in tof rasters\n",
    "        tof = rs.open(f'{country}/resampled_rasters/tof/{admin}.tif').read(1)\n",
    "        assert tof.max() <= 255\n",
    "        tof = None\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Admin Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_polygons(country, multi_analysis):\n",
    "    '''\n",
    "    Takes in a country's resampled rasters and identifies\n",
    "    which admin boundaries are composed of multipolygons. Combines individual files\n",
    "    into one for the admin district, then deletes the individual files.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries_exp.geojson')\n",
    "    admin_boundaries_all = list(shapefile.NAME_1)\n",
    "    \n",
    "    # creates a list of admins that need to be merged (digits in filename)\n",
    "    no_ints = []\n",
    "    for admin in admin_boundaries_all:\n",
    "        \n",
    "        # if any characters are digits, remove them and ad admin to list\n",
    "        if any(char.isdigit() for char in admin):\n",
    "            clean_admin = ''.join([char for char in admin if not char.isdigit()])\n",
    "            no_ints.append(clean_admin)\n",
    "\n",
    "    no_ints = list(set(no_ints))\n",
    "    print(f'{len(no_ints)} admins will be merged: {no_ints}')\n",
    "\n",
    "    datasets = ['tof', 'esa']\n",
    "    if multi_analysis:\n",
    "        datasets.append('hansen')\n",
    "    \n",
    "    for data in datasets:\n",
    "        for admin_2 in no_ints:\n",
    "\n",
    "            # gather list of files for that admin (ex: Puntarenas1.tif, Puntarenas2.tif, Puntarenas3.tif)\n",
    "            files_to_merge = [] # items need to be in dataset reader mode\n",
    "            files_to_delete = [] # items are just string of the file name\n",
    "\n",
    "            for path in glob.glob(f'{country}/resampled_rasters/{data}/{admin_2}?.tif'):\n",
    "                filename = os.path.basename(path) \n",
    "                files_to_delete.append(filename)\n",
    "                src = rs.open(f'{country}/resampled_rasters/{data}/{filename}')\n",
    "                files_to_merge.append(src)\n",
    "\n",
    "            # capture double digits\n",
    "            for path in glob.glob(f'{country}/resampled_rasters/{data}/{admin_2}??.tif'):\n",
    "                filename = os.path.basename(path) \n",
    "                files_to_delete.append(filename)\n",
    "                src = rs.open(f'{country}/resampled_rasters/{data}/{filename}')\n",
    "                files_to_merge.append(src)\n",
    "\n",
    "            # capture triple digits\n",
    "            for path in glob.glob(f'{country}/resampled_rasters/{data}/{admin_2}???.tif'):\n",
    "                filename = os.path.basename(path) \n",
    "                files_to_delete.append(filename)\n",
    "                src = rs.open(f'{country}/resampled_rasters/{data}/{filename}')\n",
    "                files_to_merge.append(src)\n",
    "\n",
    "            if len(files_to_merge) < 1:\n",
    "                print(f'No files to merge in {data}.')\n",
    "\n",
    "            mosaic, out_transform = merge(files_to_merge)\n",
    "\n",
    "            outpath = f'{country}/resampled_rasters/{data}/{admin_2}.tif'\n",
    "            out_meta = src.meta.copy()\n",
    "            out_meta.update({'driver': \"GTiff\",\n",
    "                             'dtype': 'uint8',\n",
    "                             'height': mosaic.shape[1],\n",
    "                             'width': mosaic.shape[2],\n",
    "                             'transform': out_transform,\n",
    "                             'compress':'lzw'})\n",
    "\n",
    "            with rs.open(outpath, \"w\", **out_meta) as dest:\n",
    "                dest.write(mosaic)\n",
    "\n",
    "            # delete the old separated tifs\n",
    "            for file in files_to_delete:\n",
    "                os.remove(f'{country}/resampled_rasters/{data}/{file}')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_check(country):\n",
    "    '''\n",
    "    Calculate the area of an admin district in hectares. Convert hectares to bytes to determine\n",
    "    if the admin can be processed on r5a.2xlarge instance. If it exceeds the processing threshold\n",
    "    flag the country and save to a csv file.\n",
    "    '''\n",
    "    # print size of TML tif\n",
    "    \n",
    "\n",
    "    # Return the current process id\n",
    "    process = psutil.Process(os.getpid())\n",
    "\n",
    "    # get rss and calculate return the memory usage in MB\n",
    "    print(f'Current memory usage: {process.memory_info()[0] / float(2 ** 20)} MB')\n",
    "\n",
    "    # return the memory usage in percentage like top\n",
    "    mem = process.memory_percent()\n",
    "    print(f'Perc memory usage: %{round(mem, 2)}')\n",
    "    \n",
    "    # import and create a copy\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries.geojson')\n",
    "    shapefile = shapefile.copy()\n",
    "    \n",
    "    # convert the crs to an equal-area projection to get polygon area in m2\n",
    "    # then convert to hectares (divide the area value by 10000)\n",
    "    shapefile['area'] = shapefile['geometry'].to_crs({'init': 'epsg:3395'}).map(lambda x: x.area / 10**4)\n",
    "    \n",
    "    # calculate the size of the largest area, ha --> bytes\n",
    "    max_area = shapefile['area'].max()\n",
    "    max_bytes = max_area * 3200 \n",
    "    admin = shapefile.loc[shapefile['area'] == max_area]['NAME_1'].item()\n",
    "    \n",
    "    # create a dataframe to store details\n",
    "    too_large = pd.DataFrame(columns=['country','admin','file_size','date'], dtype=object)\n",
    "    \n",
    "    # check if it can fit into RAM, otherwise save to csv\n",
    "    # should be checking the max area in ha?\n",
    "    if max_bytes >= 6.4e10:\n",
    "        print(f'The largest admin in {country} is {admin}. Area: {round(max_area, 2)} ha')\n",
    "        print(f'Warning: That largest admin {admin} is too large to process. As np.float32, array is ({round(max_bytes/10e9, 2)} GB)')\n",
    "        too_large.append({'country': country,\n",
    "                         'largest_admin': admin,\n",
    "                         'file_size': max_bytes,\n",
    "                         'area': round(max_area, 2),\n",
    "                         'date': datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")}, ignore_index=True)\n",
    "    \n",
    "        too_large.to_csv('bigtiff_full_2020.csv', mode='a', header=False)\n",
    "    else:\n",
    "        print('Passed processing check.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_4d(raster):\n",
    "    \n",
    "    '''\n",
    "    Takes in a GTiff, identifies the dimensions and them down to the nearest 10th.\n",
    "    Then uses those dimensions and reshapes to a 4 dimensional, 10x10 grid.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    raster : str\n",
    "        GTiff that will be reshaped\n",
    "    '''\n",
    "    \n",
    "    def round_down(num, divisor):\n",
    "         return num - (num%divisor)\n",
    "   \n",
    "    # round down rows and cols to nearest 10th\n",
    "    rows, cols = round_down(raster.shape[0], 10), round_down(raster.shape[1], 10)\n",
    "    \n",
    "    # clip according to rounded numbers and reshape\n",
    "    rounded = raster[:rows, :cols]\n",
    "    reshaped = np.reshape(rounded, (rounded.shape[0] // 10, 10, rounded.shape[1] // 10, 10))\n",
    "        \n",
    "    return reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def calculate_stats_tml(country, extent):\n",
    "    \n",
    "    '''\n",
    "    Takes in a country and extent (full or partial) to import appropriate rasters. Returns a csv \n",
    "    with statistics per administrative district, per land cover class and per tree cover\n",
    "    threshold. Only produces statistics for TML.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(f'{country}/stats'):\n",
    "        os.makedirs(f'{country}/stats')\n",
    "        \n",
    "    df = pd.DataFrame({'country': pd.Series(dtype='str'),\n",
    "                       'admin': pd.Series(dtype='str'),\n",
    "                       'esa_id': pd.Series(dtype='str'),\n",
    "                       'esa_class': pd.Series(dtype='str'),\n",
    "                       'esa_sampled_ha': pd.Series(dtype='float64'),\n",
    "                       'esa_total_ha': pd.Series(dtype='float64'),\n",
    "                       'tree_cover_class': pd.Series(dtype='str'),\n",
    "                       'tof_ha': pd.Series(dtype='int64'),\n",
    "                       'tof_mean': pd.Series(dtype='float64')})\n",
    "    counter = 0\n",
    "    \n",
    "    folder_contents = [f for f in os.listdir(f'{country}/resampled_rasters/tof') if f != '.ipynb_checkpoints']\n",
    "    \n",
    "    # iterate through the admins \n",
    "    for file in folder_contents:\n",
    "        counter += 1\n",
    "        \n",
    "        tof = rs.open(f'{country}/resampled_rasters/tof/{file}').read(1)\n",
    "        esa = rs.open(f'{country}/resampled_rasters/esa/{file}').read(1)\n",
    "        \n",
    "        lower_rng = [x for x in range(0, 100, 10)]\n",
    "        upper_rng = [x for x in range(10, 110, 10)]\n",
    "\n",
    "        # convert values to their median for binning\n",
    "        for lower, upper in zip(lower_rng, upper_rng):\n",
    "            tof[(tof >= lower) & (tof < upper)] = lower + 4.5\n",
    "    \n",
    "        # iterate through the land cover classes\n",
    "        esa_classes = np.unique(esa)\n",
    "        \n",
    "        for cover in esa_classes:\n",
    "            \n",
    "            # replace all values not equal to the current lcc with no data values\n",
    "            tof_class = tof.copy()\n",
    "            tof_class[esa != cover] = 255\n",
    "\n",
    "            # reshape to a 4d array and apply mask \n",
    "            tof_reshaped = reshape_to_4d(tof_class) \n",
    "            tof_reshaped = np.ma.masked_equal(tof_reshaped, 255)\n",
    "            \n",
    "            # count the number of non-masked entries per hectare\n",
    "            tof_class_count_per_ha = np.sum(~tof_reshaped.mask, axis=(1,3), dtype=np.uint8) \n",
    "                        \n",
    "            # get sum of values themselves that are not masked\n",
    "            tof_class_sum_per_ha = np.sum(tof_reshaped, axis=(1,3), dtype=np.uint16)\n",
    "            \n",
    "            # divide the sum by the count (to avoid using np.mean which will use np.float)\n",
    "            tof_class_mean_per_ha = np.divide(tof_class_sum_per_ha, tof_class_count_per_ha, dtype=np.float32)\n",
    "                        \n",
    "            # Return all the non-masked data as a 1-D array (prevent mask from propagating)\n",
    "            tof_class_mean_per_ha = tof_class_mean_per_ha.compressed()    \n",
    "                        \n",
    "            tof_class_mean = np.round(np.mean(tof_class_mean_per_ha), 2)\n",
    "                            \n",
    "            # calculate the area sampled \n",
    "            lc_total = np.sum(esa == cover)/100\n",
    "            lc_sampled = np.sum(~tof_reshaped.mask)/100\n",
    "\n",
    "            # iterate through the thresholds (0-10, 10-20, 20-30)\n",
    "            for lower, upper in zip(lower_rng, upper_rng):\n",
    "\n",
    "                # calculate total ha for that threshold \n",
    "                tof_bin = np.sum((tof_class_mean_per_ha >= lower) & (tof_class_mean_per_ha < upper))\n",
    "                bin_name = (f'{str(lower)}-{str(upper - 1)}')\n",
    "                                \n",
    "                # confirm masked array doesn't propogate\n",
    "                vars_to_check = [lc_sampled, lc_total, tof_bin, tof_class_mean]\n",
    "                \n",
    "                for index, var in enumerate(vars_to_check):\n",
    "                    if var == '--':\n",
    "                        var = 0\n",
    "                \n",
    "                for index, var in enumerate(vars_to_check):\n",
    "                    if np.ma.isMaskedArray(var):\n",
    "                        print(f'Masked array at {index}.')\n",
    "                \n",
    "                # check for erroneous values\n",
    "                if lc_sampled > lc_total:\n",
    "                    raise ValueError(f'Sampled area is greater than total area for land cover {cover} in {file}.')\n",
    "                    \n",
    "                df = df.append({'country': country, \n",
    "                               'admin': file[:-4],\n",
    "                               'esa_id': cover,\n",
    "                               'esa_sampled_ha': lc_sampled,\n",
    "                               'esa_total_ha': lc_total,\n",
    "                               'tree_cover_class': bin_name,\n",
    "                               'tof_ha': tof_bin,\n",
    "                               'tof_mean': tof_class_mean},\n",
    "                                ignore_index=True)\n",
    "                \n",
    "                # reinforce datatypes\n",
    "                convert_dict = {'esa_sampled_ha':'float64',\n",
    "                                'esa_total_ha':'float64',\n",
    "                                'tof_ha':'int64',\n",
    "                                'tof_mean': 'float64'}\n",
    "                df = df.astype(convert_dict)\n",
    "            \n",
    "        # map ESA id numbers to lcc labels\n",
    "        esa_legend = {0: 'ESA No Data',\n",
    "                10: 'Cropland, rainfed',\n",
    "                11: 'Cropland, rainfed',\n",
    "                12: 'Cropland, rainfed',\n",
    "                20: 'Cropland, irrigated or post-flooding',\n",
    "                30: 'Mosaic cropland / natural vegetation',\n",
    "                40: 'Mosaic natural vegetation / cropland',\n",
    "                50: 'Tree cover, broadleaved, evergreen',\n",
    "                60: 'Tree cover, broadleaved, deciduous',\n",
    "                61: 'Tree cover, broadleaved, deciduous',\n",
    "                62: 'Tree cover, broadleaved, deciduous',\n",
    "                70: 'Tree cover, needleleaved, evergreen',\n",
    "                71: 'Tree cover, needleleaved, evergreen',\n",
    "                72: 'Tree cover, needleleaved, evergreen',\n",
    "                80: 'Tree cover, needleleaved, deciduous',\n",
    "                81: 'Tree cover, needleleaved, deciduous',\n",
    "                82: 'Tree cover, needleleaved, deciduous',\n",
    "                90: 'Tree cover, mixed leaf type',\n",
    "                100: 'Mosaic tree and shrub / herbaceous cover',\n",
    "                110: 'Mosaic herbaceous cover / tree and shrub',\n",
    "                120: 'Shrubland',\n",
    "                121: 'Shrubland',\n",
    "                122: 'Shrubland',\n",
    "                130: 'Grassland',\n",
    "                140: 'Lichens and mosses',\n",
    "                150: 'Sparse vegetation',\n",
    "                151: 'Sparse vegetation',\n",
    "                152: 'Sparse vegetation',\n",
    "                153: 'Sparse vegetation',\n",
    "                160: 'Tree cover, flooded, fresh or brakish water',\n",
    "                170: 'Tree cover, flooded, saline water',\n",
    "                180: 'Shrub or herbaceous cover, flooded, fresh/saline/brakish water',\n",
    "                190: 'Urban areas',\n",
    "                200: 'Bare areas',\n",
    "                201: 'Bare areas',\n",
    "                202: 'Bare areas',\n",
    "                210: 'Water bodies',\n",
    "                220: 'Permanent snow and ice',\n",
    "                255: 'No Data (flag)'}\n",
    "     \n",
    "        df['esa_class'] = df['esa_id'].map(esa_legend)\n",
    "        \n",
    "        tof = None\n",
    "        esa = None\n",
    "        \n",
    "        if counter % 3 == 0:\n",
    "            print(f'{counter}/{len(folder_contents)} admins processed...')\n",
    "    \n",
    "    cols_to_check = ['esa_sampled_ha', 'esa_total_ha', 'tof_ha', 'tof_mean']\n",
    "    assert all(ptypes.is_numeric_dtype(df[col]) for col in cols_to_check)\n",
    "    \n",
    "    df.to_csv(f'{country}/stats/{country}_statistics_{extent}_tmlonly.csv', index=False)\n",
    "    print('Analysis complete.')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def calculate_stats(country, extent):\n",
    "    \n",
    "    '''\n",
    "    Takes in a country and extent (partial or full) to import appropriate tml/hansen/esa rasters. \n",
    "    Returns a csv with statistics per administrative district, per land cover class and per tree cover\n",
    "    threshold. Includes Hansen and TML statistics.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(f'{country}/stats'):\n",
    "        os.makedirs(f'{country}/stats')\n",
    "        \n",
    "    df = pd.DataFrame({'country': pd.Series(dtype='str'),\n",
    "                       'admin': pd.Series(dtype='str'),\n",
    "                       'esa_id': pd.Series(dtype='str'),\n",
    "                       'esa_class': pd.Series(dtype='str'),\n",
    "                       'esa_sampled_ha': pd.Series(dtype='float64'),\n",
    "                       'esa_total_ha': pd.Series(dtype='float64'),\n",
    "                       'tree_cover_class': pd.Series(dtype='str'),\n",
    "                       'tof_ha': pd.Series(dtype='int64'),\n",
    "                       'hans_ha': pd.Series(dtype='int64'),\n",
    "                       'tof_mean': pd.Series(dtype='float64'),\n",
    "                       'hans_mean': pd.Series(dtype='float64')})\n",
    "    counter = 0\n",
    "    \n",
    "    folder_contents = [f for f in os.listdir(f'{country}/resampled_rasters/tof') if f != '.ipynb_checkpoints']\n",
    "    \n",
    "    # iterate through the admins \n",
    "    for file in folder_contents:\n",
    "        counter += 1\n",
    "        \n",
    "        tof = rs.open(f'{country}/resampled_rasters/tof/{file}').read(1)\n",
    "        hans = rs.open(f'{country}/resampled_rasters/hansen/{file}').read(1)\n",
    "        esa = rs.open(f'{country}/resampled_rasters/esa/{file}').read(1)\n",
    "        \n",
    "        lower_rng = [x for x in range(0, 100, 10)]\n",
    "        upper_rng = [x for x in range(10, 110, 10)]\n",
    "\n",
    "        # convert values to their median for binning\n",
    "        for lower, upper in zip(lower_rng, upper_rng):\n",
    "            tof[(tof >= lower) & (tof < upper)] = lower + 4.5\n",
    "            hans[(hans >= lower) & (hans < upper)] = lower + 4.5\n",
    "    \n",
    "        # iterate through the land cover classes\n",
    "        esa_classes = np.unique(esa)\n",
    "         \n",
    "        for cover in esa_classes:\n",
    "            \n",
    "            # replace all values not equal to the current lcc with no data values\n",
    "            tof_class = tof.copy()\n",
    "            tof_class[esa != cover] = 255\n",
    "\n",
    "            # reshape to a 4d array and apply mask \n",
    "            tof_reshaped = reshape_to_4d(tof_class) \n",
    "            tof_reshaped = np.ma.masked_equal(tof_reshaped, 255)\n",
    "            \n",
    "            # count the number of non-masked entries per hectare\n",
    "            tof_class_count_per_ha = np.sum(~tof_reshaped.mask, axis=(1,3), dtype=np.uint8) \n",
    "             \n",
    "            # get sum of values themselves that are not masked\n",
    "            tof_class_sum_per_ha = np.sum(tof_reshaped, axis=(1,3), dtype=np.uint16)\n",
    "            \n",
    "            # divide the sum by the count (to avoid using np.mean which will use np.float)\n",
    "            tof_class_mean_per_ha = np.divide(tof_class_sum_per_ha, tof_class_count_per_ha, dtype=np.float32)\n",
    "            \n",
    "            # check the conversion to hectares - should be 10x smaller than tof_class\n",
    "            #print(f'Conversion check. Original: {tof_class.shape} New: {tof_class_mean_per_ha.shape}')\n",
    "            \n",
    "            # Return all the non-masked data as a 1-D array (prevent mask from propagating)\n",
    "            tof_class_mean_per_ha = tof_class_mean_per_ha.compressed()    \n",
    "                        \n",
    "            tof_class_mean = np.round(np.mean(tof_class_mean_per_ha), 2)\n",
    "                \n",
    "            # apply same steps to hansen\n",
    "            hans_class = hans.copy()\n",
    "            hans_class[esa != cover] = 255\n",
    "            hans_reshaped = reshape_to_4d(hans_class) \n",
    "            hans_reshaped = np.ma.masked_equal(hans_reshaped, 255)\n",
    "            hans_class_count_per_ha = np.sum(~hans_reshaped.mask, axis=(1,3), dtype=np.uint8) \n",
    "            hans_class_sum_per_ha = np.sum(hans_reshaped, axis=(1,3), dtype=np.uint16)\n",
    "            hans_class_mean_per_ha = np.divide(hans_class_sum_per_ha, hans_class_count_per_ha, dtype=np.float32)\n",
    "            hans_class_mean_per_ha = hans_class_mean_per_ha.compressed()\n",
    "            hans_class_mean = np.round(np.mean(hans_class_mean_per_ha), 2)\n",
    "            \n",
    "            # calculate the area sampled \n",
    "            lc_total = np.sum(esa == cover)/100\n",
    "            lc_sampled = np.sum(~tof_reshaped.mask)/100\n",
    "\n",
    "            # iterate through the thresholds (0-10, 10-20, 20-30)\n",
    "            for lower, upper in zip(lower_rng, upper_rng):\n",
    "\n",
    "                # calculate total ha for that threshold \n",
    "                tof_bin = np.sum((tof_class_mean_per_ha >= lower) & (tof_class_mean_per_ha < upper))\n",
    "                hans_bin = np.sum((hans_class_mean_per_ha >= lower) & (hans_class_mean_per_ha < upper))\n",
    "                bin_name = (f'{str(lower)}-{str(upper - 1)}')\n",
    "                                \n",
    "                # confirm masked array doesn't propogate\n",
    "                vars_to_check = [lc_sampled, lc_total, tof_bin, hans_bin, tof_class_mean, hans_class_mean]\n",
    "                \n",
    "                for index, var in enumerate(vars_to_check):\n",
    "                    if var == '--':\n",
    "                        var = 0\n",
    "                \n",
    "                for index, var in enumerate(vars_to_check):\n",
    "                    if np.ma.isMaskedArray(var):\n",
    "                        print(f'Masked array at {index}.')\n",
    "                \n",
    "                # check for erroneous values\n",
    "                assert ~np.ma.isMaskedArray(var)\n",
    "                if lc_sampled > lc_total:\n",
    "                    raise ValueError(f'Sampled area is greater than total area for land cover {cover} in {file}.')\n",
    "                    \n",
    "                df = df.append({'country': country, \n",
    "                               'admin': file[:-4],\n",
    "                               'esa_id': cover,\n",
    "                               'esa_sampled_ha': lc_sampled,\n",
    "                               'esa_total_ha': lc_total,\n",
    "                               'tree_cover_class': bin_name,\n",
    "                               'tof_ha': tof_bin,\n",
    "                               'hans_ha': hans_bin,\n",
    "                               'tof_mean': tof_class_mean, \n",
    "                               'hans_mean': hans_class_mean},\n",
    "                                ignore_index=True)\n",
    "                \n",
    "                # reinforce datatypes\n",
    "                convert_dict = {'esa_sampled_ha':'float64',\n",
    "                                'esa_total_ha':'float64',\n",
    "                                'tof_ha':'int64',\n",
    "                                'hans_ha': 'int64',\n",
    "                                'tof_mean': 'float64',\n",
    "                                'hans_mean': 'float64'}\n",
    "                df = df.astype(convert_dict)\n",
    "            \n",
    "        # map ESA id numbers to lcc labels\n",
    "        esa_legend = {0: 'ESA No Data',\n",
    "                10: 'Cropland, rainfed',\n",
    "                11: 'Cropland, rainfed',\n",
    "                12: 'Cropland, rainfed',\n",
    "                20: 'Cropland, irrigated or post-flooding',\n",
    "                30: 'Mosaic cropland / natural vegetation',\n",
    "                40: 'Mosaic natural vegetation / cropland',\n",
    "                50: 'Tree cover, broadleaved, evergreen',\n",
    "                60: 'Tree cover, broadleaved, deciduous',\n",
    "                61: 'Tree cover, broadleaved, deciduous',\n",
    "                62: 'Tree cover, broadleaved, deciduous',\n",
    "                70: 'Tree cover, needleleaved, evergreen',\n",
    "                71: 'Tree cover, needleleaved, evergreen',\n",
    "                72: 'Tree cover, needleleaved, evergreen',\n",
    "                80: 'Tree cover, needleleaved, deciduous',\n",
    "                81: 'Tree cover, needleleaved, deciduous',\n",
    "                82: 'Tree cover, needleleaved, deciduous',\n",
    "                90: 'Tree cover, mixed leaf type',\n",
    "                100: 'Mosaic tree and shrub / herbaceous cover',\n",
    "                110: 'Mosaic herbaceous cover / tree and shrub',\n",
    "                120: 'Shrubland',\n",
    "                121: 'Shrubland',\n",
    "                122: 'Shrubland',\n",
    "                130: 'Grassland',\n",
    "                140: 'Lichens and mosses',\n",
    "                150: 'Sparse vegetation',\n",
    "                151: 'Sparse vegetation',\n",
    "                152: 'Sparse vegetation',\n",
    "                153: 'Sparse vegetation',\n",
    "                160: 'Tree cover, flooded, fresh or brakish water',\n",
    "                170: 'Tree cover, flooded, saline water',\n",
    "                180: 'Shrub or herbaceous cover, flooded, fresh/saline/brakish water',\n",
    "                190: 'Urban areas',\n",
    "                200: 'Bare areas',\n",
    "                201: 'Bare areas',\n",
    "                202: 'Bare areas',\n",
    "                210: 'Water bodies',\n",
    "                220: 'Permanent snow and ice',\n",
    "                255: 'No Data (flag)'}\n",
    "     \n",
    "        df['esa_class'] = df['esa_id'].map(esa_legend)\n",
    "        \n",
    "        tof = None\n",
    "        esa = None\n",
    "        hans = None\n",
    "        \n",
    "        if counter % 3 == 0:\n",
    "            print(f'{counter}/{len(folder_contents)} admins processed...')\n",
    "    \n",
    "    cols_to_check = ['esa_sampled_ha', 'esa_total_ha', 'tof_ha', 'hans_ha', 'tof_mean', 'hans_mean']\n",
    "    assert all(ptypes.is_numeric_dtype(df[col]) for col in cols_to_check)\n",
    "    \n",
    "    df.to_csv(f'{country}/stats/{country}_statistics_{extent}.csv', index=False)\n",
    "    print('Analysis complete.')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def execute_pipe(country, extent, incl_hansen=True):\n",
    "    print(f'Started at: {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "    print('Converting shapefile to geojson...')\n",
    "    shape_to_gjson(country)\n",
    "    if incl_hansen:\n",
    "        print('Building Hansen tree cover raster...')\n",
    "        create_hansen_tif(country)\n",
    "        print('Removing tree cover loss...')\n",
    "        remove_loss(country)\n",
    "    print('Padding tml raster...')\n",
    "    pad_tml_raster(country)\n",
    "    print('Clipping rasters by admin boundary...')\n",
    "    create_clippings(country, multi_analysis=incl_hansen)\n",
    "    print('Resampling to match raster extents and resolutions...')\n",
    "    apply_extent_res(country, multi_analysis=incl_hansen)\n",
    "    print('Merging admins containing multiple polygons...')\n",
    "    merge_polygons(country, multi_analysis=incl_hansen)\n",
    "    print('Checking size...')\n",
    "    processing_check(country)\n",
    "    print('Calculating statistics...')\n",
    "    if incl_hansen:\n",
    "        calculate_stats(country, extent)\n",
    "    else:\n",
    "        calculate_stats_tml(country, extent)\n",
    "    print(f'Finished {extent} processing at: {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "\n",
    "\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tml-analysis",
   "language": "python",
   "name": "tml-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "174px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
