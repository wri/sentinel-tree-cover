{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Cover Statistics: Data Prep & Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rs\n",
    "from rasterio.mask import mask\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show, show_hist, adjust_band\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio import Affine, MemoryFile\n",
    "from rasterio.windows import Window\n",
    "\n",
    "import numpy as np \n",
    "import numpy.ma as ma \n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import pyproj\n",
    "import geopandas as gpd \n",
    "import shapely\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "import pandas as pd\n",
    "import fiona\n",
    "from contextlib import contextmanager  \n",
    "from skimage.transform import resize\n",
    "import math\n",
    "import urllib.request\n",
    "import osgeo\n",
    "from osgeo import gdal\n",
    "from osgeo import gdalconst\n",
    "import glob\n",
    "from copy import copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.5'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapefile to Geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shp_to_gjson(country):\n",
    "    shapefile = glob.glob(f'{country}/shapefile/*.shp')\n",
    "    new_shp = gpd.read_file(shapefile[0])\n",
    "    new_shp.to_file(f'{country}/{country}_adminboundaries.geojson', driver='GeoJSON')\n",
    "    print(f'There are {len(new_shp)} admins in {country}.')\n",
    "    assert new_shp.crs == 'epsg:4326'\n",
    "    assert new_shp.NAME_1.duplicated().sum() == 0\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hansen Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hansen_tif(country):\n",
    "    '''\n",
    "    Identifies the latitude and longitude coordinates for a country \n",
    "    to download Hansen tree cover and tree cover loss tif files. \n",
    "    Returns combined tifs as one file in the country's folder.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    \n",
    "    '''\n",
    "    gdal.UseExceptions()\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries.geojson')\n",
    "    \n",
    "    # identify min/max bounds for the country\n",
    "    bounds = shapefile.geometry.bounds\n",
    "    min_x = bounds.minx.min() \n",
    "    min_y = bounds.miny.min()\n",
    "    max_x = bounds.maxx.max()\n",
    "    max_y = bounds.maxy.max()\n",
    "\n",
    "    # identify the lowest and highest 10 lat/lon increments for the country\n",
    "    # index on top left corner\n",
    "    lower_x = math.floor(min_x / 10) * 10 \n",
    "    lower_y = math.ceil(min_y / 10) * 10 \n",
    "    upper_x = math.ceil(max_x / 10) * 10 \n",
    "    upper_y = math.ceil(max_y / 10) * 10\n",
    "    #print(f'Rounded coords for {country}: ({lower_x}, {lower_y}, {upper_x}, {upper_y})')\n",
    "    \n",
    "    lon = 'N' if lower_y >= 0 else 'S'\n",
    "    lat = 'E' if lower_x >= 0 else 'W'\n",
    "    print(f'{country} has lon {lon} and lat {lat}.')\n",
    "\n",
    "    # create a list of tif file names for the country\n",
    "    tree_cover_files = []\n",
    "    loss_files = []\n",
    "    \n",
    "    print('Downloading files from GLAD...')\n",
    "    \n",
    "    for x_grid in range(lower_x, upper_x, 10):\n",
    "        for y_grid in range(lower_y, upper_y + 10, 10):\n",
    "            \n",
    "#             lon = 'N' if y_grid == 0 # to pull the correct Hansen files this must be set to N\n",
    "#             lat = 'E' if\n",
    "#             print(y_grid, x_grid)\n",
    " \n",
    "            # download tree cover and loss files from UMD\n",
    "            cover_url =  f'https://glad.umd.edu/Potapov/TCC_2010/treecover2010_' \\\n",
    "                         f'{str(y_grid).zfill(2)}{lon}_{str(np.absolute(x_grid)).zfill(3)}{lat}.tif'\n",
    "            cover_dest = f'/Users/jessicaertel/wri/restoration-mapper/notebooks/analysis/hansen_treecover2010/'\\\n",
    "                         f'{str(y_grid).zfill(2)}{lon}_{str(np.absolute(x_grid)).zfill(3)}{lat}.tif'\n",
    "            urllib.request.urlretrieve(cover_url, cover_dest)\n",
    "            \n",
    "            loss_url =  f'https://storage.googleapis.com/earthenginepartners-hansen/GFC-2020-v1.8/Hansen_GFC-2020-v1.8_lossyear_' \\\n",
    "                         f'{str(y_grid).zfill(2)}{lon}_{str(np.absolute(x_grid)).zfill(3)}{lat}.tif'\n",
    "            loss_dest = f'/Users/jessicaertel/wri/restoration-mapper/notebooks/analysis/hansen_lossyear2020/'\\\n",
    "                         f'{str(y_grid).zfill(2)}{lon}_{str(np.absolute(x_grid)).zfill(3)}{lat}.tif'\n",
    "            urllib.request.urlretrieve(loss_url, loss_dest)\n",
    "            \n",
    "            if not os.path.exists(cover_dest) or not os.path.exists(loss_dest):\n",
    "                print(f'Files were not downloaded.')\n",
    "                \n",
    "            tree_cover_files.append(cover_dest)\n",
    "            loss_files.append(loss_dest)\n",
    "    \n",
    "    # remove duplicate file names\n",
    "    tree_tifs = [x for x in tree_cover_files if os.path.exists(x)] \n",
    "    loss_tifs = [x for x in loss_files if os.path.exists(x)]\n",
    "    \n",
    "    # convert tree cover and loss tifs into a virtual raster tile  \n",
    "    gdal.BuildVRT(f'{country}/{country}_hansen_treecover2010.vrt', tree_tifs)\n",
    "    gdal.BuildVRT(f'{country}/{country}_hansen_loss2020.vrt', loss_tifs)\n",
    "\n",
    "    # open vrts and convert to a single .tif\n",
    "    # FLAG -- adding tfw=yes increased file size significantly\n",
    "    translateoptions = gdal.TranslateOptions(format='Gtiff', \n",
    "                                              outputSRS='EPSG:4326',\n",
    "                                              outputType=gdal.GDT_Byte,\n",
    "                                              noData=255,\n",
    "                                              creationOptions=['COMPRESS=LZW'],\n",
    "                                              resampleAlg='nearest')\n",
    " \n",
    "    source = gdal.Open(f'{country}/{country}_hansen_treecover2010.vrt', )\n",
    "    ds = gdal.Translate(f'{country}/{country}_hansen_treecover2010.tif', source, options=translateoptions)\n",
    "    os.remove(f'{country}/{country}_hansen_treecover2010.vrt')\n",
    "                      \n",
    "    source = gdal.Open(f'{country}/{country}_hansen_loss2020.vrt')\n",
    "    ds = gdal.Translate(f'{country}/{country}_hansen_loss2020.tif', source, options=translateoptions)\n",
    "    os.remove(f'{country}/{country}_hansen_loss2020.vrt')\n",
    "    \n",
    "    assert os.path.exists(f'{country}/{country}_hansen_treecover2010.tif')\n",
    "    assert os.path.exists(f'{country}/{country}_hansen_loss2020.tif')\n",
    "\n",
    "    # if new files are properly create, delete what is not needed\n",
    "    for file in tree_cover_files:\n",
    "        os.remove(file)\n",
    "    \n",
    "    for file in loss_files:\n",
    "        os.remove(file)\n",
    "    \n",
    "    print('Hansen raster built.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_loss(country):\n",
    "    '''\n",
    "    Takes in a country name to import hansen tree cover loss tifs. Updates tree cover \n",
    "    to 0 if loss was detected between 2011-2020. Returns updated tif in the country's \n",
    "    folder.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "      '''\n",
    "    gdal.UseExceptions()\n",
    "    hansen_cover = rs.open(f'{country}/{country}_hansen_treecover2010.tif').read(1) \n",
    "    hansen_loss = rs.open(f'{country}/{country}_hansen_loss2020.tif').read(1)\n",
    "    \n",
    "     # assert raster shape, datatype and max/min values\n",
    "    assert hansen_cover.dtype == 'uint8'\n",
    "    assert hansen_cover.shape != (0, ) and len(hansen_cover.shape) <= 2\n",
    "    assert hansen_cover.max() <= 100 and hansen_cover.min() >= 0\n",
    "    assert hansen_loss.dtype == 'uint8'\n",
    "    assert hansen_loss.shape != (0, ) and len(hansen_loss.shape) <= 2\n",
    "    assert hansen_loss.max() <= 20 and hansen_cover.min() >= 0\n",
    "    \n",
    "    # If there was loss between 2011-2020 (values between 11-20, make then 0 in tree cover\n",
    "    hansen_cover_new = np.where((hansen_loss >= 11) & (hansen_loss <= 20), 0, hansen_cover)\n",
    "    # hansen_cover[np.logical_and(hansen_loss >= 11, hansen_loss <= 20)] = 0  # logical_and isn't working\n",
    "    \n",
    "    # check bin counts after loss removed\n",
    "    print(f'{(np.sum(hansen_cover > 0)) - (np.sum(hansen_cover_new > 0))} pixels converted to loss.')\n",
    "    \n",
    "    # save updated raster\n",
    "    out_meta = rs.open(f'{country}/{country}_hansen_treecover2010.tif').meta\n",
    "    out_meta.update({'driver': 'GTiff',    \n",
    "                     'dtype': 'uint8',\n",
    "                     'height': hansen_cover_new.shape[0],\n",
    "                     'width': hansen_cover_new.shape[1],\n",
    "                     'count': 1,\n",
    "                     'compress':'lzw'})\n",
    "    outpath = f'{country}/{country}_hansen_treecover2010_wloss.tif'\n",
    "    with rs.open(outpath, 'w', **out_meta) as dest:\n",
    "            dest.write(hansen_cover_new, 1) \n",
    "    \n",
    "    # remove original hansen tree cover and loss files\n",
    "    os.remove(f'{country}/{country}_hansen_treecover2010.tif')\n",
    "    os.remove(f'{country}/{country}_hansen_loss2020.tif')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad TOF Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tof_raster(country):\n",
    "    \n",
    "    '''\n",
    "    Increase the raster extent to match the boundas of a country's shapefile\n",
    "    and fill with no data value.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "    \n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries.geojson')\n",
    "\n",
    "    # identify min/max bounds for the country\n",
    "    bounds = shapefile.geometry.bounds\n",
    "    min_x = bounds.minx.min() \n",
    "    min_y = bounds.miny.min()\n",
    "    max_x = bounds.maxx.max()\n",
    "    max_y = bounds.maxy.max()\n",
    "    #print(f'Original bounds: ({min_x}, {min_y}, {max_x}, {max_y})')\n",
    "    \n",
    "    # round to the nearest .1 lat/lon\n",
    "    lower_x = math.floor(min_x * 10) / 10 \n",
    "    lower_y = math.floor(min_y * 10) / 10 \n",
    "    upper_x = math.ceil(max_x * 10) / 10\n",
    "    upper_y = math.ceil(max_y * 10) / 10\n",
    "    #print(f'Padding bounds: ({lower_x}, {lower_y}, {upper_x}, {upper_y}')\n",
    "          \n",
    "    # create tif with new output bounds, filled with no data value\n",
    "    warp_options = gdal.WarpOptions(format='Gtiff', \n",
    "                                    dstSRS='EPSG:4326',\n",
    "                                    dstNodata=255,\n",
    "                                    outputBounds=[lower_x, lower_y, upper_x, upper_y],\n",
    "                                    resampleAlg='near',\n",
    "                                    outputType=osgeo.gdalconst.GDT_Byte,\n",
    "                                    creationOptions=['TFW=YES', 'COMPRESS=LZW'])\n",
    "          \n",
    "    ds = gdal.Warp(f'{country}/{country}_tof_padded.tif', \n",
    "                   f'{country}/{country}_tof.tif',      \n",
    "                   options=warp_options)                  \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip Rasters by Admin Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clippings(country):\n",
    "    '''\n",
    "    Takes in a country name to import tof/hansen rasters and masks out administrative \n",
    "    boundaries based on the shapefile. Saves exploded shapefile as a geojson with polygons \n",
    "    split/numbered for each admin boundary. Returns clipped rasters as individual \n",
    "    files in the country's \"clipped_rasters\" folder. Deletes the original Hansen file. \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import        \n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(f'{country}/clipped_rasters/hansen'):\n",
    "        os.makedirs(f'{country}/clipped_rasters/hansen')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/clipped_rasters/tof'):\n",
    "        os.makedirs(f'{country}/clipped_rasters/tof')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/clipped_rasters/esa'):\n",
    "        os.makedirs(f'{country}/clipped_rasters/esa')\n",
    "    \n",
    "    orig_shapefile = gpd.read_file(f'{country}/{country}_adminboundaries.geojson')\n",
    "    tof_raster = rs.open(f'{country}/{country}_tof_padded.tif') \n",
    "    hansen_raster = rs.open(f'{country}/{country}_hansen_treecover2010_wloss.tif')\n",
    "    esa_raster = rs.open('ESACCI-LC-L4-LCCS-Map-300m-P1Y-2015-v2.0.7.tif')\n",
    "    \n",
    "    # preprocess shapefile from multipolygon to single\n",
    "    counter = 0\n",
    "    for idx, row in orig_shapefile.iterrows():\n",
    "        counter += 1 if type(row.geometry) == MultiPolygon else 0\n",
    "\n",
    "    if counter > 0:\n",
    "        shapefile = orig_shapefile.explode()\n",
    "        \n",
    "        # add integer to admin name if multi polys\n",
    "        shapefile.NAME_1 = np.where(shapefile.NAME_1.duplicated(keep=False), \n",
    "                                     shapefile.NAME_1 + shapefile.groupby('NAME_1').cumcount().add(1).astype(str),\n",
    "                                     shapefile.NAME_1)\n",
    "\n",
    "        shapefile = shapefile.reset_index()\n",
    "        shapefile.drop(columns=['level_0', 'level_1'], inplace=True)\n",
    "    \n",
    "    # if no multi polys save original shapefile under new name\n",
    "    else:\n",
    "        shapefile = orig_shapefile\n",
    "        print(f'No MultiPolygons in {country}.')\n",
    "    \n",
    "    shapefile.to_file(f'{country}/{country}_adminboundaries_exp.geojson', driver='GeoJSON')\n",
    "    \n",
    "    def mask_raster(polygon, admin, raster, folder):\n",
    "        out_img, out_transform = mask(dataset=raster, shapes=[polygon], crop=True, nodata=0)\n",
    "        out_meta = raster.meta\n",
    "        out_meta.update({'driver': 'GTiff',    \n",
    "                         'dtype': 'uint8',\n",
    "                         'height': out_img.shape[1],\n",
    "                         'width': out_img.shape[2],\n",
    "                         'transform': out_transform})\n",
    "        outpath = f'{country}/clipped_rasters/{folder}/{admin}.tif'\n",
    "        with rs.open(outpath, 'w', **out_meta) as dest:\n",
    "            dest.write(out_img)\n",
    "        return None\n",
    "    \n",
    "    for polygon, admin in zip(shapefile.geometry, shapefile.NAME_1):\n",
    "        mask_raster(polygon, admin, tof_raster, 'tof')\n",
    "        mask_raster(polygon, admin, hansen_raster, 'hansen')\n",
    "        mask_raster(polygon, admin, esa_raster, 'esa')\n",
    "        \n",
    "    \n",
    "    # delete Tof and Hansen files once clippings created\n",
    "    os.remove(f'{country}/{country}_hansen_treecover2010_wloss.tif')\n",
    "    os.remove(f'{country}/{country}_tof_padded.tif')\n",
    "    os.remove(f'{country}/{country}_tof_padded.tfw')\n",
    "    \n",
    "    print(f\"{country}'s rasters clipped and saved.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No MultiPolygons in Burundi.\n",
      "Burundi's rasters clipped and saved.\n"
     ]
    }
   ],
   "source": [
    "create_clippings('Burundi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample to Match Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_extent_and_res(source, reference, out_filename, tof=False, esa=False):\n",
    "\n",
    "    '''\n",
    "    Matches the projection, bounding box, and dimensions of source to reference\n",
    "    '''\n",
    "    \n",
    "    # set up the source file \n",
    "    src = gdal.Open(source, gdalconst.GA_ReadOnly)\n",
    "    src_proj = src.GetProjection()\n",
    "    src_geotrans = src.GetGeoTransform()\n",
    "\n",
    "    # set up the reference file (esa)\n",
    "    ref_ds = gdal.Open(reference, gdalconst.GA_ReadOnly)\n",
    "    ref_proj = ref_ds.GetProjection()\n",
    "    ref_geotrans = ref_ds.GetGeoTransform()\n",
    "    \n",
    "    # create height/width for the interpolation (ref dataset except for tof)\n",
    "    width = ref_ds.RasterXSize if not tof else src.RasterXSize\n",
    "    height = ref_ds.RasterYSize if not tof else src.RasterYSize\n",
    "\n",
    "    out = gdal.GetDriverByName('GTiff').Create(out_filename, width, height, 1, gdalconst.GDT_Byte)\n",
    "    \n",
    "    # do not adjust the bounds for esa, use source (esa)\n",
    "    if esa:\n",
    "        ref_proj = src_proj\n",
    "        #ref_geotrans = src.GetGeoTransform()\n",
    "    \n",
    "    # set geotrans and proj for the out file\n",
    "    out.SetGeoTransform(ref_geotrans)\n",
    "    out.SetProjection(ref_proj)\n",
    "\n",
    "    interpolation = gdalconst.GRA_NearestNeighbour\n",
    "    gdal.ReprojectImage(src, out, src_proj, ref_proj, interpolation)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_extent_res(country):\n",
    "    \n",
    "    '''\n",
    "    Applies match_raster_extent_and_res() to all admin files\n",
    "    for a country.\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(f'{country}/resampled_rasters/hansen'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/hansen')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/resampled_rasters/tof'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/tof')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/resampled_rasters/esa'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/esa')\n",
    "        \n",
    "    \n",
    "    # import new shapefile containing only polygons\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries_exp.geojson')\n",
    "    admin_boundaries = list(shapefile.NAME_1)\n",
    "    \n",
    "    for admin in admin_boundaries:\n",
    "        \n",
    "        # apply to esa\n",
    "        match_extent_and_res(f'{country}/clipped_rasters/esa/{admin}.tif', # source\n",
    "                             f'{country}/clipped_rasters/tof/{admin}.tif', # reference\n",
    "                             f'{country}/resampled_rasters/esa/{admin}.tif', # outpath\n",
    "                             tof = False, # is this tof?\n",
    "                             esa = True) # is this esa?\n",
    "        \n",
    "        # apply to tof\n",
    "        match_extent_and_res(f'{country}/clipped_rasters/tof/{admin}.tif', \n",
    "                             f'{country}/resampled_rasters/esa/{admin}.tif', \n",
    "                             f'{country}/resampled_rasters/tof/{admin}.tif', \n",
    "                             tof = True, \n",
    "                             esa = False) \n",
    "        \n",
    "        # apply to hansen\n",
    "        match_extent_and_res(f'{country}/clipped_rasters/hansen/{admin}.tif', \n",
    "                             f'{country}/resampled_rasters/esa/{admin}.tif', \n",
    "                             f'{country}/resampled_rasters/hansen/{admin}.tif', \n",
    "                             tof = False, \n",
    "                             esa = False) \n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Admin Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_polygons(country):\n",
    "    '''\n",
    "    Takes in a country to iterate through the resampled rasters and identify\n",
    "    which admin boundaries are composed of multipolygons. Combines individual files\n",
    "    into one for the admin district, then deletes the individual files.\n",
    "    '''\n",
    "\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries_exp.geojson')\n",
    "    admin_boundaries_all = list(shapefile.NAME_1)\n",
    "    \n",
    "    # creates a list of admins that need to be merged (digits in filename)\n",
    "    no_ints = []\n",
    "    for admin in admin_boundaries_all:\n",
    "        \n",
    "        # if any characters are digits, remove them and ad admin to list\n",
    "        if any(char.isdigit() for char in admin):\n",
    "            clean_admin = ''.join([char for char in admin if not char.isdigit()])\n",
    "            no_ints.append(clean_admin)\n",
    "\n",
    "    no_ints = list(set(no_ints))\n",
    "    print(f'{len(no_ints)} admins will be merged: {no_ints}')\n",
    "\n",
    "    datasets = ['tof', 'hansen', 'esa']\n",
    "    \n",
    "    for data in datasets:\n",
    "        for admin_2 in no_ints:\n",
    "\n",
    "            # gather list of files for that admin (ex: Puntarenas1.tif, Puntarenas2.tif, Puntarenas3.tif)\n",
    "            files_to_merge = [] # items need to be in dataset reader mode\n",
    "            files_to_delete = [] # items are just string of the file name\n",
    "\n",
    "            for path in glob.glob(f'{country}/resampled_rasters/{data}/{admin_2}?.tif'):\n",
    "                filename = os.path.basename(path) \n",
    "                files_to_delete.append(filename)\n",
    "                src = rs.open(f'{country}/resampled_rasters/{data}/{filename}')\n",
    "                files_to_merge.append(src)\n",
    "\n",
    "            # capture double digits\n",
    "            for path in glob.glob(f'{country}/resampled_rasters/{data}/{admin_2}??.tif'):\n",
    "                filename = os.path.basename(path) \n",
    "                files_to_delete.append(filename)\n",
    "                src = rs.open(f'{country}/resampled_rasters/{data}/{filename}')\n",
    "                files_to_merge.append(src)\n",
    "\n",
    "            # capture triple digits\n",
    "            for path in glob.glob(f'{country}/resampled_rasters/{data}/{admin_2}???.tif'):\n",
    "                filename = os.path.basename(path) \n",
    "                files_to_delete.append(filename)\n",
    "                src = rs.open(f'{country}/resampled_rasters/{data}/{filename}')\n",
    "                files_to_merge.append(src)\n",
    "\n",
    "            if len(files_to_merge) < 1:\n",
    "                print(f'No files to merge in {data}.')\n",
    "\n",
    "            mosaic, out_transform = merge(files_to_merge)\n",
    "\n",
    "            outpath = f'{country}/resampled_rasters/{data}/{admin_2}.tif'\n",
    "            out_meta = src.meta.copy()\n",
    "            out_meta.update({'driver': \"GTiff\",\n",
    "                             'dtype': 'uint8',\n",
    "                             'height': mosaic.shape[1],\n",
    "                             'width': mosaic.shape[2],\n",
    "                             'transform': out_transform})\n",
    "\n",
    "            with rs.open(outpath, \"w\", **out_meta) as dest:\n",
    "                dest.write(mosaic)\n",
    "\n",
    "            # delete the old separated tifs\n",
    "            for file in files_to_delete:\n",
    "                os.remove(f'{country}/resampled_rasters/{data}/{file}')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_4d(raster):\n",
    "    \n",
    "    '''\n",
    "    Takes in a raster, identifies the dimensions and them down to the nearest 10th.\n",
    "    Returns a reshaped 10x10 grid array. \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    raster : str\n",
    "        tree cover raster file to be reshaped\n",
    "    '''\n",
    "    \n",
    "    def round_down(num, divisor):\n",
    "         return num - (num%divisor)\n",
    "   \n",
    "    # round down rows and cols to nearest 10th\n",
    "    rows, cols = round_down(raster.shape[0], 10), round_down(raster.shape[1], 10)\n",
    "    \n",
    "    # clip according to rounded numbers and reshape\n",
    "    rounded = raster[:rows, :cols]\n",
    "    reshaped = np.reshape(rounded, (rounded.shape[0] // 10, 10, rounded.shape[1] // 10, 10))\n",
    "        \n",
    "    return reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(country):\n",
    "    \n",
    "    '''\n",
    "    Takes in a country to import appropriate tof/hansen/esa rasters. Returns a csv \n",
    "    with statistics per administrative district, per land cover class and per tree cover\n",
    "    threshold.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(f'{country}/stats'):\n",
    "        os.makedirs(f'{country}/stats')\n",
    "        \n",
    "    # set up the dataframe\n",
    "    df = pd.DataFrame(columns=['country','admin','esa_id','esa_class',\n",
    "                               'esa_sampled_ha','esa_total_ha','tree_cover_class',\n",
    "                               'tof_ha','hans_ha', 'tof_mean', 'hans_mean']) \n",
    "    counter = 0\n",
    "    \n",
    "    folder_contents = [f for f in os.listdir(f'{country}/resampled_rasters/tof') if f != '.ipynb_checkpoints']\n",
    "    \n",
    "    # iterate through the admins\n",
    "    for file in folder_contents:\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        tof = rs.open(f'{country}/resampled_rasters/tof/{file}').read(1).astype(np.float32)\n",
    "        hans = rs.open(f'{country}/resampled_rasters/hansen/{file}').read(1).astype(np.float32)\n",
    "        esa = rs.open(f'{country}/resampled_rasters/esa/{file}').read(1).astype(np.float32)\n",
    "        \n",
    "        lower_rng = [x for x in range(0, 100, 10)]\n",
    "        upper_rng = [x for x in range(10, 110, 10)]\n",
    "\n",
    "        # convert values to their median for binning\n",
    "        for lower, upper in zip(lower_rng, upper_rng):\n",
    "            \n",
    "            tof[(tof >= lower) & (tof < upper)] = lower + 4.5\n",
    "            hans[(hans >= lower) & (hans < upper)] = lower + 4.5\n",
    "    \n",
    "        # iterate through the land cover classes\n",
    "        esa_classes = np.unique(esa)\n",
    "        for cover in esa_classes:\n",
    "            print(cover)\n",
    "            # change all values that are not equal to the lcc to NaN including no data vals\n",
    "            tof_class = tof.copy()\n",
    "            tof_class[esa != cover] = np.nan \n",
    "            tof_class[tof_class == 255] = np.nan\n",
    "\n",
    "            # reshape and calculate stats\n",
    "            tof_reshaped = reshape_to_4d(tof_class) \n",
    "            tof_class_mean = np.nanmean(tof_reshaped)\n",
    "            tof_class_mean_per_ha = np.nanmean(tof_reshaped, axis=(1,3))\n",
    "\n",
    "            # same for Hansen\n",
    "            hans_class = hans.copy()\n",
    "            hans_class[esa != cover] = np.nan\n",
    "            hans_class[hans_class == 255] = np.nan\n",
    "\n",
    "            hans_reshaped = reshape_to_4d(hans_class)\n",
    "            hans_class_mean = np.nanmean(hans_reshaped)\n",
    "            hans_class_mean_per_ha = np.nanmean(hans_reshaped, axis=(1,3)) \n",
    "\n",
    "            # iterate through the thresholds (0-10, 10-20, 20-30)\n",
    "            for lower, upper in zip(lower_rng, upper_rng):\n",
    "\n",
    "                # calculate total ha for that threshold \n",
    "                tof_bin = np.sum((tof_class_mean_per_ha >= lower) & (tof_class_mean_per_ha < upper))\n",
    "                hans_bin = np.sum((hans_class_mean_per_ha >= lower) & (hans_class_mean_per_ha < upper))\n",
    "                bin_name = (f'{str(lower)}-{str(upper - 1)}')\n",
    "    \n",
    "                # do we need to account for the total admin sampled?\n",
    "                # area of lc sampled (tof is NOT null) and total area (esa raster equals cover)\n",
    "                print(f'esa:{esa.shape}')\n",
    "                print(f'tof:{tof.shape}')\n",
    "                lc_sampled = np.sum(~np.isnan(tof_class)) / 100   \n",
    "                # need to ensure this counts the no data class correctly (no data label is 0.0)\n",
    "                lc_total = np.count_nonzero(esa == 0.0) / 100 if cover == 0.0 else np.sum(esa[esa == cover]) / 100\n",
    "    \n",
    "                # check for erroneous calculations\n",
    "                if lc_sampled > lc_total:\n",
    "                    raise ValueError(f'Sampled area is greater than total area for land cover {cover} in {file}.')\n",
    "                    \n",
    "                df = df.append({'country': country, \n",
    "                               'admin': file[:-4],\n",
    "                               'esa_id': cover,\n",
    "                               'esa_sampled_ha': lc_sampled,\n",
    "                               'esa_total_ha': lc_total,\n",
    "                               'tree_cover_class': bin_name,\n",
    "                               'tof_ha': tof_bin,\n",
    "                               'hans_ha': hans_bin,\n",
    "                               'tof_mean': tof_class_mean, \n",
    "                               'hans_mean': hans_class_mean},\n",
    "                                ignore_index=True)\n",
    "        \n",
    "        # map ESA id numbers to lcc labels\n",
    "        esa_legend = {0: 'No Data',\n",
    "                10: 'Cropland, rainfed',\n",
    "                11: 'Cropland, rainfed, herbaceous cover',\n",
    "                20: 'Cropland, irrigated or post-flooding',\n",
    "                30: 'Mosaic cropland / natural vegetation',\n",
    "                40: 'Mosaic natural vegetation / cropland',\n",
    "                50: 'Tree cover, broadleaved, evergreen',\n",
    "                60: 'Tree cover, broadleaved, deciduous',\n",
    "                70: 'Tree cover, needleleaved, evergreen',\n",
    "                80: 'Tree cover, needleleaved, deciduous',\n",
    "                90: 'Tree cover, mixed leaf type',\n",
    "                100: 'Mosaic tree and shrub / herbaceous cover',\n",
    "                110: 'Mosaic herbaceous cover / tree and shrub',\n",
    "                120: 'Shrubland',\n",
    "                130: 'Grassland',\n",
    "                140: 'Lichens and mosses',\n",
    "                150: 'Sparse vegetation',\n",
    "                160: 'Tree cover, flooded, fresh or brakish water',\n",
    "                170: 'Tree cover, flooded, saline water',\n",
    "                180: 'Shrub or herbaceous cover, flooded, fresh/saline/brakish water',\n",
    "                190: 'Urban areas',\n",
    "                200: 'Bare areas',\n",
    "                210: 'Water bodies',\n",
    "                220: 'Permanent snow and ice'}\n",
    "     \n",
    "        df['esa_class'] = df['esa_id'].map(esa_legend)\n",
    "        \n",
    "        if counter % 3 == 0:\n",
    "            print(f'{counter}/{len(folder_contents)} admins processed...')\n",
    "      \n",
    "    df.to_csv(f'{country}/stats/{country}_statistics.csv', index=False)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_pipe(country):\n",
    "    print('Converting shapefile to geojson...')\n",
    "    shp_to_gjson(country)\n",
    "    print('Building Hansen tree cover raster...')\n",
    "    create_hansen_tif(country)\n",
    "    print('Removing tree cover loss...')\n",
    "    remove_loss(country)\n",
    "    print('Padding tof raster...')\n",
    "    pad_tof_raster(country)\n",
    "    print('Clipping rasters by admin boundary...')\n",
    "    create_clippings(country)\n",
    "    print('Resampling to match raster extents and resolutions...')\n",
    "    apply_extent_res(country)\n",
    "    print('Merging admins containing multiple polygons...')\n",
    "    merge_polygons(country)\n",
    "    print('Data preparation complete.')\n",
    "    print('Calculating statistics...')\n",
    "    calculate_stats(country)\n",
    "    return 'Analysis complete.'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Hansen tree cover raster...\n",
      "Belize has lon N and lat W.\n",
      "Beginning file download...\n",
      "Building the tif...\n",
      "Hansen raster built.\n",
      "Removing tree cover loss...\n",
      "35697769 pixels converted to loss.\n",
      "Padding tof raster...\n",
      "Clipping rasters by admin boundary...\n",
      "Resampling to match raster extents and resolutions...\n",
      "Merging admins with multiple polygons...\n",
      "4 admins will be merged: ['Corozal', 'Stann Creek', 'Toledo', 'Belize']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data preparation complete.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_pipe('Belize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Hansen tree cover raster...\n",
      "Costa Rica has lon N and lat W.\n",
      "Downloading files from GLAD...\n",
      "Hansen raster built.\n",
      "Removing tree cover loss...\n",
      "38050442 pixels converted to loss.\n",
      "Padding tof raster...\n",
      "Clipping rasters by admin boundary...\n",
      "Costa Rica's rasters clipped and saved.\n",
      "Resampling to match raster extents and resolutions...\n",
      "Merging admins with multiple polygons...\n",
      "3 admins will be merged: ['Puntarenas', 'Limón', 'Guanacaste']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data preparation complete.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_pipe('Costa Rica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Hansen tree cover raster...\n",
      "El Salvador has lon N and lat W.\n",
      "Downloading files from GLAD...\n",
      "Hansen raster built.\n",
      "Removing tree cover loss...\n",
      "59400753 pixels converted to loss.\n",
      "Padding tof raster...\n",
      "Clipping rasters by admin boundary...\n",
      "El Salvador's rasters clipped and saved.\n",
      "Resampling to match raster extents and resolutions...\n",
      "Merging admins with multiple polygons...\n",
      "3 admins will be merged: ['Usulután', 'La Paz', 'La Unión']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data preparation complete.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_pipe('El Salvador')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Hansen tree cover raster...\n",
      "Guatemala has lon N and lat W.\n",
      "Downloading files from GLAD...\n",
      "Hansen raster built.\n",
      "Removing tree cover loss...\n",
      "59400753 pixels converted to loss.\n",
      "Padding tof raster...\n",
      "Clipping rasters by admin boundary...\n",
      "Guatemala's rasters clipped and saved.\n",
      "Resampling to match raster extents and resolutions...\n",
      "Merging admins with multiple polygons...\n",
      "3 admins will be merged: ['Jutiapa', 'Escuintla', 'Izabal']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data preparation complete.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_pipe('Guatemala')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Hansen tree cover raster...\n",
      "Honduras has lon N and lat W.\n",
      "Downloading files from GLAD...\n",
      "Hansen raster built.\n",
      "Removing tree cover loss...\n",
      "35697769 pixels converted to loss.\n",
      "Padding tof raster...\n",
      "Clipping rasters by admin boundary...\n",
      "Honduras's rasters clipped and saved.\n",
      "Resampling to match raster extents and resolutions...\n",
      "Merging admins with multiple polygons...\n",
      "7 admins will be merged: ['Colón', 'Islas de la Bahía', 'Valle', 'Cortés', 'Choluteca', 'Atlántida', 'Gracias a Dios']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data preparation complete.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_pipe('Honduras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Hansen tree cover raster...\n",
      "Nicaragua has lon N and lat W.\n",
      "Downloading files from GLAD...\n",
      "Hansen raster built.\n",
      "Removing tree cover loss...\n",
      "35697769 pixels converted to loss.\n",
      "Padding tof raster...\n",
      "Clipping rasters by admin boundary...\n",
      "Nicaragua's rasters clipped and saved.\n",
      "Resampling to match raster extents and resolutions...\n",
      "Merging admins with multiple polygons...\n",
      "8 admins will be merged: ['Atlántico Norte', 'Río San Juan', 'Rivas', 'Granada', 'Chinandega', 'Chontales', 'León', 'Atlántico Sur']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data preparation complete.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_pipe('Nicaragua')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Hansen tree cover raster...\n",
      "Panama has lon N and lat W.\n",
      "Downloading files from GLAD...\n",
      "Hansen raster built.\n",
      "Removing tree cover loss...\n",
      "38598272 pixels converted to loss.\n",
      "Padding tof raster...\n",
      "Clipping rasters by admin boundary...\n",
      "Panama's rasters clipped and saved.\n",
      "Resampling to match raster extents and resolutions...\n",
      "Merging admins containing multiple polygons...\n",
      "12 admins will be merged: ['Emberá', 'Panamá Oeste', 'Coclé', 'Darién', 'Colón', 'Kuna Yala', 'Panamá', 'Chiriquí', 'Veraguas', 'Bocas del Toro', 'Los Santos', 'Ngöbe Buglé']\n",
      "Data preparation complete.\n",
      "Calculating statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicaertel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:56: RuntimeWarning: Mean of empty slice\n",
      "/Users/jessicaertel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:65: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/13 admins processed...\n",
      "10/13 admins processed...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Analysis complete.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_pipe('Panama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting shapefile to geojson...\n",
      "There are 6 admins in Gambia.\n",
      "Building Hansen tree cover raster...\n",
      "Gambia has lon N and lat W.\n",
      "Downloading files from GLAD...\n",
      "Hansen raster built.\n",
      "Removing tree cover loss...\n",
      "21982618 pixels converted to loss.\n",
      "Padding tof raster...\n",
      "Clipping rasters by admin boundary...\n",
      "Gambia's rasters clipped and saved.\n",
      "Resampling to match raster extents and resolutions...\n",
      "Merging admins containing multiple polygons...\n",
      "3 admins will be merged: ['Banjul', 'North Bank', 'Western']\n",
      "Data preparation complete.\n",
      "Calculating statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicaertel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:56: RuntimeWarning: Mean of empty slice\n",
      "/Users/jessicaertel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:65: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/6 admins processed...\n",
      "6/6 admins processed...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Analysis complete.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_pipe('Gambia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting shapefile to geojson...\n",
      "There are 17 admins in Burundi.\n",
      "Building Hansen tree cover raster...\n",
      "Burundi has lon N and lat E.\n",
      "Downloading files from GLAD...\n",
      "Hansen raster built.\n",
      "Removing tree cover loss...\n",
      "96869725 pixels converted to loss.\n",
      "Padding tof raster...\n",
      "Clipping rasters by admin boundary...\n",
      "No MultiPolygons in Burundi.\n",
      "Burundi's rasters clipped and saved.\n",
      "Resampling to match raster extents and resolutions...\n",
      "Merging admins containing multiple polygons...\n",
      "0 admins will be merged: []\n",
      "Data preparation complete.\n",
      "Calculating statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicaertel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:56: RuntimeWarning: Mean of empty slice\n",
      "/Users/jessicaertel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:65: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/17 admins processed...\n",
      "6/17 admins processed...\n",
      "9/17 admins processed...\n",
      "12/17 admins processed...\n",
      "15/17 admins processed...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Analysis complete.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_pipe('Burundi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting shapefile to geojson...\n",
      "There are 5 admins in Rwanda.\n",
      "Building Hansen tree cover raster...\n",
      "Rwanda has lon N and lat E.\n",
      "Downloading files from GLAD...\n",
      "Hansen raster built.\n",
      "Removing tree cover loss...\n",
      "96869725 pixels converted to loss.\n",
      "Padding tof raster...\n",
      "Clipping rasters by admin boundary...\n",
      "Rwanda's rasters clipped and saved.\n",
      "Resampling to match raster extents and resolutions...\n",
      "Merging admins containing multiple polygons...\n",
      "1 admins will be merged: ['Iburengerazuba']\n",
      "Data preparation complete.\n",
      "Calculating statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicaertel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:56: RuntimeWarning: Mean of empty slice\n",
      "/Users/jessicaertel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:65: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/5 admins processed...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Analysis complete.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_pipe('Rwanda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ESA no data land cover class\n",
    "sons_esa = rs.open('El Salvador/resampled_rasters/esa/Sonsonate.tif').read(1)\n",
    "sons_tof = rs.open('El Salvador/resampled_rasters/tof/Sonsonate.tif').read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  20,  40,  60,  80, 100], dtype=uint8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check vals for Costa Rica admin\n",
    "np.unique(currid_tof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52,\n",
       "       53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69,\n",
       "       70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86,\n",
       "       87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], dtype=uint8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check vals for El Salvador admin\n",
    "np.unique(apaneca_tof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics01:\n",
      "TOF avg in ES: 16.18607142857143, Hans avg in ES: 17.820428571428568\n",
      "Statistics02:\n",
      "TOF avg in ES: 23.421297258496576, Hans avg in ES: 17.658193335813635\n",
      "Statistics03:\n",
      "TOF avg in ES: 23.421297258496576, Hans avg in ES: 17.658193335813635\n"
     ]
    }
   ],
   "source": [
    "# confirm mean tree cover counts\n",
    "# will be different in stats01 because it's mean tree cover per admin\n",
    "# versus mean tree cover per admin per land cover type\n",
    "\n",
    "es1 = pd.read_csv('El Salvador/stats/El Salvador_statistics01.csv')\n",
    "es2 = pd.read_csv('El Salvador/stats/El Salvador_statistics02.csv')\n",
    "es3 = pd.read_csv('El Salvador/stats/El Salvador_statistics03.csv')\n",
    "print('Statistics01:')\n",
    "print(f'TOF avg in ES: {es1.tof_mean_tc.mean()}, Hans avg in ES: {es1.hans_mean_tc.mean()}')\n",
    "print('Statistics02:')\n",
    "print(f'TOF avg in ES: {es2.tof_mean_tc_lc.mean()}, Hans avg in ES: {es2.hans_mean_tc_lc.mean()}')\n",
    "print('Statistics03:')\n",
    "print(f'TOF avg in ES: {es3.tof_mean_tc_lc.mean()}, Hans avg in ES: {es3.hans_mean_tc_lc.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics01:\n",
      "TOF avg in CR: 7.111285714285714, Hans avg in CR: 25.302999999999997\n",
      "Statistics02:\n",
      "TOF avg in CR: 16.921263565891472, Hans avg in CR: 32.848781838316725\n",
      "Statistics03:\n",
      "TOF avg in CR: 16.921263565891472, Hans avg in CR: 32.848781838316725\n"
     ]
    }
   ],
   "source": [
    "cr1 = pd.read_csv('Costa Rica/stats/Costa Rica_statistics01.csv')\n",
    "cr2 = pd.read_csv('Costa Rica/stats/Costa Rica_statistics02.csv')\n",
    "cr3 = pd.read_csv('Costa Rica/stats/Costa Rica_statistics03.csv')\n",
    "\n",
    "print('Statistics01:')\n",
    "print(f'TOF avg in CR: {cr1.tof_mean_tc.mean()}, Hans avg in CR: {cr1.hans_mean_tc.mean()}')\n",
    "print('Statistics02:')\n",
    "print(f'TOF avg in CR: {cr2.tof_mean_tc_lc.mean()}, Hans avg in CR: {cr2.hans_mean_tc_lc.mean()}')\n",
    "print('Statistics03:')\n",
    "print(f'TOF avg in CR: {cr3.tof_mean_tc_lc.mean()}, Hans avg in CR: {cr3.hans_mean_tc_lc.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES statistics for 2nd function not calculating correctly\n",
    "es_ipcc_means = es_above[['tof_mean_tc_lc', \n",
    "                          'hans_mean_tc_lc', \n",
    "                          'ipcc_class']].groupby('ipcc_class').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipcc_class</th>\n",
       "      <th>tof_mean_tc_lc</th>\n",
       "      <th>hans_mean_tc_lc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agriculture</td>\n",
       "      <td>29.250907</td>\n",
       "      <td>17.826823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest</td>\n",
       "      <td>32.203412</td>\n",
       "      <td>17.515225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grassland</td>\n",
       "      <td>15.547973</td>\n",
       "      <td>18.052502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Other</td>\n",
       "      <td>9.602041</td>\n",
       "      <td>17.427191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Settlement</td>\n",
       "      <td>23.999687</td>\n",
       "      <td>17.826823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wetland</td>\n",
       "      <td>14.905611</td>\n",
       "      <td>17.346923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ipcc_class  tof_mean_tc_lc  hans_mean_tc_lc\n",
       "0  Agriculture       29.250907        17.826823\n",
       "1       Forest       32.203412        17.515225\n",
       "2    Grassland       15.547973        18.052502\n",
       "3        Other        9.602041        17.427191\n",
       "4   Settlement       23.999687        17.826823\n",
       "5      Wetland       14.905611        17.346923"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_ipcc_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_esa_means = es_above[['tof_mean_tc_lc', \n",
    "                          'hans_mean_tc_lc', \n",
    "                          'esa_class']].groupby('esa_class').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_ipcc_means = cr_above[['tof_mean_tc_lc', \n",
    "                          'hans_mean_tc_lc', \n",
    "                          'ipcc_class']].groupby('ipcc_class').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipcc_class</th>\n",
       "      <th>tof_mean_tc_lc</th>\n",
       "      <th>hans_mean_tc_lc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agriculture</td>\n",
       "      <td>21.557142</td>\n",
       "      <td>29.621502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest</td>\n",
       "      <td>21.932651</td>\n",
       "      <td>53.478920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grassland</td>\n",
       "      <td>14.237551</td>\n",
       "      <td>25.316936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Other</td>\n",
       "      <td>2.697094</td>\n",
       "      <td>17.305210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Settlement</td>\n",
       "      <td>14.240675</td>\n",
       "      <td>10.024260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wetland</td>\n",
       "      <td>7.563579</td>\n",
       "      <td>19.563395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ipcc_class  tof_mean_tc_lc  hans_mean_tc_lc\n",
       "0  Agriculture       21.557142        29.621502\n",
       "1       Forest       21.932651        53.478920\n",
       "2    Grassland       14.237551        25.316936\n",
       "3        Other        2.697094        17.305210\n",
       "4   Settlement       14.240675        10.024260\n",
       "5      Wetland        7.563579        19.563395"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_ipcc_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_esa_means = cr_above[['tof_mean_tc_lc', \n",
    "                          'hans_mean_tc_lc', \n",
    "                          'esa_class']].groupby('esa_class').mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_01(country):\n",
    "    \n",
    "    '''\n",
    "    Takes in a country to import appropriate tof/hansen rasters and calculates mean tree cover \n",
    "    per admin and the total number of hectares that fall within 5 20% thresholds (0-20%, 20-40%, \n",
    "    40-60%, 60-80%). Returns a pandas dataframe with statistics saved as a csv file in the country's\n",
    "    stats folder.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(f'{country}/stats'):\n",
    "        os.makedirs(f'{country}/stats')\n",
    "    \n",
    "    tree_cover = pd.DataFrame(columns=['admin', \n",
    "                                       'tof_mean_tc',\n",
    "                                       'hans_mean_tc',\n",
    "                                       'tof_0_20',\n",
    "                                       'tof_20_40',\n",
    "                                       'tof_40_60',\n",
    "                                       'tof_60_80',\n",
    "                                       'tof_80_100',\n",
    "                                       'hans_0_20',\n",
    "                                       'hans_20_40',\n",
    "                                       'hans_40_60',\n",
    "                                       'hans_60_80',\n",
    "                                       'hans_80_100'])\n",
    "    \n",
    "    for file in [f for f in os.listdir(f'{country}/resampled_rasters/tof') if f != '.ipynb_checkpoints']:\n",
    "        \n",
    "        tof = rs.open(f'{country}/resampled_rasters/tof/{file}').read(1)\n",
    "        hansen = rs.open(f'{country}/resampled_rasters/hansen/{file}').read(1)\n",
    "\n",
    "        # reshape to 10x10 grid - ex: (88, 10, 63, 10)\n",
    "        tof_reshaped = reshape_to_4d(tof)\n",
    "        hansen_reshaped = reshape_to_4d(hansen)\n",
    "         \n",
    "        # calculate mean tree cover for admin boundary\n",
    "        tof_mean = round(np.mean(tof_reshaped), 3)   \n",
    "        hansen_mean = round(np.mean(hansen_reshaped), 3)   \n",
    "\n",
    "        # calculate mean tree cover for each hectare\n",
    "        tof_mean_per_ha = np.mean(tof_reshaped, axis=(1,3)) \n",
    "        hansen_mean_per_ha = np.mean(hansen_reshaped, axis=(1,3)) \n",
    "        \n",
    "        # calculate num of hectares with mean tree cover 0-20, 20-40, 40-60, 60-80, 80-100\n",
    "        tof_0_20 = np.sum((tof_mean_per_ha >= 0) & (tof_mean_per_ha <= 19)) \n",
    "        tof_20_40 = np.sum((tof_mean_per_ha >= 20) & (tof_mean_per_ha <= 39)) \n",
    "        tof_40_60 = np.sum((tof_mean_per_ha >= 40) & (tof_mean_per_ha <= 59)) \n",
    "        tof_60_80 = np.sum((tof_mean_per_ha >= 60) & (tof_mean_per_ha <= 79)) \n",
    "        tof_80_100 = np.sum((tof_mean_per_ha >= 80) & (tof_mean_per_ha <= 100)) \n",
    "        \n",
    "        hans_0_20 = np.sum((hansen_mean_per_ha >= 0) & (hansen_mean_per_ha <= 19)) \n",
    "        hans_20_40 = np.sum((hansen_mean_per_ha >= 20) & (hansen_mean_per_ha <= 39)) \n",
    "        hans_40_60 = np.sum((hansen_mean_per_ha >= 40) & (hansen_mean_per_ha <= 59)) \n",
    "        hans_60_80 = np.sum((hansen_mean_per_ha >= 60) & (hansen_mean_per_ha <= 79)) \n",
    "        hans_80_100 = np.sum((hansen_mean_per_ha >= 80) & (hansen_mean_per_ha <= 100)) \n",
    "        \n",
    "        tree_cover = tree_cover.append({'admin': file[:-4], \n",
    "                                        'tof_mean_tc': tof_mean,\n",
    "                                        'hans_mean_tc': hansen_mean,\n",
    "                                        'tof_0_20': tof_0_20,\n",
    "                                        'tof_20_40': tof_20_40,\n",
    "                                        'tof_40_60': tof_40_60,\n",
    "                                        'tof_60_80': tof_60_80,\n",
    "                                        'tof_80_100': tof_80_100,\n",
    "                                        'hans_0_20': hans_0_20,\n",
    "                                        'hans_20_40': hans_20_40,\n",
    "                                        'hans_40_60': hans_40_60,\n",
    "                                        'hans_60_80': hans_60_80,\n",
    "                                        'hans_80_100': hans_80_100}, ignore_index=True)\n",
    "        # save to csv      \n",
    "        tree_cover.to_csv(f'{country}/stats/{country}_statistics01.csv', index=False)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_02_03(country):\n",
    "    '''\n",
    "    Takes in a country to import appropriate tof/hansen rasters. Rounds the values\n",
    "    to their median. For each ESA land cover class within each admin district, calculates the mean \n",
    "    tree cover and the total number of contiguous hectares of tree cover above 10% \n",
    "    thresholds. Then calculates the total hectares of tree cover within 5\n",
    "    20% thresholds (0-20%, 20-40%, 40-60%, 60-80%). The land cover categories and numbers \n",
    "    are aggregated to display the same statistics per IPCC land cover class.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # set up the dataframes\n",
    "    ipcc_above = pd.DataFrame(columns=['admin', \n",
    "                                       'esa_id',\n",
    "                                       'esa_class',\n",
    "                                       'ipcc_class',\n",
    "                                       'tof_mean_tc_lc',\n",
    "                                       'hans_mean_tc_lc',\n",
    "                                       'tof_10+','tof_20+','tof_30+','tof_40+',\n",
    "                                       'tof_50+', 'tof_60+','tof_70+','tof_80+',\n",
    "                                       'tof_90+','hans_10+','hans_20+','hans_30+', \n",
    "                                       'hans_40+','hans_50+', 'hans_60+','hans_70+',\n",
    "                                       'hans_80+','hans_90+']) \n",
    "    \n",
    "    \n",
    "    ipcc_btw = pd.DataFrame(columns=['admin', \n",
    "                                     'esa_id',\n",
    "                                     'esa_class',\n",
    "                                     'ipcc_class',\n",
    "                                     'tof_mean_tc_lc',\n",
    "                                     'hans_mean_tc_lc',\n",
    "                                     'tof_0_20','tof_20_40','tof_40_60','tof_60_80',\n",
    "                                     'tof_80_100','hans_0_20','hans_20_40','hans_40_60',\n",
    "                                     'hans_60_80','hans_80_100'])\n",
    "    \n",
    "    counter = 0\n",
    "    folder_contents = [f for f in os.listdir(f'{country}/resampled_rasters/tof') if f != '.ipynb_checkpoints']\n",
    "    \n",
    "    for file in folder_contents[:1]:\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        # read in \n",
    "        tof = rs.open(f'{country}/resampled_rasters/tof/{file}').read(1).astype(np.float32)\n",
    "        hans = rs.open(f'{country}/resampled_rasters/hansen/{file}').read(1).astype(np.float32)\n",
    "        esa = rs.open(f'{country}/resampled_rasters/esa/{file}').read(1).astype(np.float32)\n",
    "                \n",
    "        # convert values to their median for binning\n",
    "        for lower, upper in zip(lower_rng, upper_rng):\n",
    "            \n",
    "            tof[(tof >= lower) & (tof < upper)] = lower + 4.5\n",
    "            hans[(hans >= lower) & (hans < upper)] = lower + 4.5\n",
    "\n",
    "        \n",
    "        # identify the lccs for that admin district\n",
    "        esa_classes = np.unique(esa)\n",
    "        \n",
    "        for cover in esa_classes:\n",
    "            \n",
    "            # change all values that are not equal to the lcc to NaN including no data vals\n",
    "            tof_class = tof.copy()\n",
    "            tof_class[esa != cover] = np.nan \n",
    "            tof_class[tof_class == 255] = np.nan\n",
    "            \n",
    "            # check - count the number of non nan instances \n",
    "            #print(f'For {cover} there are {np.count_nonzero(~np.isnan(tof_class))} non NaNs') \n",
    "\n",
    "            # reshape to a 10x10 grid to calculate stats\n",
    "            # calc mean tree cover for the lcc (entire admin) and mean tree cover for the lcc per hectare\n",
    "            # note that runtime warning: mean of empty slice indicates array has nothing but nan values\n",
    "            tof_reshaped = reshape_to_4d(tof_class) \n",
    "            tof_class_mean = np.nanmean(tof_reshaped)\n",
    "            tof_class_mean_per_ha = np.nanmean(tof_reshaped, axis=(1,3))\n",
    "            \n",
    "            # same for Hansen\n",
    "            hans_class = hans.copy()\n",
    "            hans_class[esa != cover] = np.nan\n",
    "            hans_class[hans_class == 255] = np.nan\n",
    "            \n",
    "            # reshape and calculate stats\n",
    "            hans_reshaped = reshape_to_4d(hans_class)\n",
    "            hans_class_mean = np.nanmean(hans_reshaped)\n",
    "            hans_class_mean_per_ha = np.nanmean(hans_reshaped, axis=(1,3))\n",
    "\n",
    "            # calculate num of hectares above each threshold for the lcc\n",
    "            tof_ha_over10 = np.sum(tof_class_mean_per_ha > 10.0) \n",
    "            tof_ha_over20 = np.sum(tof_class_mean_per_ha > 20.0) \n",
    "            tof_ha_over30 = np.sum(tof_class_mean_per_ha > 30.0) \n",
    "            tof_ha_over40 = np.sum(tof_class_mean_per_ha > 40.0) \n",
    "            tof_ha_over50 = np.sum(tof_class_mean_per_ha > 50.0)\n",
    "            tof_ha_over60 = np.sum(tof_class_mean_per_ha > 60.0) \n",
    "            tof_ha_over70 = np.sum(tof_class_mean_per_ha > 70.0) \n",
    "            tof_ha_over80 = np.sum(tof_class_mean_per_ha > 80.0) \n",
    "            tof_ha_over90 = np.sum(tof_class_mean_per_ha > 90.0) \n",
    "            \n",
    "            hans_ha_over10 = np.sum(hans_class_mean_per_ha > 10.0) \n",
    "            hans_ha_over20 = np.sum(hans_class_mean_per_ha > 20.0) \n",
    "            hans_ha_over30 = np.sum(hans_class_mean_per_ha > 30.0) \n",
    "            hans_ha_over40 = np.sum(hans_class_mean_per_ha > 40.0) \n",
    "            hans_ha_over50 = np.sum(hans_class_mean_per_ha > 50.0) \n",
    "            hans_ha_over60 = np.sum(hans_class_mean_per_ha > 60.0) \n",
    "            hans_ha_over70 = np.sum(hans_class_mean_per_ha > 70.0) \n",
    "            hans_ha_over80 = np.sum(hans_class_mean_per_ha > 80.0) \n",
    "            hans_ha_over90 = np.sum(hans_class_mean_per_ha > 90.0)           \n",
    "            \n",
    "            # calculate num of hectares between thresholds for the lcc\n",
    "            tof_0_20 = np.sum((tof_class_mean_per_ha >= 0) & (tof_class_mean_per_ha <= 19)) \n",
    "            tof_20_40 = np.sum((tof_class_mean_per_ha >= 20) & (tof_class_mean_per_ha <= 39)) \n",
    "            tof_40_60 = np.sum((tof_class_mean_per_ha >= 40) & (tof_class_mean_per_ha <= 59)) \n",
    "            tof_60_80 = np.sum((tof_class_mean_per_ha >= 60) & (tof_class_mean_per_ha <= 79)) \n",
    "            tof_80_100 = np.sum((tof_class_mean_per_ha >= 80) & (tof_class_mean_per_ha <= 100)) \n",
    "\n",
    "            hans_0_20 = np.sum((hans_class_mean_per_ha >= 0) & (hans_class_mean_per_ha <= 19)) \n",
    "            hans_20_40 = np.sum((hans_class_mean_per_ha >= 20) & (hans_class_mean_per_ha <= 39)) \n",
    "            hans_40_60 = np.sum((hans_class_mean_per_ha >= 40) & (hans_class_mean_per_ha <= 59)) \n",
    "            hans_60_80 = np.sum((hans_class_mean_per_ha >= 60) & (hans_class_mean_per_ha <= 79)) \n",
    "            hans_80_100 = np.sum((hans_class_mean_per_ha >= 80) & (hans_class_mean_per_ha <= 100)) \n",
    "            \n",
    "            \n",
    "            ipcc_above = ipcc_above.append({'admin': file[:-4], \n",
    "                                            'esa_id': cover,\n",
    "                                            'tof_mean_tc_lc': tof_class_mean,\n",
    "                                            'hans_mean_tc_lc': hans_class_mean,\n",
    "                                            'tof_10+':tof_ha_over10,\n",
    "                                            'tof_20+':tof_ha_over20,\n",
    "                                            'tof_30+':tof_ha_over30,\n",
    "                                            'tof_40+':tof_ha_over40,\n",
    "                                            'tof_50+':tof_ha_over50, \n",
    "                                            'tof_60+':tof_ha_over60,\n",
    "                                            'tof_70+':tof_ha_over70,\n",
    "                                            'tof_80+':tof_ha_over80,\n",
    "                                            'tof_90+':tof_ha_over90,\n",
    "                                            'hans_10+':hans_ha_over10,\n",
    "                                            'hans_20+':hans_ha_over20,\n",
    "                                            'hans_30+':hans_ha_over30,\n",
    "                                            'hans_40+':hans_ha_over40,\n",
    "                                            'hans_50+':hans_ha_over50,\n",
    "                                            'hans_60+':hans_ha_over60,\n",
    "                                            'hans_70+':hans_ha_over70,\n",
    "                                            'hans_80+':hans_ha_over80,\n",
    "                                            'hans_90+':hans_ha_over90}, ignore_index=True)\n",
    "            \n",
    "            ipcc_btw = ipcc_btw.append({'admin': file[:-4], \n",
    "                                        'esa_id': cover,\n",
    "                                        'tof_mean_tc_lc': tof_class_mean,\n",
    "                                        'hans_mean_tc_lc': hans_class_mean,\n",
    "                                        'tof_0_20': tof_0_20,\n",
    "                                        'tof_20_40': tof_20_40,\n",
    "                                        'tof_40_60': tof_40_60,\n",
    "                                        'tof_60_80': tof_60_80,\n",
    "                                        'tof_80_100': tof_80_100,\n",
    "                                        'hans_0_20': hans_0_20,\n",
    "                                        'hans_20_40': hans_20_40,\n",
    "                                        'hans_40_60': hans_40_60,\n",
    "                                        'hans_60_80': hans_60_80,\n",
    "                                        'hans_80_100': hans_80_100}, ignore_index=True)\n",
    "            \n",
    "        # map ESA id numbers to lcc labels\n",
    "        esa_legend = {0: 'No Data',\n",
    "                10: 'Cropland, rainfed',\n",
    "                11: 'Cropland, rainfed, herbaceous cover',\n",
    "                20: 'Cropland, irrigated or post-flooding',\n",
    "                30: 'Mosaic cropland (>50%) / natural vegetation (tree, shrub, herbaceous cover)(<50%)',\n",
    "                40: 'Mosaic natural vegetation (tree, shrub, herbaceous cover) (>50%) / cropland (<50%)',\n",
    "                50: 'Tree cover, broadleaved, evergreen, closed to open (>15%)',\n",
    "                60: 'Tree cover, broadleaved, deciduous, closed to open (>15%)',\n",
    "                70: 'Tree cover, needleleaved, evergreen, closed to open (>15%)',\n",
    "                80: 'Tree cover, needleleaved, deciduous, closed to open (>15%)',\n",
    "                90: 'Tree cover, mixed leaf type (broadleaved and needleleaved)',\n",
    "                100: 'Mosaic tree and shrub (>50%) / herbaceous cover (<50%)',\n",
    "                110: 'Mosaic herbaceous cover (>50%) / tree and shrub (<50%)',\n",
    "                120: 'Shrubland',\n",
    "                130: 'Grassland',\n",
    "                140: 'Lichens and mosses',\n",
    "                150: 'Sparse vegetation (tree, shrub, herbaceous cover) (<15%)',\n",
    "                160: 'Tree cover, flooded, fresh or brakish water',\n",
    "                170: 'Tree cover, flooded, saline water',\n",
    "                180: 'Shrub or herbaceous cover, flooded, fresh/saline/brakish water',\n",
    "                190: 'Urban areas',\n",
    "                200: 'Bare areas',\n",
    "                210: 'Water bodies',\n",
    "                220: 'Permanent snow and ice'}\n",
    "        \n",
    "        # map ESA id numbers to ipcc labels\n",
    "        ipcc = {0: 'Other',\n",
    "                10: 'Agriculture',\n",
    "                11: 'Agriculture',\n",
    "                20: 'Agriculture',\n",
    "                30: 'Agriculture',\n",
    "                40: 'Agriculture',\n",
    "                50: 'Forest',\n",
    "                60: 'Forest',\n",
    "                70: 'Forest',\n",
    "                80: 'Forest',\n",
    "                90: 'Forest',\n",
    "                100: 'Forest',\n",
    "                110: 'Grassland',\n",
    "                120: 'Other',\n",
    "                130: 'Grassland',\n",
    "                140: 'Other',\n",
    "                150: 'Other',\n",
    "                160: 'Forest',\n",
    "                170: 'Forest',\n",
    "                180: 'Wetland',\n",
    "                190: 'Settlement',\n",
    "                200: 'Other',\n",
    "                210: 'Other',\n",
    "                220: 'Other'}\n",
    "        \n",
    "        ipcc_above['esa_class'] = ipcc_above['esa_id'].map(esa_legend)\n",
    "        ipcc_above['ipcc_class'] = ipcc_above['esa_id'].map(ipcc)\n",
    "        ipcc_btw['esa_class'] = ipcc_btw['esa_id'].map(esa_legend)\n",
    "        ipcc_btw['ipcc_class'] = ipcc_btw['esa_id'].map(ipcc)\n",
    "        \n",
    "        if counter % 2 == 0:\n",
    "            print(f'{counter}/{len(folder_contents)} admins processed...')\n",
    "    \n",
    "    # save to csv      \n",
    "    ipcc_above.to_csv(f'{country}/stats/{country}_statistics02_new.csv', index=False)\n",
    "    ipcc_btw.to_csv(f'{country}/stats/{country}_statistics03_new.csv', index=False)\n",
    "                                \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tof[(tof > 10) & (tof < 19)] = 14.5 \n",
    "tof[(tof > 20) & (tof < 29)] = 24.5\n",
    "tof[(tof > 30) & (tof < 39)] = 34.5\n",
    "tof[(tof > 40) & (tof < 49)] = 44.5\n",
    "tof[(tof > 50) & (tof < 59)] = 54.5\n",
    "tof[(tof > 60) & (tof < 69)] = 64.5\n",
    "tof[(tof > 70) & (tof < 79)] = 74.5\n",
    "tof[(tof > 80) & (tof < 89)] = 84.5\n",
    "tof[(tof > 90) & (tof < 99)] = 94.5\n",
    "\n",
    "hans[(hans > 0) & (hans < 9)] = 4.5 \n",
    "hans[(hans > 10) & (hans < 19)] = 14.5 \n",
    "hans[(hans > 20) & (hans < 29)] = 24.5\n",
    "hans[(hans > 30) & (hans < 39)] = 34.5\n",
    "hans[(hans > 40) & (hans < 49)] = 44.5\n",
    "hans[(hans > 50) & (hans < 59)] = 54.5\n",
    "hans[(hans > 60) & (hans < 69)] = 64.5\n",
    "hans[(hans > 70) & (hans < 79)] = 74.5\n",
    "hans[(hans > 80) & (hans < 89)] = 84.5\n",
    "hans[(hans > 90) & (hans < 99)] = 94.5\n",
    "\n",
    "tof_0_10 = np.sum((tof_class_mean_per_ha >= 0) & (tof_class_mean_per_ha <= 9))\n",
    "tof_10_20 = np.sum((tof_class_mean_per_ha >= 10) & (tof_class_mean_per_ha <= 19))   \n",
    "tof_20_30 = np.sum((tof_class_mean_per_ha >= 20) & (tof_class_mean_per_ha <= 29)) \n",
    "tof_30_40 = np.sum((tof_class_mean_per_ha >= 30) & (tof_class_mean_per_ha <= 39)) \n",
    "tof_40_50 = np.sum((tof_class_mean_per_ha >= 40) & (tof_class_mean_per_ha <= 49)) \n",
    "tof_50_60 = np.sum((tof_class_mean_per_ha >= 50) & (tof_class_mean_per_ha <= 59))\n",
    "tof_60_70 = np.sum((tof_class_mean_per_ha >= 60) & (tof_class_mean_per_ha <= 69)) \n",
    "tof_70_80 = np.sum((tof_class_mean_per_ha >= 70) & (tof_class_mean_per_ha <= 79)) \n",
    "tof_80_90 = np.sum((tof_class_mean_per_ha >= 80) & (tof_class_mean_per_ha <= 89))\n",
    "tof_90_100 = np.sum((tof_class_mean_per_ha >= 90) & (tof_class_mean_per_ha <= 100))\n",
    "\n",
    "hans_0_10 = np.sum((hans_class_mean_per_ha >= 0) & (hans_class_mean_per_ha <= 9))\n",
    "hans_10_20 = np.sum((hans_class_mean_per_ha >= 10) & (hans_class_mean_per_ha <= 19))\n",
    "hans_20_30 = np.sum((hans_class_mean_per_ha >= 20) & (hans_class_mean_per_ha <= 29))\n",
    "hans_30_40 = np.sum((hans_class_mean_per_ha >= 30) & (hans_class_mean_per_ha <= 39)) \n",
    "hans_40_50 = np.sum((hans_class_mean_per_ha >= 40) & (hans_class_mean_per_ha <= 49))\n",
    "hans_50_60 = np.sum((hans_class_mean_per_ha >= 50) & (hans_class_mean_per_ha <= 59)) \n",
    "hans_60_70 = np.sum((hans_class_mean_per_ha >= 60) & (hans_class_mean_per_ha <= 69)) \n",
    "hans_70_80 = np.sum((hans_class_mean_per_ha >= 70) & (hans_class_mean_per_ha <= 79)) \n",
    "hans_80_90 = np.sum((hans_class_mean_per_ha >= 80) & (hans_class_mean_per_ha <= 89)) \n",
    "hans_90_100 = np.sum((hans_class_mean_per_ha >= 90) & (hans_class_mean_per_ha <= 100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESA bounding box stays same, becomes same shape as TOF\n",
    "# esa_out.tif has the proper resolution\n",
    "match_raster_extent_and_res(esa, tof, esa_out.tif, tof=False, esa=True) \n",
    "\n",
    "# TOF bounding box becomes ESA, TOF stays same shape\n",
    "# tof=true will not resample tof\n",
    "match_raster_extent_and_res(tof, esa_out, tof_out.tif, tof=True, esa=False) \n",
    "\n",
    "# Hansen bounding box becomes TOF/ESA, Hansen shape becomes TOF/ESA\n",
    "match_raster_extent_and_res(hansen, esa_out, hansen_out.tif, tof=False, esa=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_resize(country):\n",
    "    \n",
    "    '''\n",
    "    Takes in a country name to import a clipped raster and resamples the raster\n",
    "    to convert it to higher resolution. Crops or pads TOF/Hansen rasters to match\n",
    "    the size of the ESA raster. Returns the new rasters as individuals files\n",
    "    in the country's \"resampled_rasters\" folder. \n",
    "\n",
    "    Resampling a raster involves multiplying the pixel size by the scale factor \n",
    "    and dividing the dimensions by the scale factor. A scale >1 is an upsample\n",
    "    and a scale <1: downsample.\n",
    "\n",
    "        i.e. given a pixel size of 250m, dimensions of (1024, 1024) and a scale of 2,\n",
    "        the resampled raster would have an output pixel size of 500m and dimensions of (512, 512)\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(f'{country}/resampled_rasters/hansen'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/hansen')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/resampled_rasters/tof'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/tof')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/resampled_rasters/esa'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/esa')\n",
    "    \n",
    "    # import new shapefile containing only polygons\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries_exp.geojson')\n",
    "    admin_boundaries = list(shapefile.NAME_1)\n",
    "    \n",
    "    for admin in admin_boundaries[:1]:\n",
    "        print(admin)\n",
    "         \n",
    "    # Resample ESA\n",
    "        esa_raster = rs.open(f'{country}/clipped_rasters/esa/{admin}.tif')\n",
    "        print(f'ESA original: {esa_raster.shape}')\n",
    "        \n",
    "        height = int(esa_raster.height)\n",
    "        width = int(esa_raster.width)\n",
    "        scale = 30\n",
    "        \n",
    "        # resample data to new resolution\n",
    "        esa_resampled = esa_raster.read(out_shape=(esa_raster.count, (height * scale), (width * scale)),\n",
    "                                        resampling=Resampling.nearest)\n",
    "        \n",
    "        # removes extra index\n",
    "        esa_resampled = esa_resampled.squeeze()\n",
    "        \n",
    "        # scale image transform\n",
    "        esa_transform = esa_raster.transform * esa_raster.transform.scale((width / esa_resampled.shape[-1]),\n",
    "                                                                      (height / esa_resampled.shape[-2]))\n",
    "        \n",
    "        \n",
    "        # assert raster shape, datatype and max/min values\n",
    "        assert esa_resampled.dtype == 'uint8'\n",
    "        assert esa_resampled.shape != (0, ) and len(esa_resampled.shape) <= 2\n",
    "        assert esa_resampled.max() <= 255 and esa_resampled.min() >= 0\n",
    "        \n",
    "        esa_outpath = f'{country}/resampled_rasters/esa/{admin}.tif'\n",
    "        esa_new = rs.open(esa_outpath, 'w', \n",
    "                              driver='GTiff',\n",
    "                              height=esa_resampled.shape[0], \n",
    "                              width=esa_resampled.shape[1], \n",
    "                              count=1,\n",
    "                              dtype=\"uint8\",\n",
    "                              crs='+proj=longlat +datum=WGS84 +no_defs',\n",
    "                              transform=esa_transform,\n",
    "                              compress='lzw')\n",
    "        \n",
    "        print(f'ESA after resample: {esa_new.shape}')\n",
    "        esa_new.write(esa_resampled, 1)\n",
    "        esa_new.close()\n",
    "        \n",
    "        # define parameters for gdal translate\n",
    "        esa = rs.open(f'{country}/resampled_rasters/esa/{admin}.tif')\n",
    "        esa_bounds = esa.bounds\n",
    "        translateoptions = gdal.TranslateOptions(format='Gtiff', \n",
    "                                                  outputSRS='EPSG:4326',\n",
    "                                                  outputType=gdal.GDT_Byte,\n",
    "                                                  noData=255,\n",
    "                                                  creationOptions=['COMPRESS=LZW'],\n",
    "                                                  resampleAlg='nearest')\n",
    "        \n",
    "        # Crop Hansen to ESA bounds \n",
    "        hans_raster = f'{country}/clipped_rasters/hansen/{admin}.tif'\n",
    "        hans_outpath = f'{country}/resampled_rasters/hansen/{admin}.tif'\n",
    "        print(f'Hansen original: {rs.open(hans_raster).shape}')\n",
    "        \n",
    "        source = gdal.Open(hans_raster)\n",
    "        ds = gdal.Translate(hans_outpath, \n",
    "                            source, \n",
    "                            projWin=[-90.22499999999281, 13.75833333332723, -89.78888888888166, 14.166666666660595], \n",
    "                            options=translateoptions)\n",
    "        ds = None\n",
    "        print(f'Hansen after gdal translate: {rs.open(hans_outpath).shape}')\n",
    "        \n",
    "        # Resample hansen\n",
    "        hans_raster = rs.open(hans_raster)\n",
    "        height = int(hans_raster.height)\n",
    "        width = int(hans_raster.width)\n",
    "        scale = 3\n",
    "        \n",
    "        # resample data to target shape -- use Resampling.nearest not Resampling.bilinear\n",
    "        hans_resampled = hans_raster.read(out_shape=(hans_raster.count, (height * scale), (width * scale)),\n",
    "                                          resampling=Resampling.nearest)\n",
    "        \n",
    "        # removes extra index\n",
    "        hans_resampled = hans_resampled.squeeze()\n",
    "        \n",
    "        # scale image transform\n",
    "        hans_transform = hans_raster.transform * hans_raster.transform.scale((width / hans_resampled.shape[-1]),\n",
    "                                                                           (height / hans_resampled.shape[-2]))\n",
    "        \n",
    "        \n",
    "        # assert raster shape, datatype and max/min values\n",
    "        assert hans_resampled.dtype == 'uint8'\n",
    "        assert hans_resampled.shape != (0, ) and len(hans_resampled.shape) <= 2\n",
    "        assert hans_resampled.max() <= 255 and hans_resampled.min() >= 0\n",
    "        \n",
    "        # write the resampled raster to the new folder \n",
    "        hans_new = rs.open(hans_outpath, 'w', \n",
    "                              driver='GTiff',\n",
    "                              height=hans_resampled.shape[0], \n",
    "                              width=hans_resampled.shape[1], \n",
    "                              count=1,\n",
    "                              dtype=\"uint8\",\n",
    "                              crs='+proj=longlat +datum=WGS84 +no_defs',\n",
    "                              transform=hans_transform,\n",
    "                              compress='lzw')\n",
    "        \n",
    "        print(f'Hansen after resample: {hans_new.shape}')\n",
    "        hans_new.write(hans_resampled, 1)\n",
    "        hans_new.close()\n",
    "        \n",
    "        \n",
    "        # Crop TOF to ESA bounds (move to resampled folder, skip resample)\n",
    "        tof_raster = f'{country}/clipped_rasters/tof/{admin}.tif'\n",
    "        tof_outpath = f'{country}/resampled_rasters/tof/{admin}.tif'\n",
    "        source = gdal.Open(tof_raster)\n",
    "        ds = gdal.Translate(tof_outpath, \n",
    "                            source, \n",
    "                            projWin=[-90.22499999999281, 13.75833333332723, -89.78888888888166, 14.166666666660595], \n",
    "                            options=translateoptions)\n",
    "        ds = None\n",
    "        print(f'TOF original: {rs.open(tof_raster).shape}')\n",
    "        print(f'TOF after gdal translate: {rs.open(tof_outpath).shape}')\n",
    "        print(' ')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_extent_and_res(country):\n",
    "    \n",
    "    '''\n",
    "    Matches the projection, bounding box, and dimensions of source to reference\n",
    "    ''' \n",
    "   \n",
    "    # import new shapefile containing only polygons\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries_exp.geojson')\n",
    "    admin_boundaries = list(shapefile.NAME_1)\n",
    "    \n",
    "    for admin in admin_boundaries:\n",
    "        \n",
    "        # import, get the projection and geotrans for each dataset\n",
    "        tof = gdal.Open(f'{country}/clipped_rasters/tof/{admin}.tif', gdalconst.GA_ReadOnly)     \n",
    "        tof_proj = tof.GetProjection()\n",
    "        tof_geotrans = tof.GetGeoTransform()\n",
    "        tof_outpath = f'{country}/resampled_rasters/tof/{admin}.tif'\n",
    "        \n",
    "        hans = gdal.Open(f'{country}/clipped_rasters/hansen/{admin}.tif', gdalconst.GA_ReadOnly)\n",
    "        hans_proj = hans.GetProjection()\n",
    "        hans_geotrans = hans.GetGeoTransform()\n",
    "        hans_outpath = f'{country}/resampled_rasters/hansen/{admin}.tif'\n",
    "        \n",
    "        esa = gdal.Open(f'{country}/clipped_rasters/esa/{admin}.tif', gdalconst.GA_ReadOnly)\n",
    "        esa_proj = esa.GetProjection()\n",
    "        esa_geotrans = esa.GetGeoTransform() # not used remove\n",
    "        esa_outpath = f'{country}/resampled_rasters/esa/{admin}.tif'\n",
    "                \n",
    "        # esa upsample to 10m\n",
    "        width = esa.RasterXSize\n",
    "        height = esa.RasterYSize\n",
    "        esa_out = gdal.GetDriverByName('GTiff').Create(esa_outpath, width, height, 1, gdalconst.GDT_Byte)\n",
    "        interpolation = gdalconst.GRA_NearestNeighbour\n",
    "        gdal.ReprojectImage(esa, esa_outpath, esa_proj, tof_proj, interpolation)\n",
    "\n",
    "        # set reference as resampled esa\n",
    "        esa_resampled = gdal.Open(esa_outpath, gdalconst.GA_ReadOnly)\n",
    "        esa_proj = esa_resampled.GetProjection()\n",
    "        esa_geotrans = esa_resampled.GetGeoTransform()\n",
    "        width = esa_resampled.RasterXSize\n",
    "        height = esa_resampled.RasterYSize\n",
    "        \n",
    "        # tof transform\n",
    "        tof_out = gdal.GetDriverByName('GTiff').Create(tof_outpath, width, height, 1, gdalconst.GDT_Byte)\n",
    "        tof_out.SetGeoTransform(esa_geotrans)\n",
    "        tof_out.SetProjection(esa_proj)\n",
    "        \n",
    "        interpolation = gdalconst.GRA_NearestNeighbour\n",
    "        gdal.ReprojectImage(hans, hans_out, hans_proj, tof_proj, interpolation)\n",
    "        \n",
    "        # Hansen transform\n",
    "        hans_out = gdal.GetDriverByName('GTiff').Create(hans_outpath, width, height, 1, gdalconst.GDT_Byte)\n",
    "        hans_out.SetGeoTransform(esa_geotrans)\n",
    "        hans_out.SetProjection(esa_proj)\n",
    "        \n",
    "        # resample hansen to tof\n",
    "#         interpolation = gdalconst.GRA_NearestNeighbour\n",
    "#         gdal.ReprojectImage(hans, hans_out, hans_proj, tof_proj, interpolation)\n",
    "                \n",
    "    return None\n",
    "\n",
    "def resample_resize(country):\n",
    "    \n",
    "    '''\n",
    "    Takes in a country name to import a clipped raster and resamples the raster\n",
    "    to convert it to higher resolution. Crops or pads TOF/Hansen rasters to match\n",
    "    the size of the ESA raster. Returns the new rasters as individuals files\n",
    "    in the country's \"resampled_rasters\" folder. \n",
    "\n",
    "    Resampling a raster involves multiplying the pixel size by the scale factor \n",
    "    and dividing the dimensions by the scale factor. A scale >1 is an upsample\n",
    "    and a scale <1: downsample.\n",
    "\n",
    "        i.e. given a pixel size of 250m, dimensions of (1024, 1024) and a scale of 2,\n",
    "        the resampled raster would have an output pixel size of 500m and dimensions of (512, 512)\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(f'{country}/resampled_rasters/hansen'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/hansen')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/resampled_rasters/tof'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/tof')\n",
    "    \n",
    "    if not os.path.exists(f'{country}/resampled_rasters/esa'):\n",
    "        os.makedirs(f'{country}/resampled_rasters/esa')\n",
    "    \n",
    "    # import new shapefile containing only polygons\n",
    "    shapefile = gpd.read_file(f'{country}/{country}_adminboundaries_exp.geojson')\n",
    "    admin_boundaries = list(shapefile.NAME_1)\n",
    "    \n",
    "    for admin in admin_boundaries[:1]:\n",
    "        print(admin)\n",
    "         \n",
    "    # Resample ESA\n",
    "        esa_raster = rs.open(f'{country}/clipped_rasters/esa/{admin}.tif')\n",
    "        print(f'ESA original: {esa_raster.shape}')\n",
    "        \n",
    "        height = int(esa_raster.height)\n",
    "        width = int(esa_raster.width)\n",
    "        scale = 30\n",
    "        \n",
    "        # resample data to new resolution\n",
    "        esa_resampled = esa_raster.read(out_shape=(esa_raster.count, (height * scale), (width * scale)),\n",
    "                                        resampling=Resampling.nearest)\n",
    "        \n",
    "        # removes extra index\n",
    "        esa_resampled = esa_resampled.squeeze()\n",
    "        \n",
    "        # scale image transform\n",
    "        esa_transform = esa_raster.transform * esa_raster.transform.scale((width / esa_resampled.shape[-1]),\n",
    "                                                                      (height / esa_resampled.shape[-2]))\n",
    "        \n",
    "        \n",
    "        # assert raster shape, datatype and max/min values\n",
    "        assert esa_resampled.dtype == 'uint8'\n",
    "        assert esa_resampled.shape != (0, ) and len(esa_resampled.shape) <= 2\n",
    "        assert esa_resampled.max() <= 255 and esa_resampled.min() >= 0\n",
    "        \n",
    "        esa_outpath = f'{country}/resampled_rasters/esa/{admin}.tif'\n",
    "        esa_new = rs.open(esa_outpath, 'w', \n",
    "                              driver='GTiff',\n",
    "                              height=esa_resampled.shape[0], \n",
    "                              width=esa_resampled.shape[1], \n",
    "                              count=1,\n",
    "                              dtype=\"uint8\",\n",
    "                              crs='+proj=longlat +datum=WGS84 +no_defs',\n",
    "                              transform=esa_transform,\n",
    "                              compress='lzw')\n",
    "        \n",
    "        print(f'ESA after resample: {esa_new.shape}')\n",
    "        esa_new.write(esa_resampled, 1)\n",
    "        esa_new.close()\n",
    "        \n",
    "        # define parameters for gdal translate\n",
    "        esa = rs.open(f'{country}/resampled_rasters/esa/{admin}.tif')\n",
    "        esa_bounds = esa.bounds\n",
    "        translateoptions = gdal.TranslateOptions(format='Gtiff', \n",
    "                                                  outputSRS='EPSG:4326',\n",
    "                                                  outputType=gdal.GDT_Byte,\n",
    "                                                  noData=255,\n",
    "                                                  creationOptions=['COMPRESS=LZW'],\n",
    "                                                  resampleAlg='nearest')\n",
    "        \n",
    "        # Crop Hansen to ESA bounds \n",
    "        hans_raster = f'{country}/clipped_rasters/hansen/{admin}.tif'\n",
    "        hans_outpath = f'{country}/resampled_rasters/hansen/{admin}.tif'\n",
    "        print(f'Hansen original: {rs.open(hans_raster).shape}')\n",
    "        \n",
    "        source = gdal.Open(hans_raster)\n",
    "        ds = gdal.Translate(hans_outpath, \n",
    "                            source, \n",
    "                            projWin=[-90.22499999999281, 13.75833333332723, -89.78888888888166, 14.166666666660595], \n",
    "                            options=translateoptions)\n",
    "        ds = None\n",
    "        print(f'Hansen after gdal translate: {rs.open(hans_outpath).shape}')\n",
    "        \n",
    "        # Resample hansen\n",
    "        hans_raster = rs.open(hans_raster)\n",
    "        height = int(hans_raster.height)\n",
    "        width = int(hans_raster.width)\n",
    "        scale = 3\n",
    "        \n",
    "        # resample data to target shape -- use Resampling.nearest not Resampling.bilinear\n",
    "        hans_resampled = hans_raster.read(out_shape=(hans_raster.count, (height * scale), (width * scale)),\n",
    "                                          resampling=Resampling.nearest)\n",
    "        \n",
    "        # removes extra index\n",
    "        hans_resampled = hans_resampled.squeeze()\n",
    "        \n",
    "        # scale image transform\n",
    "        hans_transform = hans_raster.transform * hans_raster.transform.scale((width / hans_resampled.shape[-1]),\n",
    "                                                                           (height / hans_resampled.shape[-2]))\n",
    "        \n",
    "        \n",
    "        # assert raster shape, datatype and max/min values\n",
    "        assert hans_resampled.dtype == 'uint8'\n",
    "        assert hans_resampled.shape != (0, ) and len(hans_resampled.shape) <= 2\n",
    "        assert hans_resampled.max() <= 255 and hans_resampled.min() >= 0\n",
    "        \n",
    "        # write the resampled raster to the new folder \n",
    "        hans_new = rs.open(hans_outpath, 'w', \n",
    "                              driver='GTiff',\n",
    "                              height=hans_resampled.shape[0], \n",
    "                              width=hans_resampled.shape[1], \n",
    "                              count=1,\n",
    "                              dtype=\"uint8\",\n",
    "                              crs='+proj=longlat +datum=WGS84 +no_defs',\n",
    "                              transform=hans_transform,\n",
    "                              compress='lzw')\n",
    "        \n",
    "        print(f'Hansen after resample: {hans_new.shape}')\n",
    "        hans_new.write(hans_resampled, 1)\n",
    "        hans_new.close()\n",
    "        \n",
    "        \n",
    "        # Crop TOF to ESA bounds (move to resampled folder, skip resample)\n",
    "        tof_raster = f'{country}/clipped_rasters/tof/{admin}.tif'\n",
    "        tof_outpath = f'{country}/resampled_rasters/tof/{admin}.tif'\n",
    "        source = gdal.Open(tof_raster)\n",
    "        ds = gdal.Translate(tof_outpath, \n",
    "                            source, \n",
    "                            projWin=[-90.22499999999281, 13.75833333332723, -89.78888888888166, 14.166666666660595], \n",
    "                            options=translateoptions)\n",
    "        ds = None\n",
    "        print(f'TOF original: {rs.open(tof_raster).shape}')\n",
    "        print(f'TOF after gdal translate: {rs.open(tof_outpath).shape}')\n",
    "        print(' ')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tof = np.array([88, 84, 80, 10, 12, 17, 91, 42, 20, 23, 26, 29, 31, 39, 90, 92, 97]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4093998, 14244852)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of non nan and nan instances in the dataset\n",
    "np.count_nonzero(~np.isnan(tof_class)), np.count_nonzero(np.isnan(tof_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([88., 84., 80., 10., 12., 17., 91., 42., 20., 23., 26., 29., 31.,\n",
       "       39., 95., 92., 97.], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data < 90] += 4.5\n",
    "data[data == 90] += 5\n",
    "\n",
    "\n",
    "tof[(tof > 90) & (tof < 99)] += 4.5 # if its 91, 92, 93, etc. make it 94.5\n",
    "tof[tof == 90] += 5  # if its 90 make it 95\n",
    "tof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25.4, 24.5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = np.array([21, 24, 25, 28, 29]).astype('float32')\n",
    "test2 = np.array([24.5, 24.5, 24.5, 24.5, 24.5])\n",
    "test1.mean(), test2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 5. , 5. , 4.5, 4.5, 4.5,\n",
       "       4.5, 4.5], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tof = np.array([0, 1, 2, 5, 6, 3, 8, 9, 10, 10, 2, 4, 6, 7, 8]).astype('float32')\n",
    "\n",
    "tof[(tof > 0) & (tof < 10)] = 4.5 # if it's between 0-9 make it 4.5\n",
    "tof[tof == 10] = 5 # if it's 10 make it 15\n",
    "tof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing...\n",
      "Filtering to 2.0 for Toledo.tif...\n"
     ]
    }
   ],
   "source": [
    "for file in [f for f in os.listdir(f'Belize/resampled_rasters/tof') if f != '.ipynb_checkpoints'][:1]:\n",
    "\n",
    "    # read in as a float in order to replace no data with NaNs\n",
    "    tof = rs.open(f'Belize/resampled_rasters/tof/{file}').read(1).astype(np.float32)\n",
    "    hans = rs.open(f'Belize/resampled_rasters/hansen/{file}').read(1).astype(np.float32)\n",
    "    esa = rs.open(f'Belize/resampled_rasters/esa/{file}').read(1).astype(np.float32)\n",
    "\n",
    "    esa[esa == 0.0] = 2.0\n",
    "    \n",
    "    # resize esa/hansen to have the same shape as tof and reshape for stats\n",
    "    print('Resizing...')\n",
    "    if esa.shape != tof.shape:\n",
    "        esa = resize(esa, tof.shape, order=0, preserve_range=True)\n",
    "\n",
    "    if hans.shape != tof.shape:\n",
    "        hans = resize(hans, tof.shape, order=0, preserve_range=True)\n",
    "\n",
    "    # identify the lccs for that admin district\n",
    "    esa_classes = np.unique(esa)\n",
    "\n",
    "    for cover in esa_classes[:1]:\n",
    "\n",
    "        # change all values that are not equal to the lcc to NaN\n",
    "        print(f'Filtering to {cover} for {file}...')\n",
    "        tof_class_new = tof.copy()\n",
    "        tof_class_new[esa != cover] = np.nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the value counts for tof_class\n",
    "(unique, counts) = np.unique(tof_class_new, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000e+00, 1.03472e+05],\n",
       "       [2.00000e+01, 1.45800e+03],\n",
       "       [4.00000e+01, 5.77000e+03],\n",
       "       [6.00000e+01, 4.12600e+03],\n",
       "       [8.00000e+01, 5.88300e+03],\n",
       "       [1.00000e+02, 6.60200e+03],\n",
       "       [        nan, 1.00000e+00],\n",
       "       [        nan, 1.00000e+00],\n",
       "       [        nan, 1.00000e+00],\n",
       "       [        nan, 1.00000e+00]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       0, 93976598],\n",
       "       [      20,    47917],\n",
       "       [      40,   194512],\n",
       "       [      60,   160780],\n",
       "       [      80,   296476],\n",
       "       [     100,  1512940]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tof = rs.open(f'Belize/resampled_rasters/tof/Toledo.tif').read(1)\n",
    "(unique, counts) = np.unique(tof, return_counts=True)\n",
    "toledo_tof = np.asarray((unique, counts)).T\n",
    "toledo_tof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esa: (22, 26), tof: (638, 746), hans: (232, 271)\n"
     ]
    }
   ],
   "source": [
    "tof = rs.open(f'Costa Rica/clipped_rasters/tof/Puntarenas1.tif').read(1)\n",
    "hans = rs.open(f'Costa Rica/clipped_rasters/hansen/Puntarenas1.tif').read(1)\n",
    "esa = rs.open(f'Costa Rica/clipped_rasters/esa/Puntarenas1.tif').read(1)\n",
    "print(f'esa: {esa.shape}, tof: {tof.shape}, hans: {hans.shape}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(660, 780) (638, 746) (696, 813)\n"
     ]
    }
   ],
   "source": [
    "print((22*30, 26*30),(638, 746),(232*3, 271*3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(884, 631)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tof = rs.open('El Salvador/resampled_rasters/tof/Apaneca.tif').read(1)\n",
    "tof.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(884, 631)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tof = rs.open('El Salvador/resampled_rasters/tof/Apaneca.tif').read(1).astype(np.float32)\n",
    "tof[tof == 255] = np.nan\n",
    "tof.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3557.54 non-contiguous hectares above 10% canopy cover\n",
      "There are 3709 contiguous hectares above 10% canopy cover\n"
     ]
    }
   ],
   "source": [
    "## EXAMPLE CODE\n",
    "\n",
    "\n",
    "tof_data = rs.open('El Salvador/resampled_rasters/tof/Apaneca.tif')\n",
    "width = tof_data.shape[1]\n",
    "height = tof_data.shape[0]\n",
    "\n",
    "tof_array = tof_data.read(1)[:13590, :25810].astype(np.float32)\n",
    "\n",
    "# non contiguous hectares\n",
    "tof_array[tof_array == 255] = np.nan\n",
    "print(f\"There are {np.sum(tof_array > 10) / 100} non-contiguous hectares above 10% canopy cover\")\n",
    "\n",
    "# contiguous hectares\n",
    "tof_array = reshape_for_stats(tof_array) #there was another reshape that didn't work\n",
    "tof_array = np.nanmean(tof_array, axis = (1, 3))\n",
    "tof_array = (np.floor(tof_array)).astype(np.uint8)\n",
    "print(f\"There are {np.sum(tof_array > 10)} contiguous hectares above 10% canopy cover\")\n",
    "bounds = tof_data.bounds\n",
    "transform = rs.transform.from_bounds(west = bounds[0], \n",
    "                                     south = bounds[1],\n",
    "                                     east = bounds[2], \n",
    "                                     north = bounds[3],\n",
    "                                     width = tof_array.shape[1], \n",
    "                                     height = tof_array.shape[0])\n",
    "\n",
    "new_dataset = rs.open(\"el salvador-one-hectare-treecover.tif\", 'w', \n",
    "                      driver='GTiff',\n",
    "                      height=tof_array.shape[0], \n",
    "                      width=tof_array.shape[1], \n",
    "                      count=1,\n",
    "                      dtype=\"uint8\",\n",
    "                      crs='+proj=longlat +datum=WGS84 +no_defs',\n",
    "                      transform=transform)\n",
    "\n",
    "new_dataset.write(np.array(tof_array), 1)\n",
    "new_dataset.close()\n",
    "\n",
    "new_dataset = rs.open(\"el salvador-one-hectare-binary.tif\", 'w', \n",
    "                      driver='GTiff',\n",
    "                      height=tof_array.shape[0], \n",
    "                      width=tof_array.shape[1], \n",
    "                      count=1,\n",
    "                      dtype=\"uint8\",\n",
    "                      crs='+proj=longlat +datum=WGS84 +no_defs',\n",
    "                      transform=transform)\n",
    "\n",
    "new_dataset.write(np.array(tof_array >= 10).astype(np.uint8), 1)\n",
    "new_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "jurisdictions = gpd.read_file('El Salvador/El Salvador_admin.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADM0_PCODE</th>\n",
       "      <th>ADM0_ES</th>\n",
       "      <th>ADM1_PCODE</th>\n",
       "      <th>ADM1_ES</th>\n",
       "      <th>ADM2_REF</th>\n",
       "      <th>ADM2_PCODE</th>\n",
       "      <th>ADM2_ES</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CR</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>CR03</td>\n",
       "      <td>Guanacaste</td>\n",
       "      <td>Guanacaste</td>\n",
       "      <td>CR0301</td>\n",
       "      <td>Abangares</td>\n",
       "      <td>MULTIPOLYGON (((375722.282 1121031.337, 375696...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CR</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>CR07</td>\n",
       "      <td>San José</td>\n",
       "      <td>San José</td>\n",
       "      <td>CR0701</td>\n",
       "      <td>Acosta</td>\n",
       "      <td>POLYGON ((482255.682 1090408.565, 482281.722 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CR</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>CR06</td>\n",
       "      <td>Puntarenas</td>\n",
       "      <td>Puntarenas</td>\n",
       "      <td>CR0601</td>\n",
       "      <td>Aguirre</td>\n",
       "      <td>MULTIPOLYGON (((486711.014 1033380.147, 486700...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CR</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>CR01</td>\n",
       "      <td>Alajuela</td>\n",
       "      <td>Alajuela</td>\n",
       "      <td>CR0101</td>\n",
       "      <td>Alajuela</td>\n",
       "      <td>POLYGON ((482377.488 1151480.958, 482376.471 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CR</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>CR07</td>\n",
       "      <td>San José</td>\n",
       "      <td>San José</td>\n",
       "      <td>CR0702</td>\n",
       "      <td>Alajuelita</td>\n",
       "      <td>POLYGON ((487024.603 1098008.676, 487070.576 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ADM0_PCODE     ADM0_ES ADM1_PCODE     ADM1_ES    ADM2_REF ADM2_PCODE  \\\n",
       "0         CR  Costa Rica       CR03  Guanacaste  Guanacaste     CR0301   \n",
       "1         CR  Costa Rica       CR07    San José    San José     CR0701   \n",
       "2         CR  Costa Rica       CR06  Puntarenas  Puntarenas     CR0601   \n",
       "3         CR  Costa Rica       CR01    Alajuela    Alajuela     CR0101   \n",
       "4         CR  Costa Rica       CR07    San José    San José     CR0702   \n",
       "\n",
       "      ADM2_ES                                           geometry  \n",
       "0   Abangares  MULTIPOLYGON (((375722.282 1121031.337, 375696...  \n",
       "1      Acosta  POLYGON ((482255.682 1090408.565, 482281.722 1...  \n",
       "2     Aguirre  MULTIPOLYGON (((486711.014 1033380.147, 486700...  \n",
       "3    Alajuela  POLYGON ((482377.488 1151480.958, 482376.471 1...  \n",
       "4  Alajuelita  POLYGON ((487024.603 1098008.676, 487070.576 1...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_admins = gpd.read_file('Costa Rica/cri_admbnda_adm2_2020.shp')\n",
    "cr_admins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproject the shapefile if not espg 4326\n",
    "cr_admins = cr_admins.to_crs('\"EPSG:4326\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minx</th>\n",
       "      <th>miny</th>\n",
       "      <th>maxx</th>\n",
       "      <th>maxy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-85.225659</td>\n",
       "      <td>10.109038</td>\n",
       "      <td>-84.827092</td>\n",
       "      <td>10.389386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-84.341693</td>\n",
       "      <td>9.622506</td>\n",
       "      <td>-84.132088</td>\n",
       "      <td>9.861633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-84.253801</td>\n",
       "      <td>9.254671</td>\n",
       "      <td>-83.840040</td>\n",
       "      <td>9.572033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-84.367680</td>\n",
       "      <td>9.914267</td>\n",
       "      <td>-84.160955</td>\n",
       "      <td>10.413694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-84.145894</td>\n",
       "      <td>9.847564</td>\n",
       "      <td>-84.084156</td>\n",
       "      <td>9.931675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>-84.592696</td>\n",
       "      <td>9.569590</td>\n",
       "      <td>-84.403502</td>\n",
       "      <td>9.925489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-85.449169</td>\n",
       "      <td>10.670787</td>\n",
       "      <td>-84.880578</td>\n",
       "      <td>11.065570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-84.362149</td>\n",
       "      <td>10.048149</td>\n",
       "      <td>-84.226587</td>\n",
       "      <td>10.276312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-84.034999</td>\n",
       "      <td>9.964382</td>\n",
       "      <td>-83.858583</td>\n",
       "      <td>10.190042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-84.518360</td>\n",
       "      <td>10.167054</td>\n",
       "      <td>-84.315016</td>\n",
       "      <td>10.270422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         minx       miny       maxx       maxy\n",
       "0  -85.225659  10.109038 -84.827092  10.389386\n",
       "1  -84.341693   9.622506 -84.132088   9.861633\n",
       "2  -84.253801   9.254671 -83.840040   9.572033\n",
       "3  -84.367680   9.914267 -84.160955  10.413694\n",
       "4  -84.145894   9.847564 -84.084156   9.931675\n",
       "..        ...        ...        ...        ...\n",
       "76 -84.592696   9.569590 -84.403502   9.925489\n",
       "77 -85.449169  10.670787 -84.880578  11.065570\n",
       "78 -84.362149  10.048149 -84.226587  10.276312\n",
       "79 -84.034999   9.964382 -83.858583  10.190042\n",
       "80 -84.518360  10.167054 -84.315016  10.270422\n",
       "\n",
       "[81 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_bounds = cr_admins.geometry.bounds\n",
    "cr_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-90.03624725299994, 13.417916298000023, -88.95311737099996, 13.99701213800006)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds = jurisdictions.geometry.bounds\n",
    "min_x = bounds.minx.min()\n",
    "min_y = bounds.miny.min()\n",
    "max_x = bounds.maxx.max()\n",
    "max_y = bounds.maxy.max()\n",
    "min_x, min_y, max_x, max_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_x = math.floor(min_x / 10) * 10\n",
    "lower_y = math.ceil(min_y / 10) * 10\n",
    "upper_y = math.ceil(max_y / 10) * 10\n",
    "upper_x = math.ceil(max_x / 10) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-100, 20, -80, 20)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_x, lower_y, upper_x, upper_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-100\n",
      "-90\n"
     ]
    }
   ],
   "source": [
    "for x in range(lower_x, upper_x, 10):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "for x in range(lower_y, upper_y + 10, 10):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower_x = np.absolute(math.floor(min_x / 10) * 10)\n",
    "# lower_y = np.absolute(math.floor(min_y / 10) * 10)\n",
    "# upper_y = np.absolute(math.ceil(max_y / 10) * 10)\n",
    "# upper_x = np.absolute(math.ceil(max_x / 10) * 10)\n",
    "# lower_x, lower_y, upper_x, upper_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats_esa_lc(country, shapefile):\n",
    "    \n",
    "    '''\n",
    "    Takes in a country name to import tof/hansen/esa rasters and calculates mean \n",
    "    tree cover thresholds per administrative boundary and ESA land cover \n",
    "    class. Returns a pandas dataframe with statistics.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    country : str\n",
    "        a string indicating the country files to import\n",
    "    shapefile : .geojson or .shp file\n",
    "        shapefile containing subnational administrative boundaries level 0-2\n",
    "    '''\n",
    "    \n",
    "    tree_cover = pd.DataFrame(columns=['admin', \n",
    "                                       'esa_id', \n",
    "                                       'lc_class',\n",
    "                                       'ipcc_class',\n",
    "                                       'tof_mean_tc',\n",
    "                                       'tof_total_ha', \n",
    "                                       'hansen_mean_tc',\n",
    "                                       'hansen_total_ha',\n",
    "                                       'tof_hans'])\n",
    "    \n",
    "    admin_boundaries = list(shapefile.NAME_1)\n",
    "    \n",
    "    for admin in admin_boundaries:\n",
    "        \n",
    "        # resize esa to tof boundaries then reshape both\n",
    "        esa = rs.open(f'{country}/resampled_rasters/esa/{admin}.tif').read(1)\n",
    "        tof = rs.open(f'{country}/resampled_rasters/tof/{admin}.tif').read(1)\n",
    "        hansen = rs.open(f'{country}/resampled_rasters/hansen/{admin}.tif').read(1)\n",
    "        \n",
    "        # hard code esa to have the same shape as tof/hansen\n",
    "        if esa.shape != tof.shape:\n",
    "            esa = resize(esa, tof.shape, order=0, preserve_range=True)\n",
    "        \n",
    "        if hansen.shape != tof.shape:\n",
    "            hansen = resize(hansen, tof.shape, order=0, preserve_range=True)\n",
    "            \n",
    "        esa = reshape_for_stats(esa)\n",
    "        tof = reshape_for_stats(tof)\n",
    "        hansen = reshape_for_stats(hansen)\n",
    "        \n",
    "        # get a list of land cover classes in that jurisdiction\n",
    "        esa_classes = np.unique(esa)\n",
    "        #print(f'{len(esa_classes)} land cover classes in {juris}.')\n",
    "   \n",
    "        for cover in esa_classes:\n",
    "        \n",
    "            tof_class = tof[esa == cover]\n",
    "            hansen_class = hansen[esa == cover]\n",
    "            \n",
    "            # calculate mean tree cover for each lc class in the jurisdiction\n",
    "            tof_tc_by_class = round(np.mean(tof_class), 3)\n",
    "            hansen_tc_by_class = round(np.mean(hansen_class), 3)\n",
    "            \n",
    "            # calculate number of hectareas in each jurisdiction above 10% canopy cover per class\n",
    "            # get sum of 10m pixels above 10% and divide by 100 to convert to to non-contiguous hectares\n",
    "            tof_ha_over10 = np.sum(tof_class > 10.0) / 100 \n",
    "            hansen_ha_over10 = np.sum(hansen_class > 10.0) / 100\n",
    "            \n",
    "            \n",
    "            tree_cover = tree_cover.append({'admin': admin, \n",
    "                                            'esa_id': cover,\n",
    "                                            'tof_mean_tc': tof_tc_by_class,\n",
    "                                            'tof_total_ha': tof_ha_over10,\n",
    "                                            'hansen_mean_tc': hansen_tc_by_class,\n",
    "                                            'hansen_total_ha': hansen_ha_over10}, ignore_index=True)\n",
    "        \n",
    "        # from Appdx 1 of ESA product user guide\n",
    "        legend = {0: 'No Data',\n",
    "                10: 'Cropland, rainfed',\n",
    "                11: 'Cropland, rainfed, herbaceous cover',\n",
    "                20: 'Cropland, irrigated or post-flooding',\n",
    "                30: 'Mosaic cropland (>50%) / natural vegetation (tree, shrub, herbaceous cover)(<50%)',\n",
    "                40: 'Mosaic natural vegetation (tree, shrub, herbaceous cover) (>50%) / cropland (<50%)',\n",
    "                50: 'Tree cover, broadleaved, evergreen, closed to open (>15%)',\n",
    "                60: 'Tree cover, broadleaved, deciduous, closed to open (>15%)',\n",
    "                70: 'Tree cover, needleleaved, evergreen, closed to open (>15%)',\n",
    "                80: 'Tree cover, needleleaved, deciduous, closed to open (>15%)',\n",
    "                90: 'Tree cover, mixed leaf type (broadleaved and needleleaved)',\n",
    "                100: 'Mosaic tree and shrub (>50%) / herbaceous cover (<50%)',\n",
    "                110: 'Mosaic herbaceous cover (>50%) / tree and shrub (<50%)',\n",
    "                120: 'Shrubland',\n",
    "                130: 'Grassland',\n",
    "                140: 'Lichens and mosses',\n",
    "                150: 'Sparse vegetation (tree, shrub, herbaceous cover) (<15%)',\n",
    "                160: 'Tree cover, flooded, fresh or brakish water',\n",
    "                170: 'Tree cover, flooded, saline water',\n",
    "                180: 'Shrub or herbaceous cover, flooded, fresh/saline/brakish water',\n",
    "                190: 'Urban areas',\n",
    "                200: 'Bare areas',\n",
    "                210: 'Water bodies',\n",
    "                220: 'Permanent snow and ice'}\n",
    "        \n",
    "        # add line to map to IPCC land cover classes\n",
    "        ipcc = {0: 'Other',\n",
    "                10: 'Agriculture',\n",
    "                11: 'Agriculture',\n",
    "                20: 'Agriculture',\n",
    "                30: 'Agriculture',\n",
    "                40: 'Agriculture',\n",
    "                50: 'Forest',\n",
    "                60: 'Forest',\n",
    "                70: 'Forest',\n",
    "                80: 'Forest',\n",
    "                90: 'Forest',\n",
    "                100: 'Forest',\n",
    "                110: 'Grassland',\n",
    "                120: 'Other',\n",
    "                130: 'Grassland',\n",
    "                140: 'Other',\n",
    "                150: 'Other',\n",
    "                160: 'Forest',\n",
    "                170: 'Forest',\n",
    "                180: 'Wetland',\n",
    "                190: 'Settlement',\n",
    "                200: 'Other',\n",
    "                210: 'Other',\n",
    "                220: 'Other'}\n",
    "        \n",
    "        tree_cover['lc_class'] = tree_cover['esa_id'].map(legend)\n",
    "        tree_cover['ipcc_class'] = tree_cover['esa_id'].map(ipcc)\n",
    "        tree_cover['tof_hans'] = tree_cover['tof_total_ha'] - tree_cover['hansen_total_ha']\n",
    "        \n",
    "    print(f'{country} has {len(tree_cover.esa_id.value_counts())} land cover classes.')   \n",
    "    return tree_cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile = gpd.read_file(f'insert new str')\n",
    "admin_boundaries_all = list(shapefile.NAME_1)\n",
    "\n",
    "# goals: create a list of admin names without numbers\n",
    "# identify whether the admin has 1 or 2 digits in order to properly remove them\n",
    "no_ints = []\n",
    "for admin in admin_boundaries_all:\n",
    "    # if any character is a digit\n",
    "    if any(char.isdigit() for char in admin):\n",
    "        # remove the digits and add it to the list of no ints\n",
    "        clean_admin = ''.join([char for char in admin if not char.isdigit()])\n",
    "        no_ints.append(clean_admin)\n",
    "no_ints = list(set(no_ints))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for admin_2 in no_ints:\n",
    "    # gather list of files for that admin (ex: Puntarenas1.tif, Puntarenas2.tif, Puntarenas3.tif)\n",
    "    files_to_merge = [] # this needs to be in dataset reader mode\n",
    "    files_to_delete = [] # just a string of the file name\n",
    "    for path in glob.glob(f'Costa Rica/resampled_rasters/tof/{admin_2}?.tif') and glob.glob(f'Costa Rica/resampled_rasters/tof/{admin_2}??.tif'):\n",
    "        filename = os.path.basename(path) \n",
    "#         files_to_delete.append(filename)\n",
    "#         src = rs.open(f'{country}/resampled_rasters/{data}/{filename}')\n",
    "#         files_to_merge.append(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // matches all non-digits, replaces it with \"\" and returns the length.\n",
    "s.replaceAll(\"\\\\D\", \"\").length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "files_to_merge = []\n",
    "for path in glob.glob(f'Costa Rica/resampled_rasters/hansen/Acosta?.tif'):\n",
    "    filename = os.path.basename(path) \n",
    "    files_to_merge.append(filename)\n",
    "print(files_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "177.003px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
