{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b77154b",
   "metadata": {},
   "source": [
    "# Comparison of Zonal Stats (from different processing methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a71e0b",
   "metadata": {},
   "source": [
    "This notebook is an exploratory analyses that compares statistics for different processing extents and methods. Primarily, it explores differences between partially processed and the fully processed (wall to wall) TML data for specific regions. It additionally looks at the differences between zonal statistics produced by the Data API / GFW backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1ae40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rs\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd \n",
    "import pandas as pd\n",
    "import fiona\n",
    "\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import show_hist\n",
    "from osgeo import gdal\n",
    "\n",
    "from numpy.ma import masked_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe723088",
   "metadata": {},
   "source": [
    "## Comparison of statistics for full vs partial processing extent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f15c3bd",
   "metadata": {},
   "source": [
    "## Area sampled (per land cover class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97368349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_lc_sampled(country):\n",
    "    \n",
    "    ## This needs to include a check for land cover class IDs that were missed in partial processing ##\n",
    "    \n",
    "    # get the full and partial processing extent for the country\n",
    "    full_proc = pd.read_csv(f'comparisons/full_processing_area.csv')\n",
    "    partial_proc = pd.read_csv(f'comparisons/partial_processing_area.csv')\n",
    "    extent_full = full_proc[full_proc.country == country]\n",
    "    extent_partial = partial_proc[partial_proc.country == country]\n",
    "    \n",
    "    full = pd.read_csv(f'comparisons/{country}_statistics_full.csv')\n",
    "    partial = pd.read_csv(f'comparisons/{country}_statistics_partial.csv')\n",
    "\n",
    "    # set up the dataframe and calculate the area sampled for the full\n",
    "    full = full[['country', 'admin', 'esa_class', 'esa_id', 'esa_sampled_ha', 'esa_total_ha']]\n",
    "    full = full.drop_duplicates(keep='first', ignore_index=True)\n",
    "    full['esa_not_sampled'] = full['esa_total_ha'] - full['esa_sampled_ha']\n",
    "    full = full.groupby(by=['esa_class']).sum().reset_index()\n",
    "    full = full[full.esa_class != 'No Data (flag)']\n",
    "    #full['esa_perc_sampled'] = round((full['esa_sampled_ha'] / full['esa_total_ha']) * 100, 1)\n",
    "\n",
    "    # apply the same to the partial \n",
    "    partial = partial[['country', 'admin', 'esa_class','esa_id','esa_sampled_ha', 'esa_total_ha']]\n",
    "    partial = partial.drop_duplicates(keep='first', ignore_index=True)\n",
    "    partial['esa_not_sampled'] = partial['esa_total_ha'] - partial['esa_sampled_ha']\n",
    "    partial = partial.groupby(by=['esa_class']).sum().reset_index()\n",
    "    partial = partial[partial.esa_class != 'No Data (flag)']\n",
    "    #partial['esa_perc_sampled'] = round((partial['esa_sampled_ha'] / partial['esa_total_ha']) * 100, 1)\n",
    "    \n",
    "    print(f'Processing extent for {country}:')\n",
    "    print(f'Calculated extent sampled (full): {round(full.esa_sampled_ha.sum(),1)} ha')\n",
    "    print(f'Actual extent sampled (full): {extent_full.full_area_ha.item()} ha')\n",
    "          \n",
    "    # check for differences in lccs sampled\n",
    "    full_classes = list(full.esa_class.values)\n",
    "    partial_classes = list(partial.esa_class.values)\n",
    "    list_difference = [item for item in partial_classes if item not in full_classes]\n",
    "    if len(list_difference) > 0:\n",
    "        partial.drop(partial[partial.esa_class == list_difference[0]].index, inplace = True)\n",
    "        print(f'{list_difference} caused shape mismatch and was dropped from the partial dataset.')\n",
    "\n",
    "    # create the position of the grouped bars\n",
    "    width = 0.4\n",
    "    pos1 = np.arange(len(full))\n",
    "    pos2 = pos1 + width\n",
    "\n",
    "    plt.figure(figsize=(15,9))\n",
    "\n",
    "    # create the position of the stacked bars\n",
    "    classes = list(full.esa_class.values)\n",
    "    bars1 = full.esa_sampled_ha\n",
    "    bars2 = full.esa_not_sampled\n",
    "    bars3 = partial.esa_sampled_ha\n",
    "    bars4 = partial.esa_not_sampled\n",
    "\n",
    "    plt.barh(pos1, bars1, width, color=\"gold\", edgecolor='white', label='sampled (full)')  \n",
    "    plt.barh(pos1, bars2, width, left=bars1, color=\"darkslateblue\", edgecolor='white', label='not sampled (full)')\n",
    "    #f3e151  #6c3376\n",
    "    plt.barh(pos2, bars3, width, color=\"palegoldenrod\", edgecolor='white', label='sampled (partial)')  \n",
    "    plt.barh(pos2, bars4, width, left=bars3, color=\"slateblue\", edgecolor='white', label='not sampled (partial)')\n",
    "\n",
    "    plt.title(f'Land Cover Sampled in {country}')\n",
    "    plt.xlabel('Tree Cover (ha)')\n",
    "    plt.yticks(pos1 + width / 2, classes)\n",
    "    plt.ticklabel_format(useOffset=False, style='plain', axis='x')\n",
    "    plt.grid(axis='x', linestyle='-', linewidth=.2)\n",
    "    plt.legend(loc='lower right');\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lc_sampled('El Salvador')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c4e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lc_sampled('Costa Rica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68686392",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lc_sampled('Belize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5570d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lc_sampled('Panama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lc_sampled('Honduras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37270819",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lc_sampled('Nicaragua')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lc_sampled('Guatemala')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b526c66",
   "metadata": {},
   "source": [
    "## Total hectares of tree cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_lc_totalha(country):\n",
    "    \n",
    "    full = pd.read_csv(f'comparisons/{country}_statistics_full.csv')\n",
    "    partial = pd.read_csv(f'comparisons/{country}_statistics.csv')\n",
    "    \n",
    "    full_sum = full.groupby('esa_class').sum()\n",
    "    full_sum = full_sum[['tof_ha', 'hans_ha']]\n",
    "    partial_sum = partial.groupby('esa_class').sum()\n",
    "    partial_sum = partial_sum[['tof_ha', 'hans_ha']]\n",
    "\n",
    "    diverge_sum = full_sum - partial_sum\n",
    "    colors = ['red' if x < 0 else 'green' for x in diverge_sum.tof_ha]\n",
    "\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.hlines(y=diverge_sum.index,\n",
    "               xmin=0,\n",
    "               xmax=diverge_sum.tof_ha,\n",
    "               alpha=0.7,\n",
    "               linewidth=7,\n",
    "               colors=colors)\n",
    "               #label=str(diverge_sum.tof_ha))\n",
    "    \n",
    "    plt.ticklabel_format(useOffset=False, style='plain', axis='x')\n",
    "    plt.grid(axis='x', linestyle='-', linewidth=.2)\n",
    "    plt.title(f'Difference in TML tree cover (hectares) per land cover class: {country}')\n",
    "    plt.xlabel('tree cover (ha)');\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acda738",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lc_totalha('Panama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a69a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lc_totalha('Guatemala')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337839b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lc_totalha('El Salvador')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c957b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lc_totalha('Nicaragua')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa733c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lc_totalha('Belize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lc_totalha('Honduras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43cc7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_lc_mean(country):\n",
    "\n",
    "    full = pd.read_csv(f'comparisons/{country}_statistics_full.csv')\n",
    "    partial = pd.read_csv(f'comparisons/{country}_statistics.csv')\n",
    "\n",
    "    full_mean = full.groupby('esa_class').mean()\n",
    "    full_mean = full_mean[['tof_mean']]\n",
    "    partial_mean = partial.groupby('esa_class').mean()\n",
    "    partial_mean = partial_mean[['tof_mean']]\n",
    "    mean_merged = full_mean.merge(partial_mean, on='esa_class')\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "\n",
    "    width = 0.4\n",
    "    pos1 = np.arange(len(mean_merged))\n",
    "    pos2 = pos1 + width\n",
    "\n",
    "    plt.barh(pos1, mean_merged.tof_mean_x, width, color='gold', edgecolor='white', label='Full')\n",
    "    plt.barh(pos2, mean_merged.tof_mean_y, width, color='palegoldenrod', edgecolor='white', label='Partial')\n",
    "\n",
    "    plt.xlabel('% Tree Cover')\n",
    "    plt.yticks(pos1 + width / 2, mean_merged.index.values)\n",
    "    plt.title(f'Mean Tree Cover per Land Cover Class: {country}')\n",
    "    plt.grid(axis='x', linestyle='-', linewidth=.3)\n",
    "    plt.legend();\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dcd3c7",
   "metadata": {},
   "source": [
    "## Forest Cover Compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a15e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regional_csv(list_of_countries, region, extent):\n",
    "    '''\n",
    "    Merges the statistics for a list of countries into a single csv \n",
    "    file to permit regional analyses.\n",
    "    '''\n",
    "    \n",
    "    regional_df = pd.DataFrame()\n",
    "    dfs_to_concat = []\n",
    "    \n",
    "    for country in list_of_countries:\n",
    "        country_df = pd.read_csv(f'comparisons/{country}_statistics_{extent}.csv')\n",
    "        dfs_to_concat.append(country_df)\n",
    "    \n",
    "    regional_df = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "    regional_df.to_csv(f'comparisons/{region}_{extent}.csv', index=False)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# central america full processing\n",
    "create_regional_csv(['Belize', \n",
    "                     'Honduras', \n",
    "                     'Guatemala', \n",
    "                     'El Salvador', \n",
    "                     'Costa Rica', \n",
    "                     'Nicaragua', \n",
    "                     'Panama'], 'central_america', 'full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f61e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# central america partial processing\n",
    "create_regional_csv(['Belize', \n",
    "                     'Honduras', \n",
    "                     'Guatemala', \n",
    "                     'El Salvador', \n",
    "                     'Costa Rica', \n",
    "                     'Nicaragua', \n",
    "                     'Panama'], 'central_america', 'partial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5618a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# west africa full processing (missing Cape Verde, Ghana, Mali, Nigeria)\n",
    "create_regional_csv(['Benin',\n",
    "                    'Burkina Faso',\n",
    "                    'Ivory Coast',\n",
    "                    'Gambia',\n",
    "                    'Guinea',\n",
    "                    'Liberia',\n",
    "                    'Mauritania',\n",
    "                    'Niger',\n",
    "                    'Senegal',\n",
    "                    'Sierra Leone',\n",
    "                    'Togo'], 'west_africa', 'full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f1fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_forest_cover_compliant(region, extent, figsize, rotation):\n",
    "    \n",
    "    region_df = pd.read_csv(f'comparisons/{region}_{extent}.csv')\n",
    "    \n",
    "    countries = list(set(region_df.country.values))\n",
    "\n",
    "    x_labels, ag_bar1, ag_bar2, urban_bar1, urban_bar2 = [],[],[],[],[]\n",
    "    \n",
    "    for country in countries:\n",
    "        \n",
    "        # filter to one country and only urban/ag land cover classes\n",
    "        country_df = region_df[region_df.country == country]\n",
    "        ag_ids = [10.0, 11.0, 12.0, 20.0, 30.0, 40.0]\n",
    "        urban_ids = [190.0]\n",
    "        ag_df = country_df[country_df.esa_id.isin(ag_ids)].sort_values('country')\n",
    "        urban_df = country_df[country_df.esa_id.isin(urban_ids)].sort_values('country')\n",
    "        \n",
    "        # get total tof ha per tree cover threshold\n",
    "        ag_df = ag_df.groupby(by=['country', 'tree_cover_class']).sum().reset_index() \n",
    "        ag_df = ag_df[['country', 'tree_cover_class', 'tof_ha']] \n",
    "        urban_df = urban_df.groupby(by=['country', 'tree_cover_class']).sum().reset_index() \n",
    "        urban_df = urban_df[['country', 'tree_cover_class', 'tof_ha']] \n",
    "        \n",
    "        # calculate # ha <10% tree cover and >10% tree cover\n",
    "        ag_under10 = ag_df.tof_ha[0] \n",
    "        ag_over10 = sum(ag_df.tof_ha[1:])\n",
    "        urban_under10 = urban_df.tof_ha[0]\n",
    "        urban_over10 = sum(urban_df.tof_ha[1:])\n",
    "        \n",
    "        # normalize\n",
    "        ag_under10_norm = ag_under10/(ag_under10 + ag_over10)*100\n",
    "        ag_over10_norm = ag_over10/(ag_under10 + ag_over10)*100\n",
    "        urban_under10_norm = urban_under10/(urban_under10 + urban_over10)*100\n",
    "        urban_over10_norm = urban_over10/(urban_under10 + urban_over10)*100\n",
    "        \n",
    "        x_labels.append(country)\n",
    "        ag_bar1.append(ag_over10_norm) # >10% is on the bottom\n",
    "        ag_bar2.append(ag_under10_norm) # <10% is on the top\n",
    "        urban_bar1.append(urban_over10_norm) \n",
    "        urban_bar2.append(urban_under10_norm) \n",
    "   \n",
    "    # convert to array in order to add data labels\n",
    "    ag_bar1 = np.asarray(ag_bar1)\n",
    "    ag_bar2 = np.asarray(ag_bar2)\n",
    "    urban_bar1 = np.asarray(urban_bar1)\n",
    "    urban_bar2 = np.asarray(urban_bar2)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "        \n",
    "    # Ag plot\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.bar(x_labels, ag_bar1, color=\"seagreen\", label='>10% tree cover', capsize=4)  \n",
    "    plt.bar(x_labels, ag_bar2, bottom=ag_bar1, color=\"honeydew\", label='<10% tree cover')    \n",
    "    \n",
    "    # labels\n",
    "    for xpos, ypos, yval in zip(x_labels, ag_bar1/2, ag_bar1):\n",
    "        plt.text(xpos, ypos, f'{round(yval)}%', ha=\"center\", va=\"center\")\n",
    "    for xpos, ypos, yval in zip(x_labels, ag_bar1 + ag_bar2/2, ag_bar2):\n",
    "        plt.text(xpos, ypos, f'{round(yval)}%', ha=\"center\", va=\"center\")\n",
    "    \n",
    "    plt.xlabel(' ')\n",
    "    plt.xticks(rotation=rotation)\n",
    "    plt.ylabel('% Land')\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    plt.title(f'% Agricultural Land Meeting \\n 10% Forest Cover Criteria \\n ({extent})')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    # Urban plot\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.bar(x_labels, urban_bar1, color=\"seagreen\", label='>10% tree cover', capsize=4)  \n",
    "    plt.bar(x_labels, urban_bar2, bottom=urban_bar1, color=\"honeydew\", label='<10% tree cover')    \n",
    "    \n",
    "    # labels\n",
    "    for xpos, ypos, yval in zip(x_labels, urban_bar1/2, urban_bar1):\n",
    "        plt.text(xpos, ypos, f'{round(yval)}%', ha=\"center\", va=\"center\")\n",
    "    for xpos, ypos, yval in zip(x_labels, urban_bar1 + urban_bar2/2, urban_bar2):\n",
    "        plt.text(xpos, ypos, f'{round(yval)}%', ha=\"center\", va=\"center\")\n",
    "    \n",
    "    \n",
    "    plt.xlabel(' ')\n",
    "    plt.xticks(rotation=rotation)\n",
    "    plt.ylabel('% Land')\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    plt.title(f'% Urban Land Meeting \\n 10% Forest Cover Criteria \\n ({extent})')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65418626",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_forest_cover_compliant('central_america', 'partial', (14,7), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb00b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_forest_cover_compliant('central_america', 'full', (14,7), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_forest_cover_compliant('west_africa', 'full', (14,7), 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a42ccd",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "**Area sampled (per land cover class)**\n",
    "- The 'No Data (flag)' land cover classes is removed.\n",
    "- We expect and see that the dark yellow bars are higher and the dark purple bars are smaller, indicating an increase in the sampling area during full processing. The most significant increases are in the tree cover (broadleaved/deciduous, broadleaved/evergreen) classes, as these were intentionally omitted in the initial processing extent.\n",
    "- Additional labels for some land cover classes (same ESA label but different ESA ID number) were identified. This could result in a higher count of hectares for the sampled and total area for a land cover class if these labels were not previously identified.\n",
    "- The no data glag is removed. If a land cover class does not appear in the visualization (a warning will print) the country did not have data for that land cover class in the initial processing, but does include that land cover class in the full processing.\n",
    "\n",
    "**Total hectares of tree cover (per land cover class)**\n",
    "- We expect and see the greatest differences in the tree cover (broadleaved/deciduous, broadleaved/evergreen) class. This is logical given the initial processing extent intentionally omitted these classes.\n",
    "- We see an increase in the total hectares on cropland, which could be a result of the additional encoding/labeling of ESA ID's in that land cover category.\n",
    "- Note: hectares of tree cover are not weighted by the percentage of tree cover within that hectare. A hectare with 10% tree cover is counted the same as a hectare with 90% tree cover.\n",
    "\n",
    "**Forest cover compliant (per country)**\n",
    "- In Central America, there's a fluctution of ~1% between the partial and full processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524104d6",
   "metadata": {},
   "source": [
    "# Comparison with GFW Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae49a54",
   "metadata": {},
   "source": [
    "## Compare boundaries\n",
    "Get shapefile from geostore through RW API and run through the pipeline, compare to downloaded zip from GADM. Since geostore only returns a single admin, compare the results for a single admin across three different countries (DRC, El Salvador, South Sudan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = confuse.Configuration('sentinel-tree-cover')\n",
    "config.set_file('/Users/jessica.ertel/sentinel-tree-cover/jessica-config.yaml')\n",
    "api_token = config['rw']['token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gjson(country):\n",
    "    \n",
    "    '''\n",
    "    Takes in a country to request admin 1 shapefile from geostore.\n",
    "    Saves the geostore object (single admin shapefile) as a geojson.\n",
    "    '''\n",
    "    \n",
    "    # use pycountry to get country ISO\n",
    "    iso = pycountry.countries.get(name = country).alpha_3\n",
    "\n",
    "    # if errors, try pycountry.countries.search_fuzzy('country')\n",
    "    \n",
    "    # Get geostore object by ISO endpoint and GADM admin boundary 1\n",
    "    rw_url = (f'https://api.resourcewatch.org/v2/geostore/admin/{iso}/1')\n",
    "    my_headers = {'Authorization': 'Bearer ' + str(api_token)}\n",
    "    response = requests.get(url=rw_url, headers=my_headers)\n",
    "    print(response)\n",
    "    \n",
    "    # save response as geojson\n",
    "    data = response.json()\n",
    "    geojson = data['data']['attributes'] \n",
    "    geojson['geojson']['crs'] = 'epsg:4326'\n",
    "    \n",
    "    # check which admin was returned\n",
    "    print(geojson['info']['name'])\n",
    "    \n",
    "    with open(f'{country}_adminboundaries.geojson', 'w') as f:\n",
    "        dump(geojson, f)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b0f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# admin is Central Equatoria\n",
    "get_gjson('South Sudan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# admin is Bas-Uélé\n",
    "get_gjson('Congo, The Democratic Republic of the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7765fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# admin is Ahuachapán\n",
    "get_gjson('El Salvador')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78288724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add NAME_1 column so geojsons are compatible with tml analysis pipeline reqs\n",
    "geostore_salvador = gpd.read_file('Ahuachapán_adminboundaries.geojson')\n",
    "geostore_salvador['NAME_1'] = 'Ahuachapán'\n",
    "geostore_salvador.to_file(f'Ahuachapán_adminboundaries.geojson', driver='GeoJSON')\n",
    "check = gpd.read_file('Ahuachapán_adminboundaries.geojson')\n",
    "check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "geostore_drc = gpd.read_file('Bas-Uélé_adminboundaries.geojson')\n",
    "geostore_drc['NAME_1'] = 'Bas-Uélé'\n",
    "geostore_drc.to_file(f'Bas-Uélé_adminboundaries.geojson', driver='GeoJSON')\n",
    "check = gpd.read_file('Bas-Uélé_adminboundaries.geojson')\n",
    "check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f9396",
   "metadata": {},
   "outputs": [],
   "source": [
    "geostore_ssudan = gpd.read_file('Central Equatoria_adminboundaries.geojson')\n",
    "geostore_ssudan['NAME_1'] = 'Central Equatoria'\n",
    "geostore_ssudan.to_file(f'Central Equatoria_adminboundaries.geojson', driver='GeoJSON')\n",
    "check = gpd.read_file('Central Equatoria_adminboundaries.geojson')\n",
    "check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ab727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gadm_vs_geostore(country, admin):\n",
    "    \n",
    "    '''\n",
    "    Takes in a country and admin in order to import and filter the statistics\n",
    "    to enable side by side comparison of differences.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # for gadm, import country stats and filter df to admin\n",
    "    gadm = pd.read_csv(f'statistics/{country}_statistics_full.csv')\n",
    "    gadm = gadm[gadm.admin == admin]\n",
    "    gadm = gadm[['esa_id', 'esa_class', 'tof_mean']]\n",
    "    gadm = gadm.drop_duplicates(keep='first', ignore_index=True).rename(columns={'tof_mean': 'tof_mean_gadm'}) \n",
    "\n",
    "    # for geostore, import admin stats\n",
    "    geostore = pd.read_csv(f'statistics/{admin}_statistics_full_tmlonly.csv')\n",
    "    geostore = geostore[['esa_id', 'esa_class','tof_mean']]\n",
    "    geostore = geostore.drop_duplicates(keep='first', ignore_index=True).rename(columns={'tof_mean': 'tof_mean_geostore'})\n",
    "\n",
    "    # compare tof mean for geostore and gadm\n",
    "    comb = gadm.join(geostore.tof_mean_geostore)\n",
    "    comb['diff'] = round(comb.tof_mean_gadm - comb.tof_mean_geostore, 3)\n",
    "        \n",
    "    return comb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "gadm_vs_geostore('El Salvador', 'Ahuachapán')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b11743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gadm_vs_geostore('South Sudan', 'Central Equatoria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146adb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required different approach due to issue with accents on admin name\n",
    "# for gadm, import country stats and filter df to admin\n",
    "gadm = pd.read_csv(f'statistics/DRC_statistics_full_tmlonly.csv')\n",
    "gadm = gadm[gadm.admin == 'Bas-Uele']\n",
    "gadm = gadm[['esa_id', 'esa_class', 'tof_mean']]\n",
    "gadm = gadm.drop_duplicates(keep='first', ignore_index=True).rename(columns={'tof_mean': 'tof_mean_gadm'}) \n",
    "\n",
    "# for geostore, import admin stats\n",
    "geostore = pd.read_csv('statistics/Bas-Uélé_statistics_full_tmlonly.csv')\n",
    "geostore = geostore[['esa_id', 'esa_class','tof_mean']]\n",
    "geostore = geostore.drop_duplicates(keep='first', ignore_index=True).rename(columns={'tof_mean': 'tof_mean_geostore'})\n",
    "\n",
    "# compare tof mean for geostore and gadm\n",
    "comb = gadm.join(geostore.tof_mean_geostore)\n",
    "comb['diff'] = round(comb.tof_mean_gadm - comb.tof_mean_geostore, 3)\n",
    "comb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b9c8c3",
   "metadata": {},
   "source": [
    "## Compare tree cover by land cover class\n",
    "GFW and TML use the 2015 ESA land cover product for land cover classifications, however GFW aggregates to IPCCC land cover types. To compare, look at the total hectares of tree cover in agricultural areas for a country.\n",
    "\n",
    "geostore IDs: \n",
    "- south sudan: 566e8323abaf1b7080b179bec5946ce6\n",
    "- DRC: 2852c7accd29c848ed699cdff6bd275e\n",
    "- el salvador: 8ea11cb2347e2e93ebc7e0ede15598ba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ddefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = confuse.Configuration('sentinel-tree-cover')\n",
    "config.set_file('/Users/jessica.ertel/sentinel-tree-cover/jessica-config.yaml')\n",
    "api_token = config['rw']['token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cfbeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drc_id = '2852c7accd29c848ed699cdff6bd275e'\n",
    "es_id = \"8ea11cb2347e2e93ebc7e0ede15598ba\"\n",
    "ssudan_id = \"566e8323abaf1b7080b179bec5946ce6\"\n",
    "\n",
    "country_id = drc_id\n",
    "url = f'https://data-api.globalforestwatch.org/dataset/wri_trees_in_mosaic_landscapes/v20220218/query/json?geostore_id={country_id}&geostore_origin=rw&sql=SELECT+sum%28area__ha%29+FROM+data+GROUP+BY+esa_land_cover_2015__class%2C+wri_trees_in_mosaic_landscapes__decile'\n",
    "my_headers = {'Authorization': str(api_token)}\n",
    "response = requests.get(url=url, headers=my_headers)\n",
    "print(response)\n",
    "\n",
    "# store as json, but remove ('status', 'success')\n",
    "data = response.json()\n",
    "data.popitem() \n",
    "\n",
    "# make into dataframe with updated columns\n",
    "gfw = pd.DataFrame(data['data'])\n",
    "gfw.rename(columns={\"esa_land_cover_2015__class\": \"esa_class\", \n",
    "                   \"wri_trees_in_mosaic_landscapes__decile\": \"tree_cover_class\"},\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimated: 226,175,961  \n",
    "#Actual: 219,481,200 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058fa85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(gfw.area__ha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f92c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the total ha trees in ag areas for DRC\n",
    "gfw_ag = gfw[gfw.esa_class == 'Agriculture']\n",
    "gfw_ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dabfcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tml = pd.read_csv('statistics/DRC_statistics_full_tmlonly.csv')\n",
    "tml_ag = tml[tml.esa_id.isin([10, 11, 12, 20, 30, 40])]\n",
    "tml_ag = tml_ag[['tree_cover_class', 'tof_ha']]\n",
    "tml_ag = tml_ag.groupby(by=['tree_cover_class']).sum().reset_index()\n",
    "tml_ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2282cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esa_to_ipcc(df):\n",
    "    \n",
    "    '''\n",
    "    Aggregates ESA land cover classes to IPCC land cover classes.\n",
    "    Returns a dataframe with tree cover statistics for each IPCC class.\n",
    "    \n",
    "    '''\n",
    "    agriculture = df[df.esa_id.isin([10, 11, 12, 20, 20, 30, 40])]\n",
    "    forest = df[df.esa_id.isin([50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100, 160, 170])]\n",
    "    grassland = df[df.esa_id.isin([110, 130])]\n",
    "    wetland = df[df.esa_id == 180]\n",
    "    settlement = df[df.esa_id == 190]\n",
    "    shrubland = df[df.esa_id.isin([120, 121, 122])]\n",
    "    sparse_veg = df[df.esa_id.isin([140, 150, 151, 152, 153])]\n",
    "    bare = df[df.esa_id.isin([200, 201, 202])]\n",
    "    water = df[df.esa_id == 210]\n",
    "\n",
    "    return agriculture, forest, grassland, wetland, settlement, shrubland, sparse_veg, bare, water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa6304",
   "metadata": {},
   "outputs": [],
   "source": [
    "agriculture, forest, grassland, wetland, settlement, shrubland, sparse_veg, bare, water = esa_to_ipcc(drc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cc6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean tree cover per land cover class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be862595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total ha per threshold per country\n",
    "df = df.groupby(by=['country', 'tree_cover_class']).sum().reset_index()\n",
    "df = df[['country', 'tree_cover_class', 'tof_ha']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe183db",
   "metadata": {},
   "outputs": [],
   "source": [
    "agriculture = drc[drc.esa_id.isin([10, 11, 12, 20, 20, 30, 40])]\n",
    "agriculture[['admin', 'esa_id', 'esa_class', 'tree_cover_class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c860da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the 0-20 class total ha is \n",
    "# get total ha for TML agriculture\n",
    "tml_agriculture = agriculture[['country', 'tree_cover_class', 'tof_ha']]\n",
    "tml_agriculture.groupby('tree_cover_class').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9910a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "gfw_agriculture = gfw[gfw.esa_class == 'Agriculture']\n",
    "gfw_agriculture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4adbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total ha for TML agriculture\n",
    "tml_urban = settlement[['country', 'tree_cover_class', 'tof_ha']]\n",
    "tml_urban.groupby('tree_cover_class').sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fabe442",
   "metadata": {},
   "source": [
    "## Compare resampling effects\n",
    "\n",
    "**Processing Extents**   \n",
    "DRC  \n",
    "- Calculated from original TML tif: 229,218,208.13 ha\n",
    "- Estimated area sampled (from TML pipeline): 226,175,961 ha  \n",
    "- Extent Spreadsheet: 219,481,200 ha (difference from above -6,694,761 ha)\n",
    "- GFW: 217,834,921.067 ha\n",
    "- Wiki: 226,704,800 ha\n",
    "\n",
    "El Salvador \n",
    "- Calculated from original TML tif: 2,101,131.25 ha\n",
    "- Estimated area sampled (from TML pipeline): 1,957,471\n",
    "- Extent Spreadsheet: 2,066,400 (difference from above 108,929 ha)\n",
    "- GFW: 1,532,594.297 ha\n",
    "- Wiki: 2,072,100 ha\n",
    " \n",
    " \n",
    "South Sudan \n",
    "- Calculated from original TML tif: 62,429,864.11 ha\n",
    "- Estimated area sampled (from TML pipeline): 60,928,916\n",
    "- Extent Spreadsheet: 59,709,600 (difference from above -1,219,316 ha)\n",
    "- GFW: 57,831,316.129 ha\n",
    "- Wiki: 64,432,900 ha\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e2afa7",
   "metadata": {},
   "source": [
    "### Option 1\n",
    "Compare total area_ha from GFW stats to total pixels in TML country raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898f9339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get drc stats and save as df\n",
    "drc_id = '2852c7accd29c848ed699cdff6bd275e'\n",
    "es_id = \"8ea11cb2347e2e93ebc7e0ede15598ba\"\n",
    "ssudan_id = \"566e8323abaf1b7080b179bec5946ce6\"\n",
    "\n",
    "country_id = es_id\n",
    "url = f'https://data-api.globalforestwatch.org/dataset/wri_trees_in_mosaic_landscapes/v20220218/query/json?geostore_id={country_id}&geostore_origin=rw&sql=SELECT+sum%28area__ha%29+FROM+data+GROUP+BY+esa_land_cover_2015__class%2C+wri_trees_in_mosaic_landscapes__decile'\n",
    "my_headers = {'Authorization': str(api_token)}\n",
    "response = requests.get(url=url, headers=my_headers)\n",
    "print(response)\n",
    "\n",
    "# store as json, but remove ('status', 'success')\n",
    "data = response.json()\n",
    "data.popitem() \n",
    "\n",
    "# make into dataframe with updated columns\n",
    "gfw = pd.DataFrame(data['data'])\n",
    "gfw.rename(columns={\"esa_land_cover_2015__class\": \"esa_class\", \n",
    "                   \"wri_trees_in_mosaic_landscapes__decile\": \"tree_cover_class\"},\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sum of gfw pixels for El Salvador\n",
    "# note this is not filtering out no data values\n",
    "es_gfw_count = round(sum(gfw.area__ha),3)\n",
    "es_gfw_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed60017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sum of gfw pixels for South Sudan\n",
    "# note this is not filtering out no data values\n",
    "ssudan_gfw_count = round(sum(gfw.area__ha),3)\n",
    "ssudan_gfw_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70dacd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssudan = rs.open(f'South Sudan.tif').read(1)\n",
    "ssudan_tml_count = np.sum(ssudan != 255)\n",
    "ssudan_tml_count / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100fb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = rs.open(f'El Salvador.tif').read(1)\n",
    "es_tml_count = np.sum(es != 255)\n",
    "es_tml_count / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82b4ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "drc_tml_count / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc147c67",
   "metadata": {},
   "source": [
    "### Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3d1176",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get GFW data from gfw-data-lake\n",
    "#aws s3 cp s3://gfw-data-lake/esa_land_cover_2015/v2016/raster/epsg-4326/10/100000/class/geotiff/20N_100W.tif jessica.ertel/sentinel-tree-cover/notebooks/analysis/ --request-payer\n",
    "\n",
    "tml1 = rs.open('20N_100W.tif').read(1)\n",
    "tml2 = rs.open('20N_090W.tif').read(1)\n",
    "\n",
    "\n",
    "# mask out no data values for plotting\n",
    "#tml_ma = np.ma.masked_where(tml == 255, tml, copy=True)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "fontsize = 18\n",
    "\n",
    "# plot admin with urban land cover overlay\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(tml1, cmap='Greens')\n",
    "plt.title(f'20N_100W', fontsize=fontsize)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(tml2, cmap='Greens')\n",
    "plt.title(f'20N_090W', fontsize=fontsize)\n",
    "plt.axis('off')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e11b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get TML pixel count for each admin in Belize\n",
    "# to do this, downloaded the 10x10 degree tiles from gfw-data-lake\n",
    "# ran them through the pipeline as if they were Hansen data\n",
    "# skipped resampling and merged multipolys to calculate pixel size\n",
    "\n",
    "admins = [x for x in natsorted(os.listdir(\"Belize/tml_clipped/\")) if \".tif\" in x]\n",
    "\n",
    "for i in admins:\n",
    "    tml = rs.open(f'Belize/tml_clipped/{i}').read(1)\n",
    "    gfw = rs.open(f'Belize/gfw_clipped/{i}').read(1)\n",
    "    tml_count = np.sum(tml != 255)\n",
    "    gfw_count = np.sum(gfw != 255)\n",
    "    print(i[:-4]) \n",
    "    print(f'TML pixel count: {tml_count}, GFW pixel count: {gfw_count}')\n",
    "    print(f'Difference: {tml_count - gfw_count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb9c733",
   "metadata": {},
   "source": [
    "## Removal of Median Bracketing for Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f26c52",
   "metadata": {},
   "source": [
    "In comparison with the statistics that use bracketing, removing the brackets results in lower total ha and average tree cover.\n",
    "\n",
    "Bangladesh area  \n",
    "Estimated: 13,521,943  \n",
    "Actual: 8,405,739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first compare the difference in statistics between the newly bracketed data and the old bracket\n",
    "# for the 10m resolution tif (v1 and v4)\n",
    "\n",
    "def compare_stats(filename):\n",
    "    \n",
    "    # create ha stats table\n",
    "    df = pd.read_csv(filename)\n",
    "    table = df[['admin', 'esa_class', 'tree_cover_class', 'tof_ha', 'hans_ha']]\n",
    "    table = table.groupby('esa_class').sum().reset_index()\n",
    "    \n",
    "    # print ha totals\n",
    "    stats = df.groupby(by='tree_cover_class').sum().reset_index()\n",
    "    stats = stats[['tree_cover_class', 'tof_ha', 'hans_ha']] \n",
    "\n",
    "    # remove tree cover classes <10%\n",
    "    over10 = stats[stats.tree_cover_class != '0-9']\n",
    "    over20 = stats[(stats.tree_cover_class != '0-9') & (stats.tree_cover_class != '10-19')]\n",
    "    over30 = stats[(stats.tree_cover_class != '0-9') & (stats.tree_cover_class != '10-19') & (stats.tree_cover_class != '20-29')]\n",
    "\n",
    "    # calculate totals\n",
    "    total = sum(stats.tof_ha)\n",
    "    over10_ha = sum(over10.tof_ha)\n",
    "    over10_perc = sum(over10.tof_ha) / total * 100\n",
    "    over20_perc = sum(over20.tof_ha) / total * 100\n",
    "    over30_perc = sum(over30.tof_ha) / total * 100\n",
    "\n",
    "    print(f'ha >10%: {round(over10_perc,2)}')\n",
    "    print(f'ha >20%: {round(over20_perc,2)}')\n",
    "    print(f'ha >30%: {round(over30_perc,2)}')\n",
    "    \n",
    "    return table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164712c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = pd.read_csv('statistics/Belize_statistics_full.csv')\n",
    "v1_ha = compare_stats('statistics/Belize_statistics_full.csv')\n",
    "v1_ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af654811",
   "metadata": {},
   "outputs": [],
   "source": [
    "v4 = pd.read_csv('Belize/stats/Belize_statistics_full.csv')\n",
    "v4_ha = compare_stats('Belize/stats/Belize_statistics_full.csv')\n",
    "v4_ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9590f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = v1[['admin', 'esa_class', 'esa_sampled_ha', 'esa_total_ha']]\n",
    "v1 = v1.drop_duplicates()\n",
    "v1 = v1.groupby('admin').sum().reset_index()\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dfc40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "v4 = v4[['admin', 'esa_class', 'esa_sampled_ha', 'esa_total_ha']]\n",
    "v4 = v4.drop_duplicates()\n",
    "v4 = v4.groupby('admin').sum().reset_index()\n",
    "v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133af057",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1.esa_total_ha - v4.esa_total_ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a13a66",
   "metadata": {},
   "outputs": [],
   "source": [
    " # create v1 avg table\n",
    "df = pd.read_csv('statistics/Bangladesh_statistics_full.csv')\n",
    "table = df[['admin', 'esa_class', 'tof_mean', 'hans_mean']]\n",
    "table = table.drop_duplicates()\n",
    "table = table.groupby('esa_class').mean().reset_index()\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    " # create v4 avg table\n",
    "df = pd.read_csv('statistics/Bangladesh_v4_statistics_full.csv')\n",
    "table = df[['admin', 'esa_class', 'tof_mean', 'hans_mean']]\n",
    "table = table.drop_duplicates()\n",
    "table = table.groupby('esa_class').mean().reset_index()\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = pd.read_csv('statistics/Bangladesh_statistics_full.csv')\n",
    "\n",
    "v5 = pd.read_csv('statistics/Bangladesh_statistics_full_statscheck.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_stats('statistics/Bangladesh_statistics_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_stats('statistics/Bangladesh_statistics_full_statscheck.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db040105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the % land with >10% tree cover\n",
    "zonals = pd.read_csv('statistics/centralamzonalstats.csv')\n",
    "zonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e62723",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pd.read_csv('statistics/central_am.csv')\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493d11f",
   "metadata": {},
   "source": [
    "## Check Hansen histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses a Hansen 10m resampled tif\n",
    "file = '/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Panama/hansen/Bocas del Toro.tif'\n",
    "ds = gdal.Open(file)\n",
    "array = ds.GetRasterBand(1).ReadAsArray()\n",
    "plt.imshow(array)\n",
    "plt.colorbar()\n",
    "print(ds.GetGeoTransform())\n",
    "print(ds.GetProjection())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea32bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f354089",
   "metadata": {},
   "outputs": [],
   "source": [
    "resamp = gdal.Warp('/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Panama/hansen/Bocas del Toro_resamp.tif',\n",
    "                    ds,\n",
    "                    xRes=0.001,\n",
    "                    yRes=0.001)\n",
    "# always remember to close\n",
    "ds = None\n",
    "resamp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "ds = gdal.Open('/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Panama/hansen/Bocas del Toro.tif')\n",
    "array = ds.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "# resampled\n",
    "resamp = rs.open('/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Panama/hansen/Bocas del Toro_resamp.tif')\n",
    "\n",
    "#fig, ax = plt.subplots(1, 2, figsize=(14,7))\n",
    "show_hist(resamp, alpha=0.3)\n",
    "plt.imshow(resamp_array)\n",
    "plt.colorbar();\n",
    "ds = None\n",
    "resamp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac662ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampled\n",
    "plt.figure(figsize=(10,7))\n",
    "resamp = rs.open('/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Panama/hansen/Bocas del Toro_resamp.tif')\n",
    "show_hist(resamp, alpha=0.3, title='Bocas del Toro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428588f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bocas de Toro\n",
    "plt.figure(figsize=(10,7))\n",
    "#convert to array in order to get subset of values\n",
    "toro = resamp.read(1)\n",
    "under15 = toro[np.where(toro <= 13)]\n",
    "show_hist(under15, alpha=0.5, title='Bocas del Toro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f967bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test other admins in Belize\n",
    "admin = 'Colón'\n",
    "ds = gdal.Open(f'/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Panama/hansen/{admin}.tif')\n",
    "resamp = gdal.Warp(f'/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Panama/hansen/{admin}_resamp.tif',\n",
    "                    ds,\n",
    "                    xRes=0.001,\n",
    "                    yRes=0.001)\n",
    "\n",
    "# always remember to close\n",
    "ds = None\n",
    "resamp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82236c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "resamp = rs.open(f'/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Panama/hansen/{admin}_resamp.tif')\n",
    "show_hist(resamp, alpha=0.3, title=admin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom in on <10%\n",
    "plt.figure(figsize=(10,7))\n",
    "array = resamp.read(1)\n",
    "under15 = array[np.where(array <= 15)]\n",
    "show_hist(under15, alpha=0.5, title=admin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin = 'Los Santos'\n",
    "ds = gdal.Open(f'/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Panama/hansen/{admin}.tif')\n",
    "resamp = gdal.Warp(f'/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Panama/hansen/{admin}_resamp.tif',\n",
    "                    ds,\n",
    "                    xRes=0.001,\n",
    "                    yRes=0.001)\n",
    "\n",
    "# always remember to close\n",
    "ds = None\n",
    "resamp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705dae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "resamp = rs.open(f'/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Panama/hansen/{admin}_resamp.tif')\n",
    "show_hist(resamp, alpha=0.3, title=admin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc54326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom in on <10%\n",
    "plt.figure(figsize=(10,7))\n",
    "array = resamp.read(1)\n",
    "under15 = array[np.where(array <= 15)]\n",
    "show_hist(under15, alpha=0.5, title=admin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827946eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin = 'Herrera'\n",
    "ds = gdal.Open(f'/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Panama/hansen/{admin}.tif')\n",
    "resamp = gdal.Warp(f'/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Panama/hansen/{admin}_resamp.tif',\n",
    "                    ds,\n",
    "                    xRes=0.001,\n",
    "                    yRes=0.001)\n",
    "\n",
    "# always remember to close\n",
    "ds = None\n",
    "resamp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f69076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "resamp = rs.open(f'/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Panama/hansen/{admin}_resamp.tif')\n",
    "show_hist(resamp, alpha=0.3, title=admin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom in on <10%\n",
    "plt.figure(figsize=(10,7))\n",
    "array = resamp.read(1)\n",
    "under15 = array[np.where(array <= 15)]\n",
    "show_hist(under15, alpha=0.5, title=admin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb06f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "coro = rs.open('/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Belize/resampled_rasters/hansen/Corozal.tif')\n",
    "show(coro);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 ha resampled\n",
    "coro_resamp = rs.open('/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Belize/resampled_rasters/hansen/Corozal_resamp.tif')\n",
    "show(coro_resamp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist(coro_resamp, alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8178759",
   "metadata": {},
   "outputs": [],
   "source": [
    "coro_resamp.read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93600e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom in on the values <10% and analyze curve\n",
    "coro = coro_resamp.read(1)\n",
    "under15 = coro[np.where(coro <= 15)]\n",
    "under15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27849e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist(under15, alpha=0.5, legend=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e03c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = gdal.Open('/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Belize/resampled_rasters/hansen/Belize.tif')\n",
    "array = ds.GetRasterBand(1).ReadAsArray()\n",
    "plt.imshow(array)\n",
    "plt.colorbar()\n",
    "print(ds.GetGeoTransform())\n",
    "print(ds.GetProjection())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb1056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "belize_resamp = gdal.Warp('/Users/jessica.ertel/sentinel-tree-cover/notebooks/analysis/Belize/resampled_rasters/hansen/Belize_resamp.tif',\n",
    "                            ds,\n",
    "                            xRes=0.001,\n",
    "                            yRes=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d75d5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48e8888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76b9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tml-analysis",
   "language": "python",
   "name": "tml-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "184px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
