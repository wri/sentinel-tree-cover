{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package import, API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "from sentinelhub import DataSource\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import splu\n",
    "from skimage.transform import resize\n",
    "from sentinelhub import CustomUrlParam\n",
    "from time import time as timer\n",
    "import multiprocessing\n",
    "import math\n",
    "import reverse_geocoder as rg\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "import hickle as hkl\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import math\n",
    "import boto3\n",
    "from pyproj import Proj, transform\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, List\n",
    "import warnings\n",
    "from scipy.ndimage import median_filter\n",
    "import datetime\n",
    "from time import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/preprocessing/slope.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/downloading/utils.py\n",
    "%run ../src/preprocessing/cloud_removal.py\n",
    "%run ../src/preprocessing/whittaker_smoother.py\n",
    "%run ../src/downloading/upload.py\n",
    "%run ../src/tof/tof_downloading.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../config.yaml\"):\n",
    "    with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key']\n",
    "        AWSKEY = key['awskey']\n",
    "        AWSSECRET = key['awssecret']\n",
    "else:\n",
    "    API_KEY = \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Make the tiling ID structure\n",
    "\n",
    "The data is processed in 6 x 6 km tiles. The following code will calculate the x, y tile IDs for a given CSV file containing tile centroids as lat, lon pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"processing_area.csv\")\n",
    "\n",
    "make_tile = False\n",
    "\n",
    "if make_tile:\n",
    "    def id_tile_number(idx, col):\n",
    "        vals = data[col]\n",
    "        vals_set = sorted(np.unique(vals))\n",
    "        no = np.argwhere(vals_set == idx).flatten()\n",
    "        return str(no[0])\n",
    "\n",
    "    y_tiles = np.empty((len(data)))\n",
    "    data = data.reset_index()\n",
    "    for idx in tnrange(len(data)):\n",
    "        y_tiles[idx] = (id_tile_number(data['Y'][idx], 'Y'))\n",
    "\n",
    "    y_tiles = list(y_tiles)\n",
    "    y_tiles = [str(x) for x in y_tiles]\n",
    "    data['Y_tile'] = y_tiles\n",
    "    data.to_csv(\"final_processing_area_noclip.csv\")\n",
    "\n",
    "    x_tiles = np.empty((len(data)))\n",
    "    data = data.reset_index()\n",
    "    for idx in tnrange(len(data)):\n",
    "        x_tiles[idx] = (id_tile_number(data['X'][idx], 'X'))\n",
    "\n",
    "    x_tiles = list(x_tiles)\n",
    "    x_tiles = [str(x) for x in x_tiles]\n",
    "    data['X_tile'] = x_tiles\n",
    "    data.to_csv(\"final_processing_area_noclip.csv\")\n",
    "    \n",
    "    \n",
    "tracker = pd.DataFrame({'X_tile': [], 'Y_tile': []})\n",
    "x_tiles = [x for x in os.listdir(\"../project-monitoring/tof/\") if '.DS' not in x]\n",
    "x_tiles = [x for x in x_tiles if '.csv' not in x]\n",
    "for x_tile in x_tiles:\n",
    "    y_tiles = os.listdir(\"../project-monitoring/tof/\" + x_tile)\n",
    "    y_tiles = [y for y in y_tiles if '.DS' not in y]\n",
    "    y_tiles = [y for y in y_tiles if '.tif' not in y]\n",
    "    for y_tile in y_tiles:\n",
    "        tracker = tracker.append({'X_tile': int(x_tile), 'Y_tile': int(y_tile)}, ignore_index = True)\n",
    "\n",
    "tracker = pd.merge(tracker, data)\n",
    "tracker.to_csv(\"../project-monitoring/tof/tracker.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "MDL_PATH = \"../models/supres/\"\n",
    "\n",
    "model = tf.train.import_meta_graph(MDL_PATH + 'model.meta')\n",
    "model.restore(sess, tf.train.latest_checkpoint(MDL_PATH))\n",
    "\n",
    "logits = tf.get_default_graph().get_tensor_by_name(\"Add_6:0\")\n",
    "inp = tf.get_default_graph().get_tensor_by_name(\"Placeholder:0\")\n",
    "inp_bilinear = tf.get_default_graph().get_tensor_by_name(\"Placeholder_1:0\")\n",
    "\n",
    "\n",
    "def superresolve_tile(arr: np.ndarray, sess = sess) -> np.ndarray:\n",
    "    \"\"\"Superresolves each 56x56 subtile in a 646x646 input tile\n",
    "       by padding the subtiles to 64x64 and removing the pad after prediction,\n",
    "       eliminating boundary artifacts\n",
    "\n",
    "        Parameters:\n",
    "         arr (arr): (?, 646, 646, 10) array\n",
    "\n",
    "        Returns:\n",
    "         superresolved (arr): (?, 646, 646, 10) array\n",
    "    \"\"\"\n",
    "\n",
    "    to_resolve = np.pad(arr, ((0, 0), (4, 4), (4, 4), (0, 0)), 'reflect')\n",
    "\n",
    "    bilinear = to_resolve[..., 4:]\n",
    "    resolved = sess.run([logits], \n",
    "                 feed_dict={inp: to_resolve,\n",
    "                            inp_bilinear: bilinear})[0]\n",
    "    resolved = resolved[:, 4:-4, 4:-4, :]\n",
    "    arr[..., 4:] = resolved\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output_and_temp_folders(output_folder: str) -> None:\n",
    "    \"\"\"Makes necessary folder structures for IO of raw and processed data\n",
    "\n",
    "        Parameters:\n",
    "         idx (str)\n",
    "         output_folder (path)\n",
    "\n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    def _find_and_make_dirs(dirs):\n",
    "        if not os.path.exists(os.path.realpath(dirs)):\n",
    "            os.makedirs(os.path.realpath(dirs))\n",
    "            \n",
    "    folders = ['raw/', 'raw/clouds/', 'raw/misc/', 'raw/s1/',\n",
    "              'raw/s2_10/', 'raw/s2_20/']\n",
    "    \n",
    "    for folder in folders:\n",
    "        _find_and_make_dirs(output_folder + folder)\n",
    "        \n",
    "def make_bbox(initial_bbx: list, expansion: int = 10) -> list:\n",
    "    \"\"\"\n",
    "    Makes a (min_x, min_y, max_x, max_y) bounding box that\n",
    "    is 2 * expansion 300 x 300 meter ESA LULC pixels\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    multiplier = 1/360\n",
    "    bbx = copy.deepcopy(initial_bbx)\n",
    "    bbx[0] -= expansion * multiplier\n",
    "    bbx[1] -= expansion * multiplier\n",
    "    bbx[2] += expansion * multiplier\n",
    "    bbx[3] += expansion * multiplier\n",
    "    return bbx\n",
    "    \n",
    "\n",
    "def download_tile(x: int, y: int, data: pd.DataFrame) -> None:\n",
    "    data = data[data['Y_tile'] == int(y)]\n",
    "    data = data[data['X_tile'] == int(x)]\n",
    "    data = data.reset_index(drop = True)\n",
    "    x = str(int(x))\n",
    "    y = str(int(y))\n",
    "    if \".0\" in x:\n",
    "        x = x[:-2]\n",
    "    if \".0\" in y:\n",
    "        y = y[:-2]\n",
    "        \n",
    "    initial_bbx = [data['X'][0], data['Y'][0], data['X'][0], data['Y'][0]]\n",
    "\n",
    "    bbx = make_bbox(initial_bbx, expansion = 300/30)\n",
    "    dem_bbx = make_bbox(initial_bbx, expansion = 301/30)\n",
    "        \n",
    "    folder = f\"../project-monitoring/tof/{str(x)}/{str(y)}/\"\n",
    "    tile_idx = f'{str(x)}X{str(y)}Y'\n",
    "    \n",
    "    make_output_and_temp_folders(folder)        \n",
    "    clouds_file = f'{folder}raw/clouds/clouds_{tile_idx}.hkl'\n",
    "    shadows_file = f'{folder}raw/clouds/shadows_{tile_idx}.hkl'\n",
    "    s1_file = f'{folder}raw/s1/{tile_idx}.hkl'\n",
    "    s1_dates_file = f'{folder}raw/misc/s1_dates_{tile_idx}.hkl'\n",
    "    s2_10_file = f'{folder}raw/s2_10/{tile_idx}.hkl'\n",
    "    s2_20_file = f'{folder}raw/s2_20/{tile_idx}.hkl'\n",
    "    s2_dates_file = f'{folder}raw/misc/s2_dates_{tile_idx}.hkl'\n",
    "    s2_file = f'{folder}raw/s2/{tile_idx}.hkl'\n",
    "    clean_steps_file = f'{folder}raw/clouds/clean_steps_{tile_idx}.hkl'\n",
    "    dem_file = f'{folder}raw/misc/dem_{tile_idx}.hkl'\n",
    "    \n",
    "    print(f\"Downloading {x, y} tile to {folder}, with a\"\n",
    "          f\" bounding box of {bbx}\")\n",
    "    \n",
    "    \n",
    "    if not (os.path.exists(clouds_file)):# or processed):\n",
    "        print(f\"Downloading {clouds_file}\")\n",
    "\n",
    "        cloud_probs, shadows, _, image_dates, _ = identify_clouds(bbox = bbx,\n",
    "                                                               dates = dates,\n",
    "                                                              imsize = 600,\n",
    "                                                              api_key = API_KEY,\n",
    "                                                              year = 2020)\n",
    "\n",
    "        to_remove, _ = calculate_cloud_steps(cloud_probs, image_dates)\n",
    "\n",
    "        if len(to_remove) > 0:\n",
    "            clean_dates = np.delete(image_dates, to_remove)\n",
    "            cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "        else:\n",
    "            clean_dates = image_dates\n",
    "            \n",
    "        to_remove = subset_contiguous_sunny_dates(clean_dates)\n",
    "        if len(to_remove) > 0:\n",
    "            clean_dates = np.delete(clean_dates, to_remove)\n",
    "            cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "\n",
    "        hkl.dump(cloud_probs, clouds_file, mode='w', compression='gzip')\n",
    "        hkl.dump(shadows, shadows_file, mode='w', compression='gzip')\n",
    "        hkl.dump(clean_dates, clean_steps_file, mode='w', compression='gzip')\n",
    "            \n",
    "    \n",
    "    if not (os.path.exists(s2_10_file)):\n",
    "        print(f\"Downloading {s2_10_file}\")\n",
    "        clean_steps = list(hkl.load(clean_steps_file))\n",
    "        cloud_probs = hkl.load(clouds_file)\n",
    "        shadows = hkl.load(shadows_file)    \n",
    "        s2_10, s2_20, s2_dates = download_sentinel_2(bbx, clean_steps = clean_steps,\n",
    "                                                     api_key = API_KEY,\n",
    "                                                     dates = dates,\n",
    "                                                     year = 2020\n",
    "                                                    )\n",
    "\n",
    "        # Steps to ensure that L2A, L1C derived products have exact matching dates\n",
    "        print(f\"Shadows {shadows.shape}, clouds {cloud_probs.shape},\"\n",
    "              f\" S2, {s2_10.shape}, S2d, {s2_dates.shape}\")\n",
    "        to_remove_clouds = [i for i, val in enumerate(clean_steps) if val not in s2_dates]\n",
    "        to_remove_dates = [val for i, val in enumerate(clean_steps) if val not in s2_dates]\n",
    "        if len(to_remove_clouds) >= 1:\n",
    "            print(f\"Removing {to_remove_dates} from clouds because not in S2\")\n",
    "            cloud_probs = np.delete(cloud_probs, to_remove_clouds, 0)\n",
    "            shadows = np.delete(shadows, to_remove_clouds, 0)\n",
    "            hkl.dump(cloud_probs, clouds_file, mode='w', compression='gzip')\n",
    "            hkl.dump(shadows, shadows_file, mode='w', compression='gzip')\n",
    "\n",
    "        assert cloud_probs.shape[0] == s2_10.shape[0], \"There is a date mismatch\"\n",
    "        hkl.dump(to_int16(s2_10), s2_10_file, mode='w', compression='gzip')\n",
    "        hkl.dump(to_int16(s2_20), s2_20_file, mode='w', compression='gzip')\n",
    "        hkl.dump(s2_dates, s2_dates_file, mode='w', compression='gzip')\n",
    "        \n",
    "        s210_arr = to_int16(s2_10[0, ..., 0])\n",
    "        s220_arr = to_int16(s2_20[0, ..., 0])\n",
    "            \n",
    "    if not (os.path.exists(s1_file)):\n",
    "        print(f\"Downloading {s1_file}\")\n",
    "        s1_layer = identify_s1_layer((data['X'][0], data['Y'][0]))\n",
    "        s1, s1_dates = download_sentinel_1(bbx,\n",
    "                                           layer = s1_layer,\n",
    "                                           api_key = API_KEY,\n",
    "                                           year = 2020,\n",
    "                                           dates = dates_sentinel_1)\n",
    "        if s1.shape[0] == 0:\n",
    "            s1_layer = \"SENT_DESC\" if s1_layer == \"SENT\" else \"SENT\"\n",
    "            print(f'Switching to {s1_layer}')\n",
    "            s1, s1_dates = download_sentinel_1(bbx,\n",
    "                                               layer = s1_layer,\n",
    "                                               api_key = API_KEY,\n",
    "                                               year = 2020,\n",
    "                                               dates = dates_sentinel_1)\n",
    "        s1 = process_sentinel_1_tile(s1, s1_dates)\n",
    "        hkl.dump(to_int16(s1), s1_file, mode='w', compression='gzip')\n",
    "        hkl.dump(s1_dates, s1_dates_file, mode='w', compression='gzip')\n",
    "        \n",
    "        s1_arr = to_int16(s1[0, ..., 0])\n",
    "        \n",
    "    if not os.path.exists(dem_file):\n",
    "        print(f'Downloading {dem_file}')\n",
    "        dem = download_dem(dem_bbx, api_key = API_KEY)\n",
    "        hkl.dump(dem, dem_file, mode='w', compression='gzip')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_missing_px(sentinel2: np.ndarray, thresh: int = 11) -> np.ndarray:\n",
    "    \"\"\"Identifies missing (na) values in input array\n",
    "    \"\"\"\n",
    "    missing_images_0 = np.sum(sentinel2[..., :10] == 0.0, axis = (1, 2, 3))\n",
    "    missing_images_p = np.sum(sentinel2[..., :10] >= 1., axis = (1, 2, 3))\n",
    "    missing_images = missing_images_0 + missing_images_p\n",
    "    \n",
    "    missing_images = np.argwhere(missing_images >= (sentinel2.shape[1]**2) / thresh)\n",
    "    missing_images = missing_images.flatten()\n",
    "    if len(missing_images) > 0:\n",
    "        print(f\"The missing image bands (0) are: {missing_images_0}\")\n",
    "        print(f\"The missing image bands (1.0) are: {missing_images_p}\")\n",
    "    return missing_images\n",
    "\n",
    "def process_tile(x: int, y: int, data: pd.DataFrame):\n",
    "    \n",
    "    x = str(int(x))\n",
    "    y = str(int(y))\n",
    "    if \".0\" in x:\n",
    "        x = x[:-2]\n",
    "    if \".0\" in y:\n",
    "        y = y[:-2]\n",
    "            \n",
    "    folder = f\"../project-monitoring/tof/{str(x)}/{str(y)}/\"\n",
    "    tile_idx = f'{str(x)}X{str(y)}Y'\n",
    "    \n",
    "    clouds_file = f'{folder}raw/clouds/clouds_{tile_idx}.hkl'\n",
    "    shadows_file = f'{folder}raw/clouds/shadows_{tile_idx}.hkl'\n",
    "    s1_file = f'{folder}raw/s1/{tile_idx}.hkl'\n",
    "    s1_dates_file = f'{folder}raw/misc/s1_dates_{tile_idx}.hkl'\n",
    "    s2_10_file = f'{folder}raw/s2_10/{tile_idx}.hkl'\n",
    "    s2_20_file = f'{folder}raw/s2_20/{tile_idx}.hkl'\n",
    "    s2_dates_file = f'{folder}raw/misc/s2_dates_{tile_idx}.hkl'\n",
    "    s2_file = f'{folder}raw/s2/{tile_idx}.hkl'\n",
    "    clean_steps_file = f'{folder}raw/clouds/clean_steps_{tile_idx}.hkl'\n",
    "    dem_file = f'{folder}raw/misc/dem_{tile_idx}.hkl'\n",
    "    \n",
    "    \n",
    "    clouds = hkl.load(clouds_file)\n",
    "    shadows = hkl.load(shadows_file)\n",
    "    s1 = hkl.load(s1_file)\n",
    "    s2_10 = to_float32(hkl.load(s2_10_file))\n",
    "    s2_20 = to_float32(hkl.load(s2_20_file))\n",
    "    dem = hkl.load(dem_file)\n",
    "    image_dates = hkl.load(s2_dates_file)\n",
    "    \n",
    "    \n",
    "    width = s2_10.shape[1]\n",
    "    height = s2_20.shape[2] * 2\n",
    "    \n",
    "    if clouds.shape[1] < width:\n",
    "        pad_amt =  (width - clouds.shape[1]) // 2\n",
    "        clouds = np.pad(clouds, ((0, 0), (pad_amt, pad_amt), (0,0)), 'edge')\n",
    "        \n",
    "    if shadows.shape[1] < width:\n",
    "        pad_amt =  (width - shadows.shape[1]) // 2\n",
    "        shadows = np.pad(shadows, ((0, 0), (pad_amt, pad_amt), (0,0)), 'edge')\n",
    "        \n",
    "    if dem.shape[0] < width:\n",
    "        pad_amt =  (width - dem.shape[0]) // 2\n",
    "        dem = np.pad(dem, ((pad_amt, pad_amt), (0, 0)), 'edge')\n",
    "        \n",
    "    if s2_10.shape[2] < height:\n",
    "        pad_amt =  (height - s2_10.shape[2]) / 2\n",
    "        if pad_amt % 2 == 0:\n",
    "            pad_amt = int(pad_amt)\n",
    "            s2_10 = np.pad(s2_10, ((0, 0), (0, 0), (pad_amt, pad_amt), (0,0)), 'edge')\n",
    "        else:\n",
    "            s2_10 = np.pad(s2_10, ((0, 0), (0, 0), (0, int(pad_amt * 2)), (0,0)), 'edge')\n",
    "    \n",
    "    if s2_10.shape[2] > height:\n",
    "        pad_amt =  abs(height - s2_10.shape[2])\n",
    "        s2_10 = s2_10[:, :, :-pad_amt, :]\n",
    "        print(s2_10.shape)\n",
    "       \n",
    "    if dem.shape[1] < height:\n",
    "        pad_amt =  (height - dem.shape[1]) / 2\n",
    "        if pad_amt % 2 == 0:\n",
    "            pad_amt = int(pad_amt)\n",
    "            dem = np.pad(dem, ((0, 0), (pad_amt, pad_amt)), 'edge')\n",
    "        else:\n",
    "            dem = np.pad(dem, ( (0, 0), (0, int(pad_amt * 2))), 'edge')\n",
    "            \n",
    "    if dem.shape[1] > height:\n",
    "        pad_amt =  abs(height - dem.shape[1])\n",
    "        dem = dem[:, :-pad_amt]\n",
    "        \n",
    "        \n",
    "    print(f'Clouds: {clouds.shape}, \\nShadows: {shadows.shape} \\n'\n",
    "          f'S1: {s1.shape} \\nS2: {s2_10.shape}, {s2_20.shape} \\nDEM: {dem.shape}')\n",
    "            \n",
    "  \n",
    "    sentinel2 = np.empty((s2_10.shape[0], width, height, 10))\n",
    "    sentinel2[..., :4] = s2_10\n",
    "    for band in range(6):\n",
    "        for time in range(sentinel2.shape[0]):\n",
    "            sentinel2[time, ..., band + 4] = resize(s2_20[time,..., band], (width, height), 1)\n",
    "\n",
    "    missing_px = id_missing_px(sentinel2, 3)\n",
    "    if len(missing_px) > 0:\n",
    "        print(f\"Removing {missing_px} dates due to missing data\")\n",
    "        clouds = np.delete(clouds, missing_px, axis = 0)\n",
    "        shadows = np.delete(shadows, missing_px, axis = 0)\n",
    "        image_dates = np.delete(image_dates, missing_px)\n",
    "        sentinel2 = np.delete(sentinel2, missing_px, axis = 0)\n",
    "\n",
    "    x, interp = remove_cloud_and_shadows(sentinel2, clouds, shadows, image_dates) \n",
    "\n",
    "    dem_i = np.tile(dem[np.newaxis, :, :, np.newaxis], (x.shape[0], 1, 1, 1))\n",
    "    dem_i = dem_i / 90\n",
    "    x = np.concatenate([x, dem_i], axis = -1)\n",
    "    x = np.clip(x, 0, 1)\n",
    "    return x, image_dates, interp, s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subtiles(folder, tiles):\n",
    "    \n",
    "    y_tiles = np.unique(tiles[:, 1])\n",
    "    x_tiles = np.unique(tiles[:, 0])\n",
    "    \n",
    "    def _find_and_make_dirs(dirs):\n",
    "        if not os.path.exists(os.path.realpath(dirs)):\n",
    "            os.makedirs(os.path.realpath(dirs))\n",
    "    \n",
    "    for y_tile in y_tiles:\n",
    "        _find_and_make_dirs(folder + str(y_tile) + '/')\n",
    "            \n",
    "def interpolate_na_vals(s2):\n",
    "    '''Interpolates NA values with closest time steps, to deal with\n",
    "       the small potential for NA values in calculating indices'''\n",
    "    for x_loc in range(s2.shape[1]):\n",
    "        for y_loc in range(s2.shape[2]):\n",
    "            n_na = np.sum(np.isnan(s2[:, x_loc, y_loc, :]), axis = 1)\n",
    "            for date in range(s2.shape[0]):\n",
    "                if n_na.flatten()[date] > 0:\n",
    "                    before, after = calculate_proximal_steps(date, np.argwhere(n_na == 0))\n",
    "                    s2[date, x_loc, y_loc, :] = ((s2[date + before, x_loc, y_loc] + \n",
    "                                                 s2[date + after, x_loc, y_loc]) / 2)\n",
    "    numb_na = np.sum(np.isnan(s2), axis = (1, 2, 3))\n",
    "    if np.sum(numb_na) > 0:\n",
    "        print(f\"There are {numb_na} NA values\")\n",
    "    return s2\n",
    "    \n",
    "\n",
    "def process_subtiles(x, y, s2: np.ndarray = None, \n",
    "                       dates: np.ndarray = None,\n",
    "                       interp: np.ndarray = None, s1 = None) -> None:\n",
    "    '''Wrapper function to interpolate clouds and temporal gaps, superresolve tiles,\n",
    "       calculate relevant indices, and save analysis-ready data to the output folder\n",
    "       \n",
    "       Parameters:\n",
    "        coord (tuple)\n",
    "        step_x (int):\n",
    "        step_y (int):\n",
    "        folder (str):\n",
    "\n",
    "       Returns:\n",
    "        None\n",
    "    '''\n",
    "    x = str(int(x))\n",
    "    y = str(int(y))\n",
    "    if \".0\" in x:\n",
    "        x = x[:-2]\n",
    "    if \".0\" in y:\n",
    "        y = y[:-2]\n",
    "    \n",
    "    s2 = interpolate_na_vals(s2)\n",
    "    \n",
    "    tiles_folder = tile_window(s1.shape[2], s1.shape[1], window_size = 140)\n",
    "    tiles_array = make_overlapping_windows(tiles_folder)\n",
    "    \n",
    "    \n",
    "    make_subtiles(f'../project-monitoring/tof/{str(x)}/{str(y)}/processed/',\n",
    "                  tiles_folder)\n",
    "    path = f'../project-monitoring/tof/{str(x)}/{str(y)}/processed/'\n",
    "    for t in range(len(tiles_folder)):\n",
    "        tile_folder = tiles_folder[t]\n",
    "        tile_array = tiles_array[t]\n",
    "        \n",
    "        start_x, start_y = tile_array[0], tile_array[1]\n",
    "        folder_x, folder_y = tile_folder[0], tile_folder[1]\n",
    "        end_x = start_x + tile_array[2]\n",
    "        end_y = start_y + tile_array[3]\n",
    "        subset = s2[:, start_x:end_x, start_y:end_y, :]\n",
    "        interp_tile = interp[:, start_x:end_x, start_y:end_y]\n",
    "        interp_tile = np.sum(interp_tile, axis = (1, 2))\n",
    "        \n",
    "        dates_tile = np.copy(dates)\n",
    "        to_remove = np.argwhere(interp_tile > ((150*150) / 6.67)).flatten()\n",
    "        if len(to_remove) > 0:\n",
    "            dates_tile = np.delete(dates_tile, to_remove)\n",
    "            subset = np.delete(subset, to_remove, 0)\n",
    "            print(f\"Removing {to_remove} interp, leaving {len(dates_tile)} / {len(dates)}\")\n",
    "\n",
    "        missing_px = id_missing_px(subset)\n",
    "        if len(missing_px) > 0:\n",
    "            dates_tile = np.delete(dates_tile, missing_px)\n",
    "            subset = np.delete(subset, missing_px, 0)\n",
    "\n",
    "        to_remove = remove_missed_clouds(subset)\n",
    "        if len(to_remove) > 0:\n",
    "            subset = np.delete(subset, to_remove, axis = 0)\n",
    "            dates_tile = np.delete(dates_tile, to_remove)\n",
    "        try:\n",
    "            subtile, _ = calculate_and_save_best_images(subset, dates_tile)\n",
    "        except:\n",
    "            subtile = np.zeros((72, end_x-start_x, end_y - start_y, 11))\n",
    "            dates_tile = [0,]\n",
    "        output = f\"{path}{str(folder_y)}/{str(folder_x)}.hkl\"\n",
    "        s1_subtile = s1[:, start_x:end_x, start_y:end_y, :]\n",
    "        \n",
    "        if subtile.shape[2] == 145: \n",
    "            pad_u, pad_d = 0, 0\n",
    "            if start_y == 0:\n",
    "                pad_u = 5\n",
    "            else:\n",
    "                pad_d = 5\n",
    "            subtile = np.pad(subtile, ((0, 0,), (0, 0), (pad_u, pad_d), (0, 0)), 'reflect')\n",
    "            s1_subtile = np.pad(s1_subtile, ((0, 0,), (0, 0), (pad_u, pad_d), (0, 0)), 'reflect')\n",
    "        if subtile.shape[1] == 145:\n",
    "            pad_l, pad_r = 0, 0\n",
    "            if start_x == 0:\n",
    "                pad_l = 5\n",
    "            else:\n",
    "                pad_r = 5\n",
    "            subtile = np.pad(subtile, ((0, 0,), (pad_l, pad_r), (0, 0), (0, 0)), 'reflect')\n",
    "            s1_subtile = np.pad(s1_subtile, ((0, 0,), (pad_l, pad_r), (0, 0), (0, 0)), 'reflect')\n",
    "        \n",
    "        dem = subtile[..., -1]\n",
    "        sm = Smoother(lmbd = 800, size = subtile.shape[0], nbands = 10, dim = subtile.shape[1])\n",
    "        subtile = sm.interpolate_array(subtile[..., :-1])\n",
    "        subtile = superresolve_tile(subtile)\n",
    "        \n",
    "        subtile = np.concatenate([subtile, dem[:12, :, :, np.newaxis]], axis = -1)\n",
    "        subtile = np.concatenate([subtile,  s1_subtile], axis = -1)\n",
    "        subtile[..., -2:] = subtile[..., -2:] / 65535\n",
    "        \n",
    "        output_folder = \"/\".join(output.split(\"/\")[:-1])\n",
    "        if not os.path.exists(os.path.realpath(output_folder)):\n",
    "            os.makedirs(os.path.realpath(output_folder))\n",
    "            \n",
    "        \n",
    "        subtile = np.clip(subtile, 0, 1)\n",
    "        subtile = to_int16(subtile)\n",
    "        print(f\"Writing {output}\")\n",
    "        assert subtile.shape[1] >= 145, f\"subtile shape is {subtile.shape}\"\n",
    "        assert subtile.shape[0] == 12, f\"subtile shape is {subtile.shape}\"\n",
    "        if len(dates_tile) < 5:\n",
    "            subtile = np.zeros_like(subtile)\n",
    "        hkl.dump(subtile, output, mode='w', compression='gzip')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ('2052', '1177') tile to ../project-monitoring/tof/2052/1177/, with a bounding box of [20.388888892222223, 11.944444442222222, 20.44444444777778, 11.99999999777778]\n",
      "The original max value is 51360\n",
      "The original max value is 52310\n",
      "Clouds: (20, 618, 608), \n",
      "Shadows: (20, 618, 608) \n",
      "S1: (12, 618, 604, 2) \n",
      "S2: (20, 618, 604, 4), (20, 309, 302, 6) \n",
      "DEM: (618, 604)\n",
      "Interpolated 922395.0 px 0.11546564552956257%\n",
      "Removing [ 1 10 12 13 14 19] interp, leaving 14 / 20\n",
      "Missed shadow 12: 0.16642092746730083\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/0/0.hkl\n",
      "Removing [ 1 10 12 13 14 19] interp, leaving 14 / 20\n",
      "Missed shadow 12: 0.1462528735632184\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/116/0.hkl\n",
      "Removing [ 1 12 13 14 19] interp, leaving 15 / 20\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/232/0.hkl\n",
      "Removing [12 13 14 19] interp, leaving 16 / 20\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/348/0.hkl\n",
      "Removing [12 13 14 19] interp, leaving 16 / 20\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/464/0.hkl\n",
      "Removing [ 0  1 10 13 14 19] interp, leaving 14 / 20\n",
      "Missed shadow 12: 0.42970114942528737\n",
      "Missed shadow 13: 0.10266666666666667\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/0/119.hkl\n",
      "Removing [ 1 10 12 13 14 19] interp, leaving 14 / 20\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/116/119.hkl\n",
      "Removing [ 1 10 12 13 14 19] interp, leaving 14 / 20\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/232/119.hkl\n",
      "Removing [12 13 14] interp, leaving 17 / 20\n",
      "Missed shadow 16: 0.18208888888888888\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/348/119.hkl\n",
      "Removing [13 14] interp, leaving 18 / 20\n",
      "Missed shadow 17: 0.8154942528735633\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/464/119.hkl\n",
      "Removing [ 1 10 12 13 14 19] interp, leaving 14 / 20\n",
      "Missed shadow 11: 0.18524137931034482\n",
      "Missed shadow 12: 0.5075402298850574\n",
      "Missed shadow 13: 0.24110344827586208\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/0/238.hkl\n",
      "Removing [10 12 13 14] interp, leaving 16 / 20\n",
      "Missed shadow 12: 0.19346666666666668\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/116/238.hkl\n",
      "Removing [10 13 14] interp, leaving 17 / 20\n",
      "Missed shadow 13: 0.17657777777777778\n",
      "Missed shadow 16: 0.1368888888888889\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/232/238.hkl\n",
      "Removing [13 14] interp, leaving 18 / 20\n",
      "Missed shadow 17: 0.8238222222222222\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/348/238.hkl\n",
      "Removing [13 14] interp, leaving 18 / 20\n",
      "Missed shadow 17: 0.9774712643678161\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/464/238.hkl\n",
      "Removing [10 13 14] interp, leaving 17 / 20\n",
      "Missed shadow 14: 0.45489655172413795\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/0/358.hkl\n",
      "Removing [10 13 14] interp, leaving 17 / 20\n",
      "Missed shadow 13: 0.18382222222222222\n",
      "Missed shadow 14: 0.2572888888888889\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/116/358.hkl\n",
      "Removing [10 13 14 16] interp, leaving 16 / 20\n",
      "Missed shadow 13: 0.23564444444444443\n",
      "Missed shadow 15: 0.2596\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/232/358.hkl\n",
      "Removing [13 14] interp, leaving 18 / 20\n",
      "Missed shadow 17: 0.8389777777777778\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/348/358.hkl\n",
      "Removing [ 1 13 14] interp, leaving 17 / 20\n",
      "Missed shadow 16: 0.8677701149425288\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/464/358.hkl\n",
      "Removing [ 1 10 13 14] interp, leaving 16 / 20\n",
      "Missed shadow 13: 0.5405945303210463\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/0/478.hkl\n",
      "Removing [10 13 14] interp, leaving 17 / 20\n",
      "Missed shadow 14: 0.531448275862069\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/116/478.hkl\n",
      "Removing [ 0 10 13 14] interp, leaving 16 / 20\n",
      "Missed shadow 11: 0.11282758620689655\n",
      "Missed shadow 12: 0.12997701149425286\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/232/478.hkl\n",
      "Removing [ 0 13 14] interp, leaving 17 / 20\n",
      "Missed shadow 12: 0.16813793103448277\n",
      "Missed shadow 16: 0.23710344827586208\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/348/478.hkl\n",
      "Removing [ 1 13 14 15] interp, leaving 16 / 20\n",
      "Missed shadow 12: 0.1941973840665874\n",
      "Missed shadow 15: 0.29032104637336503\n",
      "Writing ../project-monitoring/tof/2052/1177/processed/464/478.hkl\n",
      "Finished in 155.4 seconds\n"
     ]
    }
   ],
   "source": [
    "%run ../src/tof/tof_downloading.py\n",
    "time1 = time()\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "year = 2020\n",
    "dates = (f'{str(year - 1)}-11-15' , f'{str(year + 1)}-02-15')\n",
    "dates_sentinel_1 = (f'{str(year)}-01-01' , f'{str(year)}-12-31')\n",
    "days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "starting_days = np.cumsum(days_per_month)\n",
    "\n",
    "\n",
    "x = 2052#  729, 2319\n",
    "y = 1177\n",
    "\n",
    "\n",
    "download_tile(x = x, y = y, data = data)\n",
    "s2, dates, interp, s1 = process_tile(x = x, y = y, data = data)\n",
    "process_subtiles(x, y, s2, dates, interp, s1)\n",
    "time2 = time()\n",
    "print(f\"Finished in {np.around(time2 - time1, 1)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary uploading of .tif files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "uploader = FileUploader(awskey = AWSKEY, awssecret = AWSSECRET)\n",
    "\n",
    "path_to_folders = \"../project-monitoring/tof/\"\n",
    "\n",
    "uploaded = 0\n",
    "folders = glob(path_to_folders + \"*/*/\")\n",
    "for folder in folders:\n",
    "    files = [x for x in os.listdir(folder) if x[-4:] == '.tif']\n",
    "    for file in files:\n",
    "        internal_folder = folder[len(path_to_folders):]\n",
    "        key = f'2020/tiles/{internal_folder}{file}'\n",
    "        _file = folder + file\n",
    "        print(uploaded)\n",
    "        uploader.upload(bucket = 'tof-output', key = key, file = _file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra GDAL code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from osgeo import gdal\n",
    "from glob import glob\n",
    "li_dirs = glob(\"../project-monitoring/tof/*/*\")\n",
    "\n",
    "\n",
    "print(li_dirs)\n",
    "li_all_files = list()\n",
    "for folder in li_dirs:\n",
    "    files = [file for file in os.listdir(folder) if os.path.splitext(file)[-1] == '.tif']\n",
    "    for file in files:\n",
    "        li_all_files.append(os.path.join(folder, file))\n",
    "\n",
    "gdal.BuildVRT('out.vrt', li_all_files)\n",
    "\n",
    "#!gdal_translate -of GTiff out.vrt out.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdalwarp -s_srs \"EPSG:4326\" -t_srs \"EPSG:32663\" -of vrt in2.tif out.vrt\n",
    "!gdal_translate -co compress=LZW out.vrt out.tif"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
