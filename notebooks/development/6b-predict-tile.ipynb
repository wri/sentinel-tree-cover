{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This Jupyter notebook predicts large-area tiles downloaded in `4a-download-large-area` with a trained model from `3-model-master`. The notebook is broken down into the following sections:\n",
    "\n",
    "   * **Model loading**:\n",
    "   * **Coordinate identification**\n",
    "   * **Tiling**\n",
    "   * **Loading and predicting**\n",
    "   * **Mosaicing**\n",
    "   * **Writing TIF**\n",
    "   * **Writing COG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "from osgeo import ogr, osr\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize\n",
    "import hickle as hkl\n",
    "from time import sleep\n",
    "import copy\n",
    "\n",
    "%run ../src/downloading/utils.py\n",
    "%run ../src/models/utils.py\n",
    "%run ../src/models/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "x = hkl.load(\"../project-monitoring/tiles/1857/1041/raw/s2_10/1857X1041Y.hkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,13))\n",
    "sns.heatmap(x[0, ..., 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Parameter definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../models/master-28/'\n",
    "new_saver = tf.train.import_meta_graph(path + 'model.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    try:\n",
    "        logits = tf.get_default_graph().get_tensor_by_name(\"conv2d_{}/Sigmoid:0\".format(i))\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "inp = tf.get_default_graph().get_tensor_by_name(\"Placeholder:0\")\n",
    "#length = tf.get_default_graph().get_tensor_by_name(\"Placeholder_1:0\")\n",
    "length =tf.get_default_graph().get_tensor_by_name(\"PlaceholderWithDefault:0\")\n",
    "#rmax = tf.get_default_graph().get_tensor_by_name(\"Placeholder_4:0\")\n",
    "#rmin = tf.get_default_graph().get_tensor_by_name(\"Placeholder_5:0\")\n",
    "#dmax = tf.get_default_graph().get_tensor_by_name(\"Placeholder_6:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tiling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fspecial_gauss(size, sigma):\n",
    "\n",
    "    \"\"\"Function to mimic the 'fspecial' gaussian MATLAB function\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
    "    g = np.exp(-((x**2 + y**2)/(2.0*sigma**2)))\n",
    "    return g\n",
    "\n",
    "arr = fspecial_gauss(14, 4.5)\n",
    "arr = arr[:7, :7]\n",
    "\n",
    "SIZE = 10\n",
    "SIZE_N = SIZE*SIZE\n",
    "SIZE_UR = (SIZE - 1) * (SIZE - 1)\n",
    "SIZE_R = (SIZE - 1) * SIZE\n",
    "SIZE_U = SIZE_R\n",
    "TOTAL = SIZE_N + SIZE_UR + SIZE_R + SIZE_U\n",
    "\n",
    "arr = np.concatenate([arr, np.flip(arr, 0)], 0)\n",
    "base_filter = np.concatenate([arr, np.flip(arr, 1)], 1)\n",
    "normal = np.tile(base_filter, (SIZE, SIZE))\n",
    "normal[:, 0:7] = 1.\n",
    "normal[:, -7:] = 1.\n",
    "normal[0:7, :] = 1.\n",
    "normal[-7:, :] = 1.\n",
    "upright = np.tile(base_filter, (SIZE - 1, SIZE - 1))\n",
    "upright = np.pad(upright, (7, 7), 'constant', constant_values = 0)\n",
    "\n",
    "\n",
    "sums = (upright + normal)\n",
    "\n",
    "upright /= sums\n",
    "normal /= sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_all = [0.006607156481269551, 0.0162050812542916, 0.010055695429922943, 0.013351644159609368, 0.01965362020294499, 0.014251926451514458, 0.015289539940489814, 0.011993591210803388, 0.008239871824216068, 0.006576638437476158, 0.0, 0.0, 0.0, -0.14089041542883884, -0.4973007582496804, -0.09727903335259765, -0.7193163251773491]\n",
    "max_all = [0.26909261463349116, 0.3739681086442359, 0.5171129930571451, 0.6027161058976119, 0.5649805447470817, 0.5746852826733806, 0.5933623254749371, 0.6034790569924467, 0.7471885252155337, 0.6999771114671549, 0.5081406881818875, 0.9483111607060141, 0.6728625967127161, 0.8176895380653232, 0.3576770175739082, 0.7545675799120575, 0.7602480781910504]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%run ../src/models/utils.py\n",
    "\n",
    "\n",
    "\n",
    "x = 1642#  729, 2319\n",
    "y = 1094\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"processing_area.csv\")\n",
    "\n",
    "data = data[data['Y_tile'] == int(y)]\n",
    "data = data[data['X_tile'] == int(x)]\n",
    "\n",
    "y = str(y)\n",
    "x = str(x)\n",
    "\n",
    "\n",
    "def make_bbox(initial_bbx, expansion = 10):\n",
    "    \n",
    "    multiplier = 1/360\n",
    "    bbx = copy.deepcopy(initial_bbx)\n",
    "    bbx[0] -= expansion * multiplier\n",
    "    bbx[1] -= expansion * multiplier\n",
    "    bbx[2] += expansion * multiplier\n",
    "    bbx[3] += expansion * multiplier\n",
    "    return bbx\n",
    "\n",
    "\n",
    "\n",
    "data = data.reset_index(drop = True)\n",
    "if \".0\" in x:\n",
    "    x = x[:-2]\n",
    "if \".0\" in y:\n",
    "    y = y[:-2]\n",
    "\n",
    "initial_bbx = [data['X'][0], data['Y'][0], data['X'][0], data['Y'][0]]\n",
    "point = make_bbox(initial_bbx, expansion = 300/30)\n",
    "\n",
    "\n",
    "inp_folder = f'../project-monitoring/tof/{str(x)}/{str(y)}/processed/'\n",
    "out_folder = f'../project-monitoring/tof/{str(x)}/{str(y)}/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_db(x, min_db):\n",
    "    x = 10 * np.log10(x + 1/65535)\n",
    "    x[x < -min_db] = -min_db\n",
    "    x = x + min_db\n",
    "    x = x / min_db\n",
    "    x = np.clip(x, 0, 1)\n",
    "    return x\n",
    "\n",
    "def grndvi(array):\n",
    "    nir = np.clip(array[..., 3], 0, 1)\n",
    "    green = np.clip(array[..., 1], 0, 1)\n",
    "    red = np.clip(array[..., 2], 0, 1)\n",
    "    denominator = (nir+(green+red)) + 1e-5\n",
    "    return (nir-(green+red)) / denominator\n",
    "\n",
    "def evi(x: np.ndarray, verbose: bool = False) -> np.ndarray:\n",
    "    '''\n",
    "    Calculates the enhanced vegetation index\n",
    "    2.5 x (08 - 04) / (08 + 6 * 04 - 7.5 * 02 + 1)\n",
    "    '''\n",
    "\n",
    "    BLUE = x[..., 0]\n",
    "    GREEN = x[..., 1]\n",
    "    RED = x[..., 2]\n",
    "    NIR = x[..., 3]\n",
    "    evis = 2.5 * ( (NIR-RED) / (NIR + (6*RED) - (7.5*BLUE) + 1))\n",
    "    evis = np.clip(evis, -1.5, 1.5)\n",
    "    return evis\n",
    "\n",
    "def msavi2(x: np.ndarray, verbose: bool = False) -> np.ndarray:\n",
    "    '''\n",
    "    Calculates the modified soil-adjusted vegetation index 2\n",
    "    (2 * NIR + 1 - sqrt((2*NIR + 1)^2 - 8*(NIR-RED)) / 2\n",
    "    '''\n",
    "    BLUE = x[..., 0]\n",
    "    GREEN = x[..., 1]\n",
    "    RED = np.clip(x[..., 2], 0, 1)\n",
    "    NIR = np.clip(x[..., 3], 0, 1)\n",
    "\n",
    "    msavis = (2 * NIR + 1 - np.sqrt( (2*NIR+1)**2 - 8*(NIR-RED) )) / 2\n",
    "    return msavis\n",
    "\n",
    "def bi(x: np.ndarray, verbose: bool = False) -> np.ndarray:\n",
    "    B11 = np.clip(x[..., 8], 0, 1)\n",
    "    B4 = np.clip(x[..., 2], 0, 1)\n",
    "    B8 = np.clip(x[..., 3], 0, 1)\n",
    "    B2 = np.clip(x[..., 0], 0, 1)\n",
    "    bis = ((B11 + B4) - (B8 + B2)) / ((B11 + B4) + (B8 + B2))\n",
    "    return bis\n",
    " \n",
    "\n",
    "def load_and_predict_folder(pred_files, overlap_filter = upright,\n",
    "                            normal_filter = normal, histogram_match = False):\n",
    "    \"\"\"Insert documentation here\n",
    "    \"\"\"\n",
    "      \n",
    "    #clipping_params = {\n",
    "    #    'rmax': rmax,\n",
    "    #    'rmin': rmin,\n",
    "    #    'dmax': dmax\n",
    "    #}\n",
    "    \n",
    "    pred_x = []\n",
    "    x = hkl.load(pred_files)\n",
    "    \n",
    "        \n",
    "    if np.sum(x) > 0:\n",
    "        if not isinstance(x.flat[0], np.floating):\n",
    "            assert np.max(x) > 1\n",
    "            x = x / 65535.\n",
    "            \n",
    "        s1 = x[..., -2:]\n",
    "       #s1 = np.reshape(s1, (12, 75, 2, 75, 2, 2))\n",
    "       # s1 = np.mean(s1, axis = (2, 4))\n",
    "        #s1 = resize(s1, (12, 150, 150, 2), order = 1)\n",
    "\n",
    "        x[..., -1] = convert_to_db(x[..., -1], 22)\n",
    "        x[..., -2] = convert_to_db(x[..., -2], 22)\n",
    "        \n",
    "        print(len(np.unique(x[..., -1])))\n",
    "\n",
    "        indices = np.empty((12, x.shape[1], x.shape[2], 4))\n",
    "        indices[..., 0] = evi(x)\n",
    "        indices[..., 1] = bi(x)\n",
    "        indices[..., 2] = msavi2(x)\n",
    "        indices[..., 3] = grndvi(x)\n",
    "\n",
    "        x = np.concatenate([x, indices], axis = -1)\n",
    "\n",
    "        med = np.median(x, axis = 0)\n",
    "        med = med[np.newaxis, :, :, :]\n",
    "        x = np.concatenate([x, med], axis = 0)\n",
    "\n",
    "        filtered = median_filter(x[0, :, :, 10], size = 5)\n",
    "        x[:, :, :, 10] = np.stack([filtered] * x.shape[0])\n",
    "        x = tile_images(x)\n",
    "        \n",
    "        pred_x = np.stack(x)   \n",
    "        for band in range(0, pred_x.shape[-1]):\n",
    "            mins = min_all[band]\n",
    "            maxs = max_all[band]\n",
    "            pred_x[..., band] = np.clip(pred_x[..., band], mins, maxs)\n",
    "            midrange = (maxs + mins) / 2\n",
    "            rng = maxs - mins\n",
    "            standardized = (pred_x[..., band] - midrange) / (rng / 2)\n",
    "            pred_x[..., band] = standardized\n",
    "\n",
    "        preds = []\n",
    "        batches = [x for x in range(0, 180, 40)] + [181]\n",
    "        for i in range(len(batches)-1):\n",
    "            batch_x = pred_x[batches[i]:batches[i+1]]\n",
    "            lengths = np.full((batch_x.shape[0]), 12)\n",
    "            batch_pred = sess.run(logits,\n",
    "                                  feed_dict={inp:batch_x, \n",
    "                                             #clipping_params['rmax']: 5,\n",
    "                                             #clipping_params['rmin']: 0,\n",
    "                                             #clipping_params['dmax']: 3,\n",
    "                                             length:lengths}).reshape(batch_x.shape[0], 14, 14)\n",
    "            for sample in range(batch_pred.shape[0]):\n",
    "                preds.append(batch_pred[sample, :, :])\n",
    "\n",
    "\n",
    "        preds_stacked = []\n",
    "        for i in range(0, SIZE_N, SIZE):\n",
    "            preds_stacked.append(np.concatenate(preds[i:i + SIZE], axis = 1))\n",
    "        stacked = np.concatenate(preds_stacked, axis = 0) #* normal\n",
    "\n",
    "        preds_overlap = []\n",
    "        for scene in range(SIZE_N, SIZE_N+SIZE_UR, SIZE - 1):\n",
    "            to_concat = np.concatenate(preds[scene:scene+ (SIZE - 1)], axis = 1)\n",
    "            preds_overlap.append(to_concat)    \n",
    "        overlapped = np.concatenate(preds_overlap, axis = 0)\n",
    "        overlapped = np.pad(overlapped, (7, 7), 'constant', constant_values = 0)\n",
    "        overlapped = overlapped * upright\n",
    "\n",
    "\n",
    "        stacked = stacked #+ overlapped# + right + up\n",
    "    else:\n",
    "        stacked = np.full((140, 140), 255)\n",
    "    \n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_images(arr: np.ndarray) -> list:\n",
    "    \"\"\" Converts a 142x142 array to a 289, 24, 24 array\n",
    "        \n",
    "        Parameters:\n",
    "         arr (np.ndaray): (142, 142) float array\n",
    "    \n",
    "        Returns:\n",
    "         images (list): \n",
    "    \"\"\"\n",
    "\n",
    "    # Normal\n",
    "    images = []\n",
    "    for x_offset, cval in enumerate([x for x in range(0, 140, 14)]):\n",
    "        for y_offset, rval in enumerate([x for x in range(0, 140, 14)]):\n",
    "            min_x = np.max([cval - 0, 0])\n",
    "            max_x = np.min([cval + 24, 150])\n",
    "            min_y = np.max([rval - 0, 0])\n",
    "            max_y = np.min([rval + 24, 150])\n",
    "            subs = arr[:, min_x:max_x, min_y:max_y]\n",
    "            images.append(subs)\n",
    "            \n",
    "    # Upright  \n",
    "    for x_offset, cval in enumerate([x for x in range(7,  140-7, 14)]):\n",
    "        for y_offset, rval in enumerate([x for x in range(7, 140-7, 14)]):\n",
    "            base_id = 9*9\n",
    "            min_x = np.max([cval - 0, 0])\n",
    "            max_x = np.min([cval + 24, 150])\n",
    "            min_y = np.max([rval - 0, 0])\n",
    "            max_y = np.min([rval + 24, 150])\n",
    "            subs = arr[:, min_x:max_x, min_y:max_y]\n",
    "            images.append(subs)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"../project-monitoring/tof/344/1271/processed/77/0.hkl\"\n",
    "\n",
    "prediction = load_and_predict_folder(input_file, histogram_match = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no Partial Conv\n",
    "plt.figure(figsize=(15,13))\n",
    "sns.heatmap(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial conv\n",
    "plt.figure(figsize=(15,13))\n",
    "sns.heatmap(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"../project-monitoring/tof/344/1271/processed/77/0.hkl\"\n",
    "x = hkl.load(input_file)\n",
    "\n",
    "if not isinstance(x.flat[0], np.floating):\n",
    "    assert np.max(x) > 1\n",
    "    x = x / 65535.\n",
    "print(np.max(x))\n",
    "\n",
    "\n",
    "def convert_to_db(x, min_db):\n",
    "    x = 10 * np.log10(x + 1/65535)\n",
    "    x[x < -min_db] = -min_db\n",
    "    x = x + min_db\n",
    "    x = x / min_db\n",
    "    x = np.clip(x, 0, 1)\n",
    "    return x\n",
    "\n",
    "x[..., -1] = convert_to_db(x[..., -1], 22)\n",
    "x[..., -2] = convert_to_db(x[..., -2], 22)\n",
    "print(len(np.unique(x[..., -1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(x.flat[0], np.floating):\n",
    "    assert np.max(x) > 1\n",
    "    x = x / 65535.\n",
    "\n",
    "s1 = x[..., -2:]\n",
    "s1 = np.reshape(s1, (12, 75, 2, 75, 2, 2))\n",
    "s1 = np.mean(s1, axis = (2, 4))\n",
    "s1 = resize(s1, (12, 150, 150, 2), order = 1)\n",
    "x[..., -1] = convert_to_db(x[..., -1], 22)\n",
    "x[..., -2] = convert_to_db(x[..., -2], 22)\n",
    "\n",
    "indices = np.empty((12, x.shape[1], x.shape[2], 4))\n",
    "indices[..., 0] = evi(x)\n",
    "indices[..., 1] = bi(x)\n",
    "indices[..., 2] = msavi2(x)\n",
    "indices[..., 3] = grndvi(x)\n",
    "\n",
    "x = np.concatenate([x, indices], axis = -1)\n",
    "\n",
    "med = np.median(x, axis = 0)\n",
    "med = med[np.newaxis, :, :, :]\n",
    "x = np.concatenate([x, med], axis = 0)\n",
    "\n",
    "filtered = median_filter(x[0, :, :, 10], size = 5)\n",
    "x[:, :, :, 10] = np.stack([filtered] * x.shape[0])\n",
    "pred_x = x\n",
    "for band in range(0, pred_x.shape[-1]):\n",
    "    mins = min_all[band]\n",
    "    maxs = max_all[band]\n",
    "    pred_x[..., band] = np.clip(pred_x[..., band], mins, maxs)\n",
    "    midrange = (maxs + mins) / 2\n",
    "    rng = maxs - mins\n",
    "    standardized = (pred_x[..., band] - midrange) / (rng / 2)\n",
    "    pred_x[..., band] = standardized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "to_run = pred_x[np.newaxis, :, 70:, :80, :]\n",
    "lengths = np.full((1), 12)\n",
    "batch_pred = sess.run(logits,\n",
    "                      feed_dict={inp:to_run, \n",
    "                                 #clipping_params['rmax']: 5,\n",
    "                                 #clipping_params['rmin']: 0,\n",
    "                                 #clipping_params['dmax']: 3,\n",
    "                                 length:lengths}).reshape(1, 70, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,13))\n",
    "sns.heatmap(batch_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,17))\n",
    "sns.heatmap(pred_x[0, ..., -5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"../data/train-s1-radiometric/138948588.npy\")\n",
    "x = np.reshape(x, (12, 12, 2, 12, 2, 2))\n",
    "x = np.mean(x, axis = (2, 4))\n",
    "x = resize(x, (12, 24, 24, 2), order = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(x[0, ..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "time1 = time()\n",
    "\n",
    "x_tiles = [int(x) for x in os.listdir(inp_folder) if '.DS' not in x]\n",
    "max_x = np.max(x_tiles) + 140\n",
    "\n",
    "for x_tile in x_tiles:\n",
    "    y_tiles = [int(y[:-4]) for y in os.listdir(inp_folder + str(x_tile) + \"/\") if '.DS' not in y]\n",
    "    max_y = np.max(y_tiles) + 140\n",
    "    for y_tile in y_tiles:\n",
    "        output_file = f\"{out_folder}{str(x_tile)}/{str(y_tile)}.npy\"\n",
    "        input_file = f\"{inp_folder}{str(x_tile)}/{str(y_tile)}.hkl\"\n",
    "        if os.path.exists(input_file) and not os.path.exists(output_file):\n",
    "            print(output_file)\n",
    "            prediction = load_and_predict_folder(input_file, histogram_match = False)\n",
    "            if not os.path.exists(f\"{out_folder}{str(x_tile)}/\"):\n",
    "                os.makedirs(f\"{out_folder}{str(x_tile)}/\")\n",
    "            prediction = prediction\n",
    "            np.save(output_file, prediction)\n",
    "            \n",
    "time2 = time()\n",
    "print(f\"Finished in {np.around(time2 - time1, 1)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2.5 Mosaic predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (max_x or max_y):\n",
    "    x_tiles = [int(x) for x in os.listdir(out_folder) if '.DS' not in x]\n",
    "    max_x = np.max(x_tiles) + 140\n",
    "\n",
    "    for x_tile in x_tiles:\n",
    "        y_tiles = [int(y[:-4]) for y in os.listdir(out_folder + str(x_tile) + \"/\") if '.DS' not in y]\n",
    "        max_y = np.max(y_tiles) + 140\n",
    "\n",
    "predictions = np.full(\n",
    "    (max_x,\n",
    "     max_y), 0, dtype = np.uint8 )\n",
    "\n",
    "x_tiles = [int(x) for x in os.listdir(out_folder) if '.DS' not in x]\n",
    "\n",
    "for x_tile in x_tiles:\n",
    "    y_tiles = [int(y[:-4]) for y in os.listdir(out_folder + str(x_tile) + \"/\") if '.DS' not in y]\n",
    "    for y_tile in y_tiles:\n",
    "        output_file = out_folder + str(x_tile) + \"/\" + str(y_tile) + \".npy\"\n",
    "        if os.path.exists(output_file):\n",
    "            prediction = np.load(output_file)\n",
    "            prediction = prediction * 100\n",
    "            prediction = prediction.T.astype(np.uint8)\n",
    "            predictions_tile = predictions[ (x_tile ): (x_tile+140),\n",
    "                       y_tile:y_tile + 140]\n",
    "\n",
    "            if np.max(prediction) <= 100:\n",
    "                predictions_tile[np.logical_and(predictions_tile != 0, predictions_tile <= 100)] = (\n",
    "                    predictions_tile[np.logical_and(predictions_tile != 0, predictions_tile <= 100)] + \n",
    "                    prediction[np.logical_and(predictions_tile != 0, predictions_tile <= 100)] ) / 2\n",
    "                predictions_tile[predictions_tile == 0] = prediction[predictions_tile == 0]\n",
    "            else:\n",
    "                predictions[ (x_tile ): (x_tile+140),\n",
    "                       y_tile:y_tile + 140] = prediction\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_i in range(0, predictions.shape[0] - 3):\n",
    "    for y_i in range(0, predictions.shape[1] - 3):\n",
    "        window = predictions[x_i:x_i+3, y_i:y_i+3]\n",
    "        if np.max(window) < 40:\n",
    "            if np.sum(np.logical_and(window > 10, window < 40)) > 5:\n",
    "                predictions[x_i:x_i+3, y_i:y_i+3] = 0.\n",
    "\n",
    "predictions[predictions <= .20*100] = 0.        \n",
    "predictions = np.around(predictions / 20, 0) * 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Sharpen predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = predictions\n",
    "stacked[stacked == 156] = 255.\n",
    "\n",
    "plot = True\n",
    "if plot:\n",
    "    plt.figure(figsize=(20,17))\n",
    "    plt.imshow(stacked.T, cmap='Greens', vmin=0, vmax=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Write GeoTiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = out_folder[:-7] + f\"{str(x)}X{str(y)}Y_POST_NEW.tif\"\n",
    "\n",
    "west = point[0]\n",
    "east = point[2]\n",
    "north = point[3]\n",
    "south = point[1]\n",
    "\n",
    "arr = stacked.T.astype(np.uint8)\n",
    "\n",
    "transform = rasterio.transform.from_bounds(west = west, south = south,\n",
    "                                           east = east, north = north,\n",
    "                                           width = arr.shape[1], \n",
    "                                           height = arr.shape[0])\n",
    "\n",
    "print(\"Writing\", file)\n",
    "new_dataset = rasterio.open(file, 'w', driver = 'GTiff',\n",
    "                           height = arr.shape[0], width = arr.shape[1], count = 1,\n",
    "                           dtype = \"uint8\",\n",
    "                           crs = '+proj=longlat +datum=WGS84 +no_defs',\n",
    "                           transform=transform)\n",
    "new_dataset.write(arr, 1)\n",
    "new_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdal_translate ../../ce-hosting/includes/niger-koure.tif ../tile_data/cog/niger-koure.tif \\\n",
    "               -co TILED=YES -co COMPRESS=LZW\n",
    "!gdaladdo -r average -ro ../tile_data/cog/niger-koure.tif 2 4 8 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 One-hectare tree cover Geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed = np.reshape(stacked, (stacked.shape[0] // 10, 10, stacked.shape[1] // 10, 10))\n",
    "summed = np.mean(summed, (1, 3))\n",
    "\n",
    "summed = summed.astype(np.float32)\n",
    "transform = rasterio.transform.from_bounds(west = west, south = south,\n",
    "                                           east = east, north = north,\n",
    "                                           width = summed.shape[1], height = summed.shape[1])\n",
    "\n",
    "new_dataset = rasterio.open('../../ce-hosting/includes/bonanza1.tif', 'w', driver = 'GTiff',\n",
    "                           height = summed.shape[1], width = summed.shape[1], count = 1,\n",
    "                           dtype = 'float32',#str(stacked.dtype),\n",
    "                           crs = '+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs',\n",
    "                           transform=transform)\n",
    "new_dataset.write(summed, 1)\n",
    "new_dataset.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
