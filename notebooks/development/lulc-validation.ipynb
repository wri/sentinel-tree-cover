{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bb820e",
   "metadata": {},
   "source": [
    "# Validation Analysis\n",
    "\n",
    "## Objective\n",
    "Develop accuracy assessments for tree cover predictions in each country, region, subregion, ecoregion and biome to understand how the model performs differently in each class and determine where we need more data.  \n",
    "Ultimately we want to be able to say \"the error of the model at predicting tree cover was x +/- y at the 95% confidence level\". When we report on numbers we will directly reference these confidence intervals.\n",
    "\n",
    "## Steps\n",
    "- Start with biomes because they will have tight error bars\n",
    "- Bootstrap sample n times from each biome\n",
    "- Calculate precision, recall and F1 scores for each bootstrap\n",
    "- Calculate 75%, 95%, 99% confidence intervals for the samples\n",
    "- Repeat for other categories and see what the error bars look like\n",
    "- Create visualizations to see the error bars (heatmap? examples [here?](https://clauswilke.com/dataviz/visualizing-uncertainty.html))\n",
    "\n",
    "## Key Questions\n",
    "- How granular can we make these validation assessments for each region/subregion/ecoregion? \n",
    "- Confidence intervals: How confident are we that a certain range around these estimates of tree cover actually contains the true parameter?\n",
    "- What do the error bars look like for precision, recall and percent error? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ee812",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('validation_data_1000.csv')\n",
    "df2 = pd.read_csv(\"eval-numbers-lulc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426dac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d662e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e640e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df2, on = 'plot_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b7172",
   "metadata": {},
   "source": [
    "## Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0246d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('validation_data_1000.csv')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ebbc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred'] = df.tp + df.fp\n",
    "df['true'] = df.tp + df.fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4663d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('LULC')['tp'].sum() / (df.groupby('LULC')['fp'].sum() + df.groupby('LULC')['tp'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c2c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('LULC')['pred'].sum() / df.groupby('LULC')['true'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22051315",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('tree_class')['pred'].sum() / df.groupby('tree_class')['true'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aae88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.subregion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ba218",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.biome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tree_class.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162955ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9e979",
   "metadata": {},
   "source": [
    "## Bootstrap from each scale, calculate scores and confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ddacc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(df, scale, n):\n",
    "    \n",
    "    '''\n",
    "    Function takes in a df, bootstraps n samples from each category in a scale, \n",
    "    calculates scores, then takes percentiles of the bootstrap \n",
    "    to calculate confidence intervals and returns 2 dfs with results.\n",
    "    '''\n",
    "\n",
    "    # calculation for precision, recall, f1\n",
    "    def calculate_metrics(tp, fp, fn):\n",
    "        tp = np.sum(tp)\n",
    "        fp = np.sum(fp)\n",
    "        fn = np.sum(fn)\n",
    "        \n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "        oe = (tp + fp) / (tp + fn)\n",
    "        return precision, recall, f1, oe\n",
    "    \n",
    "    # get a list of categories within the scale\n",
    "    categories = set(df[scale].dropna().unique())\n",
    "    print(f'{scale} has {len(categories)} subcategories. {len(categories)*n} samples in total.')\n",
    "                     \n",
    "    # empty df to store samples, scores and CIs\n",
    "    bootsamples = pd.DataFrame()\n",
    "    val_df = pd.DataFrame(columns = ['scale', 'category', 'precision', 'recall', 'f1', 'oe'])\n",
    "    conf_int = pd.DataFrame(columns = ['scale', 'category',\n",
    "                                       'p_lower_95', 'p_upper_95', 'r_lower_95',\n",
    "                                       'r_upper_95', 'f1_lower_95','f1_upper_95',\n",
    "                                       'p_lower_90', 'p_upper_90', 'r_lower_90',\n",
    "                                       'r_upper_90', 'f1_lower_90','f1_upper_90',\n",
    "                                       'p_lower_80', 'p_upper_80', 'r_lower_80',\n",
    "                                       'r_upper_80', 'f1_lower_80','f1_upper_80',\n",
    "                                       'oe_lower_80', 'oe_upper_80', 'oe_lower_95',\n",
    "                                      'oe_upper_95'])\n",
    "\n",
    "    \n",
    "    # for each category in scale, calculate scores\n",
    "    for i in categories:\n",
    "        grouped = df[df[scale] == i]\n",
    "        \n",
    "        for num in range(n):   \n",
    "            bootstrap = grouped.sample(len(grouped), replace = True)\n",
    "            #bootsamples = bootsamples.append(bootstrap, ignore_index = True)\n",
    "            precision, recall, f1, oe = calculate_metrics(sum(bootstrap.tp.values), \n",
    "                                                      sum(bootstrap.fp.values), \n",
    "                                                      sum(bootstrap.fn.values))\n",
    "            \n",
    "            # add calculations to empty df\n",
    "            val_df = val_df.append({'scale': scale, \n",
    "                                   'category': i,\n",
    "                                   'precision': precision,\n",
    "                                   'recall': recall,\n",
    "                                   'f1': f1,\n",
    "                                   'oe': oe}, ignore_index = True)\n",
    "    \n",
    "    # use scores in val_df to calculate CIs\n",
    "    for i in categories:\n",
    "        grouped2 = val_df[val_df.category == i] \n",
    "\n",
    "        # calculate 80, 90, 95th percentile\n",
    "        p_lower_95, r_lower_95, f1_lower_95, oe_lower_95 = np.percentile(grouped2.precision.values, 2.5),\\\n",
    "                                              np.percentile(grouped2.recall.values, 2.5),\\\n",
    "                                              np.percentile(grouped2.f1.values, 2.5),\\\n",
    "                                              np.percentile(grouped2.oe.values, 2.5)\n",
    "                                                    \n",
    "        \n",
    "        p_upper_95, r_upper_95, f1_upper_95, oe_upper_95 = np.percentile(grouped2.precision.values, 97.5),\\\n",
    "                                              np.percentile(grouped2.recall.values, 97.5),\\\n",
    "                                              np.percentile(grouped2.f1.values, 97.5),\\\n",
    "                                                           np.percentile(grouped2.oe.values, 97.5)\n",
    "                \n",
    "\n",
    "        p_lower_90, r_lower_90, f1_lower_90, oe_lower_90 = np.percentile(grouped2.precision.values, 5),\\\n",
    "                                              np.percentile(grouped2.recall.values, 5),\\\n",
    "                                              np.percentile(grouped2.f1.values, 5),\\\n",
    "                                                           np.percentile(grouped2.oe.values, 5,) \n",
    "                                                            \n",
    "        p_upper_90, r_upper_90, f1_upper_90, oe_upper_90 = np.percentile(grouped2.precision.values, 95),\\\n",
    "                                              np.percentile(grouped2.recall.values, 95),\\\n",
    "                                              np.percentile(grouped2.f1.values, 95),\\\n",
    "                                                           np.percentile(grouped2.oe.values, 95)\n",
    "        \n",
    "\n",
    "        p_lower_80, r_lower_80, f1_lower_80, oe_lower_90 = np.percentile(grouped2.precision.values, 10),\\\n",
    "                                              np.percentile(grouped2.recall.values, 10),\\\n",
    "                                              np.percentile(grouped2.f1.values, 10),\\\n",
    "                                                           np.percentile(grouped2.oe.values, 10) \n",
    "                                                            \n",
    "        p_upper_80, r_upper_80, f1_upper_80, oe_upper_80 = np.percentile(grouped2.precision.values, 90),\\\n",
    "                                              np.percentile(grouped2.recall.values, 90),\\\n",
    "                                              np.percentile(grouped2.f1.values, 90),\\\n",
    "                                                           np.percentile(grouped2.oe.values, 90)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # add percentiles to empty df\n",
    "        conf_int = conf_int.append({'scale': scale, \n",
    "                                    'category': i,\n",
    "                                   'p_lower_95': p_lower_95, \n",
    "                                   'p_upper_95': p_upper_95, \n",
    "                                   'r_lower_95': r_lower_95,\n",
    "                                   'r_upper_95': r_upper_95, \n",
    "                                   'f1_lower_95': f1_lower_95,\n",
    "                                   'f1_upper_95': f1_upper_95,\n",
    "                                   'p_lower_90': p_lower_90, \n",
    "                                   'p_upper_90': p_upper_90, \n",
    "                                   'r_lower_90': r_lower_90,\n",
    "                                   'r_upper_90': r_upper_90, \n",
    "                                   'f1_lower_90': f1_lower_90,\n",
    "                                   'f1_upper_90': f1_upper_90,\n",
    "                                   'p_lower_80': p_lower_80, \n",
    "                                   'p_upper_80': p_upper_80, \n",
    "                                   'r_lower_80': r_lower_80,\n",
    "                                   'r_upper_80': r_upper_80, \n",
    "                                   'f1_lower_80': f1_lower_80,\n",
    "                                   'f1_upper_80': f1_upper_80,\n",
    "                                   'oe_upper_95': oe_upper_95,\n",
    "                                   'oe_lower_95': oe_lower_95}, ignore_index = True)\n",
    "\n",
    "    # returns scores and conf intervals separately\n",
    "    return val_df, conf_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a54696",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_val, lc_conf = bootstrap(df, 'LULC', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2cd17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376aeabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "biome_val, biome_conf = bootstrap(df, 'biome', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81fb1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "subregion_val, subregion_conf = bootstrap(df, 'subregion', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6cd54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_val, country_conf = bootstrap(df, 'country', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4999f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_val[country_val.category == 'Ethiopia'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c09622",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethiopia = country_conf[country_conf.category == 'Ethiopia']\n",
    "ethiopia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb464ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_conf[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ce2f4",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_precision(conf_int_df, linewidth, figsize, title):\n",
    "\n",
    "    plt.figure(figsize = figsize)\n",
    "\n",
    "    # plot 95% CI\n",
    "    for lower, upper, y in zip(conf_int_df['p_lower_90'],\n",
    "                               conf_int_df['p_upper_90'],\n",
    "                               range(len(conf_int_df))):\n",
    "\n",
    "        plt.plot((lower, upper), (y, y), color = 'lightsteelblue', linestyle='-', linewidth = linewidth*3)\n",
    "    \n",
    "    # plot 90% CI\n",
    "    for lower, upper, y in zip(conf_int_df['p_lower_90'],\n",
    "                               conf_int_df['p_upper_90'],\n",
    "                               range(len(conf_int_df))):\n",
    "\n",
    "        plt.plot((lower, upper), (y, y), color = 'royalblue', linestyle='-', linewidth = linewidth*2)\n",
    "        \n",
    "    # plot 80% CI\n",
    "    for lower, upper, y in zip(conf_int_df['p_lower_80'],\n",
    "                               conf_int_df['p_upper_80'],\n",
    "                               range(len(conf_int_df))):\n",
    "\n",
    "        plt.plot((lower, upper), (y, y), color = 'tomato', linestyle='-', linewidth = linewidth)\n",
    "\n",
    "    plt.yticks(range(len(conf_int_df)), list(conf_int_df['category']))\n",
    "    plt.title(f'Precision Confidence Intervals for {title}');\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4528f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_recall(conf_int_df, linewidth, figsize, title):\n",
    "\n",
    "    plt.figure(figsize = figsize)\n",
    "\n",
    "    # plot 95% CI\n",
    "    for lower, upper, y in zip(conf_int_df['r_lower_90'],\n",
    "                               conf_int_df['r_upper_90'],\n",
    "                               range(len(conf_int_df))):\n",
    "\n",
    "        plt.plot((lower, upper), (y, y), color = 'darkseagreen', linestyle='-', linewidth = linewidth*3)\n",
    "    \n",
    "    # plot 90% CI\n",
    "    for lower, upper, y in zip(conf_int_df['r_lower_90'],\n",
    "                               conf_int_df['r_upper_90'],\n",
    "                               range(len(conf_int_df))):\n",
    "\n",
    "        plt.plot((lower, upper), (y, y), color = 'seagreen', linestyle='-', linewidth = linewidth*2)\n",
    "        \n",
    "    # plot 80% CI\n",
    "    for lower, upper, y in zip(conf_int_df['r_lower_80'],\n",
    "                               conf_int_df['r_upper_80'],\n",
    "                               range(len(conf_int_df))):\n",
    "\n",
    "        plt.plot((lower, upper), (y, y), color = 'tomato', linestyle='-', linewidth = linewidth)\n",
    "\n",
    "    plt.yticks(range(len(conf_int_df)), list(conf_int_df['category']))\n",
    "    plt.title(f'Recall Confidence Intervals for {title}');\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f0523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_f1(conf_int_df, linewidth, figsize, title):\n",
    "\n",
    "    plt.figure(figsize = figsize)\n",
    "\n",
    "    # plot 95% CI\n",
    "    for lower, upper, y in zip(conf_int_df['f1_lower_90'],\n",
    "                               conf_int_df['f1_upper_90'],\n",
    "                               range(len(conf_int_df))):\n",
    "\n",
    "        plt.plot((lower, upper), (y, y), color = 'thistle', linestyle='-', linewidth = linewidth*3)\n",
    "    \n",
    "    # plot 90% CI\n",
    "    for lower, upper, y in zip(conf_int_df['f1_lower_90'],\n",
    "                               conf_int_df['f1_upper_90'],\n",
    "                               range(len(conf_int_df))):\n",
    "\n",
    "        plt.plot((lower, upper), (y, y), color = 'darkslateblue', linestyle='-', linewidth = linewidth*2)\n",
    "        \n",
    "    # plot 80% CI\n",
    "    for lower, upper, y in zip(conf_int_df['f1_lower_80'],\n",
    "                               conf_int_df['f1_upper_80'],\n",
    "                               range(len(conf_int_df))):\n",
    "\n",
    "        plt.plot((lower, upper), (y, y), color = 'tomato', linestyle='-', linewidth = linewidth)\n",
    "\n",
    "    plt.yticks(range(len(conf_int_df)), list(conf_int_df['category']))\n",
    "    plt.title(f'F1 Confidence Intervals for {title}')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f879312c",
   "metadata": {},
   "source": [
    "### Biome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_precision(biome_conf, 6, (13,6), 'Biomes')\n",
    "visualize_recall(biome_conf, 6, (13,6), 'Biomes')\n",
    "visualize_f1(biome_conf, 6, (13,6), 'Biomes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf47ee1",
   "metadata": {},
   "source": [
    "### Subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc3678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_precision(subregion_conf, 6, (10,6), 'Subregions')\n",
    "visualize_recall(subregion_conf, 6, (10,6), 'Subregions')\n",
    "visualize_f1(subregion_conf, 6, (10,6), 'Subregions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_precision(lc_conf, 6, (10,6), 'LULC')\n",
    "visualize_recall(lc_conf, 6, (10,6), 'LULC')\n",
    "visualize_f1(lc_conf, 6, (10,6), 'LULC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3749300",
   "metadata": {},
   "source": [
    "### Country Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4772a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_precision(country_conf[:10], 6, (13,6), 'Sample Countries')\n",
    "visualize_recall(country_conf[:10], 6, (13,6), 'Sample Countries')\n",
    "visualize_f1(country_conf[:10], 6, (13,6), 'Sample Countries')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
