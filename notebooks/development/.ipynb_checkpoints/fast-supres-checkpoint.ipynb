{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "#import tensorflow_probability as tfp\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras.layers import Conv2D, Layer, InputSpec\n",
    "from tensorflow.initializers import glorot_normal, lecun_normal\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize\n",
    "\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downPixelAggr(img, SCALE=2):\n",
    "    from scipy import signal\n",
    "    import skimage.measure\n",
    "    from scipy.ndimage.filters import gaussian_filter\n",
    "    img = resize(img, ((24, 24, 6)), 0)\n",
    "\n",
    "    img_blur = np.zeros(img.shape)\n",
    "    # Filter the image with a Gaussian filter\n",
    "    for i in range(0, img.shape[2]):\n",
    "        img_blur[:, :, i] = gaussian_filter(img[:, :, i], 1/SCALE)\n",
    "    # New image dims\n",
    "    new_dims = tuple(s//SCALE for s in img.shape)\n",
    "    img_lr = np.zeros(new_dims[0:2]+(img.shape[-1],))\n",
    "    # Iterate through all the image channels with avg pooling (pixel aggregation)\n",
    "    for i in range(0, img.shape[2]):\n",
    "        img_lr[:, :, i] = skimage.measure.block_reduce(img_blur[:, :, i], (SCALE, SCALE), np.mean)\n",
    "        \n",
    "    img_lr = resize(img_lr, ((48, 48, 6)), 0)\n",
    "\n",
    "    return np.squeeze(img_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_data(data):\n",
    "    # 10 meter band, 20 -> 40 meter band, 20 meter band\n",
    "    twentym = data[..., 4:]\n",
    "    labels = np.copy(twentym)\n",
    "    twentym = np.reshape(twentym, ((twentym.shape[0], 24, 2, 24, 2, 6)))\n",
    "    twentym = np.mean(twentym, axis = (2, 4))\n",
    "    tenm = data[..., :4]\n",
    "\n",
    "    fourty_m = np.zeros_like(data[..., 4:])\n",
    "    for sample in range(fourty_m.shape[0]):\n",
    "        fourty_m[sample] = downPixelAggr(twentym[sample])\n",
    "\n",
    "    bilinear_upsample = resize(fourty_m, (fourty_m.shape[0], 48, 48, 6), 2)\n",
    "    input_data = np.concatenate([tenm, fourty_m], axis = -1)\n",
    "    \n",
    "    return bilinear_upsample, input_data, labels\n",
    "\n",
    "def evaluate(test_data):\n",
    "    n_batches = test_data.shape[0] // 32\n",
    "    print(n_batches)\n",
    "    test_ids = np.arange(test_data.shape[0])\n",
    "    rmses = np.empty((20, 6))\n",
    "    for batch in range(20):\n",
    "        batch_ids = test_ids[batch*32: (batch+1) * 32]\n",
    "        bilinear_upsample, input_data, labels_ = make_input_data(test_data[batch_ids])\n",
    "        y = sess.run([skipconnect], feed_dict={inp: input_data,\n",
    "                                         bilinear_input: bilinear_upsample,\n",
    "                                         labels: labels_,\n",
    "                                         is_training: True,\n",
    "                                         })\n",
    "        y = np.concatenate(y)\n",
    "        se = (labels_ - y)**2\n",
    "        mse = np.mean(se, axis = (0, 1, 2))\n",
    "        mse = np.clip(mse, 0, 1)\n",
    "        rmse = np.sqrt(mse)\n",
    "        rmse = np.clip(rmse, 0, 1)\n",
    "        rmses[batch] = rmse\n",
    "    rmses = np.mean(rmses, 0)\n",
    "    return rmses\n",
    "\n",
    "def make_test_data(data):\n",
    "    input_data = data\n",
    "    twentym = data[..., 4:]\n",
    "    twentym = np.reshape(twentym, ((twentym.shape[0], 24, 2, 24, 2, 6)))\n",
    "    twentym = np.mean(twentym, axis = (2, 4))\n",
    "    bilinear_upsample = resize(twentym, (twentym.shape[0], 48, 48, 6), 2)\n",
    "    return bilinear_upsample, input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "files = [x for x in os.listdir(\"../../data/train-raw/\") if \".npy\" in x]\n",
    "for file in files:\n",
    "    data.append(np.load(\"../../data/train-raw/\" + file))\n",
    "data = np.concatenate(data, axis = 0)\n",
    "data = np.float32(data) / 65535\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "files = [x for x in os.listdir(\"../../data/test-raw/\") if \".npy\" in x]\n",
    "for file in files:\n",
    "    test_data.append(np.load(\"../../data/test-raw/\" + file)[:5])\n",
    "test_data = np.concatenate(test_data, axis = 0)\n",
    "test_data = np.float32(test_data) / 65535\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu(inp, \n",
    "                 is_training, \n",
    "                 scope,\n",
    "                 kernel_size,\n",
    "                 filters, \n",
    "                 stride = (1, 1),\n",
    "                 activation = True,\n",
    "                 use_bias = True):\n",
    "    '''2D convolution, batch renorm, relu block, 3x3 drop block. \n",
    "       Use_bias must be set to False for batch normalization to work. \n",
    "       He normal initialization is used with batch normalization.\n",
    "       RELU is better applied after the batch norm.\n",
    "       DropBlock performs best when applied last, according to original paper.\n",
    "          \n",
    "    '''\n",
    "\n",
    "    with tf.variable_scope(scope + \"_conv\"):\n",
    "        conv = Conv2D(filters = filters, kernel_size = (kernel_size, kernel_size),  strides = stride,\n",
    "                      activation = None, padding = 'valid', use_bias = use_bias,\n",
    "                      kernel_initializer = tf.keras.initializers.he_normal())(inp)\n",
    "    if activation:\n",
    "        conv = tf.nn.relu(conv)\n",
    "    return conv\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "    \n",
    "def resblock(inp, is_training, scope, filters):\n",
    "    inp_pad = ReflectionPadding2D()(inp)\n",
    "    conv = conv_relu(inp_pad, is_training, scope + \"1\", 3, filters, activation = True, use_bias = True)\n",
    "    conv_pad = ReflectionPadding2D()(conv)\n",
    "    conv2 =  conv_relu(conv_pad, is_training, scope + \"2\", 3, filters, activation = False, use_bias = True)\n",
    "    conv2 = tf.multiply(conv2, tf.constant(0.2))\n",
    "    add = tf.add(inp, conv2)\n",
    "    return add\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.placeholder(tf.float32, shape=(None, 48, 48, 10))\n",
    "bilinear_input = tf.placeholder(tf.float32, shape = (None, 48, 48, 6))\n",
    "labels =  tf.placeholder(tf.float32, shape = (None, 48, 48, 6))\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_pad = ReflectionPadding2D()(inp)\n",
    "conv1 = conv_relu(inp_pad, is_training, \"out\", 3, 48, activation = True)\n",
    "print(conv1.shape)\n",
    "conv2 = resblock(conv1, is_training, \"one\", 48)\n",
    "conv4 = resblock(conv2, is_training, \"three\", 48)\n",
    "print(conv4.shape)\n",
    "conv5 = resblock(conv4, is_training, \"four\", 48)\n",
    "print(conv5.shape)\n",
    "outconv = conv_relu(conv5, is_training, \"out\", 1, 6, activation = False)\n",
    "skipconnect = tf.add(bilinear_input, outconv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(\"This model has {} parameters\".format(total_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "loss_fn = tf.keras.losses.MAE(labels, skipconnect)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = optimizer.minimize(loss_fn)   \n",
    "        \n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "saver = tf.train.Saver(max_to_keep = 150)\n",
    "print(\"The graph has been finalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses = np.empty((150, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(np.arange(rmses.shape[0])[1:15],\n",
    "               rmses[:, 0][1:15] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tnrange, tqdm_notebook\n",
    "losses = []\n",
    "train_ids = np.arange(data.shape[0])\n",
    "n_batches = data.shape[0] // 32\n",
    "for i in range(0, 150):\n",
    "    np.random.shuffle(train_ids)\n",
    "    for k in tnrange(50):\n",
    "        batch_ids = train_ids[k*32: (k+1) * 32]\n",
    "        batch = data[batch_ids]\n",
    "        bilinear_upsample, input_data, labels_ = make_input_data(batch)\n",
    "        opt, tr = sess.run([train_op, loss_fn],\n",
    "                          feed_dict={inp: input_data,\n",
    "                                     bilinear_input: bilinear_upsample,\n",
    "                                     labels: labels_,\n",
    "                                     is_training: True,\n",
    "                                     })\n",
    "        losses.append(np.mean(tr))\n",
    "    print(i, np.mean(tr))\n",
    "    rmse = evaluate(test_data)\n",
    "    print(rmse)\n",
    "    rmses[i, :] = rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilinear_upsample, input_data = make_test_data(test_data[135][np.newaxis, :])\n",
    "\n",
    "%time y = sess.run([skipconnect], feed_dict={inp: input_data, bilinear_input: bilinear_upsample, labels: labels_, is_training: True, })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7.5))\n",
    "sns.heatmap(y[0][0, :, :,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7.5))\n",
    "sns.heatmap(input_data[0, :, :, 4 + 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
