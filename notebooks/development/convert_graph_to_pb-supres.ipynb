{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras.layers import Conv2D, Lambda, Dense, Multiply, Add\n",
    "from tensorflow.initializers import glorot_normal, lecun_normal\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.layers import batch_normalization\n",
    "from tensorflow.python.util import deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../src/layers/zoneout.py\n",
    "%run ../../src/layers/adabound.py\n",
    "%run ../../src/layers/convgru.py\n",
    "%run ../../src/layers/dropblock.py\n",
    "%run ../../src/layers/extra_layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu(inp, \n",
    "                 is_training, \n",
    "                 scope,\n",
    "                 kernel_size,\n",
    "                 filters, \n",
    "                 stride = (1, 1),\n",
    "                 activation = True,\n",
    "                 use_bias = True):\n",
    "    '''2D convolution, batch renorm, relu block, 3x3 drop block. \n",
    "       Use_bias must be set to False for batch normalization to work. \n",
    "       He normal initialization is used with batch normalization.\n",
    "       RELU is better applied after the batch norm.\n",
    "       DropBlock performs best when applied last, according to original paper.\n",
    "          \n",
    "    '''\n",
    "\n",
    "    with tf.variable_scope(scope + \"_conv\"):\n",
    "        conv = Conv2D(filters = filters, kernel_size = (kernel_size, kernel_size),  strides = stride,\n",
    "                      activation = None, padding = 'valid', use_bias = use_bias,\n",
    "                      kernel_initializer = tf.keras.initializers.he_uniform())(inp)\n",
    "    if activation:\n",
    "        conv = tf.nn.relu(conv)\n",
    "    return conv\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "    \n",
    "def resblock(inp, is_training, scope, filters):\n",
    "    inp_pad = ReflectionPadding2D()(inp)\n",
    "    conv = conv_relu(inp_pad, is_training, scope + \"1\", 3, filters, activation = True, use_bias = True)\n",
    "    conv_pad = ReflectionPadding2D()(conv)\n",
    "    conv2 =  conv_relu(conv_pad, is_training, scope + \"2\", 3, filters, activation = False, use_bias = True)\n",
    "    conv2 = tf.multiply(conv2, tf.constant(0.10))\n",
    "    add = tf.add(inp, conv2)\n",
    "    return add\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.placeholder(tf.float32, shape=(None, None, None, 10))\n",
    "bilinear_input = tf.placeholder(tf.float32, shape = (None, None, None, 6))\n",
    "labels =  tf.placeholder(tf.float32, shape = (None, None, None, 6))\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = [2, 3, 4, 5, 6]\n",
    "width = [32, 48, 64, 80, 96]\n",
    "\n",
    "depth = 2\n",
    "width = 32\n",
    "\n",
    "inp_pad = ReflectionPadding2D()(inp)\n",
    "conv = conv_relu(inp_pad, is_training, \"in\", 3, width, activation = True)\n",
    "\n",
    "for d in range(depth):\n",
    "    conv = resblock(conv, is_training, str(d), width)\n",
    "    print(d, conv.shape)\n",
    "    \n",
    "conv = ReflectionPadding2D()(conv)\n",
    "outconv = conv_relu(conv, is_training, \"out\", 3, 6, activation = False)\n",
    "outconv = tf.nn.tanh(outconv)\n",
    "skipconnect = tf.add(bilinear_input, outconv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(5e-4)\n",
    "loss_fn = tf.keras.losses.MAE(labels, skipconnect)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = optimizer.minimize(loss_fn)   \n",
    "        \n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "saver = tf.train.Saver(max_to_keep = 150)\n",
    "print(\"The graph has been finalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../models/supres/nov-40k-swir/\"\n",
    "saver.restore(sess, tf.train.latest_checkpoint(path))\n",
    "save_path = saver.save(sess, f\"../../models/supres/nov-40k-swir/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = '../../models/supres/nov-40k-swir/' # Your .meta file\n",
    "output_node_names = ['Add_2']    # Output nodes\n",
    "#output_node_names = ['conv2d_12/Sigmoid']\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore the graph\n",
    "    saver = tf.train.import_meta_graph(meta_path + \"model.meta\")\n",
    "\n",
    "    # Load weights\n",
    "    saver.restore(sess,tf.train.latest_checkpoint(meta_path))\n",
    "    #output_node_names = [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    #print(output_node_names)\n",
    "    \n",
    "    # Freeze the graph\n",
    "    frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "        sess,\n",
    "        sess.graph_def,\n",
    "        output_node_names)\n",
    "\n",
    "    # Save the frozen graph\n",
    "    with open('../../models/supres/nov-40k-swir/superresolve_graph.pb', 'wb') as f:\n",
    "        f.write(frozen_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
