{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This Jupyter notebook calculates change detection predictions for multi-year results from `4a-download-large-area` and `4b-predict-large-area`. This uses a combination of deep iterative slow features analysis and bayesian soft fusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hickle as hkl\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from matplotlib import image\n",
    "from scipy.cluster.vq import kmeans as km\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from scipy.misc import imread, imresize, imsave\n",
    "\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%run ../src/downloading/utils.py\n",
    "%run ../src/models/utils.py\n",
    "from skimage.transform import resize\n",
    "from sklearn.cross_decomposition import CCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Parameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landscape = 'mantiquiera'\n",
    "max_x = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_csv(\"../project-monitoring/database.csv\")\n",
    "coords = database[database['landscape'] == landscape]\n",
    "path = coords['path'].tolist()[0]\n",
    "coords = (float(coords['longitude']), float(coords['latitude']))\n",
    "\n",
    "IO_PARAMS = {'prefix': '../',\n",
    "             'bucket': 'restoration-monitoring',\n",
    "             'coords': coords,\n",
    "             'bucket-prefix': '',\n",
    "             'path': path}\n",
    "\n",
    "TIF_OUTPUT = IO_PARAMS['prefix'] + IO_PARAMS['path'] + \"gain.tif\"\n",
    "INPUT_FOLDER = IO_PARAMS['prefix'] + IO_PARAMS['path']\n",
    "\n",
    "print(coords, INPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Processed data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = np.empty((646, 646*max_x, 16))\n",
    "im2 = np.empty((646, 646*max_x, 16))\n",
    "\n",
    "for x_val in range(0, max_x):\n",
    "    image1 = hkl.load(f\"{INPUT_FOLDER}/2017/interim/0_{x_val}.hkl\")\n",
    "    print(f\"There are {np.sum(np.isnan(image1))} NA values in 2017\")\n",
    "    image1 = image1[..., :8]\n",
    "\n",
    "    im1_med = np.median(image1, axis = 0)\n",
    "    im1_stdev = np.std(image1, axis = 0)\n",
    "    image1 = np.concatenate([im1_med, im1_stdev], axis = -1)\n",
    "    im1[:, 646 * (x_val):646 * (x_val+1), :] = image1\n",
    "\n",
    "\n",
    "    image2 = hkl.load(f\"{INPUT_FOLDER}/2019/interim/0_{x_val}.hkl\")\n",
    "    print(f\"There are {np.sum(np.isnan(image2))} NA values in 2019\")\n",
    "    image2 = image2[..., :8]\n",
    "\n",
    "    im2_med = np.median(image2, axis = 0)\n",
    "    im2_stdev = np.std(image2, axis = 0)\n",
    "    image2 = np.concatenate([im2_med, im2_stdev], axis = -1)\n",
    "    im2[:, 646 * (x_val):646 * (x_val+1), :] = image2\n",
    "net_shape = [128, 128, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Change vector analysis with canonical correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = im1[:, ..., :16].reshape(646*646*max_x, 16)\n",
    "y = im2[:, ..., :16].reshape(646*646*max_x, 16)\n",
    "cca = CCA(n_components=3)\n",
    "cca.fit(x, y)\n",
    "xs = cca.transform(x)\n",
    "ys = cca.transform(y)\n",
    "diffs = abs(xs - ys)\n",
    "diffs = np.sum(diffs, axis = 1)\n",
    "diffs = diffs.reshape(646, 646*max_x)\n",
    "cutoff = np.percentile(diffs.flatten(), 1)\n",
    "\n",
    "new = np.ones_like(diffs)\n",
    "new[np.where(diffs < cutoff)] = 0.\n",
    "plt.figure(figsize=(12, 9))\n",
    "sns.heatmap(new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.2 Deep slow feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainSamples(index, im1, im2, number=4000):\n",
    "    '''Identifies training samples for the DSFA\n",
    "    '''\n",
    "\n",
    "    loc = np.where(index != 1)[0]\n",
    "    perm = np.random.permutation(np.shape(loc)[0])\n",
    "\n",
    "    ind = loc[perm[0:number]]\n",
    "\n",
    "    return im1[ind, :], im2[ind, :]\n",
    "\n",
    "\n",
    "def normalize(data):\n",
    "    ''' Standardizes data to 0 mean unit variance\n",
    "    '''\n",
    "    meanv = np.mean(data, axis=0)\n",
    "    stdv = np.std(data, axis=0)\n",
    "\n",
    "    delta = data - meanv\n",
    "    data = delta / stdv\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def linear_sfa(fcx, fcy, vp, shape):\n",
    "    \n",
    "    delta = np.matmul(fcx, vp) - np.matmul(fcy, vp)\n",
    "    differ_map = delta\n",
    "    magnitude = np.sum(delta**2 / np.var(delta, axis = 0), axis=1)\n",
    "    vv = magnitude / np.max(magnitude)\n",
    "    im = np.reshape(kmeans(vv), shape[0:-1])\n",
    "    return im, magnitude, differ_map\n",
    "\n",
    "\n",
    "def kmeans(data):\n",
    "    shape = np.shape(data)\n",
    "    ctr, _ = km(data, 2)\n",
    "\n",
    "    for k1 in range(shape[0]):\n",
    "        if abs(ctr[0] - data[k1]) >= abs(ctr[1] - data[k1]):\n",
    "            data[k1] = 0\n",
    "        else:\n",
    "            data[k1] = 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)-15s %(levelname)s: %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n",
    "\n",
    "def dsfa(xtrain, ytrain, xtest, ytest, net_shape=None):\n",
    "\n",
    "    train_num = np.shape(xtrain)[0]\n",
    "    bands = np.shape(xtrain)[-1]\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    activation = tf.nn.softsign\n",
    "\n",
    "    xd = tf.placeholder(dtype=tf.float32, shape=[None, bands])\n",
    "    yd = tf.placeholder(dtype=tf.float32, shape=[None, bands])\n",
    "\n",
    "    # fc1\n",
    "    fc1w1 = tf.Variable(tf.truncated_normal(shape=[bands, net_shape[0]], dtype=tf.float32, stddev=1e-1))\n",
    "    fc1w2 = tf.Variable(tf.truncated_normal(shape=[bands, net_shape[0]], dtype=tf.float32, stddev=1e-1))\n",
    "    fc1b1 = tf.Variable(tf.constant(1e-1, shape=[net_shape[0]], dtype=tf.float32))\n",
    "    fc1b2 = tf.Variable(tf.constant(1e-1, shape=[net_shape[0]], dtype=tf.float32))\n",
    "\n",
    "    fc1x = tf.nn.bias_add(tf.matmul(xd, fc1w1), fc1b1)\n",
    "    fc1y = tf.nn.bias_add(tf.matmul(yd, fc1w2), fc1b2)\n",
    "\n",
    "    fc11 = activation(fc1x)\n",
    "    fc12 = activation(fc1y)\n",
    "\n",
    "    # fc2\n",
    "    fc2w1 = tf.Variable(tf.truncated_normal(shape=[net_shape[0], net_shape[1]], dtype=tf.float32, stddev=1e-1))\n",
    "    fc2w2 = tf.Variable(tf.truncated_normal(shape=[net_shape[0], net_shape[1]], dtype=tf.float32, stddev=1e-1))\n",
    "    fc2b1 = tf.Variable(tf.constant(1e-1, shape=[net_shape[1]], dtype=tf.float32))\n",
    "    fc2b2 = tf.Variable(tf.constant(1e-1, shape=[net_shape[1]], dtype=tf.float32))\n",
    "\n",
    "    fc2x = tf.nn.bias_add(tf.matmul(fc11, fc2w1), fc2b1)\n",
    "    fc2y = tf.nn.bias_add(tf.matmul(fc12, fc2w2), fc2b2)\n",
    "\n",
    "    fc21 = activation(fc2x)\n",
    "    fc22 = activation(fc2y)\n",
    "\n",
    "    # fc3\n",
    "    fc3w1 = tf.Variable(tf.truncated_normal(shape=[net_shape[1], net_shape[2]], dtype=tf.float32, stddev=1e-1))\n",
    "    fc3w2 = tf.Variable(tf.truncated_normal(shape=[net_shape[1], net_shape[2]], dtype=tf.float32, stddev=1e-1))\n",
    "    fc3b1 = tf.Variable(tf.constant(1e-1, shape=[net_shape[2]], dtype=tf.float32))\n",
    "    fc3b2 = tf.Variable(tf.constant(1e-1, shape=[net_shape[2]], dtype=tf.float32))\n",
    "\n",
    "    fc3x = tf.nn.bias_add(tf.matmul(fc21, fc3w1), fc3b1)\n",
    "    fc3y = tf.nn.bias_add(tf.matmul(fc22, fc3w2), fc3b2)\n",
    "\n",
    "    fc3x = activation(fc3x)\n",
    "    fc3y = activation(fc3y)\n",
    "\n",
    "    #fc3x - tf.cast(tf.divide(1, bands), tf.float32) * tf.matmul(fc3x, tf.ones([bands, bands]))\n",
    "    m = tf.shape(fc3x)[1]\n",
    "    fc_x = fc3x - tf.cast(tf.divide(1, m), tf.float32) * tf.matmul(fc3x, tf.ones([m, m]))\n",
    "    fc_y = fc3y - tf.cast(tf.divide(1, m), tf.float32) * tf.matmul(fc3y, tf.ones([m, m]))\n",
    "\n",
    "    Differ = fc_x - fc_y\n",
    "\n",
    "    A = tf.matmul(Differ, Differ, transpose_a=True)\n",
    "    A = A / train_num\n",
    "\n",
    "    sigmaX = tf.matmul(fc_x, fc_x, transpose_a=True)\n",
    "    sigmaY = tf.matmul(fc_y, fc_y, transpose_a=True)\n",
    "    sigmaX = sigmaX / train_num + 1e-4  * tf.eye(net_shape[-1])\n",
    "    sigmaY = sigmaY / train_num + 1e-4  * tf.eye(net_shape[-1])\n",
    "\n",
    "    B = (sigmaX + sigmaY) / 2# + args.reg * tf.eye(net_shape[-1])\n",
    "\n",
    "    # B_inv, For numerical stability.\n",
    "    D_B, V_B = tf.self_adjoint_eig(B)\n",
    "    idx = tf.where(D_B > 1e-12)[:, 0]\n",
    "    D_B = tf.gather(D_B, idx)\n",
    "    V_B = tf.gather(V_B, idx, axis=1)\n",
    "    B_inv = tf.matmul(tf.matmul(V_B, tf.diag(tf.reciprocal(D_B))), tf.transpose(V_B))\n",
    "\n",
    "    sigma = tf.matmul(B_inv, A)#+ args.reg * tf.eye(net_shape[-1])\n",
    "\n",
    "    D, V = tf.self_adjoint_eig(sigma)\n",
    "    \n",
    "    #loss = tf.sqrt(tf.trace(tf.matmul(sigma,sigma)))\n",
    "    loss = tf.trace(tf.matmul(sigma,sigma))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    loss_log = []\n",
    "\n",
    "    sess = tf.Session()\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for k in range(4000):\n",
    "        sess.run(optimizer, feed_dict={xd: xtrain, yd: ytrain})\n",
    "\n",
    "        if k % 100 == 0:\n",
    "            ll = sess.run(loss, feed_dict={xd: xtrain, yd: ytrain})\n",
    "            ll = ll / net_shape[-1]\n",
    "            logging.info('The %4d-th epochs, loss is %4.4f ' % (k, ll))\n",
    "            loss_log.append(ll)\n",
    "\n",
    "    matV = sess.run(V, feed_dict={xd: xtest, yd: ytest})\n",
    "    bVal = sess.run(B, feed_dict={xd: xtest, yd: ytest})\n",
    "\n",
    "    fcx = sess.run(fc_x, feed_dict={xd: xtest, yd: ytest})\n",
    "    fcy = sess.run(fc_y, feed_dict={xd: xtest, yd: ytest})\n",
    "\n",
    "    sess.close()\n",
    "    print('')\n",
    "\n",
    "    return loss_log, matV, fcx, fcy, bVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = np.shape(im1)\n",
    "\n",
    "im1 = np.reshape(im1, newshape=[-1,img_shape[-1]])\n",
    "im2 = np.reshape(im2, newshape=[-1,img_shape[-1]])\n",
    "\n",
    "im1 = normalize(im1)\n",
    "im2 = normalize(im2)\n",
    "\n",
    "imm = None\n",
    "all_magnitude = None\n",
    "cva_ind = np.reshape(new, newshape=[-1])\n",
    "\n",
    "i1, i2 = getTrainSamples(cva_ind, im1, im2, 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log, vpro, fcx, fcy, bval = dsfa(xtrain=i1, ytrain=i2, xtest=im1, ytest=im2, net_shape=net_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm, magnitude, differ_map = linear_sfa(fcx, fcy, vpro, shape=img_shape)\n",
    "\n",
    "magnitude = np.reshape(magnitude, img_shape[0:-1])\n",
    "magnitude_float = np.copy(magnitude)\n",
    "differ = differ_map\n",
    "change_map = np.reshape(kmeans(np.reshape(magnitude, [-1])), img_shape[0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_float2 = np.copy(magnitude_float)\n",
    "magnitude_float2[np.where(magnitude_float2 > np.percentile(magnitude_float2, 95))] = np.percentile(magnitude_float2, 95)\n",
    "magnitude_float2 = magnitude_float2 / np.percentile(magnitude_float2, 100)\n",
    "\n",
    "indices = [(8, 638), (654, 1284)]\n",
    "change_map = np.empty((630, 630*max_x))\n",
    "for x_val in range(0, max_x):\n",
    "    print(indices[x_val][0], indices[x_val][1])\n",
    "    change_map[:, (630*x_val):(630*(x_val+1))] = magnitude_float2[8:-8, indices[x_val][0]:indices[x_val][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = bounding_box(coords, (5*max_x*1260)-0, ((5)*1260)-0, expansion = 0)\n",
    "west = point[1][0]\n",
    "east = point[0][0]\n",
    "north = point[0][1]\n",
    "south = point[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian soft fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "start_probs = np.array(Image.open(f'{INPUT_FOLDER}/2017.tif'))\n",
    "middle_probs = np.array(Image.open(f'{INPUT_FOLDER}/2018.tif'))\n",
    "end_probs = np.array(Image.open(f'{INPUT_FOLDER}/2019.tif'))\n",
    "start_probs = resize(start_probs, (630, 1260), 0)\n",
    "middle_probs = resize(middle_probs, (630, 1260), 0)\n",
    "end_probs = resize(end_probs, (630, 1260), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = change_map\n",
    "cv[np.where(cv > 1)] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "possibilities = [list(x) for x in product([0, 1], [0, 1], [0, 1])]\n",
    "print(possibilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_map = np.zeros((630, 630*max_x, 8))\n",
    "for i, val in enumerate(possibilities):\n",
    "    start = val[0]\n",
    "    middle = val[1]\n",
    "    end = val[2]\n",
    "    if start == 1:\n",
    "        pwi_x = start_probs\n",
    "    else:\n",
    "        pwi_x = 1 - start_probs\n",
    "        \n",
    "    if middle == 1:\n",
    "        pxk_z = middle_probs\n",
    "    else:\n",
    "        pxk_z = 1 - middle_probs\n",
    "\n",
    "    if end == 1:\n",
    "        pvj_y = end_probs\n",
    "    else:\n",
    "        pvj_y = 1 - end_probs\n",
    "\n",
    "    if start == end:\n",
    "        pwi_vj = 1 - (cv)\n",
    "    else:\n",
    "        pwi_vj = (cv)\n",
    "        \n",
    "    #change_map = pwi_x * pvj_y * pxk_z\n",
    "\n",
    "    prior = np.logical_and(pwi_x > 0.75, pvj_y > 0.75)\n",
    "    prior = np.logical_and(prior, pxk_z > 0.75)\n",
    "\n",
    "    change_map[prior, i] = pwi_x[prior] * pvj_y[prior] * pxk_z[prior]# * pwi_vj[prior] \n",
    "    change_map[~prior, i] = pwi_x[~prior] * pvj_y[~prior] * pxk_z[~prior] * pwi_vj[~prior] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_map = np.argmax(change_map, axis = -1)\n",
    "#change_new = np.zeros_like(change_map)\n",
    "#gain_areas = np.logical_or(change_map == 1, change_map == 3)\n",
    "#change_new[gain_areas] = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = np.copy(change_map)\n",
    "#for x_window in range(1, change_new.shape[0] - 2, 2):\n",
    "#    for y_window in range(1, change_new.shape[1] - 2, 2):\n",
    " #       if np.sum(change_new[x_window - 1:x_window + 1, y_window - 1:y_window + 1]) < 2:\n",
    " #           gain[x_window - 1:x_window + 1, y_window - 1:y_window + 1] *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 11))\n",
    "sns.heatmap(gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = rasterio.transform.from_bounds(west = west, south = south,\n",
    "                                           east = east, north = north,\n",
    "                                           width = change_map.shape[1], height = change_map.shape[0])\n",
    "\n",
    "print(\"Writing\", f'{INPUT_FOLDER}gain.tif')\n",
    "new_dataset = rasterio.open(f'{INPUT_FOLDER}gain.tif', 'w', driver = 'GTiff',\n",
    "                           height = change_map.shape[0], width = change_map.shape[1], count = 1,\n",
    "                           dtype = 'float32',#str(stacked.dtype),\n",
    "                           crs = '+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs',\n",
    "                           transform=transform)\n",
    "new_dataset.write(gain.astype(np.float32), 1)\n",
    "new_dataset.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
